{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/3_features_topo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yk8CnJRCYVQ"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5USlWSaxqv9Y"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOfy6gWFwHEC"
      },
      "outputs": [],
      "source": [
        "# Installs\n",
        "%%capture\n",
        "!pip install astropy\n",
        "!pip install geopandas\n",
        "!pip install whitebox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTHaRSv8wICv"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "import csv\n",
        "import geopandas as gpd\n",
        "import glob\n",
        "from google.colab import runtime\n",
        "import ipywidgets as widgets\n",
        "from math import sqrt, cos, radians\n",
        "import matplotlib.pyplot as plt\n",
        "from numba import jit\n",
        "import numpy as np\n",
        "from os import makedirs\n",
        "from os.path import exists, join\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from scipy.ndimage import maximum_filter, minimum_filter, uniform_filter, distance_transform_edt\n",
        "from scipy.ndimage import label, sum as ndi_sum\n",
        "import whitebox\n",
        "wbt = whitebox.WhiteboxTools()\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMCq50kg36Br"
      },
      "outputs": [],
      "source": [
        "# 1_areas directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "template_dir = join(areas_dir, \"template.tif\")\n",
        "\n",
        "# 3_features directories\n",
        "features_dir = join(base_dir, \"3_features\")\n",
        "\n",
        "# 6_scenarios directory\n",
        "scenario_dir = join(base_dir, \"6_scenarios\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhkgXF4foXhx"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = vector = None\n",
        "    try:\n",
        "        raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "        vector = ogr.Open(polygon_path)\n",
        "        if not raster or not vector:\n",
        "            raise ValueError(\"Cannot open input files\")\n",
        "        layer = vector.GetLayer()\n",
        "        options = [\"ALL_TOUCHED=TRUE\"] if all_touched else []\n",
        "        if fixed:\n",
        "            gdal.RasterizeLayer(raster, [1], layer, burn_values=[fixed_value], options=options)\n",
        "        else:\n",
        "            attr_name = column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()\n",
        "            options.append(f\"ATTRIBUTE={attr_name}\")\n",
        "            gdal.RasterizeLayer(raster, [1], layer, options=options)\n",
        "    finally:\n",
        "        if raster: raster.FlushCache()\n",
        "        raster = vector = None\n",
        "\n",
        "# Global function: edge effects\n",
        "# Provides spatial awareness analogous to CNN receptive fields for tabular models\n",
        "# Data_type: 'binary' or 'continuous'.\n",
        "cell_size_y_path = join(areas_dir, 'cell_size_y.tif')\n",
        "cell_size_x_path = join(areas_dir, 'cell_size_x.tif')\n",
        "\n",
        "def edge_effects(array, data_type, cell_size_x_path, cell_size_y_path, threshold_metres):\n",
        "    # Determine pixel size from cell size rasters.\n",
        "    cell_size_x = gdal.Open(cell_size_x_path)\n",
        "    cell_size_x_array = cell_size_x.ReadAsArray()\n",
        "    cell_size_x = None\n",
        "    cell_size_y = gdal.Open(cell_size_y_path)\n",
        "    cell_size_y_array = cell_size_y.ReadAsArray()\n",
        "    cell_size_y = None\n",
        "    cell_area = np.mean([np.mean(cell_size_x_array), np.mean(cell_size_y_array)])\n",
        "    # Maximum pixel distance for kernel extent.\n",
        "    max_pixel_distance = threshold_metres / cell_area\n",
        "    # 2D Gaussian weight distribution follows chi-squared with df=2.\n",
        "    # Cumulative probability within radius r: P = 1 - exp(-r² / 2σ²).\n",
        "    # Solving for r at P=0.95: r = σ * sqrt(-2 * ln(0.05)) ≈ 2.45σ.\n",
        "    # Setting r = max_pixel_distance ensures 95% of kernel weight falls within threshold.\n",
        "    gaussian_stdev = max_pixel_distance / 2.45\n",
        "    kernel_radius = int(np.ceil(max_pixel_distance))\n",
        "    kernel_size = 2 * kernel_radius + 1\n",
        "    # Gaussian kernel for spatial weighting.\n",
        "    kernel = Gaussian2DKernel(x_stddev=gaussian_stdev, y_stddev=gaussian_stdev,\n",
        "                              x_size=kernel_size, y_size=kernel_size)\n",
        "    # Circular mask enforces ecological threshold as hard boundary.\n",
        "    # Square kernels would include pixels beyond threshold at corners.\n",
        "    y, x = np.ogrid[:kernel_size, :kernel_size]\n",
        "    centre = kernel_radius\n",
        "    distance_from_centre = np.sqrt((x - centre)**2 + (y - centre)**2)\n",
        "    circular_mask = distance_from_centre <= max_pixel_distance\n",
        "    # Apply mask and renormalise to sum to 1.\n",
        "    # Renormalisation ensures consistent weighting after truncation.\n",
        "    kernel_array = kernel.array.copy()\n",
        "    kernel_array[~circular_mask] = 0\n",
        "    kernel_array /= kernel_array.sum()\n",
        "    # Gaussian smoothing captures local spatial context.\n",
        "    # For binary: represents local class density within threshold.\n",
        "    # For continuous: represents local weighted mean within threshold.\n",
        "    # boundary='extend' extrapolates edge values beyond raster extent.\n",
        "    smoothed = convolve(array.astype(float), kernel_array, boundary='extend')\n",
        "    if data_type == 'continuous': return smoothed # Without rounding\n",
        "    if data_type == 'binary': smoothed = np.round(smoothed, 2) # Round\n",
        "    # Binary data: compute signed distance to class boundary.\n",
        "    # Euclidean distance transform gives centre-to-centre pixel distance.\n",
        "    dist_from_ones = distance_transform_edt(array == 0)\n",
        "    dist_from_zeros = distance_transform_edt(array == 1)\n",
        "    # Convert to distance from pixel centre to class boundary.\n",
        "    # Class boundary lies between adjacent pixels of different classes.\n",
        "    # Subtracting 0.5 pixels approximates centre-to-boundary distance.\n",
        "    # Sign encodes class membership: positive = class 1, negative = class 0.\n",
        "    # Magnitude encodes proximity to boundary (edge effects zone).\n",
        "    signed_distance = np.where(\n",
        "        array == 1,\n",
        "        np.maximum(dist_from_zeros - 0.5, 0) * cell_area,\n",
        "        -np.maximum(dist_from_ones - 0.5, 0) * cell_area\n",
        "    )\n",
        "    # Cap at threshold: pixels beyond are interior, not edge-influenced.\n",
        "    # Round to integer metres for cleaner feature representation.\n",
        "    signed_distance = np.round(np.clip(signed_distance, -threshold_metres, threshold_metres)).astype(np.int16)\n",
        "    return signed_distance, smoothed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9x_KcII90Cw"
      },
      "source": [
        "# Define base DEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMoW0_un4dOA"
      },
      "outputs": [],
      "source": [
        "# After predicting GEDI elevation to create a GEDI DTM (Digital Terrain Model),\n",
        "# Change to True to generate DTM features to use instead of DSM (Digital Surface Model) features.\n",
        "# If no GEDI DTM has been created this will default to the DSM.\n",
        "enable_gedi_dtm = True\n",
        "\n",
        "# Select which prediction path to use as the GEDI DTM\n",
        "# Use an 'unmasked' version so area calculations (e.g. hydrography) are calculated correctly.\n",
        "if enable_gedi_dtm:\n",
        "  gedi_dtm_exists = False\n",
        "  for subdir, dirs, files in os.walk(scenario_dir):\n",
        "    for raster in files:\n",
        "      if raster.endswith('.tif'):\n",
        "        if 'gedi_elevation' in raster:\n",
        "          print('# Remember to modify montane transition values to be specific to study area.')\n",
        "          print(f'gedi_elevation_path = \"{subdir}/{raster}\"')\n",
        "          gedi_dtm_exists = True\n",
        "  if not gedi_dtm_exists: print(\"No GEDI DTM found in scenarios folder. Defaulting to DSM.\")\n",
        "else: print(f\"Using the DSM in {areas_dir}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuGr-V0ViwgE"
      },
      "outputs": [],
      "source": [
        "# This code block is only relevant (and otherwise does nothing) if using a GEDI DTM.\n",
        "# Remember to modify montane transition values to be specific to study area (see below)\n",
        "gedi_elevation_path = \"/gdrive/Shareddrives/masfi/6_scenarios/gedi_elevation_251203_121657/scenario_predictions_unmasked/2015__gedi_elevation_251203_121657_unmasked.tif\"\n",
        "\n",
        "base_dsm_path = join(areas_dir, \"base_dem_dsm.tif\")\n",
        "base_dtm_path = join(areas_dir, \"base_dem_dtm.tif\")\n",
        "# Copy GEDI DTM to areas directory\n",
        "if enable_gedi_dtm and gedi_dtm_exists:\n",
        "  if not exists(base_dtm_path):\n",
        "    # Sea level is post-processed back to 0 m, and areas that might have been predicted\n",
        "    # Below this. The original DEM was likely the true terrain height around sea level.\n",
        "\n",
        "    # Define a low transition zone between original and DTM.\n",
        "    # This delineates a transition from 0 to 100 % DTM values.\n",
        "    low_transition_lower_limit = 0\n",
        "    low_transition_upper_limit = 5\n",
        "\n",
        "    # Higher elevations are poorly predicted in some study areas due to low sample size.\n",
        "    # Original values should be used to avoid erroneous topographic metrics.\n",
        "    # Vegetation at these high elevations tend not to change much between disturbance scenarios.\n",
        "\n",
        "    # Define a montane transition zone between original and DTM.\n",
        "    # This delineates a transition from 0 to 100 % original DEM values.\n",
        "    # E.g. >1,500 is typically scrub in Peninsular Malaysia.\n",
        "    montane_transition_lower_limit = 1500\n",
        "    montane_transition_upper_limit = 1800\n",
        "\n",
        "    # Read original base DEM\n",
        "    base_dsm = gdal.Open(base_dsm_path)\n",
        "    base_dsm_array = base_dsm.ReadAsArray()\n",
        "    base_dsm = None\n",
        "\n",
        "    # Scale <=0 to >=5 m values to a ratio (0 - 1) of DTM values\n",
        "    base_dsm_array_low_ratio = base_dsm_array.copy()\n",
        "    base_dsm_array_low_ratio[base_dsm_array_low_ratio <= low_transition_lower_limit] = low_transition_lower_limit\n",
        "    base_dsm_array_low_ratio[base_dsm_array_low_ratio >= low_transition_upper_limit] = low_transition_upper_limit\n",
        "    base_dsm_array_low_ratio = base_dsm_array_low_ratio / low_transition_upper_limit\n",
        "\n",
        "    # Scale <=0 to >=5 m values to a ratio (0 - 1) of DTM values\n",
        "    base_dsm_array_montane_ratio = base_dsm_array.copy()\n",
        "    base_dsm_array_montane_ratio[base_dsm_array_montane_ratio <= montane_transition_lower_limit] = montane_transition_lower_limit\n",
        "    base_dsm_array_montane_ratio[base_dsm_array_montane_ratio >= montane_transition_upper_limit] = montane_transition_upper_limit\n",
        "    base_dsm_array_montane_ratio = (montane_transition_upper_limit - base_dsm_array_montane_ratio) / (montane_transition_upper_limit - montane_transition_lower_limit)\n",
        "\n",
        "    # Use original DEM values for surface water.\n",
        "    # The Copernicus DEM rounds all surface water values to 1 or 0 decimal places.\n",
        "    # This is used to differentiate them from land values, creating a 'land binary'.\n",
        "    base_dsm_array_land_binary = base_dsm_array.copy()\n",
        "    base_dsm_array_land_binary = np.floor(base_dsm_array_land_binary * 10) / 10 # Round DOWN 1 decimal place\n",
        "    base_dsm_array_land_binary = base_dsm_array - base_dsm_array_land_binary\n",
        "    base_dsm_array_land_binary[base_dsm_array_land_binary > 0] = 1\n",
        "    # Invert the binary array to target 0 values for sieving single water pixels (usually erroneous)\n",
        "    base_dsm_array_land_binary_inverted = np.logical_not(base_dsm_array_land_binary)\n",
        "    # Sieve to 0.5 ha, using 8-connectedness (3, 3)\n",
        "    lb_array_labelled, lb_array_features = label(base_dsm_array_land_binary_inverted, structure=np.ones((3, 3)))\n",
        "    # Determine the size of each patch\n",
        "    lb_array_sizes = ndi_sum(base_dsm_array_land_binary_inverted, lb_array_labelled, range(lb_array_features + 1))\n",
        "    # Create a mask to remove patches smaller than the threshold\n",
        "    lb_array_mask_sizes = lb_array_sizes >= 2\n",
        "    lb_array_mask_sizes[0] = 0 # Ensure non-target values are excluded\n",
        "    lb_array_mask = lb_array_mask_sizes[lb_array_labelled]\n",
        "    # Apply the mask to the inverted binary array\n",
        "    lb_array_sieved_inverted = base_dsm_array_land_binary_inverted * lb_array_mask\n",
        "    # Invert the array back to original representation\n",
        "    base_dsm_array_land_binary = np.logical_not(lb_array_sieved_inverted)\n",
        "\n",
        "    # Read the GEDI DTM and create the final modifier\n",
        "    gedi_elevation = gdal.Open(gedi_elevation_path)\n",
        "    gedi_elevation_array = gedi_elevation.ReadAsArray()\n",
        "    gedi_elevation = None\n",
        "    base_dtm_array_modifier = gedi_elevation_array.copy()\n",
        "    # Change all DTM values < sea level to 0 (most are erroneous)\n",
        "    base_dtm_array_modifier[base_dtm_array_modifier < 0] = 0\n",
        "    # Sutract DTM from the DSM as the modifier\n",
        "    base_dtm_array_modifier = base_dsm_array - base_dtm_array_modifier\n",
        "    # Multiply the DTM modifier by low ratio, montane ratio and land binary\n",
        "    base_dtm_array_modifier = base_dtm_array_modifier * base_dsm_array_low_ratio * base_dsm_array_montane_ratio * base_dsm_array_land_binary\n",
        "\n",
        "    # Apply the modifier\n",
        "    base_dtm_array = base_dsm_array - base_dtm_array_modifier\n",
        "\n",
        "    # Export uncompressed for further topographic metrics\n",
        "    export_array_as_tif(base_dtm_array, base_dtm_path, compress=False)\n",
        "    print(f\"GEDI DTM has been postprocessed and uncompressed to: {base_dtm_path}\")\n",
        "\n",
        "  else: print(f\"A base DTM already exists, first remove from {areas_dir} for replacement.\")\n",
        "\n",
        "else: print(f\"A GEDI DTM does not exist in the scenarios directory. Proceeding with DSM.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obnUPMpUit18"
      },
      "outputs": [],
      "source": [
        "# Define base DEM and properties\n",
        "if enable_gedi_dtm:\n",
        "  print(\"Post-processed GEDI DTM enabled.\")\n",
        "  if not exists(base_dtm_path):\n",
        "    print(\"A post-processed GEDI DTM does not exist. Defaulting to the original DSM.\")\n",
        "    base_dem = gdal.Open(base_dsm_path)\n",
        "    topo_temp_dir = join(features_dir, 'topo_dsm_temp')\n",
        "    topo_final_dir = join(features_dir, 'topo_dsm_final')\n",
        "    makedirs(topo_temp_dir, exist_ok=True)\n",
        "    makedirs(topo_final_dir, exist_ok=True)\n",
        "  else:\n",
        "    base_dem = gdal.Open(base_dtm_path)\n",
        "    topo_temp_dir = join(features_dir, 'topo_dtm_temp')\n",
        "    topo_final_dir = join(features_dir, 'topo_dtm_final')\n",
        "    makedirs(topo_temp_dir, exist_ok=True)\n",
        "    makedirs(topo_final_dir, exist_ok=True)\n",
        "else:\n",
        "  print(\"Post-processed GEDI DTM disabled. Using the original DSM.\")\n",
        "  base_dem = gdal.Open(base_dsm_path)\n",
        "  topo_temp_dir = join(features_dir, 'topo_dsm_temp')\n",
        "  topo_final_dir = join(features_dir, 'topo_dsm_final')\n",
        "  makedirs(topo_temp_dir, exist_ok=True)\n",
        "  makedirs(topo_final_dir, exist_ok=True)\n",
        "\n",
        "# Get base DEM attributes\n",
        "base_dem_array = base_dem.ReadAsArray()\n",
        "dem_dimensions = base_dem.GetGeoTransform()\n",
        "base_dem = None\n",
        "y_origin, pixel_height, raster_height = dem_dimensions[3], dem_dimensions[5], len(base_dem_array)\n",
        "dem_central_latitude = y_origin + (raster_height // 2) * pixel_height"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc0og_zgpQHG"
      },
      "source": [
        "# Topography metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RY2j4dh5etQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Calculates a total of 24 topographic metrics using either Whitebox or custom functions.\n",
        "# https://www.whiteboxgeo.com/manual/wbt_book/preface.html\n",
        "# These are later finalised with an automatic reduction in precision (rounding)\n",
        "# for faster moodelling, and the creation of 'unsmooth' and 'smooth' versions.\n",
        "# Smoothed versions allow the model to account for geolocation inaccuracies,\n",
        "# and adjacent topography types not captured in the various metrics.\n",
        "\n",
        "# The accuracy of most DEMs makes sub-metre values meaningless\n",
        "# Lowering precision gives better estimates for unique values of other\n",
        "# topographic metrics, and better optimisation for model training.\n",
        "elevation_precision = 0\n",
        "\n",
        "# Clear the temporary directory and recalculate topographic metrics if issues.\n",
        "clear_temp_directory = False\n",
        "if clear_temp_directory:\n",
        "  for raster in Path(topo_temp_dir).glob(\"**/*\"):\n",
        "    if raster.is_file(): raster.unlink()\n",
        "\n",
        "# Elevation\n",
        "elevation_path_temp = join(topo_temp_dir, \"elevation.tif\")\n",
        "if not exists(elevation_path_temp):\n",
        "  if elevation_precision <= 0:\n",
        "    elevation = np.round(base_dem_array, decimals=elevation_precision).astype(np.int16)\n",
        "  else: elevation = np.round(base_dem_array, decimals=elevation_precision)\n",
        "  # Whitebox doesn't support some types of compression\n",
        "  export_array_as_tif(elevation, elevation_path_temp, compress=False)\n",
        "\n",
        "# Slope\n",
        "slope_path_temp = join(topo_temp_dir, \"slope.tif\")\n",
        "if not exists(slope_path_temp):\n",
        "  wbt.slope(elevation_path_temp, slope_path_temp, units = \"degrees\")\n",
        "\n",
        "# Aspect\n",
        "aspect_path_temp = join(topo_temp_dir, \"aspect.tif\")\n",
        "if not exists(aspect_path_temp):\n",
        "  wbt.aspect(elevation_path_temp, aspect_path_temp)\n",
        "\n",
        "# Profile Curvature\n",
        "profile_curvature_path_temp = join(topo_temp_dir, \"profile_curvature.tif\")\n",
        "if not exists(profile_curvature_path_temp):\n",
        "  wbt.profile_curvature(elevation_path_temp, profile_curvature_path_temp, log=False)\n",
        "\n",
        "# Tangential Curvature\n",
        "tangential_curvature_path_temp = join(topo_temp_dir, \"tangential_curvature.tif\")\n",
        "if not exists(tangential_curvature_path_temp):\n",
        "  wbt.tangential_curvature(elevation_path_temp, tangential_curvature_path_temp, log=False)\n",
        "\n",
        "# Topographic Ruggedness Index\n",
        "topographic_ruggedness_index_path_temp = join(topo_temp_dir, \"topographic_ruggedness_index.tif\")\n",
        "if not exists(topographic_ruggedness_index_path_temp):\n",
        "  wbt.ruggedness_index(elevation_path_temp, topographic_ruggedness_index_path_temp)\n",
        "\n",
        "# Deviation from Mean Elevation\n",
        "dev_kernel_sizes = [3, 7, 11]\n",
        "for kernel_size in dev_kernel_sizes:\n",
        "  deviation_mean_elevation_path_temp = join(topo_temp_dir, f\"deviation_mean_elevation_{str(kernel_size).rjust(2, '0')}.tif\")\n",
        "  if not exists(deviation_mean_elevation_path_temp):\n",
        "    wbt.dev_from_mean_elev(elevation_path_temp, deviation_mean_elevation_path_temp, filterx=kernel_size, filtery=kernel_size)\n",
        "\n",
        "# Circular Variance of Aspect\n",
        "cva_kernel_sizes = [3, 7, 11]\n",
        "for kernel_size in cva_kernel_sizes:\n",
        "  circular_variance_aspect_path_temp = join(topo_temp_dir, f\"circular_variance_aspect_{str(kernel_size).rjust(2, '0')}.tif\")\n",
        "  if not exists(circular_variance_aspect_path_temp):\n",
        "    wbt.circular_variance_of_aspect(elevation_path_temp, circular_variance_aspect_path_temp, filter=kernel_size)\n",
        "\n",
        "# Fill Single Cell Pits for Breach Depressions\n",
        "dem_fill_single_cell_pits_path_temp = join(topo_temp_dir, \"dem_fill_single_cell_pits.tif\")\n",
        "if not exists(dem_fill_single_cell_pits_path_temp):\n",
        "  wbt.fill_single_cell_pits(elevation_path_temp, dem_fill_single_cell_pits_path_temp)\n",
        "  # Raw output doesn't work, needs to be saved again.\n",
        "  dem_fill_single_cell_pits = gdal.Open(dem_fill_single_cell_pits_path_temp)\n",
        "  dem_fill_single_cell_pits_array = dem_fill_single_cell_pits.ReadAsArray()\n",
        "  dem_fill_single_cell_pits = None\n",
        "  # Whitebox doesn't support some types of compression\n",
        "  export_array_as_tif(dem_fill_single_cell_pits_array, dem_fill_single_cell_pits_path_temp, compress=False)\n",
        "\n",
        "# Breach Depressions for Specific Contributing Area\n",
        "max_search_dist = 2 # Maximum search distance for breach paths in cells (pixels)\n",
        "dem_breach_depressions_path_temp = join(topo_temp_dir, \"dem_breach_depressions.tif\")\n",
        "if not exists(dem_breach_depressions_path_temp):\n",
        "  wbt.breach_depressions_least_cost(dem_fill_single_cell_pits_path_temp, dem_breach_depressions_path_temp, dist=max_search_dist)\n",
        "\n",
        "# Specific Contributing Area (Qin) (for TWI and SPI)\n",
        "specific_contributing_area_qin_path_temp = join(topo_temp_dir, \"specific_contributing_area_qin.tif\")\n",
        "if not exists(specific_contributing_area_qin_path_temp):\n",
        "  wbt.qin_flow_accumulation(dem_breach_depressions_path_temp, specific_contributing_area_qin_path_temp, out_type=\"specific contributing area\")\n",
        "\n",
        "# Topographic Wetness Index (TWI)\n",
        "topographic_wetness_index_path_temp = join(topo_temp_dir, \"topographic_wetness_index.tif\")\n",
        "if not exists(topographic_wetness_index_path_temp):\n",
        "  wbt.wetness_index(specific_contributing_area_qin_path_temp, slope_path_temp, topographic_wetness_index_path_temp)\n",
        "\n",
        "# Stream Power Index (SPI)\n",
        "exponent = 1.0\n",
        "stream_power_index_path_temp = join(topo_temp_dir, \"stream_power_index.tif\")\n",
        "if not exists(stream_power_index_path_temp):\n",
        "  wbt.stream_power_index(specific_contributing_area_qin_path_temp, slope_path_temp, stream_power_index_path_temp, exponent=exponent)\n",
        "\n",
        "# The whitebox algorithm 'wbt.surface_area_ratio' is not currently working correctly.\n",
        "# This SAR function below is based on the Whitebox source code:\n",
        "# https://github.com/jblindsay/whitebox-tools/blob/master/whitebox-tools-app/src/tools/terrain_analysis/surface_area_ratio.rs\n",
        "# 'jit' makes it orders of magnitude faster.\n",
        "\n",
        "surface_area_ratio_path_temp = join(topo_temp_dir, \"surface_area_ratio.tif\")\n",
        "\n",
        "if not exists(surface_area_ratio_path_temp):\n",
        "  elevation_raster = gdal.Open(elevation_path_temp)\n",
        "  transform = elevation_raster.GetGeoTransform()\n",
        "  elevation_array = elevation_raster.ReadAsArray()\n",
        "  elevation_raster = None\n",
        "  @jit(nopython=True)\n",
        "  def calculate_surface_area_ratio(dem, transform, nodata):\n",
        "      resx, resy = transform[1], -transform[5]\n",
        "      output = np.full(dem.shape, nodata, dtype=np.float32)\n",
        "      for i in range(1, dem.shape[0]-1):\n",
        "          mid_lat = transform[3] + i*transform[5]\n",
        "          resx_adjusted = abs(resx) * 111_111.0 * cos(radians(mid_lat))\n",
        "          resy_adjusted = abs(resy) * 111_111.0\n",
        "          res_diag = sqrt(resx_adjusted**2 + resy_adjusted**2)\n",
        "          cell_area = resx_adjusted * resy_adjusted\n",
        "          eigth_area = cell_area / 8.0\n",
        "          for j in range(1, dem.shape[1]-1):\n",
        "              if dem[i, j] == nodata:\n",
        "                  continue\n",
        "              window = dem[i-1:i+2, j-1:j+2]\n",
        "              dx = np.array([-1, 0, 1, -1, 0, 1, -1, 0, 1])\n",
        "              dy = np.array([-1, -1, -1, 0, 0, 0, 1, 1, 1])\n",
        "              zvals = np.array([window[dy[k]+1, dx[k]+1] for k in range(9)])\n",
        "              dist_planar = np.array([resx_adjusted]*6 + [resy_adjusted]*6 + [res_diag]*4)\n",
        "              dist_pairs = [(0, 1), (1, 2), (3, 4), (4, 5), (6, 7), (7, 8), (0, 3), (1, 4), (2, 5), (3, 6), (4, 7), (5, 8), (0, 4), (2, 4), (6, 4), (8, 4)]\n",
        "              distances = np.array([sqrt(dist_planar[k]**2 + (zvals[i] - zvals[j])**2) / 2 for k, (i, j) in enumerate(dist_pairs) if zvals[i] != nodata and zvals[j] != nodata])\n",
        "              triangle_sides = [(0, 7, 12), (1, 7, 13), (2, 6, 12), (3, 8, 13), (2, 9, 14), (3, 11, 15), (4, 10, 14), (5, 10, 15)]\n",
        "              area = 0.0\n",
        "              cell_area2 = cell_area\n",
        "              for a, b, c in triangle_sides:\n",
        "                  if a < len(distances) and b < len(distances) and c < len(distances):\n",
        "                      s = (distances[a] + distances[b] + distances[c]) / 2.0\n",
        "                      area += sqrt(s * (s - distances[a]) * (s - distances[b]) * (s - distances[c]))\n",
        "                  else:\n",
        "                      cell_area2 -= eigth_area\n",
        "              if cell_area2 > 0.0:\n",
        "                  output[i, j] = area / cell_area2\n",
        "          resx, resy = transform[1], -transform[5]\n",
        "      return output\n",
        "\n",
        "  # Run the jitted function on the entire array\n",
        "  surface_area_ratio_array = calculate_surface_area_ratio(elevation_array, transform, nodatavalue)\n",
        "  export_array_as_tif(surface_area_ratio_array, surface_area_ratio_path_temp, template=elevation_path_temp, compress=False)\n",
        "\n",
        "# Calculate Aspect Sine\n",
        "aspect_sine_path_temp = join(topo_temp_dir,\"aspect_sine.tif\")\n",
        "if not exists(aspect_sine_path_temp):\n",
        "  aspect = gdal.Open(aspect_path_temp)\n",
        "  aspect_array = aspect.ReadAsArray()\n",
        "  aspect = None\n",
        "  aspect_sine = np.sin(np.radians(aspect_array))\n",
        "  export_array_as_tif(aspect_sine, aspect_sine_path_temp, compress=False)\n",
        "\n",
        "# Calculate Aspect Cosine\n",
        "aspect_cosine_path_temp = join(topo_temp_dir,\"aspect_cosine.tif\")\n",
        "if not exists(aspect_cosine_path_temp):\n",
        "  aspect = gdal.Open(aspect_path_temp)\n",
        "  aspect_array = aspect.ReadAsArray()\n",
        "  aspect = None\n",
        "  aspect_cosine = np.cos(np.radians(aspect_array))\n",
        "  export_array_as_tif(aspect_cosine, aspect_cosine_path_temp, compress=False)\n",
        "\n",
        "# Calculate Eastness\n",
        "eastness_path_temp = join(topo_temp_dir,\"eastness.tif\")\n",
        "if not exists(eastness_path_temp):\n",
        "  slope = gdal.Open(slope_path_temp)\n",
        "  slope_array = slope.ReadAsArray()\n",
        "  slope = None\n",
        "  aspect_sine = gdal.Open(aspect_sine_path_temp).ReadAsArray()\n",
        "  eastness = aspect_sine * np.sin(np.radians(slope_array))\n",
        "  export_array_as_tif(eastness, eastness_path_temp, compress=False)\n",
        "\n",
        "# Calculate Northness\n",
        "northness_temp = join(topo_temp_dir,\"northness.tif\")\n",
        "if not exists(northness_temp):\n",
        "  slope = gdal.Open(slope_path_temp)\n",
        "  slope_array = slope.ReadAsArray()\n",
        "  slope = None\n",
        "  aspect_cosine = gdal.Open(aspect_cosine_path_temp)\n",
        "  aspect_cosine_array = aspect_cosine.ReadAsArray()\n",
        "  aspect_cosine = None\n",
        "  northness = aspect_cosine_array * np.sin(np.radians(slope_array))\n",
        "  export_array_as_tif(northness, northness_temp, compress=False)\n",
        "\n",
        "elevation = gdal.Open(elevation_path_temp)\n",
        "elevation_array = elevation.ReadAsArray()\n",
        "elevation = None\n",
        "\n",
        "# Calculate Roughness\n",
        "roughness_kernel_sizes = [3, 7, 11]\n",
        "for kernel_size in roughness_kernel_sizes:\n",
        "  roughness_path_temp = join(topo_temp_dir,f\"roughness_{str(kernel_size).rjust(2, '0')}.tif\")\n",
        "  if not exists(roughness_path_temp):\n",
        "    roughness = maximum_filter(elevation_array, size=kernel_size) - minimum_filter(elevation_array, size=kernel_size)\n",
        "    export_array_as_tif(roughness, roughness_path_temp, compress=False)\n",
        "\n",
        "# Calculate Topographic Position Index (TPI)\n",
        "tpi_kernel_sizes = [3, 7, 11]\n",
        "for kernel_size in tpi_kernel_sizes:\n",
        "  tpi_path_temp = join(topo_temp_dir, f\"topographic_position_index_{str(kernel_size).rjust(2, '0')}.tif\")\n",
        "  if not exists(tpi_path_temp):\n",
        "    topographic_position_index = elevation_array - uniform_filter(elevation_array, size=kernel_size)\n",
        "    export_array_as_tif(topographic_position_index, tpi_path_temp, compress=False)\n",
        "\n",
        "# Calculate Stream Power Index (SPI) log10\n",
        "spi_log10_path_temp = join(topo_temp_dir,\"stream_power_index_log10.tif\")\n",
        "if not exists(spi_log10_path_temp):\n",
        "  stream_power_index = gdal.Open(stream_power_index_path_temp)\n",
        "  stream_power_index_array = stream_power_index.ReadAsArray()\n",
        "  stream_power_index = None\n",
        "  stream_power_index_array[stream_power_index_array <= 0] = 1.0e-30 # Convert 0, negative or 'nodata' values\n",
        "  stream_power_index_log10 = np.log10(stream_power_index_array)\n",
        "  export_array_as_tif(stream_power_index_log10, spi_log10_path_temp, compress=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check there weren't errors in updated Whitebox algorithms\n",
        "visualise_topo_temp = False\n",
        "\n",
        "if visualise_topo_temp:\n",
        "  raster_files = [os.path.join(topo_temp_dir, file) for file in os.listdir(topo_temp_dir) if file.endswith('.tif')]\n",
        "  for raster_file in raster_files:\n",
        "      ds = gdal.Open(raster_file)\n",
        "      if ds is None:\n",
        "          print('Could not open ' + raster_file)\n",
        "          continue\n",
        "      band = ds.GetRasterBand(1)\n",
        "      raster_data = band.ReadAsArray()\n",
        "      ds = None\n",
        "      p2, p98 = np.percentile(raster_data, [2, 98]) # Compute the 2% and 98% percentiles\n",
        "      plt.figure()\n",
        "      plt.imshow(raster_data, cmap='viridis', vmin=p2, vmax=p98)\n",
        "      plt.colorbar()\n",
        "      plt.title(os.path.basename(raster_file))\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "fiTfDN2BL7P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsX4t1x7dGm4"
      },
      "source": [
        "# Round and smooth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a dictionary of optiomal precision based on number of desired unique values.\n",
        "# Limiting the number unique values avoids overfitting and reduces training time.\n",
        "# Defauly XGBoost hyperparameters use a maximum of 256 bins.\n",
        "# As a rule of thumb, set this to between 256 and the elevation range in metres.\n",
        "# E.g. project area is 200 - 2900 m, set to at least 2,700.\n",
        "# Smoothed versions may need to be re-evaluated as unique values can increase.\n",
        "#  This can be done in 4_datasets.ipynb 'Finalise dataset' section.\n",
        "\n",
        "override_max_unique_values = True\n",
        "max_unique_values = 1000\n",
        "\n",
        "if override_max_unique_values == False:\n",
        "  elevation_path_temp = join(topo_temp_dir, \"elevation.tif\")\n",
        "  elevation = gdal.Open(elevation_path_temp)\n",
        "  elevation_array = elevation.ReadAsArray()\n",
        "  elevation = None\n",
        "  max_unique_values = int(np.ptp(elevation_array)) # Precision based on elevation variance\n",
        "  elevation_array = None # Clear memory\n",
        "  print(f\"Basing max unique values on elevation integers, which has {max_unique_values}.\\n\")\n",
        "else: print(f\"Override max unique values is on, and set to {max_unique_values}.\\n\")\n",
        "\n",
        "topo_precision_dict = {}\n",
        "\n",
        "# Define list of topography metrics to finalise\n",
        "topography_list = []\n",
        "for temp_tif in os.listdir(topo_temp_dir):\n",
        "    topography_list.append(str(temp_tif))\n",
        "topography_list = sorted(topography_list)\n",
        "print(\"topography_final_list = [\")\n",
        "for topography in topography_list:\n",
        "    print(f\"'{topography}',\")\n",
        "print(\"]\")"
      ],
      "metadata": {
        "id": "7j0660GrLTCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topography_final_list = [\n",
        "# 'aspect.tif',\n",
        "'aspect_cosine.tif',\n",
        "'aspect_sine.tif',\n",
        "'circular_variance_aspect_03.tif',\n",
        "'circular_variance_aspect_07.tif',\n",
        "'circular_variance_aspect_11.tif',\n",
        "# 'dem_breach_depressions.tif',\n",
        "# 'dem_fill_single_cell_pits.tif',\n",
        "'deviation_mean_elevation_03.tif',\n",
        "'deviation_mean_elevation_07.tif',\n",
        "'deviation_mean_elevation_11.tif',\n",
        "'eastness.tif',\n",
        "'elevation.tif',\n",
        "'northness.tif',\n",
        "'profile_curvature.tif',\n",
        "'roughness_03.tif',\n",
        "'roughness_07.tif',\n",
        "'roughness_11.tif',\n",
        "'slope.tif',\n",
        "# 'specific_contributing_area_qin.tif',\n",
        "# 'stream_power_index.tif',\n",
        "'stream_power_index_log10.tif',\n",
        "'surface_area_ratio.tif',\n",
        "'tangential_curvature.tif',\n",
        "'topographic_position_index_03.tif',\n",
        "'topographic_position_index_07.tif',\n",
        "'topographic_position_index_11.tif',\n",
        "'topographic_ruggedness_index.tif',\n",
        "'topographic_wetness_index.tif',\n",
        "]"
      ],
      "metadata": {
        "id": "P7gOvqHDLRe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_precision_dict = True\n",
        "overwrite_existing_precision_dict = False\n",
        "\n",
        "precision_dict_csv_path = join(topo_temp_dir, 'rounding_dictionary.csv')\n",
        "if not exists(precision_dict_csv_path) or overwrite_existing_precision_dict:\n",
        "  for topography_final in topography_final_list:\n",
        "    print(f\"Reading {topography_final}...\")\n",
        "    # Read raster as array\n",
        "    topography_raster_path = join(topo_temp_dir, topography_final)\n",
        "    # Convert to float32 in case nodata values need to be converted to nan\n",
        "    topography_raster = gdal.Open(topography_raster_path)\n",
        "    topography_raster_array = topography_raster.ReadAsArray().astype(np.float32)\n",
        "    topography_raster = None\n",
        "    # Convert 'nodata' values to nan\n",
        "    topography_raster_array[topography_raster_array == nodatavalue] = np.nan\n",
        "    topography_raster_array_masked = np.ma.array(topography_raster_array, mask=np.isnan(topography_raster_array))\n",
        "    # Count unique values in raster\n",
        "    unique_values = len(np.unique(topography_raster_array_masked))\n",
        "    print(f\"There are {unique_values} unique values in {topography_final}\")\n",
        "    # Generate histogram from 100,000 random points\n",
        "    random_selection = np.random.choice(topography_raster_array_masked.ravel(), size = 100_000, replace = False)\n",
        "    _ = plt.hist(random_selection, bins='auto')  # arguments are passed to np.histogram\n",
        "    plt.title(f\"{topography_final}\")\n",
        "    plt.show()\n",
        "    # Remove 0 values for log10\n",
        "    topography_raster_array_masked[topography_raster_array_masked == 0] = np.nan\n",
        "    topography_raster_array_masked = np.ma.array(topography_raster_array, mask=np.isnan(topography_raster_array))\n",
        "    # Create log10 array for determining positions for rounding\n",
        "    array_log10 = np.log10(abs(topography_raster_array_masked))\n",
        "    place_value_decimal = int(abs(np.min(array_log10)))\n",
        "    place_value_integer = int(0 - np.max(array_log10))\n",
        "    # Iterate down precision levels to determine optimal number of unique values\n",
        "    min_starting_precision = len(str(max_unique_values))\n",
        "    optimal_precision = None\n",
        "    # Ensure it does not test below 0\n",
        "    lower_bound = max(place_value_integer, 0)\n",
        "    upper_bound = max(min_starting_precision, place_value_decimal + 1)\n",
        "    tested_precisions = list(reversed(range(lower_bound, upper_bound)))\n",
        "    for precision in tested_precisions:\n",
        "        rounded_array = np.round(topography_raster_array, decimals=precision)\n",
        "        round_unique_values = len(np.unique(rounded_array))\n",
        "        if round_unique_values <= max_unique_values:\n",
        "            optimal_precision = precision\n",
        "            print(f\"The optimal precision for {topography_final} is {optimal_precision}, with {round_unique_values} unique values.\")\n",
        "            topo_precision_dict.update({f'{topography_final}': f'{optimal_precision}'})\n",
        "            break\n",
        "    if optimal_precision is None:\n",
        "        integer_unique_values = len(np.unique(np.round(topography_raster_array, decimals=0)))\n",
        "        print(f\"No precision had <= {max_unique_values} unique values for {topography_final}. Using integers for {integer_unique_values} unique values.\")\n",
        "        topo_precision_dict.update({f'{topography_final}': f'{0}'})\n",
        "    print(\"___________________\\n\")\n",
        "\n",
        "  print(\"# Dictionary for optimal rounding values:\")\n",
        "  topo_precision_dict\n",
        "\n",
        "  precision_dict_csv_path = join(topo_temp_dir, 'rounding_dictionary.csv')\n",
        "  # Save rounding dictionary to CSV\n",
        "  with open(precision_dict_csv_path, 'w', newline='') as precision_dict_csv:\n",
        "      writer = csv.writer(precision_dict_csv)\n",
        "      writer.writerow(topo_precision_dict.keys())\n",
        "      writer.writerow(topo_precision_dict.values())\n",
        "\n",
        "# Open rounding dictionary and verify\n",
        "precision_dict_csv_path = join(topo_temp_dir, 'rounding_dictionary.csv')\n",
        "with open(precision_dict_csv_path, 'r') as file:\n",
        "    keys, values = list(csv.reader(file))\n",
        "    topo_precision_dict = dict(zip(keys, values))\n",
        "\n",
        "# Verify precision and correct if necessary\n",
        "print(\"topo_precision_dict = {\")\n",
        "for key, value in topo_precision_dict.items():\n",
        "    print(f'\"{key}\": {value},')\n",
        "print(\"}\")\n"
      ],
      "metadata": {
        "id": "7DF6qSXULQDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVCcwLGOeYa4"
      },
      "outputs": [],
      "source": [
        "# Dictionary for optimal rounding values:\n",
        "topo_precision_dict = {\n",
        "\"aspect_cosine.tif\": 2,\n",
        "\"aspect_sine.tif\": 2,\n",
        "\"circular_variance_aspect_03.tif\": 2,\n",
        "\"circular_variance_aspect_07.tif\": 2,\n",
        "\"circular_variance_aspect_11.tif\": 2,\n",
        "\"deviation_mean_elevation_03.tif\": 2,\n",
        "\"deviation_mean_elevation_07.tif\": 2,\n",
        "\"deviation_mean_elevation_11.tif\": 2,\n",
        "\"eastness.tif\": 2,\n",
        "\"elevation.tif\": 0,\n",
        "\"northness.tif\": 2,\n",
        "\"profile_curvature.tif\": 4,\n",
        "\"roughness_03.tif\": 3,\n",
        "\"roughness_07.tif\": 3,\n",
        "\"roughness_11.tif\": 3,\n",
        "\"slope.tif\": 1,\n",
        "\"stream_power_index_log10.tif\": 1,\n",
        "\"surface_area_ratio.tif\": 2,\n",
        "\"tangential_curvature.tif\": 4,\n",
        "\"topographic_position_index_03.tif\": 3,\n",
        "\"topographic_position_index_07.tif\": 3,\n",
        "\"topographic_position_index_11.tif\": 3,\n",
        "\"topographic_ruggedness_index.tif\": 3,\n",
        "\"topographic_wetness_index.tif\": 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates both unsmoothed and smoothed topography feature versions.\n",
        "# Smoothing topography or other continuous features is mainly to account for\n",
        "# geolocation inaccuracies. The various topographic metrics already provide\n",
        "# some spatial awareness to tabular machine learning algorithms (e.g. XGBoost),\n",
        "# without performance and optimisation issues of deep learning models (e.g. CNN).\n",
        "# The smoothing distance (m) affects the distance and strength of the kernel,\n",
        "# and should be over the max pixel size.\n",
        "\n",
        "# Determine maximum cell resolution\n",
        "cell_size_x = gdal.Open(cell_size_x_path)\n",
        "cell_size_y = gdal.Open(cell_size_y_path)\n",
        "max_cell_resolution = max(cell_size_x.ReadAsArray().max(), cell_size_y.ReadAsArray().max())\n",
        "cell_size_x = None\n",
        "cell_size_y = None\n",
        "\n",
        "# Set smoothing distance (metres) to 2.1x the resolution to account for\n",
        "# values at a distance of 2 adjacent pixels.\n",
        "smoothing_distance = round(2.1*max_cell_resolution)\n",
        "print(f\"The maximum pixel size is {max_cell_resolution}. Smoothing distance is set to {smoothing_distance} m.\\n\")\n",
        "\n",
        "# Topography progress\n",
        "topography_progress_index = 0\n",
        "topography_progress_label = widgets.Label(f\"Topography progress: {topography_progress_index}/{len(topo_precision_dict.items())}\")\n",
        "display(topography_progress_label)\n",
        "\n",
        "# Iterate over selected topography rasters\n",
        "for topography, precision in topo_precision_dict.items():\n",
        "    topo_raster_temp_path = join(topo_temp_dir, topography)\n",
        "    topo_raster_temp = gdal.Open(topo_raster_temp_path)\n",
        "    topo_raster_temp_array = topo_raster_temp.ReadAsArray()\n",
        "    topo_raster_temp = None\n",
        "    # Convert nodata values to 0\n",
        "    topo_raster_temp_array[topo_raster_temp_array == nodatavalue] = 0\n",
        "\n",
        "    # Set path and check if exists\n",
        "    if topo_temp_dir.endswith(\"dtm_temp\"):\n",
        "        topo_raster_unsmoothed_filename = f\"topo_dtm_unsmooth_{topography}\"\n",
        "    else:\n",
        "        topo_raster_unsmoothed_filename = f\"topo_dsm_unsmooth_{topography}\"\n",
        "    topo_raster_unsmoothed_path = join(topo_final_dir, topo_raster_unsmoothed_filename)\n",
        "\n",
        "    if not exists(topo_raster_unsmoothed_path):\n",
        "        # Round and export unsmoothed topography raster\n",
        "        # Preserves original elevation values at pixel level for model comparison\n",
        "        topo_raster_unsmoothed_rounded = np.round(topo_raster_temp_array, decimals=int(precision))\n",
        "        if precision <= 0: # Save as Int16 if precision = 0\n",
        "            topo_raster_unsmoothed_rounded = topo_raster_unsmoothed_rounded.astype(np.int16)\n",
        "        export_array_as_tif(topo_raster_unsmoothed_rounded, topo_raster_unsmoothed_path)\n",
        "\n",
        "    # Smooth using 2d spatial convolution\n",
        "    if topo_temp_dir.endswith(\"dtm_temp\"): topo_raster_smoothed_filename = f\"topo_dtm_smooth_{topography}\"\n",
        "    else: topo_raster_smoothed_filename = f\"topo_dsm_smooth_{topography}\"\n",
        "    topo_raster_smoothed_path = join(topo_final_dir, topo_raster_smoothed_filename)\n",
        "\n",
        "    if not exists(topo_raster_smoothed_path):\n",
        "        # Apply gaussian smoothing via edge_effects function\n",
        "        # Creates spatially-aware version capturing terrain patterns and slopes\n",
        "        # Model receives both smoothed and unsmoothed versions for enhanced feature learning\n",
        "        topo_raster_smoothed = edge_effects(topo_raster_temp_array, 'continuous', cell_size_x_path, cell_size_y_path, threshold_metres=smoothing_distance)\n",
        "        # Round and export smoothed topography raster\n",
        "        topo_raster_smoothed_rounded = np.round(topo_raster_smoothed, decimals=int(precision))\n",
        "        smoothed_round_unique_values = len(np.unique(topo_raster_smoothed_rounded))\n",
        "        smoothed_precision = int(precision)\n",
        "        if smoothed_round_unique_values > max_unique_values:\n",
        "            print(f\"Smoothed {topography} initially had {smoothed_round_unique_values} unique values.\")\n",
        "            while smoothed_round_unique_values > max_unique_values and smoothed_precision > 0:\n",
        "                smoothed_precision -= 1\n",
        "                topo_raster_smoothed_rounded = np.round(topo_raster_smoothed, decimals=smoothed_precision)\n",
        "                smoothed_round_unique_values = len(np.unique(topo_raster_smoothed_rounded))\n",
        "            if smoothed_round_unique_values > max_unique_values:\n",
        "                print(f\"No precision had <= {max_unique_values} unique values for {topography}. Using integers for {smoothed_round_unique_values} unique values.\")\n",
        "            else:\n",
        "                print(f\"Smoothed {topography} final precision: {smoothed_precision}, unique values: {smoothed_round_unique_values}.\")\n",
        "        if smoothed_precision == 0: # Save as Int16 if precision = 0\n",
        "            topo_raster_smoothed_rounded = topo_raster_smoothed_rounded.astype(np.int16)\n",
        "        export_array_as_tif(topo_raster_smoothed_rounded, topo_raster_smoothed_path)\n",
        "\n",
        "    # Update topography progress\n",
        "    topography_progress_index += 1\n",
        "    topography_progress_label.value = f\"Topography progress: {topography_progress_index}/{len(topo_precision_dict.items())}\""
      ],
      "metadata": {
        "id": "oo_EagvLk7P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mow4bIz0EBw"
      },
      "source": [
        "# Distance from coast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7PZCLj4HCqf"
      },
      "outputs": [],
      "source": [
        "# Creates a feature which accounts for coastal / continental effects.\n",
        "# Requires a polygon of a landmass or 'coastline extent', e.g. Peninsular Malaysia.\n",
        "# This must include the nearest coastline to all parts of the project area,\n",
        "# even coasts outside the project area (if they're still the nearest).\n",
        "\n",
        "# Define and create directory\n",
        "coast_dir = join(features_dir, 'coast')\n",
        "makedirs(coast_dir, exist_ok=True)\n",
        "\n",
        "# Download global coast data from https://osmdata.openstreetmap.de/data/coastlines.html\n",
        "coastlines_url = 'https://osmdata.openstreetmap.de/download/coastlines-split-4326.zip'\n",
        "coastlines_global_file_path = join(coast_dir, 'coastlines-split-4326.zip')\n",
        "if not exists(coastlines_global_file_path):\n",
        "  request = requests.get(coastlines_url, allow_redirects=True)\n",
        "  open(coastlines_global_file_path, 'wb').write(request.content)\n",
        "\n",
        "coastlines_global_dir = join(coast_dir, 'coastlines-split-4326')\n",
        "if not exists(coastlines_global_dir):\n",
        "  with zipfile.ZipFile(coastlines_global_file_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(coast_dir)\n",
        "\n",
        "# Upload and select a polygon with full coastline extent.\n",
        "# It must be the template.gpkg polygon OR a polygon that entirely contains template.gpkg.\n",
        "polygons_to_exclude = ['project_area.gpkg', 'project_area_buffered_bbox.gpkg', 'gedi_area.gpkg']\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    print(f\"coastline_extent_polygon = '{polygon}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8IUkGsiFfYs"
      },
      "outputs": [],
      "source": [
        "coastline_extent_polygon = 'peninsular_malaysia.gpkg'\n",
        "\n",
        "# Get extent of polygon\n",
        "coastline_extent_polygon_path = join(polygons_dir, coastline_extent_polygon)\n",
        "coastline_extent_bounds = gpd.read_file(coastline_extent_polygon_path).total_bounds\n",
        "coastline_min_x, coastline_max_x = coastline_extent_bounds[0], coastline_extent_bounds[2]\n",
        "coastline_min_y, coastline_max_y = coastline_extent_bounds[1], coastline_extent_bounds[3]\n",
        "\n",
        "# Set precision (in km) of distance.\n",
        "# Increase precision if focus is on coastal forest, where 100m might make a difference.\n",
        "precision = 1\n",
        "\n",
        "# Load template\n",
        "template = gdal.Open(template_dir)\n",
        "\n",
        "# Path of the new coast raster (where coastline will be rasterized)\n",
        "rasterized_coast_path = join(coast_dir, 'rasterized_coast.tif')\n",
        "\n",
        "# Create a new empty raster based on the coastline extent polygon\n",
        "if coastline_extent_polygon != 'template.gpkg':\n",
        "  # Get dimensions of template\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  template_size_x, template_size_y = template_band.XSize, template_band.YSize\n",
        "  template_res_x, template_res_y = template_dimensions[1], -template_dimensions[5]\n",
        "  template = None\n",
        "\n",
        "  # Calculate the minimum x and y of the template\n",
        "  template_min_x = template_dimensions[0]\n",
        "  template_max_y = template_dimensions[3]\n",
        "\n",
        "  # Calculate the centre x and y of the template\n",
        "  template_centre_x = template_min_x + ((template_size_x / 2) * template_res_x)\n",
        "  template_centre_y = template_max_y - ((template_size_y / 2) * template_res_y)\n",
        "\n",
        "  # Calculate the size (in pixels) difference between the polygon minimum and template minimum\n",
        "  coastline_min_diff_x = template_min_x - coastline_min_x\n",
        "  coastline_max_diff_y = coastline_max_y - template_max_y\n",
        "  # Validate coastline polygon contains template\n",
        "  if coastline_min_diff_x < 0 or coastline_max_diff_y < 0:\n",
        "      raise ValueError(\n",
        "          f\"Coastline extent polygon must entirely contain the template. \"\n",
        "          f\"Template extends beyond coastline bounds: \"\n",
        "          f\"x_diff={coastline_min_diff_x:.2f}, y_diff={coastline_max_diff_y:.2f}\")\n",
        "  coastline_min_diff_x_size = int(np.ceil(coastline_min_diff_x / template_res_x))\n",
        "  coastline_max_diff_y_size = int(np.ceil(coastline_max_diff_y / template_res_y))\n",
        "\n",
        "  # Calculate when the coastline raster should start while maintaining template resolution and position\n",
        "  coastline_start_x = template_min_x - (coastline_min_diff_x_size * template_res_x)\n",
        "  coastline_start_y = template_max_y + (coastline_max_diff_y_size * template_res_y)\n",
        "\n",
        "  # Calculate the size of the coastline raster\n",
        "  coastline_size_x = int(np.ceil((coastline_max_x - coastline_start_x)/template_res_x))\n",
        "  coastline_size_y = int(np.ceil((coastline_start_y - coastline_min_y)/template_res_y))\n",
        "\n",
        "  if not exists(rasterized_coast_path):\n",
        "    # Create coast raster dataset\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(rasterized_coast_path, coastline_size_x, coastline_size_y, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                    options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "    driver.SetProjection(template_projection)\n",
        "    driver.SetGeoTransform((coastline_start_x, template_res_x, 0, coastline_start_y, 0, -template_res_y))\n",
        "\n",
        "    #  Create and write array (all pixels with value 1)\n",
        "    raster_data = np.ones((coastline_size_y, coastline_size_x), dtype=np.float32)\n",
        "    driver.GetRasterBand(1).WriteArray(raster_data)\n",
        "\n",
        "    # Close coast raster dataset\n",
        "    driver.FlushCache()\n",
        "    driver = None\n",
        "    print(\"A blank raster at the extent of coastlines polygon has been generated, ready for rasterization.\")\n",
        "  else: print(\"A rasterization raster already exists.\")\n",
        "\n",
        "else: # If just using the template area, copy the template.\n",
        "  if not exists(rasterized_coast_path):\n",
        "    template_array = template.ReadAsArray()\n",
        "    export_array_as_tif(template_array, rasterized_coast_path)\n",
        "  else: print(\"A rasterization raster already exists.\")\n",
        "\n",
        "# Clip coastlines polygon to the extent (speeds up rasterization)\n",
        "coastlines_clipped_path = join(coast_dir, 'clipped_coastlines.gpkg')\n",
        "if not exists(coastlines_clipped_path):\n",
        "  coastlines_shp_path = join(coastlines_global_dir, 'lines.shp')\n",
        "  coastlines_shp_df = gpd.read_file(coastlines_shp_path)\n",
        "  coastlines_clipped_df = gpd.clip(coastlines_shp_df, coastline_extent_bounds)\n",
        "  coastlines_clipped_df.to_file(coastlines_clipped_path, driver='GPKG')\n",
        "  print(f\"Coastlines clipped to the polygon: {coastlines_clipped_path}\")\n",
        "else: print(f\"Coastlines have already been clipped to the polygon: {coastlines_clipped_path}\")\n",
        "\n",
        "# Rasterize coastlines (2), if not already\n",
        "rasterized_coast = gdal.Open(rasterized_coast_path)\n",
        "rasterized_coast_array = rasterized_coast.ReadAsArray()\n",
        "rasterized_coast = None\n",
        "if not np.any(rasterized_coast_array == 2):\n",
        "  burn_polygon_to_raster(rasterized_coast_path, coastlines_clipped_path, fixed_value=2)\n",
        "  print(f\"Coastlines rasterized: {rasterized_coast_path}\")\n",
        "else: print(f\"Coastlines have already been rasterized: {rasterized_coast_path}\")\n",
        "\n",
        "# Calculate proximity in pixels coast\n",
        "coast_proximity_pixels_path = join(coast_dir, \"coast_proximity_pixels.tif\")\n",
        "if not exists(coast_proximity_pixels_path):\n",
        "  rasterized_coast = gdal.Open(rasterized_coast_path)\n",
        "  rasterized_coast_array = rasterized_coast.ReadAsArray()\n",
        "  rasterized_coast = None\n",
        "  # If the rasterized coastline is different dimensions from the template\n",
        "  if coastline_extent_polygon != 'template.gpkg':\n",
        "    coast_proximity_pixels_unclipped_path = join(coast_dir, \"coast_proximity_pixels_unclipped.tif\")\n",
        "    if not exists(coast_proximity_pixels_unclipped_path):\n",
        "      coast_proximity_pixels_unclipped = distance_transform_edt(rasterized_coast_array != 2) # Target the coastal '2' pixels\n",
        "      export_array_as_tif(coast_proximity_pixels_unclipped, coast_proximity_pixels_unclipped_path, rasterized_coast_path)\n",
        "    coast_proximity_pixels_unclipped = gdal.Open(coast_proximity_pixels_unclipped_path)\n",
        "    coast_proximity_pixels_unclipped_array = coast_proximity_pixels_unclipped.ReadAsArray()\n",
        "    coast_proximity_pixels_unclipped = None\n",
        "    clip_start_x, clip_start_y = coastline_min_diff_x_size, coastline_max_diff_y_size\n",
        "    clip_size_x, clip_size_y = template_size_x, template_size_y\n",
        "    coast_proximity_pixels = coast_proximity_pixels_unclipped_array[clip_start_y:clip_start_y + clip_size_y,\n",
        "                                                            clip_start_x:clip_start_x + clip_size_x]\n",
        "  # If the rasterized coastline is the same dimensions as the template\n",
        "  else: coast_proximity_pixels = distance_transform_edt(rasterized_coast_array != 2) # Target the coastal '2' pixels\n",
        "  # Export coast proximity in pixels\n",
        "  export_array_as_tif(coast_proximity_pixels, coast_proximity_pixels_path)\n",
        "  print(f\"A proximity (pixel number) raster has been generated at: {coast_proximity_pixels_path}\")\n",
        "else: print(f\"A proximity (pixel number) raster already exists at: {coast_proximity_pixels_path}\")\n",
        "\n",
        "# Convert proximity to km (distance from coast)\n",
        "coast_distance_path = join(coast_dir, \"coast_proximity_km.tif\")\n",
        "if not exists(coast_distance_path):\n",
        "  coast_proximity = gdal.Open(coast_proximity_pixels_path)\n",
        "  coast_proximity_array = coast_proximity.ReadAsArray()\n",
        "  coast_proximity = None\n",
        "  cell_size_x = gdal.Open(cell_size_x_path)\n",
        "  cell_size_y = gdal.Open(cell_size_y_path)\n",
        "  cell_size_x_array = cell_size_x.ReadAsArray()\n",
        "  cell_size_y_array = cell_size_y.ReadAsArray()\n",
        "  cell_size_x = None\n",
        "  cell_size_y = None\n",
        "  cell_size_mean_km = ((np.mean(cell_size_x_array) + np.mean(cell_size_y_array)) / 2) / 1000\n",
        "  coast_proximity_km = coast_proximity_array * cell_size_mean_km\n",
        "  coast_proximity_round = np.round(coast_proximity_km, precision)\n",
        "  # Save as Int16 if precision <=0\n",
        "  if precision <= 0: coast_proximity_round = coast_proximity_round.astype(np.int16)\n",
        "  export_array_as_tif(coast_proximity_round, coast_distance_path)\n",
        "  print(f\"A distance from coast (km) raster has been generated at: {coast_distance_path}\")\n",
        "else: print(f\"A distance from coast (km) raster already exists at: {coast_distance_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6r7JXbijM50"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU_AC6MjNfTN"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}