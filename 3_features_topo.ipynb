{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/3_features_topo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yk8CnJRCYVQ"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5USlWSaxqv9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50004c53-b15e-4314-e611-7c74ec48439c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOfy6gWFwHEC"
      },
      "outputs": [],
      "source": [
        "# Installs\n",
        "%%capture\n",
        "!pip install astropy\n",
        "!pip install geopandas\n",
        "!pip install whitebox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTHaRSv8wICv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3103567c-980d-446a-ed8f-feb0ef3b66d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading WhiteboxTools pre-compiled binary for first time use ...\n",
            "Downloading WhiteboxTools binary from https://www.whiteboxgeo.com/WBT_Linux/WhiteboxTools_linux_musl.zip\n",
            "Decompressing WhiteboxTools_linux_musl.zip ...\n",
            "WhiteboxTools package directory: /usr/local/lib/python3.11/dist-packages/whitebox\n",
            "Downloading testdata ...\n"
          ]
        }
      ],
      "source": [
        "!# Reload imports, replacing those in the cache\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "import csv\n",
        "import geopandas as gpd\n",
        "import glob\n",
        "from google.colab import runtime\n",
        "import ipywidgets as widgets\n",
        "from math import sqrt, cos, radians\n",
        "import matplotlib.pyplot as plt\n",
        "from numba import jit\n",
        "import numpy as np\n",
        "from os import makedirs\n",
        "from os.path import exists, join\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from scipy.ndimage import maximum_filter, minimum_filter, uniform_filter, distance_transform_edt\n",
        "from scipy.ndimage import label, sum as ndi_sum\n",
        "import whitebox\n",
        "wbt = whitebox.WhiteboxTools()\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMCq50kg36Br"
      },
      "outputs": [],
      "source": [
        "# 1_areas directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "template_dir = join(areas_dir, \"template.tif\")\n",
        "\n",
        "# 3_features directories\n",
        "features_dir = join(base_dir, \"3_features\")\n",
        "\n",
        "# 6_scenarios directory\n",
        "scenario_dir = join(base_dir, \"6_scenarios\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhkgXF4foXhx"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = [\"COMPRESS=DEFLATE\", \"PREDICTOR=3\", \"ZLEVEL=9\"]\n",
        "    else: options = []\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = vector = None\n",
        "    try:\n",
        "        raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "        vector = ogr.Open(polygon_path)\n",
        "        if not raster or not vector:\n",
        "            raise ValueError(\"Cannot open input files\")\n",
        "        layer = vector.GetLayer()\n",
        "        options = [\"ALL_TOUCHED=TRUE\"] if all_touched else []\n",
        "        if fixed:\n",
        "            gdal.RasterizeLayer(raster, [1], layer, burn_values=[fixed_value], options=options)\n",
        "        else:\n",
        "            attr_name = column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()\n",
        "            options.append(f\"ATTRIBUTE={attr_name}\")\n",
        "            gdal.RasterizeLayer(raster, [1], layer, options=options)\n",
        "    finally:\n",
        "        if raster: raster.FlushCache()\n",
        "        raster = vector = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9x_KcII90Cw"
      },
      "source": [
        "# Define base DEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMoW0_un4dOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb715ec5-64eb-4eaa-f7a7-75b15c093946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Remember to modify montane transition values to be specific to study area.\n",
            "gedi_elevation_path = \"/gdrive/Shareddrives/masfi/6_scenarios/gedi_elevation_tekai_250622_103110/scenario_predictions_unmasked/2015__gedi_elevation_tekai_250622_103110_unmasked.tif\"\n",
            "# Remember to modify montane transition values to be specific to study area.\n",
            "gedi_elevation_path = \"/gdrive/Shareddrives/masfi/6_scenarios/gedi_elevation_tekai_250622_103110/scenario_predictions/2015__gedi_elevation_tekai_250622_103110.tif\"\n"
          ]
        }
      ],
      "source": [
        "# After predicting GEDI elevation to create a GEDI DTM (Digital Terrain Model),\n",
        "# Change to True to generate DTM features to use instead of DSM (Digital Surface Model) features.\n",
        "# If no GEDI DTM has been created this will default to the DSM.\n",
        "enable_gedi_dtm = True\n",
        "\n",
        "# Select which prediction path to use as the GEDI DTM\n",
        "# Use an 'unmasked' version so area calculations (e.g. hydrography) are calculated correctly.\n",
        "if enable_gedi_dtm:\n",
        "  gedi_dtm_exists = False\n",
        "  for subdir, dirs, files in os.walk(scenario_dir):\n",
        "    for raster in files:\n",
        "      if raster.endswith('.tif'):\n",
        "        if 'gedi_elevation' in raster:\n",
        "          print('# Remember to modify montane transition values to be specific to study area.')\n",
        "          print(f'gedi_elevation_path = \"{subdir}/{raster}\"')\n",
        "          gedi_dtm_exists = True\n",
        "  if not gedi_dtm_exists: print(\"No GEDI DTM found in scenarios folder. Defaulting to DSM.\")\n",
        "else: print(f\"Using the DSM in {areas_dir}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuGr-V0ViwgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657630d8-bbe9-4c95-f34b-a9efecf762d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A base DTM already exists, first remove from /gdrive/Shareddrives/masfi/1_areas for replacement.\n"
          ]
        }
      ],
      "source": [
        "# This code block is only relevant (and otherwise does nothing) if using a GEDI DTM.\n",
        "# Remember to modify montane transition values to be specific to study area (see below)\n",
        "gedi_elevation_path = \"/gdrive/Shareddrives/masfi/6_scenarios/gedi_elevation_tekai_250622_103110/scenario_predictions_unmasked/2015__gedi_elevation_tekai_250622_103110_unmasked.tif\"\n",
        "\n",
        "base_dsm_path = join(areas_dir, \"base_dem_dsm.tif\")\n",
        "base_dtm_path = join(areas_dir, \"base_dem_dtm.tif\")\n",
        "# Copy GEDI DTM to areas directory\n",
        "if enable_gedi_dtm and gedi_dtm_exists:\n",
        "  if not exists(base_dtm_path):\n",
        "    # Sea level is post-processed back to 0 m, and areas that might have been predicted\n",
        "    # Below this. At ~sea level, the original DEM was likely the true terrain height.\n",
        "\n",
        "    # Define a low transition zone between original and DTM.\n",
        "    # This delineates a transition from 0 to 100 % DTM values.\n",
        "    low_transition_lower_limit = 0\n",
        "    low_transition_upper_limit = 5\n",
        "\n",
        "    # Higher elevations are poorly predicted in some study areas due to low sample size.\n",
        "    # Original values should be used to avoid erroneous topographic metrics.\n",
        "    # Vegetation at these high elevations tend not to change much between disturbance scenarios.\n",
        "\n",
        "    # Define a montane transition zone between original and DTM.\n",
        "    # This delineates a transition from 0 to 100 % original DEM values.\n",
        "    # E.g. >1,500 is typically scrub in Peninsular Malaysia.\n",
        "    montane_transition_lower_limit = 1500\n",
        "    montane_transition_upper_limit = 1800\n",
        "\n",
        "    # Read original base DEM\n",
        "    base_dsm_array = gdal.Open(base_dsm_path).ReadAsArray()\n",
        "\n",
        "    # Scale <=0 to >=5 m values to a ratio (0 - 1) of DTM values\n",
        "    base_dsm_array_low_ratio = base_dsm_array.copy()\n",
        "    base_dsm_array_low_ratio[base_dsm_array_low_ratio <= low_transition_lower_limit] = low_transition_lower_limit\n",
        "    base_dsm_array_low_ratio[base_dsm_array_low_ratio >= low_transition_upper_limit] = low_transition_upper_limit\n",
        "    base_dsm_array_low_ratio = base_dsm_array_low_ratio / low_transition_upper_limit\n",
        "\n",
        "    # Scale <=0 to >=5 m values to a ratio (0 - 1) of DTM values\n",
        "    base_dsm_array_montane_ratio = base_dsm_array.copy()\n",
        "    base_dsm_array_montane_ratio[base_dsm_array_montane_ratio <= montane_transition_lower_limit] = montane_transition_lower_limit\n",
        "    base_dsm_array_montane_ratio[base_dsm_array_montane_ratio >= montane_transition_upper_limit] = montane_transition_upper_limit\n",
        "    base_dsm_array_montane_ratio = (montane_transition_upper_limit - base_dsm_array_montane_ratio) / (montane_transition_upper_limit - montane_transition_lower_limit)\n",
        "\n",
        "    # Use original DEM values for surface water.\n",
        "    # The Copernicus DEM rounds all surface water values to 1 or 0 decimal places.\n",
        "    # This is used to differentiate them from land values, creating a 'land binary'.\n",
        "    base_dsm_array_land_binary = base_dsm_array.copy()\n",
        "    base_dsm_array_land_binary = np.floor(base_dsm_array_land_binary * 10) / 10 # Round DOWN 1 decimal place\n",
        "    base_dsm_array_land_binary = base_dsm_array - base_dsm_array_land_binary\n",
        "    base_dsm_array_land_binary[base_dsm_array_land_binary > 0] = 1\n",
        "    # Invert the binary array to target 0 values for sieving single water pixels (usually erroneous)\n",
        "    base_dsm_array_land_binary_inverted = np.logical_not(base_dsm_array_land_binary)\n",
        "    # Sieve to 0.5 ha, using 8-connectedness (3, 3)\n",
        "    lb_array_labelled, lb_array_features = label(base_dsm_array_land_binary_inverted, structure=np.ones((3, 3)))\n",
        "    # Determine the size of each patch\n",
        "    lb_array_sizes = ndi_sum(base_dsm_array_land_binary_inverted, lb_array_labelled, range(lb_array_features + 1))\n",
        "    # Create a mask to remove patches smaller than the threshold\n",
        "    lb_array_mask_sizes = lb_array_sizes >= 2\n",
        "    lb_array_mask_sizes[0] = 0 # Ensure non-target values are excluded\n",
        "    lb_array_mask = lb_array_mask_sizes[lb_array_labelled]\n",
        "    # Apply the mask to the inverted binary array\n",
        "    lb_array_sieved_inverted = base_dsm_array_land_binary_inverted * lb_array_mask\n",
        "    # Invert the array back to original representation\n",
        "    base_dsm_array_land_binary = np.logical_not(lb_array_sieved_inverted)\n",
        "\n",
        "    # Read the GEDI DTM and create the final modifier\n",
        "    gedi_elevation_array = gdal.Open(gedi_elevation_path).ReadAsArray()\n",
        "    base_dtm_array_modifier = gedi_elevation_array.copy()\n",
        "    # Change all DTM values < sea level to 0 (most are erroneous)\n",
        "    base_dtm_array_modifier[base_dtm_array_modifier < 0] = 0\n",
        "    # Sutract DTM from the DSM as the modifier\n",
        "    base_dtm_array_modifier = base_dsm_array - base_dtm_array_modifier\n",
        "    # Multiply the DTM modifier by low ratio, montane ratio and land binary\n",
        "    base_dtm_array_modifier = base_dtm_array_modifier * base_dsm_array_low_ratio * base_dsm_array_montane_ratio * base_dsm_array_land_binary\n",
        "\n",
        "    # Apply the modifier\n",
        "    base_dtm_array = base_dsm_array - base_dtm_array_modifier\n",
        "\n",
        "    # Export uncompressed for further topographic metrics\n",
        "    export_array_as_tif(base_dtm_array, base_dtm_path, compress=False)\n",
        "    print(f\"GEDI DTM has been postprocessed and uncompressed to: {base_dtm_path}\")\n",
        "\n",
        "  else: print(f\"A base DTM already exists, first remove from {areas_dir} for replacement.\")\n",
        "\n",
        "else: print(f\"A GEDI DTM does not exist in the scenarios directory. Proceeding with DSM.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obnUPMpUit18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58324d41-3829-4d1d-fae0-a2c0fe0f7e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post-processed GEDI DTM enabled.\n"
          ]
        }
      ],
      "source": [
        "# Define base DEM and properties\n",
        "if enable_gedi_dtm:\n",
        "  print(\"Post-processed GEDI DTM enabled.\")\n",
        "  if not exists(base_dtm_path):\n",
        "    print(\"A post-processed GEDI DTM does not exist. Defaulting to the original DSM.\")\n",
        "    base_dem = gdal.Open(base_dsm_path)\n",
        "    topo_temp_dir = join(features_dir, 'topo_dsm_temp')\n",
        "    topo_final_dir = join(features_dir, 'topo_dsm_final')\n",
        "    makedirs(topo_temp_dir, exist_ok=True)\n",
        "    makedirs(topo_final_dir, exist_ok=True)\n",
        "  else:\n",
        "    base_dem = gdal.Open(base_dtm_path)\n",
        "    topo_temp_dir = join(features_dir, 'topo_dtm_temp')\n",
        "    topo_final_dir = join(features_dir, 'topo_dtm_final')\n",
        "    makedirs(topo_temp_dir, exist_ok=True)\n",
        "    makedirs(topo_final_dir, exist_ok=True)\n",
        "else:\n",
        "  print(\"Post-processed GEDI DTM disabled. Using the original DSM.\")\n",
        "  base_dem = gdal.Open(base_dsm_path)\n",
        "  topo_temp_dir = join(features_dir, 'topo_dsm_temp')\n",
        "  topo_final_dir = join(features_dir, 'topo_dsm_final')\n",
        "  makedirs(topo_temp_dir, exist_ok=True)\n",
        "  makedirs(topo_final_dir, exist_ok=True)\n",
        "\n",
        "# Get base DEM attributes\n",
        "base_dem_array = base_dem.ReadAsArray()\n",
        "dem_dimensions = base_dem.GetGeoTransform()\n",
        "y_origin, pixel_height, raster_height = dem_dimensions[3], dem_dimensions[5], len(base_dem_array)\n",
        "dem_central_latitude = y_origin + (raster_height // 2) * pixel_height"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc0og_zgpQHG"
      },
      "source": [
        "# Topography metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RY2j4dh5etQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Calculates a total of 24 topographic metrics using either Whitebox or custom functions.\n",
        "# https://www.whiteboxgeo.com/manual/wbt_book/preface.html\n",
        "# These are later finalised with an automatic reduction in precision (rounding)\n",
        "# for faster moodelling, and the creation of 'unsmooth' and 'smooth' versions.\n",
        "# Smoothed versions allow the model to account for geolocation inaccuracies,\n",
        "# and adjacent topography types not captured in the various metrics.\n",
        "\n",
        "# Clear the temporary directory and recalculate topographic metrics if issues.\n",
        "clear_temp_directory = False\n",
        "if clear_temp_directory:\n",
        "  for raster in Path(topo_temp_dir).glob(\"**/*\"):\n",
        "    if raster.is_file(): raster.unlink()\n",
        "\n",
        "# Elevation\n",
        "elevation_path_temp = join(topo_temp_dir, \"elevation.tif\")\n",
        "if not exists(elevation_path_temp):\n",
        "  elevation = base_dem_array\n",
        "  export_array_as_tif(elevation, elevation_path_temp, compress=False)\n",
        "\n",
        "# Slope\n",
        "slope_path_temp = join(topo_temp_dir, \"slope.tif\")\n",
        "if not exists(slope_path_temp):\n",
        "  wbt.slope(elevation_path_temp, slope_path_temp, units = \"degrees\")\n",
        "\n",
        "# Aspect\n",
        "aspect_path_temp = join(topo_temp_dir, \"aspect.tif\")\n",
        "if not exists(aspect_path_temp):\n",
        "  wbt.aspect(elevation_path_temp, aspect_path_temp)\n",
        "\n",
        "# Profile Curvature\n",
        "profile_curvature_path_temp = join(topo_temp_dir, \"profile_curvature.tif\")\n",
        "if not exists(profile_curvature_path_temp):\n",
        "  wbt.profile_curvature(elevation_path_temp, profile_curvature_path_temp, log=False)\n",
        "\n",
        "# Tangential Curvature\n",
        "tangential_curvature_path_temp = join(topo_temp_dir, \"tangential_curvature.tif\")\n",
        "if not exists(tangential_curvature_path_temp):\n",
        "  wbt.tangential_curvature(elevation_path_temp, tangential_curvature_path_temp, log=False)\n",
        "\n",
        "# Topographic Ruggedness Index\n",
        "topographic_ruggedness_index_path_temp = join(topo_temp_dir, \"topographic_ruggedness_index.tif\")\n",
        "if not exists(topographic_ruggedness_index_path_temp):\n",
        "  wbt.ruggedness_index(elevation_path_temp, topographic_ruggedness_index_path_temp)\n",
        "\n",
        "# Deviation from Mean Elevation\n",
        "dev_kernel_sizes = [3, 7, 11]\n",
        "for kernel_size in dev_kernel_sizes:\n",
        "  deviation_mean_elevation_path_temp = join(topo_temp_dir, f\"deviation_mean_elevation_{str(kernel_size).rjust(2, '0')}.tif\")\n",
        "  if not exists(deviation_mean_elevation_path_temp):\n",
        "    wbt.dev_from_mean_elev(elevation_path_temp, deviation_mean_elevation_path_temp, filterx=kernel_size, filtery=kernel_size)\n",
        "\n",
        "# Circular Variance of Aspect\n",
        "cva_kernel_sizes = [3, 7, 11]\n",
        "for kernel_size in cva_kernel_sizes:\n",
        "  circular_variance_aspect_path_temp = join(topo_temp_dir, f\"circular_variance_aspect_{str(kernel_size).rjust(2, '0')}.tif\")\n",
        "  if not exists(circular_variance_aspect_path_temp):\n",
        "    wbt.circular_variance_of_aspect(elevation_path_temp, circular_variance_aspect_path_temp, filter=kernel_size)\n",
        "\n",
        "# Fill Single Cell Pits for Breach Depressions\n",
        "dem_fill_single_cell_pits_path_temp = join(topo_temp_dir, \"dem_fill_single_cell_pits.tif\")\n",
        "if not exists(dem_fill_single_cell_pits_path_temp):\n",
        "  wbt.fill_single_cell_pits(elevation_path_temp, dem_fill_single_cell_pits_path_temp)\n",
        "  # Raw output doesn't work, needs to be saved again.\n",
        "  dem_fill_single_cell_pits = gdal.Open(dem_fill_single_cell_pits_path_temp).ReadAsArray()\n",
        "  export_array_as_tif(dem_fill_single_cell_pits, dem_fill_single_cell_pits_path_temp, compress=False)\n",
        "\n",
        "# Breach Depressions for Specific Contributing Area\n",
        "max_search_dist = 2 # Maximum search distance for breach paths in cells (pixels)\n",
        "dem_breach_depressions_path_temp = join(topo_temp_dir, \"dem_breach_depressions.tif\")\n",
        "if not exists(dem_breach_depressions_path_temp):\n",
        "  wbt.breach_depressions_least_cost(dem_fill_single_cell_pits_path_temp, dem_breach_depressions_path_temp, dist=max_search_dist)\n",
        "\n",
        "# Specific Contributing Area (Qin) (for TWI and SPI)\n",
        "specific_contributing_area_qin_path_temp = join(topo_temp_dir, \"specific_contributing_area_qin.tif\")\n",
        "if not exists(specific_contributing_area_qin_path_temp):\n",
        "  wbt.qin_flow_accumulation(dem_breach_depressions_path_temp, specific_contributing_area_qin_path_temp, out_type=\"specific contributing area\")\n",
        "\n",
        "# Topographic Wetness Index (TWI)\n",
        "topographic_wetness_index_path_temp = join(topo_temp_dir, \"topographic_wetness_index.tif\")\n",
        "if not exists(topographic_wetness_index_path_temp):\n",
        "  wbt.wetness_index(specific_contributing_area_qin_path_temp, slope_path_temp, topographic_wetness_index_path_temp)\n",
        "\n",
        "# Stream Power Index (SPI)\n",
        "exponent = 1.0\n",
        "stream_power_index_path_temp = join(topo_temp_dir, \"stream_power_index.tif\")\n",
        "if not exists(stream_power_index_path_temp):\n",
        "  wbt.stream_power_index(specific_contributing_area_qin_path_temp, slope_path_temp, stream_power_index_path_temp, exponent=exponent)\n",
        "\n",
        "# The whitebox algorithm 'wbt.surface_area_ratio' is not currently working correctly.\n",
        "# This SAR function below is based on the Whitebox source code:\n",
        "# https://github.com/jblindsay/whitebox-tools/blob/master/whitebox-tools-app/src/tools/terrain_analysis/surface_area_ratio.rs\n",
        "# 'jit' makes it orders of magnitude faster.\n",
        "\n",
        "surface_area_ratio_path_temp = join(topo_temp_dir, \"surface_area_ratio.tif\")\n",
        "\n",
        "if not exists(surface_area_ratio_path_temp):\n",
        "  elevation_raster = gdal.Open(elevation_path_temp)\n",
        "  transform = elevation_raster.GetGeoTransform()\n",
        "  elevation_array = elevation_raster.ReadAsArray()\n",
        "  @jit(nopython=True)\n",
        "  def calculate_surface_area_ratio(dem, transform, nodata):\n",
        "      resx, resy = transform[1], -transform[5]\n",
        "      output = np.full(dem.shape, nodata, dtype=np.float32)\n",
        "      for i in range(1, dem.shape[0]-1):\n",
        "          mid_lat = transform[3] + i*transform[5]\n",
        "          resx_adjusted = abs(resx) * 111_111.0 * cos(radians(mid_lat))\n",
        "          resy_adjusted = abs(resy) * 111_111.0\n",
        "          res_diag = sqrt(resx_adjusted**2 + resy_adjusted**2)\n",
        "          cell_area = resx_adjusted * resy_adjusted\n",
        "          eigth_area = cell_area / 8.0\n",
        "          for j in range(1, dem.shape[1]-1):\n",
        "              if dem[i, j] == nodata:\n",
        "                  continue\n",
        "              window = dem[i-1:i+2, j-1:j+2]\n",
        "              dx = np.array([-1, 0, 1, -1, 0, 1, -1, 0, 1])\n",
        "              dy = np.array([-1, -1, -1, 0, 0, 0, 1, 1, 1])\n",
        "              zvals = np.array([window[dy[k]+1, dx[k]+1] for k in range(9)])\n",
        "              dist_planar = np.array([resx_adjusted]*6 + [resy_adjusted]*6 + [res_diag]*4)\n",
        "              dist_pairs = [(0, 1), (1, 2), (3, 4), (4, 5), (6, 7), (7, 8), (0, 3), (1, 4), (2, 5), (3, 6), (4, 7), (5, 8), (0, 4), (2, 4), (6, 4), (8, 4)]\n",
        "              distances = np.array([sqrt(dist_planar[k]**2 + (zvals[i] - zvals[j])**2) / 2 for k, (i, j) in enumerate(dist_pairs) if zvals[i] != nodata and zvals[j] != nodata])\n",
        "              triangle_sides = [(0, 7, 12), (1, 7, 13), (2, 6, 12), (3, 8, 13), (2, 9, 14), (3, 11, 15), (4, 10, 14), (5, 10, 15)]\n",
        "              area = 0.0\n",
        "              cell_area2 = cell_area\n",
        "              for a, b, c in triangle_sides:\n",
        "                  if a < len(distances) and b < len(distances) and c < len(distances):\n",
        "                      s = (distances[a] + distances[b] + distances[c]) / 2.0\n",
        "                      area += sqrt(s * (s - distances[a]) * (s - distances[b]) * (s - distances[c]))\n",
        "                  else:\n",
        "                      cell_area2 -= eigth_area\n",
        "              if cell_area2 > 0.0:\n",
        "                  output[i, j] = area / cell_area2\n",
        "          resx, resy = transform[1], -transform[5]\n",
        "      return output\n",
        "\n",
        "  # Run the jitted function on the entire array\n",
        "  surface_area_ratio_array = calculate_surface_area_ratio(elevation_array, transform, nodatavalue)\n",
        "  export_array_as_tif(surface_area_ratio_array, surface_area_ratio_path_temp, template=elevation_path_temp, compress=False)\n",
        "\n",
        "# Calculate Aspect Sine\n",
        "aspect_sine_path_temp = join(topo_temp_dir,\"aspect_sine.tif\")\n",
        "if not exists(aspect_sine_path_temp):\n",
        "  aspect = gdal.Open(aspect_path_temp).ReadAsArray()\n",
        "  aspect_sine = np.sin(np.radians(aspect))\n",
        "  export_array_as_tif(aspect_sine, aspect_sine_path_temp, compress=False)\n",
        "\n",
        "# Calculate Aspect Cosine\n",
        "aspect_cosine_path_temp = join(topo_temp_dir,\"aspect_cosine.tif\")\n",
        "if not exists(aspect_cosine_path_temp):\n",
        "  aspect = gdal.Open(aspect_path_temp).ReadAsArray()\n",
        "  aspect_cosine = np.cos(np.radians(aspect))\n",
        "  export_array_as_tif(aspect_cosine, aspect_cosine_path_temp, compress=False)\n",
        "\n",
        "# Calculate Eastness\n",
        "eastness_path_temp = join(topo_temp_dir,\"eastness.tif\")\n",
        "if not exists(eastness_path_temp):\n",
        "  slope = gdal.Open(slope_path_temp).ReadAsArray()\n",
        "  aspect_sine = gdal.Open(aspect_sine_path_temp).ReadAsArray()\n",
        "  eastness = aspect_sine * np.sin(np.radians(slope))\n",
        "  export_array_as_tif(eastness, eastness_path_temp, compress=False)\n",
        "\n",
        "# Calculate Northness\n",
        "northness_temp = join(topo_temp_dir,\"northness.tif\")\n",
        "if not exists(northness_temp):\n",
        "  slope = gdal.Open(slope_path_temp).ReadAsArray()\n",
        "  aspect_cosine = gdal.Open(aspect_cosine_path_temp).ReadAsArray()\n",
        "  northness = aspect_cosine * np.sin(np.radians(slope))\n",
        "  export_array_as_tif(northness, northness_temp, compress=False)\n",
        "\n",
        "elevation = gdal.Open(elevation_path_temp).ReadAsArray()\n",
        "\n",
        "# Calculate Roughness\n",
        "roughness_kernel_sizes = [3, 7, 11]\n",
        "for kernel_size in roughness_kernel_sizes:\n",
        "  roughness_path_temp = join(topo_temp_dir,f\"roughness_{str(kernel_size).rjust(2, '0')}.tif\")\n",
        "  if not exists(roughness_path_temp):\n",
        "    roughness = maximum_filter(elevation, size=kernel_size) - minimum_filter(elevation, size=kernel_size)\n",
        "    export_array_as_tif(roughness, roughness_path_temp, compress=False)\n",
        "\n",
        "# Calculate Topographic Position Index (TPI)\n",
        "tpi_kernel_sizes = [3, 7, 11]\n",
        "for kernel_size in tpi_kernel_sizes:\n",
        "  tpi_path_temp = join(topo_temp_dir, f\"topographic_position_index_{str(kernel_size).rjust(2, '0')}.tif\")\n",
        "  if not exists(tpi_path_temp):\n",
        "    topographic_position_index = elevation - uniform_filter(elevation, size=kernel_size)\n",
        "    export_array_as_tif(topographic_position_index, tpi_path_temp, compress=False)\n",
        "\n",
        "# Calculate Stream Power Index (SPI) log10\n",
        "spi_log10_path_temp = join(topo_temp_dir,\"stream_power_index_log10.tif\")\n",
        "if not exists(spi_log10_path_temp):\n",
        "  stream_power_index = gdal.Open(stream_power_index_path_temp).ReadAsArray()\n",
        "  stream_power_index[stream_power_index <= 0] = 1.0e-30 # Convert 0, negative or 'nodata' values\n",
        "  stream_power_index_log10 = np.log10(stream_power_index)\n",
        "  export_array_as_tif(stream_power_index_log10, spi_log10_path_temp, compress=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check there weren't errors in updated Whitebox algorithms\n",
        "visualise_topo_temp = False\n",
        "\n",
        "if visualise_topo_temp:\n",
        "  raster_files = [os.path.join(topo_temp_dir, file) for file in os.listdir(topo_temp_dir) if file.endswith('.tif')]\n",
        "  for raster_file in raster_files:\n",
        "      ds = gdal.Open(raster_file)\n",
        "      if ds is None:\n",
        "          print('Could not open ' + raster_file)\n",
        "          continue\n",
        "      band = ds.GetRasterBand(1)\n",
        "      raster_data = band.ReadAsArray()\n",
        "      ds = None\n",
        "      p2, p98 = np.percentile(raster_data, [2, 98]) # Compute the 2% and 98% percentiles\n",
        "      plt.figure()\n",
        "      plt.imshow(raster_data, cmap='viridis', vmin=p2, vmax=p98)\n",
        "      plt.colorbar()\n",
        "      plt.title(os.path.basename(raster_file))\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "fiTfDN2BL7P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsX4t1x7dGm4"
      },
      "source": [
        "# Round and smooth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a dictionary of optiomal precision based on number of desired unique values.\n",
        "# Limiting the number unique values avoids overfitting and reduces training time.\n",
        "# As a rule of thumb, set this to at least 256 and above the elevation range in metres.\n",
        "# E.g. project area is 200 - 2900 m, set to at least 2,700.\n",
        "\n",
        "override_max_unique_values = False\n",
        "max_unique_values = 5000 # Should be >=10\n",
        "\n",
        "if override_max_unique_values == False:\n",
        "  max_unique_values = int(np.ptp(base_dem_array)) # Precision based on elevation variance\n",
        "topo_precision_dict = {}\n",
        "\n",
        "# Define list of topography metrics to finalise\n",
        "topography_list = []\n",
        "for temp_tif in os.listdir(topo_temp_dir):\n",
        "    topography_list.append(str(temp_tif))\n",
        "topography_list = sorted(topography_list)\n",
        "print(\"topography_final_list = [\")\n",
        "for topography in topography_list:\n",
        "    print(f\"'{topography}',\")\n",
        "print(\"]\")"
      ],
      "metadata": {
        "id": "7j0660GrLTCs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d87794b-397a-4257-cafc-4d206351d533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topography_final_list = [\n",
            "'aspect.tif',\n",
            "'aspect_cosine.tif',\n",
            "'aspect_sine.tif',\n",
            "'circular_variance_aspect_03.tif',\n",
            "'circular_variance_aspect_07.tif',\n",
            "'circular_variance_aspect_11.tif',\n",
            "'dem_breach_depressions.tif',\n",
            "'dem_fill_single_cell_pits.tif',\n",
            "'deviation_mean_elevation_03.tif',\n",
            "'deviation_mean_elevation_07.tif',\n",
            "'deviation_mean_elevation_11.tif',\n",
            "'eastness.tif',\n",
            "'elevation.tif',\n",
            "'northness.tif',\n",
            "'profile_curvature.tif',\n",
            "'roughness_03.tif',\n",
            "'roughness_07.tif',\n",
            "'roughness_11.tif',\n",
            "'rounding_dictionary.csv',\n",
            "'slope.tif',\n",
            "'specific_contributing_area_qin.tif',\n",
            "'stream_power_index.tif',\n",
            "'stream_power_index_log10.tif',\n",
            "'surface_area_ratio.tif',\n",
            "'tangential_curvature.tif',\n",
            "'topographic_position_index_03.tif',\n",
            "'topographic_position_index_07.tif',\n",
            "'topographic_position_index_11.tif',\n",
            "'topographic_ruggedness_index.tif',\n",
            "'topographic_wetness_index.tif',\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topography_final_list = [\n",
        "# 'aspect.tif',\n",
        "'aspect_cosine.tif',\n",
        "'aspect_sine.tif',\n",
        "'circular_variance_aspect_03.tif',\n",
        "'circular_variance_aspect_07.tif',\n",
        "'circular_variance_aspect_11.tif',\n",
        "# 'dem_breach_depressions.tif',\n",
        "# 'dem_fill_single_cell_pits.tif',\n",
        "'deviation_mean_elevation_03.tif',\n",
        "'deviation_mean_elevation_07.tif',\n",
        "'deviation_mean_elevation_11.tif',\n",
        "'eastness.tif',\n",
        "'elevation.tif',\n",
        "'northness.tif',\n",
        "'profile_curvature.tif',\n",
        "'roughness_03.tif',\n",
        "'roughness_07.tif',\n",
        "'roughness_11.tif',\n",
        "'slope.tif',\n",
        "# 'specific_contributing_area_qin.tif',\n",
        "# 'stream_power_index.tif',\n",
        "'stream_power_index_log10.tif',\n",
        "'surface_area_ratio.tif',\n",
        "'tangential_curvature.tif',\n",
        "'topographic_position_index_03.tif',\n",
        "'topographic_position_index_07.tif',\n",
        "'topographic_position_index_11.tif',\n",
        "'topographic_ruggedness_index.tif',\n",
        "'topographic_wetness_index.tif',\n",
        "]"
      ],
      "metadata": {
        "id": "P7gOvqHDLRe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_precision_dict = True\n",
        "overwrite_existing_precision_dict = False\n",
        "\n",
        "precision_dict_csv_path = join(topo_temp_dir, 'rounding_dictionary.csv')\n",
        "if not exists(precision_dict_csv_path) or overwrite_existing_precision_dict:\n",
        "  for topography_final in topography_final_list:\n",
        "    print(f\"Reading {topography_final}...\")\n",
        "    # Read raster as array\n",
        "    topography_raster_path = join(topo_temp_dir, topography_final)\n",
        "    topography_raster_array = gdal.Open(topography_raster_path).ReadAsArray()\n",
        "    # Convert 'nodata' values to nan\n",
        "    topography_raster_array[topography_raster_array == nodatavalue] = np.nan\n",
        "    topography_raster_array_masked = np.ma.array(topography_raster_array, mask=np.isnan(topography_raster_array))\n",
        "    # Count unique values in raster\n",
        "    unique_values = len(np.unique(topography_raster_array_masked))\n",
        "    print(f\"There are {unique_values} unique values in {topography_final}\")\n",
        "    # Generate histogram from 100,000 random points\n",
        "    random_selection = np.random.choice(topography_raster_array_masked.ravel(), size = 100_000, replace = False)\n",
        "    _ = plt.hist(random_selection, bins='auto')  # arguments are passed to np.histogram\n",
        "    plt.title(f\"{topography_final}\")\n",
        "    plt.show()\n",
        "    # Remove 0 values for log10\n",
        "    topography_raster_array_masked[topography_raster_array_masked == 0] = np.nan\n",
        "    topography_raster_array_masked = np.ma.array(topography_raster_array, mask=np.isnan(topography_raster_array))\n",
        "    # Create log10 array for determining positions for rounding\n",
        "    array_log10 = np.log10(abs(topography_raster_array_masked))\n",
        "    place_value_decimal = int(abs(np.min(array_log10)))\n",
        "    place_value_integer = int(0 - np.max(array_log10))\n",
        "    # Iterate down precision levels to determine optimal number of unique values\n",
        "    min_starting_precision = len(str(max_unique_values))\n",
        "    for precision in reversed(range(place_value_integer, max(min_starting_precision, place_value_decimal +1))):\n",
        "      rounded_array = np.round(topography_raster_array, decimals=precision)\n",
        "      round_unique_values = len(np.unique(rounded_array))\n",
        "      optimal_precision = None\n",
        "      if round_unique_values <= max_unique_values:\n",
        "        optimal_precision = precision\n",
        "        print(f\"The optimal precison for {topography_final} is {optimal_precision}, with {round_unique_values} unique values.\")\n",
        "        topo_precision_dict.update({f'{topography_final}':f'{optimal_precision}'})\n",
        "        break\n",
        "    if optimal_precision == None: print(\"There's a problem with setting precision.\")\n",
        "    print(\"___________________\\n\")\n",
        "\n",
        "  print(\"Dictionary for optimal rounding values:\")\n",
        "  topo_precision_dict\n",
        "\n",
        "  precision_dict_csv_path = join(topo_temp_dir, 'rounding_dictionary.csv')\n",
        "  # Save rounding dictionary to CSV\n",
        "  with open(precision_dict_csv_path, 'w', newline='') as precision_dict_csv:\n",
        "      writer = csv.writer(precision_dict_csv)\n",
        "      writer.writerow(topo_precision_dict.keys())\n",
        "      writer.writerow(topo_precision_dict.values())\n",
        "\n",
        "# Open rounding dictionary and verify\n",
        "precision_dict_csv_path = join(topo_temp_dir, 'rounding_dictionary.csv')\n",
        "with open(precision_dict_csv_path, 'r') as file:\n",
        "    keys, values = list(csv.reader(file))\n",
        "    topo_precision_dict = dict(zip(keys, values))\n",
        "\n",
        "# Verify precision and correct if necessary\n",
        "print(\"topo_precision_dict = {\")\n",
        "for key, value in topo_precision_dict.items():\n",
        "    print(f'\"{key}\": {value},')\n",
        "print(\"}\")"
      ],
      "metadata": {
        "id": "7DF6qSXULQDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03bb6464-4c0e-4a47-e0cf-6355d9e37a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topo_precision_dict = {\n",
            "\"aspect_cosine.tif\": 3,\n",
            "\"aspect_sine.tif\": 3,\n",
            "\"circular_variance_aspect_03.tif\": 3,\n",
            "\"circular_variance_aspect_07.tif\": 3,\n",
            "\"circular_variance_aspect_11.tif\": 3,\n",
            "\"deviation_mean_elevation_03.tif\": 2,\n",
            "\"deviation_mean_elevation_07.tif\": 2,\n",
            "\"deviation_mean_elevation_11.tif\": 2,\n",
            "\"eastness.tif\": 3,\n",
            "\"elevation.tif\": 0,\n",
            "\"northness.tif\": 3,\n",
            "\"profile_curvature.tif\": 4,\n",
            "\"roughness_03.tif\": 1,\n",
            "\"roughness_07.tif\": 0,\n",
            "\"roughness_11.tif\": 0,\n",
            "\"slope.tif\": 1,\n",
            "\"stream_power_index_log10.tif\": 2,\n",
            "\"surface_area_ratio.tif\": 3,\n",
            "\"tangential_curvature.tif\": 4,\n",
            "\"topographic_position_index_03.tif\": 1,\n",
            "\"topographic_position_index_07.tif\": 1,\n",
            "\"topographic_position_index_11.tif\": 1,\n",
            "\"topographic_ruggedness_index.tif\": 1,\n",
            "\"topographic_wetness_index.tif\": 1,\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVCcwLGOeYa4"
      },
      "outputs": [],
      "source": [
        "topo_precision_dict = {\n",
        "\"aspect_cosine.tif\": 2,\n",
        "\"aspect_sine.tif\": 2,\n",
        "\"circular_variance_aspect_03.tif\": 3,\n",
        "\"circular_variance_aspect_07.tif\": 3,\n",
        "\"circular_variance_aspect_11.tif\": 3,\n",
        "\"deviation_mean_elevation_03.tif\": 2,\n",
        "\"deviation_mean_elevation_07.tif\": 2,\n",
        "\"deviation_mean_elevation_11.tif\": 2,\n",
        "\"eastness.tif\": 2,\n",
        "\"elevation.tif\": 0,\n",
        "\"northness.tif\": 2,\n",
        "\"profile_curvature.tif\": 4,\n",
        "\"roughness_03.tif\": 1,\n",
        "\"roughness_07.tif\": 0,\n",
        "\"roughness_11.tif\": 0,\n",
        "\"slope.tif\": 1,\n",
        "\"stream_power_index_log10.tif\": 1,\n",
        "\"surface_area_ratio.tif\": 3,\n",
        "\"tangential_curvature.tif\": 4,\n",
        "\"topographic_position_index_03.tif\": 1,\n",
        "\"topographic_position_index_07.tif\": 1,\n",
        "\"topographic_position_index_11.tif\": 1,\n",
        "\"topographic_ruggedness_index.tif\": 1,\n",
        "\"topographic_wetness_index.tif\": 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Smoothed versions allow the model to account for geolocation inaccuracies,\n",
        "# and adjacent topography types not captured in the various metrics.\n",
        "\n",
        "# Set smoothing kernel\n",
        "kernel = Gaussian2DKernel(x_stddev=1, y_stddev=1)\n",
        "\n",
        "# Topography progress\n",
        "topography_progress_index = 0\n",
        "topography_progress_label = widgets.Label(f\"Topography progress: {topography_progress_index}/{len(topo_precision_dict.items())}\")\n",
        "display(topography_progress_label)\n",
        "\n",
        "# Iterate over selected topography rasters\n",
        "for topography, precision in topo_precision_dict.items():\n",
        "  topo_raster_temp_path = join(topo_temp_dir, topography)\n",
        "  topo_raster_temp_array = gdal.Open(topo_raster_temp_path).ReadAsArray()\n",
        "  # Convert nodata values to 0\n",
        "  topo_raster_temp_array[topo_raster_temp_array == nodatavalue] = 0\n",
        "  # Set path and check if exists\n",
        "  if topo_temp_dir.endswith(\"dtm_temp\"): topo_raster_unsmoothed_filename = f\"topo_dtm_unsmooth_{topography}\"\n",
        "  else: topo_raster_unsmoothed_filename = f\"topo_dsm_unsmooth_{topography}\"\n",
        "  topo_raster_unsmoothed_path = join(topo_final_dir, topo_raster_unsmoothed_filename)\n",
        "  if not exists(topo_raster_unsmoothed_path):\n",
        "    # Round and export unsmoothed topography raster\n",
        "    topo_raster_unsmoothed_rounded = np.round(topo_raster_temp_array, decimals=int(precision))\n",
        "    export_array_as_tif(topo_raster_unsmoothed_rounded, topo_raster_unsmoothed_path)\n",
        "  # Smooth using 2D spatial convolution\n",
        "  if topo_temp_dir.endswith(\"dtm_temp\"): topo_raster_smoothed_filename = f\"topo_dtm_smooth_{topography}\"\n",
        "  else: topo_raster_smoothed_filename = f\"topo_dsm_smooth_{topography}\"\n",
        "  topo_raster_smoothed_path = join(topo_final_dir, topo_raster_smoothed_filename)\n",
        "  if not exists(topo_raster_smoothed_path):\n",
        "    topo_raster_smoothed = convolve(topo_raster_temp_array, kernel, boundary='extend')\n",
        "    # Round and export smoothed topography raster\n",
        "    topo_raster_smoothed_rounded = np.round(topo_raster_smoothed, decimals=int(precision))\n",
        "    export_array_as_tif(topo_raster_smoothed_rounded, topo_raster_smoothed_path)\n",
        "  # Update topography progress\n",
        "  topography_progress_index += 1\n",
        "  topography_progress_label.value = f\"Topography progress: {topography_progress_index}/{len(topo_precision_dict.items())}\""
      ],
      "metadata": {
        "id": "oo_EagvLk7P0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7d1fb690349e424ca05241321b16df99",
            "10a12ce473154f69a732f7d47023e4dc",
            "00997364a0e3474cafe3384bb81551f7"
          ]
        },
        "outputId": "4ca84dd0-1d25-4314-aa34-24657359ffb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Label(value='Topography progress: 0/24')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d1fb690349e424ca05241321b16df99"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mow4bIz0EBw"
      },
      "source": [
        "# Distance from coast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7PZCLj4HCqf"
      },
      "outputs": [],
      "source": [
        "# Creates a feature which accounts for coastal / continental effects.\n",
        "# Requires a polygon of a landmass or 'coastline extent', e.g. Peninsular Malaysia.\n",
        "# This must include the nearest coastline to all parts of the project area,\n",
        "# even coasts outside the project area (if they're still the nearest).\n",
        "\n",
        "# Define and create directory\n",
        "coast_dir = join(features_dir, 'coast')\n",
        "makedirs(coast_dir, exist_ok=True)\n",
        "\n",
        "# Download global coast data from https://osmdata.openstreetmap.de/data/coastlines.html\n",
        "coastlines_url = 'https://osmdata.openstreetmap.de/download/coastlines-split-4326.zip'\n",
        "coastlines_global_file_path = join(coast_dir, 'coastlines-split-4326.zip')\n",
        "if not exists(coastlines_global_file_path):\n",
        "  request = requests.get(coastlines_url, allow_redirects=True)\n",
        "  open(coastlines_global_file_path, 'wb').write(request.content)\n",
        "\n",
        "coastlines_global_dir = join(coast_dir, 'coastlines-split-4326')\n",
        "if not exists(coastlines_global_dir):\n",
        "  with zipfile.ZipFile(coastlines_global_file_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(coast_dir)\n",
        "\n",
        "# Upload and select a polygon with full coastline extent.\n",
        "# It must be the template.gpkg polygon OR a polygon that entirely contains template.gpkg.\n",
        "polygons_to_exclude = ['project_area.gpkg', 'project_area_buffered_bbox.gpkg', 'gedi_area.gpkg']\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    print(f\"coastline_extent_polygon = '{polygon}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8IUkGsiFfYs"
      },
      "outputs": [],
      "source": [
        "coastline_extent_polygon = 'peninsular_malaysia.gpkg'\n",
        "\n",
        "# Get extent of polygon\n",
        "coastline_extent_polygon_path = join(polygons_dir, coastline_extent_polygon)\n",
        "coastline_extent_bounds = gpd.read_file(coastline_extent_polygon_path).total_bounds\n",
        "coastline_min_x, coastline_max_x = coastline_extent_bounds[0], coastline_extent_bounds[2]\n",
        "coastline_min_y, coastline_max_y = coastline_extent_bounds[1], coastline_extent_bounds[3]\n",
        "\n",
        "# Set precision (in km) of distance\n",
        "precision = 1\n",
        "\n",
        "# Load template\n",
        "template = gdal.Open(template_dir)\n",
        "\n",
        "# Path of the new coast raster (where coastline will be rasterized)\n",
        "rasterized_coast_path = join(coast_dir, 'rasterized_coast.tif')\n",
        "\n",
        "# Create a new empty raster based on the coastline extent polygon\n",
        "if coastline_extent_polygon != 'template.gpkg':\n",
        "  # Get dimensions of template\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  template_size_x, template_size_y = template_band.XSize, template_band.YSize\n",
        "  template_res_x, template_res_y = template_dimensions[1], -template_dimensions[5]\n",
        "\n",
        "  # Calculate the minimum x and y of the template\n",
        "  template_min_x = template_dimensions[0]\n",
        "  template_max_y = template_dimensions[3]\n",
        "\n",
        "  # Calculate the centre x and y of the template\n",
        "  template_centre_x = template_min_x + ((template_size_x / 2) * template_res_x)\n",
        "  template_centre_y = template_max_y - ((template_size_y / 2) * template_res_y)\n",
        "\n",
        "  # Calculate the size (in pixels) difference between the polygon minimum and template minimum\n",
        "  coastline_min_diff_x = template_min_x - coastline_min_x\n",
        "  coastline_max_diff_y = coastline_max_y - template_max_y\n",
        "  coastline_min_diff_x_size = int(np.ceil(coastline_min_diff_x / template_res_x))\n",
        "  coastline_max_diff_y_size = int(np.ceil(coastline_max_diff_y / template_res_y))\n",
        "\n",
        "  # Calculate when the coastline raster should start while maintaining template resolution and position\n",
        "  coastline_start_x = template_min_x - (coastline_min_diff_x_size * template_res_x)\n",
        "  coastline_start_y = template_max_y + (coastline_max_diff_y_size * template_res_y)\n",
        "\n",
        "  # Calculate the size of the coastline raster\n",
        "  coastline_size_x = int(np.ceil((coastline_max_x - coastline_start_x)/template_res_x))\n",
        "  coastline_size_y = int(np.ceil((coastline_start_y - coastline_min_y)/template_res_y))\n",
        "\n",
        "  if not exists(rasterized_coast_path):\n",
        "    # Create coast raster dataset\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(rasterized_coast_path, coastline_size_x, coastline_size_y, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                    options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "    driver.SetProjection(template_projection)\n",
        "    driver.SetGeoTransform((coastline_start_x, template_res_x, 0, coastline_start_y, 0, -template_res_y))\n",
        "\n",
        "    #  Create and write array (all pixels with value 1)\n",
        "    raster_data = np.ones((coastline_size_y, coastline_size_x), dtype=np.float32)\n",
        "    driver.GetRasterBand(1).WriteArray(raster_data)\n",
        "\n",
        "    # Close coast raster dataset\n",
        "    driver.FlushCache()\n",
        "    driver = None\n",
        "    print(\"A blank raster at the extent of coastlines polygon has been generated, ready for rasterization.\")\n",
        "  else: print(\"A rasterization raster already exists.\")\n",
        "\n",
        "else: # If just using the template area, copy the template.\n",
        "  if not exists(rasterized_coast_path):\n",
        "    template_array = template.ReadAsArray()\n",
        "    export_array_as_tif(template_array, rasterized_coast_path)\n",
        "  else: print(\"A rasterization raster already exists.\")\n",
        "\n",
        "# Clip coastlines polygon to the extent (speeds up rasterization)\n",
        "coastlines_clipped_path = join(coast_dir, 'clipped_coastlines.gpkg')\n",
        "if not exists(coastlines_clipped_path):\n",
        "  coastlines_shp_path = join(coastlines_global_dir, 'lines.shp')\n",
        "  coastlines_shp_df = gpd.read_file(coastlines_shp_path)\n",
        "  coastlines_clipped_df = gpd.clip(coastlines_shp_df, coastline_extent_bounds)\n",
        "  coastlines_clipped_df.to_file(coastlines_clipped_path, driver='GPKG')\n",
        "  print(f\"Coastlines clipped to the polygon: {coastlines_clipped_path}\")\n",
        "else: print(f\"Coastlines have already been clipped to the polygon: {coastlines_clipped_path}\")\n",
        "\n",
        "# Rasterize coastlines (2), if not already\n",
        "rasterized_coast_array = gdal.Open(rasterized_coast_path).ReadAsArray()\n",
        "if not np.any(rasterized_coast_array == 2):\n",
        "  burn_polygon_to_raster(rasterized_coast_path, coastlines_clipped_path, fixed_value=2)\n",
        "  print(f\"Coastlines rasterized: {rasterized_coast_path}\")\n",
        "else: print(f\"Coastlines have already been rasterized: {rasterized_coast_path}\")\n",
        "\n",
        "# Calculate proximity in pixels coast\n",
        "coast_proximity_pixels_path = join(coast_dir, \"coast_proximity_pixels.tif\")\n",
        "if not exists(coast_proximity_pixels_path):\n",
        "  rasterized_coast_array = gdal.Open(rasterized_coast_path).ReadAsArray()\n",
        "  # If the rasterized coastline is different dimensions from the template\n",
        "  if coastline_extent_polygon != 'template.gpkg':\n",
        "    coast_proximity_pixels_unclipped_path = join(coast_dir, \"coast_proximity_pixels_unclipped.tif\")\n",
        "    if not exists(coast_proximity_pixels_unclipped_path):\n",
        "      coast_proximity_pixels_unclipped = distance_transform_edt(rasterized_coast_array != 2) # Target the coastal '2' pixels\n",
        "      export_array_as_tif(coast_proximity_pixels_unclipped, coast_proximity_pixels_unclipped_path, rasterized_coast_path)\n",
        "    coast_proximity_pixels_unclipped = gdal.Open(coast_proximity_pixels_unclipped_path).ReadAsArray()\n",
        "    clip_start_x, clip_start_y = coastline_min_diff_x_size, coastline_max_diff_y_size\n",
        "    clip_size_x, clip_size_y = template_size_x, template_size_y\n",
        "    coast_proximity_pixels = coast_proximity_pixels_unclipped[clip_start_y:clip_start_y + clip_size_y,\n",
        "                                                            clip_start_x:clip_start_x + clip_size_x]\n",
        "  # If the rasterized coastline is the same dimensions as the template\n",
        "  else: coast_proximity_pixels = distance_transform_edt(rasterized_coast_array != 2) # Target the coastal '2' pixels\n",
        "  # Export coast proximity in pixels\n",
        "  export_array_as_tif(coast_proximity_pixels, coast_proximity_pixels_path)\n",
        "  print(f\"A proximity (pixel number) raster has been generated at: {coast_proximity_pixels_path}\")\n",
        "else: print(f\"A proximity (pixel number) raster already exists at: {coast_proximity_pixels_path}\")\n",
        "\n",
        "# Convert proximity to km (distance from coast)\n",
        "coast_distance_path = join(coast_dir, \"coast_proximity_km.tif\")\n",
        "if not exists(coast_distance_path):\n",
        "  coast_proximity_array = gdal.Open(coast_proximity_pixels_path).ReadAsArray()\n",
        "  cell_size_x = gdal.Open(join(areas_dir, 'cell_size_x.tif')).ReadAsArray()\n",
        "  cell_size_y = gdal.Open(join(areas_dir, 'cell_size_y.tif')).ReadAsArray()\n",
        "  cell_size_mean_km = ((np.mean(cell_size_x) + np.mean(cell_size_y)) / 2) / 1000\n",
        "  coast_proximity_km = coast_proximity_array * cell_size_mean_km\n",
        "  coast_proximity_round = np.round(coast_proximity_km, precision)\n",
        "  export_array_as_tif(coast_proximity_round, coast_distance_path)\n",
        "  print(f\"A distance from coast (km) raster has been generated at: {coast_distance_path}\")\n",
        "else: print(f\"A distance from coast (km) raster already exists at: {coast_distance_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6r7JXbijM50"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU_AC6MjNfTN"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7d1fb690349e424ca05241321b16df99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10a12ce473154f69a732f7d47023e4dc",
            "placeholder": "​",
            "style": "IPY_MODEL_00997364a0e3474cafe3384bb81551f7",
            "value": "Topography progress: 24/24"
          }
        },
        "10a12ce473154f69a732f7d47023e4dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00997364a0e3474cafe3384bb81551f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}