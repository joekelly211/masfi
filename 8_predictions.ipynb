{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/8_predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j0th2jhtnAR"
      },
      "source": [
        "# Imports & Subdirectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9drq4euvG0MS"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "\n",
        "# Mount Google Drive and set base directory\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGWgfUqto0F"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Imports and upgrades\n",
        "!pip install geopandas\n",
        "!pip install pmdarima\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MTERlyxuPd4"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# Imports\n",
        "import geopandas as gpd\n",
        "from google.auth import default\n",
        "from google.colab import auth\n",
        "from google.colab import runtime\n",
        "import gspread\n",
        "import glob\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from os.path import exists, join\n",
        "from os import makedirs\n",
        "from osgeo import gdal\n",
        "import pandas as pd\n",
        "import pmdarima as pm\n",
        "import rasterio\n",
        "from rasterio import mask as msk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwar9sxbts-3"
      },
      "outputs": [],
      "source": [
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "predictions_dir = join(base_dir, \"8_predictions\")\n",
        "sample_polygons_dir = join(predictions_dir, \"sample_polygons\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(sample_polygons_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)"
      ],
      "metadata": {
        "id": "atvGRSPYgwk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjy-T1TqScbE"
      },
      "source": [
        "# Select model, area and sample polygons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select if to source predictions from scenarios_dir or uncertainty_dir\n",
        "source_dir = scenarios_dir\n",
        "\n",
        "# Select the model\n",
        "for subdir in os.listdir(source_dir):\n",
        "  print(f\"selected_model = '{subdir}'\")"
      ],
      "metadata": {
        "id": "nj5RbVSuEp-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_model = 'agbd_240718_164421'\n",
        "\n",
        "selected_model_dir = join(source_dir, selected_model)\n",
        "# Select the prediction area\n",
        "for subdir in os.listdir(selected_model_dir):\n",
        "  if source_dir == scenarios_dir and not subdir.endswith('.csv') and not subdir.endswith('.json'):\n",
        "    print(f\"prediction_area = '{subdir}'\")\n",
        "  if source_dir == uncertainty_dir and subdir != 'model_iterations':\n",
        "    print(f\"prediction_area = '{subdir[10:]}'\")"
      ],
      "metadata": {
        "id": "x7UWN6aIL-dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_area = 'terengganu'\n",
        "\n",
        "# Model-area stats directory\n",
        "model_area_predictions_dir = join(predictions_dir, f\"{selected_model}_{prediction_area}\")\n",
        "makedirs(model_area_predictions_dir, exist_ok=True)\n",
        "\n",
        "# Create list of available prediction rasters and scenarios. Rasters must already be masked (e.g. to forest).\n",
        "if source_dir == scenarios_dir:\n",
        "  prediction_raster_dir = join(selected_model_dir, prediction_area, 'scenario_predictions')\n",
        "if source_dir == uncertainty_dir:\n",
        "  prediction_raster_dir = join(selected_model_dir, f'scenarios_{prediction_area}', 'statistics_masked')\n",
        "\n",
        "prediction_raster_dirs = []\n",
        "scenarios = set()\n",
        "for prediction_raster in os.listdir(prediction_raster_dir):\n",
        "  prediction_raster_dirs.append(join(prediction_raster_dir, prediction_raster))\n",
        "  scenarios.add(prediction_raster.split(\"__\")[0])\n",
        "scenarios = sorted(list(scenarios))"
      ],
      "metadata": {
        "id": "yJpsBoAKMc-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select sample area polygons. This should be a single .gpkg with the field 'name' differentiating polygons.\n",
        "sample_polygons = []\n",
        "for geopackage in os.listdir(sample_polygons_dir):\n",
        "  sample_polygons.append(geopackage)\n",
        "\n",
        "if len(sample_polygons) == 0:\n",
        "  print(f\"No sample areas found. Upload .gpkg polygons to {sample_polygons_dir}\")\n",
        "else:\n",
        "  for sample_polygon in sample_polygons: print(f\"selected_sample_polygons = '{sample_polygon}'\")"
      ],
      "metadata": {
        "id": "txTXPYFLpH6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_sample_polygons = 'terengganu_fr_lite.gpkg'\n",
        "\n",
        "selected_sample_polygons_dir = join(sample_polygons_dir, selected_sample_polygons)\n",
        "selected_sample_polygons_gpkg = gpd.read_file(selected_sample_polygons_dir)\n",
        "\n",
        "sample_polygons_predictions_dir = join(model_area_predictions_dir, selected_sample_polygons[:-5])\n",
        "makedirs(sample_polygons_predictions_dir, exist_ok=True)\n",
        "arima_input_dir = join(sample_polygons_predictions_dir, 'arima_input')\n",
        "makedirs(arima_input_dir, exist_ok=True)\n",
        "bau_forecasting_dir = join(sample_polygons_predictions_dir, 'bau_forecasting')\n",
        "makedirs(bau_forecasting_dir, exist_ok=True)\n",
        "detailed_stats_dir = join(sample_polygons_predictions_dir, 'detailed_stats')\n",
        "makedirs(detailed_stats_dir, exist_ok=True)\n",
        "detailed_stats_scenario_dir = join(sample_polygons_predictions_dir, 'detailed_stats_scenario')\n",
        "makedirs(detailed_stats_scenario_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "cwWvaX5qDANc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics"
      ],
      "metadata": {
        "id": "hlQ2iLAiwQoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select scenarios to predict\n",
        "print('selected_scenarios = [')\n",
        "for scenario in scenarios:\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(']')"
      ],
      "metadata": {
        "id": "BgC9laZXPB9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_scenarios = [\n",
        "  \"2008\",\n",
        "  \"2009\",\n",
        "  \"2010\",\n",
        "  \"2011\",\n",
        "  \"2012\",\n",
        "  \"2013\",\n",
        "  \"2014\",\n",
        "  \"2015\",\n",
        "  \"2016\",\n",
        "  \"2017\",\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2022\",\n",
        "  \"2027_nodef\",\n",
        "  \"2027_nodist\",\n",
        "  \"2032_nodef\",\n",
        "  \"2032_nodist\",\n",
        "  \"2037_nodef\",\n",
        "  \"2037_nodist\",\n",
        "  \"9999_comrec\",\n",
        "  \"9999_comrest\",\n",
        "]\n",
        "\n",
        "# Filter to selected scenarios, and separate prediction and uncertainty rasters (if latter present)\n",
        "prediction_rasters = []\n",
        "uncertainty_rasters = []\n",
        "for prediction_raster in prediction_raster_dirs:\n",
        "  for scenario in selected_scenarios:\n",
        "    if source_dir == uncertainty_dir:\n",
        "      if f\"{scenario}__\" in prediction_raster and 'mean.tif' in prediction_raster:\n",
        "        prediction_rasters.append(prediction_raster)\n",
        "      if f\"{scenario}__\" in prediction_raster and 'uncertainty.tif' in prediction_raster:\n",
        "        uncertainty_rasters.append(prediction_raster)\n",
        "    else: # If the source directory is scenarios_dir (without uncertainty values)\n",
        "      if f\"{scenario}__\" in prediction_raster: prediction_rasters.append(prediction_raster)\n",
        "# Toggle whether to predict uncertainty stats\n",
        "if len(uncertainty_rasters) > 0: generate_uncertainty_stats = True\n",
        "else: generate_uncertainty_stats = False\n",
        "\n",
        "# Sort rasters chronologically (assuming year is first in the filename)\n",
        "prediction_rasters = sorted(prediction_rasters)\n",
        "uncertainty_rasters = sorted(uncertainty_rasters)"
      ],
      "metadata": {
        "id": "uSXtZMcvQs57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate precise pixel size for the centre of the template area\n",
        "# See https://gis.stackexchange.com/questions/142326/calculating-longitude-length-in-miles\n",
        "# See https://thoughtco.com/degree-of-latitude-and-longitude-distance-4070616\n",
        "\n",
        "# Proxy distance of degrees latitude in m (actual is non-linear)\n",
        "lat_dist_equator_km = 110.567\n",
        "lat_dist_poles_km = 111.699\n",
        "lat_dist_diff = lat_dist_poles_km - lat_dist_equator_km\n",
        "lat_dist_change_deg_km = lat_dist_diff / 90\n",
        "long_dist_equator_km = 111.321 # Equation calculates at different latitudes\n",
        "\n",
        "# Get a dimensions using a prediction as a template\n",
        "template_prediction = gdal.Open(prediction_rasters[0])\n",
        "pixel_height_deg = 0 - template_prediction.GetGeoTransform()[5]\n",
        "pixel_width_deg = template_prediction.GetGeoTransform()[1]\n",
        "\n",
        "# Approximate pixel size\n",
        "approx_resolution = (np.average([pixel_height_deg, pixel_width_deg]) * np.average([lat_dist_equator_km, lat_dist_poles_km]) * 1000)\n",
        "approx_pixel_size_ha = approx_resolution**2 / 10000\n",
        "\n",
        "print(f\"Without precise correction, the approximate resolution is {approx_resolution} m, while the approximate pixel area is {approx_pixel_size_ha} ha.\\n\")\n",
        "\n",
        "print(f\"The pixel size with be further corrected based on the position of each sample polygon.\")"
      ],
      "metadata": {
        "id": "IyO8CKbaHAgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate empty dataframes for statistics\n",
        "df_base = pd.DataFrame(index = selected_scenarios)\n",
        "df_base.rename_axis('scenario', inplace=True)\n",
        "df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg = df_base.copy(), df_base.copy(), df_base.copy(), df_base.copy()\n",
        "# If uncertainty rasters are present, generate empty dataframes\n",
        "if generate_uncertainty_stats:\n",
        "  df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agb_total_tg_ci95 = df_base.copy(), df_base.copy(), df_base.copy()\n",
        "\n",
        "df_polygon_area_km2 = pd.DataFrame(columns = [\"Name\", \"Area (km^2)\"])\n",
        "\n",
        "# Using the first prediction as a template, create a raster with no 'nodata' values.\n",
        "# This will be used for counting all pixels inside a polygon mask (which uses the 'nodata' values)\n",
        "template_prediction = gdal.Open(prediction_rasters[0])\n",
        "template_array = template_prediction.ReadAsArray()\n",
        "template_nodatavalue = template_prediction.GetRasterBand(1).GetNoDataValue()\n",
        "absent_nodatavalue = template_nodatavalue + 0.1 # Modify nodata value to one that's absent.\n",
        "assert absent_nodatavalue not in template_array, \"New nodata value is present in the template. Change to a value that is absent.\"\n",
        "template_no_nodata_dir = join(sample_polygons_predictions_dir, 'template_no_nodata.tif')\n",
        "export_array_as_tif(template_array, template_no_nodata_dir, template = prediction_rasters[0], nodatavalue=absent_nodatavalue, compress=False)\n",
        "\n",
        "# Loop through each polygon stored in GPKG to generate statistics\n",
        "for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "\n",
        "  # Define the polygon\n",
        "  sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"] # Set name to start at 3rd character with [2:] (skipping number used for ordering)\n",
        "  polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "  # Latitude of the centroid\n",
        "  polygon_centroid_lat = sample_polygon_geometry.centroid.y\n",
        "\n",
        "  # Calculate latitude distance per degree at the polygon's latitude\n",
        "  latitude_m_per_degree = 1000 * (lat_dist_equator_km + (lat_dist_change_deg_km * polygon_centroid_lat))\n",
        "\n",
        "  # Rest of your calculations that depend on the latitude_m_per_degree follows here\n",
        "  # For example, if you're calculating pixel size in meters for each polygon based on its latitude:\n",
        "  precise_pixel_height_m = latitude_m_per_degree * pixel_height_deg\n",
        "  precise_pixel_width_m = (math.cos((math.pi / 180) * polygon_centroid_lat) * latitude_m_per_degree * pixel_width_deg)\n",
        "  precise_pixel_size_ha = precise_pixel_height_m * precise_pixel_width_m / 10000\n",
        "\n",
        "  # Mask the 'no nodata' raster to the polygon with an absent value to count all pixels within the polygon\n",
        "  with rasterio.open(template_no_nodata_dir) as template_no_nodata:\n",
        "    no_nodata_template_array_masked, transform_1 = msk.mask(template_no_nodata, polygons, crop=True, filled=False)\n",
        "\n",
        "  # Calculate sample_polygon_geometry area (ellipsoidal as opposed to planimetric)\n",
        "  sample_polygons_crs = selected_sample_polygons_gpkg.crs\n",
        "  temp_gdf = gpd.GeoDataFrame({'name': [sample_polygon_name], 'geometry': sample_polygon_geometry}, crs=sample_polygons_crs)\n",
        "  temp_gdf_utm = temp_gdf.estimate_utm_crs()\n",
        "  polygon_area_ha = np.divide(temp_gdf.to_crs(temp_gdf_utm).area[0], 10000, dtype='float64')\n",
        "  df_polygon_area_km2 = pd.concat([df_polygon_area_km2, pd.DataFrame([{'Name': sample_polygon_name, 'Area (km^2)': polygon_area_ha / 100}])], ignore_index=True, sort=False)\n",
        "\n",
        "  # Create an empty list for each prediction raster statistic\n",
        "  values_forest_cover_ha, values_agbd_mean_mg_ha, values_agbd_stdev_mg_ha, values_agb_total_tg = [], [], [], []\n",
        "\n",
        "  # If uncertainty rasters are present, create an empty list for each uncertainty raster statistic\n",
        "  if generate_uncertainty_stats:\n",
        "    values_agbd_mean_mg_ha_ci95, values_agbd_mean_mg_ha_uncertainty, values_agb_total_tg_ci95  = [], [], []\n",
        "\n",
        "  # Loop through prediction rasters\n",
        "  for prediction_raster in prediction_rasters:\n",
        "\n",
        "    # Mask predictor to sample_polygon_geometry\n",
        "    with rasterio.open(prediction_raster) as prediction:\n",
        "      nodatavalue = int(prediction.nodatavals[0])\n",
        "      prediction_array_masked, transform_2 = msk.mask(prediction, polygons, crop=True, filled=False)\n",
        "\n",
        "    # Count all (incl. nodata) pixels within polygon, and estimate their total area\n",
        "    pixel_count_polygon = np.ma.count(no_nodata_template_array_masked)\n",
        "    pixels_area_polygon_ha = np.multiply(pixel_count_polygon, precise_pixel_size_ha, dtype='float64')\n",
        "\n",
        "    # Correct pixel size to UTM ellipsoidal measure of the polygons position (which will decrease further North)\n",
        "    polygon_to_pixel_area_ratio = np.divide(pixels_area_polygon_ha, polygon_area_ha, dtype='float64')\n",
        "    pixel_size_ha_corrected = np.multiply(precise_pixel_size_ha, polygon_to_pixel_area_ratio, dtype='float64')\n",
        "\n",
        "    # Count pixels within polygon, excluding those previously masked i.e. nodata\n",
        "    pixel_count_polygon_masked = np.ma.count(prediction_array_masked)\n",
        "    pixels_area_polygon_masked_ha = np.multiply(pixel_count_polygon_masked, pixel_size_ha_corrected, dtype='float64')\n",
        "\n",
        "    # Calculate forest area\n",
        "    forest_cover_ha = pixels_area_polygon_masked_ha # Already masked to forest in current workflow\n",
        "\n",
        "    # Calculate total, mean and stdev of aboveground biomass\n",
        "    agbd_mean_mg_ha = np.ma.mean(prediction_array_masked, dtype='float64') # Float64 minimises error for large number of values\n",
        "    agbd_mean_stdev_ha = np.ma.std(prediction_array_masked, dtype='float64')\n",
        "    agb_total_mg = np.multiply(agbd_mean_mg_ha, forest_cover_ha, dtype='float64')\n",
        "    agb_total_tg = np.divide(agb_total_mg, 1000000, dtype='float64') # Convert Mg (megagram = ton) to Tg (teragram = megaton)\n",
        "\n",
        "    # Append results to statistics list\n",
        "    values_forest_cover_ha.append(forest_cover_ha)\n",
        "    values_agbd_mean_mg_ha.append(agbd_mean_mg_ha)\n",
        "    values_agbd_stdev_mg_ha.append(agbd_mean_stdev_ha)\n",
        "    values_agb_total_tg.append(agb_total_tg)\n",
        "\n",
        "    if generate_uncertainty_stats:\n",
        "      uncertainty_raster_present = False\n",
        "      for uncertainty_raster in uncertainty_rasters:\n",
        "        if prediction_raster.split(\"__mean.tif\")[0] in uncertainty_raster:\n",
        "          uncertainty_raster_present = True\n",
        "          matching_uncertainty_raster = uncertainty_raster\n",
        "\n",
        "      if not uncertainty_raster_present: print(f\"There is no uncertainty raster for {prediction_raster.split('/')[-1]}\")\n",
        "\n",
        "      if uncertainty_raster_present:\n",
        "        # Open and mask uncertainty raster to polygon\n",
        "          with rasterio.open(matching_uncertainty_raster) as uncertainty:\n",
        "            nodatavalue = int(uncertainty.nodatavals[0])\n",
        "            uncertainty_array_masked, transform_2 = msk.mask(uncertainty, polygons, crop=True, filled=False)\n",
        "\n",
        "          # See https://stats.stackexchange.com/questions/223924/how-to-add-up-partial-confidence-intervals-to-create-a-total-confidence-interval#comment426260_223924\n",
        "\n",
        "          # Compress masked array data to 1D\n",
        "          prediction_1d = np.ma.compressed(prediction_array_masked)\n",
        "          uncertainty_1d = np.ma.compressed(uncertainty_array_masked)\n",
        "          # Convert uncertainty percentages to ratios\n",
        "          uncertainty_ratios = np.divide(uncertainty_1d, 100, dtype='float64')\n",
        "          # Multiply the prediction values (mean AGBD Mg/ha) by uncertainty ratios for CI95 values\n",
        "          prediction_ci95s = np.multiply(prediction_1d, uncertainty_ratios, dtype='float64')\n",
        "\n",
        "          # Method 1 - Simple calculation of mean CI95 (higher estimate). Assumption: pixel values are completely correlated (all measure the same thing).\n",
        "          agbd_mean_mg_ha_ci95_1 = np.mean(prediction_ci95s, dtype = 'float64')\n",
        "          # Method 2 - Square prediction CI95s. Sum and then square root for total CI.\n",
        "          # Then divide by observations to calculate mean CI95. Assumption: pixel values are completely independent.\n",
        "          sum_squares = np.sum(np.square(prediction_ci95s, dtype='float64'), dtype='float64')\n",
        "          total_ci95 = np.sqrt(sum_squares, dtype='float64')\n",
        "          agbd_mean_mg_ha_ci95_2 = np.divide(total_ci95, np.ma.count(prediction_ci95s), dtype='float64')\n",
        "          # Method 3 - Used in Liang et al 2023 to calculate change uncertainty. Identical results to method 2.\n",
        "          predictions_x_uncertainties = np.multiply(prediction_1d, uncertainty_1d, dtype='float64')\n",
        "          sum_squares_pxu = np.sum(np.square(predictions_x_uncertainties, dtype='float64'), dtype='float64')\n",
        "          sqrt_divided_sum = np.sqrt(sum_squares_pxu, dtype='float64') / np.sum(prediction_1d, dtype='float64')\n",
        "          agbd_mean_mg_ha_ci95_3 = np.multiply(np.divide(sqrt_divided_sum, 100, dtype='float64'), agbd_mean_mg_ha, dtype='float64')\n",
        "\n",
        "          agbd_mean_mg_ha_ci95 = agbd_mean_mg_ha_ci95_1\n",
        "\n",
        "          # Calculate total AGB CI95\n",
        "          agb_total_mg_ci95 = np.multiply(agbd_mean_mg_ha_ci95, forest_cover_ha, dtype='float64')\n",
        "          agb_total_tg_ci95 = np.divide(agb_total_mg_ci95, 1000000, dtype='float64') # Convert total CI to Tg\n",
        "          # Calculate percentage uncertainty\n",
        "          agbd_mean_mg_ha_uncertainty = np.multiply(np.divide(agbd_mean_mg_ha_ci95, agbd_mean_mg_ha, dtype='float64'), 100, dtype='float64')\n",
        "          # Append results to statistics list\n",
        "          values_agbd_mean_mg_ha_ci95.append(agbd_mean_mg_ha_ci95)\n",
        "          values_agbd_mean_mg_ha_uncertainty.append(agbd_mean_mg_ha_uncertainty)\n",
        "          values_agb_total_tg_ci95.append(agb_total_tg_ci95)\n",
        "\n",
        "  # Concatenate new columns to the main DataFrames for each statistic\n",
        "  df_forest_cover_ha = pd.concat([df_forest_cover_ha, pd.DataFrame({sample_polygon_name: values_forest_cover_ha}, index=df_forest_cover_ha.index)], axis=1)\n",
        "  df_agbd_mean_mg_ha = pd.concat([df_agbd_mean_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha}, index=df_agbd_mean_mg_ha.index)], axis=1)\n",
        "  df_agbd_stdev_mg_ha = pd.concat([df_agbd_stdev_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_stdev_mg_ha}, index=df_agbd_stdev_mg_ha.index)], axis=1)\n",
        "  df_agb_total_tg = pd.concat([df_agb_total_tg, pd.DataFrame({sample_polygon_name: values_agb_total_tg}, index=df_agb_total_tg.index)], axis=1)\n",
        "\n",
        "  if generate_uncertainty_stats:\n",
        "      df_agbd_mean_mg_ha_ci95 = pd.concat([df_agbd_mean_mg_ha_ci95, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_ci95}, index=df_agbd_mean_mg_ha_ci95.index)], axis=1)\n",
        "      df_agbd_mean_mg_ha_uncertainty = pd.concat([df_agbd_mean_mg_ha_uncertainty, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_uncertainty}, index=df_agbd_mean_mg_ha_uncertainty.index)], axis=1)\n",
        "      df_agb_total_tg_ci95 = pd.concat([df_agb_total_tg_ci95, pd.DataFrame({sample_polygon_name: values_agb_total_tg_ci95}, index=df_agb_total_tg_ci95.index)], axis=1)\n",
        "\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "  df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_mean_mg_ha_ci95,\n",
        "                 df_agbd_mean_mg_ha_uncertainty, df_agbd_stdev_mg_ha, df_agb_total_tg, df_agb_total_tg_ci95]\n",
        "else: df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Set index of the polygon area km2 dataframe to 'Name' of the polygon\n",
        "df_polygon_area_km2 = df_polygon_area_km2.set_index('Name')\n",
        "\n",
        "# Export statistics for ARIMA input\n",
        "df_arima_list = [df_forest_cover_ha, df_agb_total_tg]\n",
        "for df_arima in df_arima_list:\n",
        "  df_noalts = df_arima[df_arima.index.str.contains(\"_\") == False]\n",
        "  if df_arima.equals(df_forest_cover_ha): df_filename = \"forest_cover_ha\"\n",
        "  # if df_stats.equals(df_agbd_mean_mg_ha): df_filename = \"agbd_mean_mg_ha\"\n",
        "  # if df_stats.equals(df_agbd_stdev_mg_ha): df_filename = \"agbd_stdev_mg_ha\"\n",
        "  if df_arima.equals(df_agb_total_tg): df_filename = \"agb_total_tg\"\n",
        "  df_noalts.to_csv(join(arima_input_dir, f'{df_filename}.csv'))\n",
        "\n",
        "# Generate summary stats\n",
        "df_forest_cover_ha_t = df_forest_cover_ha.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest cover (ha)\")\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB (Tg)\")\n",
        "if generate_uncertainty_stats:\n",
        "  df_agb_total_tg_ci95_t = df_agb_total_tg_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB CI95 (Tg)\")\n",
        "  summary_stats = pd.concat([df_polygon_area_km2, df_forest_cover_ha_t, df_agb_total_tg_t, df_agb_total_tg_ci95_t], axis=1).rename_axis(\"Name\", axis=1)\n",
        "else: summary_stats = pd.concat([df_polygon_area_km2, df_forest_cover_ha_t, df_agb_total_tg_t], axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_stats.to_csv(join(sample_polygons_predictions_dir, 'summary_stats.csv'))\n",
        "\n",
        "# Generate detailed stats by polygon (requires uncertainty stats)\n",
        "for polygon_area in df_stats_list[0]:\n",
        "  polygon_area_km2 = df_polygon_area_km2.loc[polygon_area][\"Area (km^2)\"]\n",
        "  df_detailed_stats = df_base\n",
        "  df_detailed_stats[\"Area (km^2)\"] = polygon_area_km2\n",
        "  for df_stats in df_stats_list:\n",
        "    if df_stats.equals(df_forest_cover_ha): stat_col = \"Forest cover (ha)\"\n",
        "    if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"Forest AGBD mean (Mg / ha)\"\n",
        "    if df_stats.equals(df_agbd_stdev_mg_ha): stat_col = \"Forest AGBD stdev (Mg / ha)\"\n",
        "    if df_stats.equals(df_agb_total_tg): stat_col = \"Forest AGB total (Tg)\"\n",
        "    if generate_uncertainty_stats:\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_ci95): stat_col = \"Forest AGBD CI95 (Mg / ha)\"\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_uncertainty): stat_col = \"Forest AGBD uncertainty (%)\"\n",
        "      if df_stats.equals(df_agb_total_tg_ci95): stat_col = \"Forest AGB total CI95 (Tg)\"\n",
        "    for stats_polygon_area in df_stats:\n",
        "      if stats_polygon_area == polygon_area:\n",
        "        df_stats_renamed = df_stats.rename(columns={stats_polygon_area:stat_col})\n",
        "        df_detailed_stats = pd.concat([df_detailed_stats, df_stats_renamed[stat_col]], axis=1)\n",
        "    df_detailed_stats.to_csv(join(detailed_stats_dir, f'{polygon_area}.csv'))\n",
        "\n",
        "# Generate detailed stats by scenario (requires uncertainty stats)\n",
        "scenarios = {}\n",
        "# Loop through all CSV files in the 'detailed_stats_dir'\n",
        "for stats_csv in os.listdir(detailed_stats_dir):\n",
        "    polygon_name = f\"{stats_csv[:-4]}\"\n",
        "    stats_csv_path = join(detailed_stats_dir, stats_csv)\n",
        "    stats_csv_df = pd.read_csv(stats_csv_path)\n",
        "    # Loop through each unique scenario in the file\n",
        "    for scenario in stats_csv_df['scenario'].unique():\n",
        "        # Filter the dataframe for the current scenario\n",
        "        scenario_df = stats_csv_df[stats_csv_df['scenario'] == scenario].copy()\n",
        "        # Drop the 'scenario' column and add the 'Name' column\n",
        "        scenario_df.drop('scenario', axis=1, inplace=True)\n",
        "        scenario_df.insert(0, 'Name', polygon_name)\n",
        "        # If this scenario's dataframe already exists, append to it; otherwise, create it\n",
        "        if scenario in scenarios: scenarios[scenario] = pd.concat([scenarios[scenario], scenario_df], ignore_index=True)\n",
        "        else: scenarios[scenario] = scenario_df\n",
        "for scenario, scenario_df in scenarios.items():\n",
        "    output_file_path = join(detailed_stats_scenario_dir,f'{scenario}.csv')\n",
        "    scenario_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Delete template raster\n",
        "os.remove(template_no_nodata_dir)"
      ],
      "metadata": {
        "id": "jizdpzwLiqH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARIMA"
      ],
      "metadata": {
        "id": "qZremduhzY2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select time series (Must have the 'scenario' (i.e. time period) in the first column.\n",
        "# Must have the names of the sample polygons in the proceeding columns.\n",
        "time_series_csv = []\n",
        "for csv in os.listdir(arima_input_dir): time_series_csv.append(csv)\n",
        "if len(time_series_csv) == 0: print(f\"Run the statistics section first to generate input for ARIMA\")\n",
        "\n",
        "print('time_series = [')\n",
        "for series in time_series_csv:\n",
        "  print(f'  \"{series}\",')\n",
        "print(']')"
      ],
      "metadata": {
        "id": "shV0_aDLztcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_series = [\n",
        "  \"forest_cover_ha.csv\",\n",
        "  \"agb_total_tg.csv\",\n",
        "]\n",
        "\n",
        "time_column = 'scenario'\n",
        "time_label = 'Year' # Name of time unit for plots\n",
        "forecast_years = 15\n",
        "\n",
        "for csv in time_series:\n",
        "  forecast_dir = join(bau_forecasting_dir, f\"{csv[:-4]}\")\n",
        "  makedirs(forecast_dir, exist_ok=True)\n",
        "  time_series_df = pd.read_csv(join(arima_input_dir,csv), header=0)\n",
        "  starting_time_unit = time_series_df[time_column].astype('int')[0]\n",
        "  study_areas = list(time_series_df.columns[1:])\n",
        "\n",
        "  if csv == 'forest_cover_ha.csv': forecast_label = 'Forest cover (ha)'\n",
        "  if csv == 'agb_total_tg.csv': forecast_label = 'Forest total AGB (Tg)'\n",
        "\n",
        "  for study_area in study_areas:\n",
        "    filename = study_area.replace(' ','_').lower()\n",
        "    verbose = False # Change verbose to True to see summary and diagnostics\n",
        "    # https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html\n",
        "    model = pm.auto_arima(time_series_df[study_area],\n",
        "                          seasonal=False,\n",
        "                          trace=verbose,\n",
        "                          # start_p=1,\n",
        "                          # start_q=1,\n",
        "                          # test='adf',       # use adftest to find optimal 'd'\n",
        "                          # max_p=3, max_q=3, # maximum p and q\n",
        "                          # m=1,              # frequency of series\n",
        "                          # d=None,           # let model determine 'd'\n",
        "                          # stationary=False,\n",
        "                          # error_action='ignore',\n",
        "                          # suppress_warnings=True,\n",
        "                          # information_criterion = 'aic', # Either ‘aic’, ‘bic’, ‘hqic’, ‘oob’\n",
        "                          stepwise=True,\n",
        "                          # n_jobs = -1,\n",
        "                          maxiter = 10000,\n",
        "                          error_action=\"ignore\"\n",
        "                          )\n",
        "    if verbose == True:\n",
        "      print(model.summary())\n",
        "      model.plot_diagnostics(figsize=(7,5))\n",
        "      plt.show()\n",
        "\n",
        "    # Forecast\n",
        "    n_periods = forecast_years\n",
        "    fc, confint = model.predict(n_periods=n_periods, return_conf_int=True)\n",
        "    index_of_fc = np.arange(\n",
        "        len(time_series_df[study_area])+starting_time_unit,\n",
        "        len(time_series_df[study_area])+n_periods+starting_time_unit\n",
        "        )\n",
        "\n",
        "    # Create series\n",
        "    fc_series = pd.Series(fc.values, index=index_of_fc, name=f'{study_area} forecast')\n",
        "    lower_series = pd.Series(confint[:, 0], index=index_of_fc, name=f'{study_area} lower CI')\n",
        "    upper_series = pd.Series(confint[:, 1], index=index_of_fc, name=f'{study_area} upper CI')\n",
        "\n",
        "    # Compile dataframe\n",
        "    df_forecast = pd.concat([fc_series,upper_series,lower_series],axis=1)\n",
        "    df_forecast.index.name = time_label\n",
        "\n",
        "    # Plot\n",
        "    plt.plot(time_series_df[time_column],time_series_df[study_area])\n",
        "    plt.plot(fc_series, color='darkgreen')\n",
        "    plt.fill_between(lower_series.index, lower_series, upper_series, color='k', alpha=.15)\n",
        "    plt.title(f'{study_area}'), plt.xlabel(f'{time_label}'), plt.ylabel(f'{forecast_label}')\n",
        "    plt.savefig(join(forecast_dir,f'plot_{filename}.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Save results\n",
        "    df_forecast.to_csv(join(forecast_dir,f'results_{filename}.csv'))\n",
        "\n",
        "  forecast_all_csv = glob.glob(os.path.join(forecast_dir, \"*.csv\"))\n",
        "  forecast_all_df = pd.concat((pd.read_csv(f) for f in forecast_all_csv), ignore_index=True)\n",
        "  forecast_final_df = forecast_all_df.groupby(time_label).agg('first')\n",
        "  forecast_final_df.to_csv(join(bau_forecasting_dir,f'{csv}'))"
      ],
      "metadata": {
        "id": "QyL8pg8fDfux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gsheet"
      ],
      "metadata": {
        "id": "ai3298kWu1R0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://docs.gspread.org/en/latest/user-guide.html#opening-a-spreadsheet\n",
        "# The following scenarios must be predicted: historic scenarios 2008 - 2022,\n",
        "# alternate scenarios 'no_def' and 'no_dist' for 2027, 2032, 2037 and '9999_comrec'.\n",
        "# 2022 - 2037 must be forecast using the ARIMA section.\n",
        "\n",
        "# Authenticate\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# Define AGB stats template, needs to be placed in the 'sample_polygons_predictions_dir'\n",
        "# Should also be the last sheet of this name opened.\n",
        "gspread_agb_stats_template = gc.open('agb_stats_template')\n",
        "\n",
        "# Define the Gsheet name\n",
        "gspread_agb_stats_name = f\"stats_{selected_model}_{selected_sample_polygons[:-5]}\"\n",
        "gpspread_agb_stats_backup_name = f\"{gspread_agb_stats_name}_backup\"\n",
        "\n",
        "gpsread_agb_stats_backup = None\n",
        "try: # Delete previous backup Gsheet (if it exists)\n",
        "  gpsread_agb_stats_backup = gc.open(gpspread_agb_stats_backup_name)\n",
        "  gc.del_spreadsheet(gpsread_agb_stats_backup.id) # Danger: deletes permanently\n",
        "  print(\"The previous stats Gsheet backup for this model, area and sample polygon combination has been deleted and replaced.\")\n",
        "except: print(\"There is no existing stats Gsheet backup.\")\n",
        "\n",
        "gspread_agb_stats = None\n",
        "try: # Copies an existing Gsheet (if it exists) to backup, then deletes it.\n",
        "  gspread_agb_stats = gc.open(gspread_agb_stats_name)\n",
        "  gc.copy(gspread_agb_stats.id, title=gpspread_agb_stats_backup_name, copy_permissions=True)\n",
        "  gc.del_spreadsheet(gspread_agb_stats.id) # Danger: deletes permanently\n",
        "  print(\"The previous stats Gsheet for this model, area and sample polygon combination has been deleted and replaced.\")\n",
        "except: print(\"Creating the stats Gsheet for this model, area and sample polygon combination for the first time.\")\n",
        "\n",
        "# Create a new gsheet from the template\n",
        "new_sheet = gc.copy(gspread_agb_stats_template.id, title=gspread_agb_stats_name, copy_permissions=False)\n",
        "gspread_agb_stats = gc.open_by_key(new_sheet.id)\n",
        "\n",
        "# Define worksheets\n",
        "summary_stats_sheet = gspread_agb_stats.worksheet(\"Summary stats\")\n",
        "arima_forest_cover_sheet = gspread_agb_stats.worksheet(\"BAU with CI forest cover (ha)\")\n",
        "arima_total_agb_sheet = gspread_agb_stats.worksheet(\"BAU with CI forest AGB (Tg)\")\n",
        "template_forest_cover = gspread_agb_stats.worksheet(\"Template forest cover (ha)\")\n",
        "template_forest_agb = gspread_agb_stats.worksheet(\"Template forest AGB (Tg)\")\n",
        "template_potential_agb = gspread_agb_stats.worksheet(\"Template potential AGB (%)\")\n",
        "\n",
        "# Add summary stats\n",
        "summary_stats_dir = join(sample_polygons_predictions_dir, 'summary_stats.csv')\n",
        "df_summary_stats = pd.read_csv(summary_stats_dir)\n",
        "df_summary_stats = df_summary_stats.rename(columns = {\"Unnamed: 0\":\"Name\"})\n",
        "summary_stats_sheet.update([df_summary_stats.columns.values.tolist()] + df_summary_stats.values.tolist())\n",
        "\n",
        "# Add ARIMA forecasts for forest cover (ha)\n",
        "arima_forest_cover_ha_dir = join(bau_forecasting_dir, 'forest_cover_ha.csv')\n",
        "df_arima_forest_cover_ha = pd.read_csv(arima_forest_cover_ha_dir)\n",
        "df_arima_forest_cover_ha = df_arima_forest_cover_ha.rename(columns = {\"Unnamed: 0\":\"Year\"})\n",
        "arima_forest_cover_sheet.update([df_arima_forest_cover_ha.columns.values.tolist()] + df_arima_forest_cover_ha.values.tolist())\n",
        "\n",
        "# Add ARIMA forecasts for forest AGB total (Tg)\n",
        "arima_agb_total_tg_dir = join(bau_forecasting_dir, 'agb_total_tg.csv')\n",
        "df_arima_agb_total_tg = pd.read_csv(arima_agb_total_tg_dir)\n",
        "df_arima_agb_total_tg = df_arima_agb_total_tg.rename(columns = {\"Unnamed: 0\":\"Year\"})\n",
        "arima_total_agb_sheet.update([df_arima_agb_total_tg.columns.values.tolist()] + df_arima_agb_total_tg.values.tolist())\n",
        "\n",
        "# Create 'Forest cover (ha)' sheets for all polygons\n",
        "gspread_agb_stats_max_index = len(gspread_agb_stats.worksheets())-1\n",
        "sample_polygon_no = 1\n",
        "for sample_polygon in df_summary_stats[\"Name\"]:\n",
        "  polygon_forest_cover_sheet = f\"Forest cover (ha): {sample_polygon}\"\n",
        "  gspread_agb_stats.duplicate_sheet(source_sheet_id = template_forest_cover.id,\n",
        "                                    insert_sheet_index = gspread_agb_stats_max_index+sample_polygon_no,\n",
        "                                    new_sheet_name=polygon_forest_cover_sheet)\n",
        "  gspread_agb_stats.worksheet(polygon_forest_cover_sheet).update_cell(1, 1, sample_polygon_no)\n",
        "  sample_polygon_no +=1\n",
        "gspread_agb_stats.del_worksheet(template_forest_cover)\n",
        "\n",
        "# Create 'Forest AGB (Tg)' sheets for all polygons\n",
        "gspread_agb_stats_max_index = len(gspread_agb_stats.worksheets())-1\n",
        "sample_polygon_no = 1\n",
        "for sample_polygon in df_summary_stats[\"Name\"]:\n",
        "  polygon_forest_agb_sheet = f\"Forest AGB (Tg): {sample_polygon}\"\n",
        "  gspread_agb_stats.duplicate_sheet(source_sheet_id = template_forest_agb.id,\n",
        "                                    insert_sheet_index = gspread_agb_stats_max_index+sample_polygon_no,\n",
        "                                    new_sheet_name=polygon_forest_agb_sheet)\n",
        "  gspread_agb_stats.worksheet(polygon_forest_agb_sheet).update_cell(1, 1, sample_polygon_no)\n",
        "  sample_polygon_no +=1\n",
        "gspread_agb_stats.del_worksheet(template_forest_agb)\n",
        "\n",
        "# Create 'Potential AGB (%)' sheets for all polygons\n",
        "gspread_agb_stats_max_index = len(gspread_agb_stats.worksheets())-1\n",
        "sample_polygon_no = 1\n",
        "for sample_polygon in df_summary_stats[\"Name\"]:\n",
        "  polygon_potential_agb_sheet = f\"Potential AGB (%): {sample_polygon}\"\n",
        "  gspread_agb_stats.duplicate_sheet(source_sheet_id = template_potential_agb.id,\n",
        "                                    insert_sheet_index = gspread_agb_stats_max_index+sample_polygon_no,\n",
        "                                    new_sheet_name=polygon_potential_agb_sheet)\n",
        "  gspread_agb_stats.worksheet(polygon_potential_agb_sheet).update_cell(1, 1, sample_polygon_no)\n",
        "  sample_polygon_no +=1\n",
        "gspread_agb_stats.del_worksheet(template_potential_agb)"
      ],
      "metadata": {
        "id": "vKlH90d-u2rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disconnected runtime"
      ],
      "metadata": {
        "id": "wXAGIvpk_KWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "y3qMsENC_MP2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}