{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/2_targets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUfm-wkD9x1i"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gUXicFlGp1W"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/gdrive/Shareddrives/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHh9EmwkDtUb"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlKpdcKG90gB"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import geopandas as gpd\n",
        "import h5py\n",
        "import ipywidgets as widgets\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import re\n",
        "import requests\n",
        "from time import sleep\n",
        "import urllib\n",
        "from datetime import datetime\n",
        "from getpass import getpass\n",
        "from google.colab import runtime\n",
        "from http.cookiejar import CookieJar\n",
        "from os import listdir, makedirs, path\n",
        "from osgeo import gdal\n",
        "gdal.UseExceptions()\n",
        "from os.path import exists, join\n",
        "import re\n",
        "from requests.auth import HTTPBasicAuth\n",
        "from shapely.geometry import MultiPolygon, Polygon, box\n",
        "from shapely.ops import orient\n",
        "from shapely.strtree import STRtree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0YGFT7P99xX"
      },
      "outputs": [],
      "source": [
        "# 1_areas directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(base_dir, \"1_areas/polygons\")\n",
        "masks_dir = join(base_dir, \"1_areas/masks\")\n",
        "\n",
        "# 2_targets directories\n",
        "targets_dir = join(base_dir, \"2_targets\")\n",
        "gedi_links_dir = join(targets_dir, \"gedi_h5_links\")\n",
        "username = password = None\n",
        "gedi_h5_downloads_dir = join(targets_dir, \"gedi_h5_downloads\")\n",
        "gedi_raster_downloads_dir = join(targets_dir, \"gedi_raster_downloads\")\n",
        "gedi_raster_final_dir = join(targets_dir, \"gedi_raster_final\")\n",
        "h5_pkl_cache_dir = join(targets_dir, \"gedi_h5_pkl_cache\")\n",
        "pkl_final_dir = join(targets_dir, \"pkl_final\")\n",
        "targets_gpkg_dir = join(targets_dir, \"gpkg_final\")\n",
        "targets_user_csv_dir = join(targets_dir, \"csv\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(targets_dir, exist_ok=True)\n",
        "makedirs(gedi_links_dir, exist_ok=True)\n",
        "makedirs(gedi_h5_downloads_dir, exist_ok=True)\n",
        "makedirs(gedi_raster_final_dir, exist_ok=True)\n",
        "makedirs(gedi_raster_downloads_dir, exist_ok=True)\n",
        "makedirs(h5_pkl_cache_dir, exist_ok=True)\n",
        "makedirs(pkl_final_dir, exist_ok=True)\n",
        "makedirs(targets_gpkg_dir, exist_ok=True)\n",
        "makedirs(targets_user_csv_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwQ99PbzH3o"
      },
      "source": [
        "# Download GEDI data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_EYCjhSPKoK"
      },
      "source": [
        "## Get links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU5rP8U6PMra"
      },
      "outputs": [],
      "source": [
        "# Define Prediction area in which to download GEDI data\n",
        "prediction_area_path = join(polygons_dir, \"prediction_area.gpkg\")\n",
        "if not exists(prediction_area_path):\n",
        "  print(f\"Run '1_areas.ipynb' to add 'prediction_area.gpkg' to {polygons_dir}.\")\n",
        "else:\n",
        "  prediction_area = gpd.read_file(prediction_area_path)\n",
        "  prediction_area.geometry = prediction_area.geometry.apply(orient, args=(1,))\n",
        "\n",
        "  # https://github.com/ornldaac/gedi_tutorials/blob/main/1_gedi_l4a_search_download.ipynb\n",
        "  # CMR API base url\n",
        "  cmrurl='https://cmr.earthdata.nasa.gov/search/'\n",
        "\n",
        "  # GEDI product DOI list\n",
        "  gedi_doi_list = {\n",
        "  'GEDI01_B': '10.5067/GEDI/GEDI01_B.002', # GEDI L1B\n",
        "  'GEDI02_A': '10.5067/GEDI/GEDI02_A.002', # GEDI L2A\n",
        "  'GEDI02_B': '10.5067/GEDI/GEDI02_B.002', # GEDI L2B\n",
        "  'GEDI04_A': '10.3334/ORNLDAAC/2056', # GEDI L4A\n",
        "  'GEDI04_B': '10.3334/ORNLDAAC/2056', # GEDI L4B\n",
        "  'GEDI04_C': '10.3334/ORNLDAAC/2338', # GEDI L4C\n",
        "  'GEDI04_D': '10.3334/ORNLDAAC/2455', # GEDI L4D\n",
        "  }\n",
        "\n",
        "  # Some concept IDs do not retrieve dynamically\n",
        "  gedi_concept_ids = {\n",
        "    'GEDI01_B': 'C2142749196-LPCLOUD',\n",
        "    'GEDI02_A': 'C2142771958-LPCLOUD',\n",
        "    'GEDI02_B': 'C2142776747-LPCLOUD',\n",
        "    'GEDI04_A': 'C2237824918-ORNL_CLOUD',\n",
        "    'GEDI04_B': 'C2792577683-ORNL_CLOUD',\n",
        "    'GEDI04_C': 'C3049900163-ORNL_CLOUD',\n",
        "    'GEDI04_D': 'C3904758954-ORNL_CLOUD',\n",
        "  }\n",
        "\n",
        "  # Provider mapping for different GEDI products\n",
        "  gedi_providers = {\n",
        "    'GEDI01_B': 'LPCLOUD',\n",
        "    'GEDI02_A': 'LPCLOUD',\n",
        "    'GEDI02_B': 'LPCLOUD',\n",
        "    'GEDI04_A': 'ORNL_CLOUD',\n",
        "    'GEDI04_B': 'ORNL_CLOUD',\n",
        "    'GEDI04_C': 'ORNL_CLOUD',\n",
        "    'GEDI04_D': 'ORNL_CLOUD',\n",
        "  }\n",
        "\n",
        "  # File extension per product\n",
        "  gedi_file_extensions = {\n",
        "      'GEDI01_B': '.h5',\n",
        "      'GEDI02_A': '.h5',\n",
        "      'GEDI02_B': '.h5',\n",
        "      'GEDI04_A': '.h5',\n",
        "      'GEDI04_B': '.tif',\n",
        "      'GEDI04_C': '.h5',\n",
        "      'GEDI04_D': '.tif',\n",
        "  }\n",
        "\n",
        "  for gedi_product, doi in gedi_doi_list.items():\n",
        "    # Use hardcoded concept_id if available, otherwise retrieve from DOI\n",
        "    if gedi_concept_ids[gedi_product]:\n",
        "      concept_id = gedi_concept_ids[gedi_product]\n",
        "    else:\n",
        "      response = requests.get(cmrurl + 'collections.json?doi=' + doi)\n",
        "      response.raise_for_status()\n",
        "      concept_id = response.json()['feed']['entry'][0]['id']\n",
        "\n",
        "    geojson = {\"shapefile\": (\"prediction_area.geojson\", prediction_area.geometry.to_json(), \"application/geo+json\")}\n",
        "    page_num, page_size = 1, 2000 # CMR page size limit\n",
        "    granule_arr = []\n",
        "    while True:\n",
        "        # Set up provider-specific parameters\n",
        "        if gedi_providers[gedi_product]:\n",
        "            cmr_param = {\"collection_concept_id\": concept_id, \"page_size\": page_size, \"page_num\": page_num,\n",
        "                \"simplify-shapefile\": 'true', # Needed to bypass 5000 coordinates limit of CMR\n",
        "                \"provider\": gedi_providers[gedi_product]  # Set correct provider\n",
        "            }\n",
        "        else:\n",
        "            # Use default parameters when provider not specified\n",
        "            cmr_param = {\"collection_concept_id\": concept_id, \"page_size\": page_size, \"page_num\": page_num,\n",
        "                \"simplify-shapefile\": 'true' # Needed to bypass 5000 coordinates limit of CMR\n",
        "            }\n",
        "\n",
        "        granulesearch = cmrurl + 'granules.json'\n",
        "        response = requests.post(granulesearch, data=cmr_param, files=geojson)\n",
        "        granules = response.json()['feed']['entry']\n",
        "        if granules:\n",
        "            for g in granules:\n",
        "                granule_url, granule_size, granule_poly = None, None, None\n",
        "                file_ext = gedi_file_extensions[gedi_product]\n",
        "                # Check file extensions match\n",
        "                for links in g['links']:\n",
        "                    if links['href'].endswith(file_ext) and not links['href'].startswith('https://opendap') and not links['href'].startswith('s3'):\n",
        "                        granule_url = links['href']\n",
        "                if granule_url != None: # Some GEDI2A granules do not have links in the metadata\n",
        "                  # Read file size\n",
        "                  granule_size = float(g['granule_size'])\n",
        "                  # Reading bounding geometries\n",
        "                  if 'polygons' in g:\n",
        "                      polygons= g['polygons']\n",
        "                      multipolygons = []\n",
        "                      for poly in polygons:\n",
        "                          i=iter(poly[0].split (\" \"))\n",
        "                          ltln = list(map(\" \".join,zip(i,i)))\n",
        "                          multipolygons.append(Polygon([[float(p.split(\" \")[1]), float(p.split(\" \")[0])] for p in ltln]))\n",
        "                      granule_poly = MultiPolygon(multipolygons)\n",
        "                  # Get URL to HDF5 files\n",
        "                  granule_arr.append([granule_url, granule_size, granule_poly])\n",
        "            page_num += 1\n",
        "        else: break\n",
        "\n",
        "    # Create pandas dataframe\n",
        "    doi_df = pd.DataFrame(granule_arr, columns=[\"granule_url\", \"granule_size\", \"granule_poly\"])\n",
        "    # Drop granules with empty geometry\n",
        "    gedi_product_granules = doi_df[doi_df['granule_poly'] != \"\"]\n",
        "    print(f\"Total granules found for {gedi_product}: {len(gedi_product_granules)}\")\n",
        "    print(f\"Total file size for {gedi_product}: {round(gedi_product_granules['granule_size'].sum() / 1024, 3)}GB\\n\")\n",
        "\n",
        "    # Export links list\n",
        "    if len(gedi_product_granules) > 0:\n",
        "        gedi_product_links_path = join(gedi_links_dir, f\"{gedi_product}_links.txt\")\n",
        "        gedi_product_granules.to_csv(gedi_product_links_path, columns=['granule_url'], index=False, header=False)\n",
        "    else:print(f\"No valid granules for {gedi_product} link export.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdXO-FMQ-E0w"
      },
      "source": [
        "## H5 files: 1B, 2A, 2B, 4A, 4C"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Earthdata account\n",
        "if not username or not password:\n",
        "  username = getpass(\"Username: \")\n",
        "  password = getpass(\"Password: \")\n",
        "  manager = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
        "  manager.add_password(None, \"https://urs.earthdata.nasa.gov\", username, password)\n",
        "  manager.add_password(None, \"https://data.ornldaac.earthdata.nasa.gov\", username, password)\n",
        "  cookie_jar = CookieJar()\n",
        "  opener = urllib.request.build_opener(\n",
        "      urllib.request.HTTPBasicAuthHandler(manager),\n",
        "      urllib.request.HTTPCookieProcessor(cookie_jar))\n",
        "  urllib.request.install_opener(opener)"
      ],
      "metadata": {
        "id": "VjJ2Bd2XPcB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKVd_Ks_-KOp"
      },
      "outputs": [],
      "source": [
        "# Get list of .txt files in the 'links' directory\n",
        "link_files = []\n",
        "for file in os.listdir(gedi_links_dir):\n",
        "  link_files.append(file)\n",
        "\n",
        "print('# Select downloads')\n",
        "print(\"downloads = [\")\n",
        "for link_file in sorted(link_files):\n",
        "  if 'GEDI04_D' not in link_file:\n",
        "    print(f'  \"{link_file}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcBdA4URR0C9"
      },
      "outputs": [],
      "source": [
        "# Select downloads\n",
        "downloads = [\n",
        "  # \"GEDI01_B_links.txt\",\n",
        "  # \"GEDI02_A_links.txt\",\n",
        "  # \"GEDI02_B_links.txt\",\n",
        "  \"GEDI04_A_links.txt\",\n",
        "  # \"GEDI04_C_links.txt\",\n",
        "]\n",
        "\n",
        "# Process URLs\n",
        "for text_file in downloads:\n",
        "  text_file_path = join(gedi_links_dir, text_file)\n",
        "  with open(text_file_path, 'r') as file:\n",
        "    url_list = file.readlines()\n",
        "  url_list = [url.strip() for url in url_list] # Remove any white space\n",
        "  product = text_file[:8]\n",
        "  # Display progress\n",
        "  index = 0\n",
        "  progress_label = widgets.Label(value=f\"{product} progress: {index}/{len(url_list)} files downloaded.\")\n",
        "  display(progress_label)\n",
        "  for url in url_list:\n",
        "    product_dir = join(gedi_h5_downloads_dir, product)\n",
        "    makedirs(product_dir, exist_ok=True)\n",
        "    product_filename = url.split('/')[-1]\n",
        "    product_file_path = join(product_dir, product_filename)\n",
        "    if not exists(product_file_path):\n",
        "      try:\n",
        "        request = urllib.request.Request(url)\n",
        "        response = urllib.request.urlopen(request)\n",
        "        body = response.read()\n",
        "        open(product_file_path, 'wb').write(body)\n",
        "      except Exception as e:\n",
        "        print(f\"Failed URL in {product}: {url}\")\n",
        "        print(f\"Error: {e}\")\n",
        "    # Update progress\n",
        "    index += 1\n",
        "    progress_label.value = f\"{product} progress: {index}/{len(url_list)} files downloaded.\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rasters: 4B, 4D"
      ],
      "metadata": {
        "id": "0Ryx6CB_MykJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Earthdata account\n",
        "if not username or not password:\n",
        "  username = getpass(\"Username: \")\n",
        "  password = getpass(\"Password: \")\n",
        "  manager = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
        "  manager.add_password(None, \"https://urs.earthdata.nasa.gov\", username, password)\n",
        "  manager.add_password(None, \"https://data.ornldaac.earthdata.nasa.gov\", username, password)\n",
        "  cookie_jar = CookieJar()\n",
        "  opener = urllib.request.build_opener(\n",
        "      urllib.request.HTTPBasicAuthHandler(manager),\n",
        "      urllib.request.HTTPCookieProcessor(cookie_jar))\n",
        "  urllib.request.install_opener(opener)"
      ],
      "metadata": {
        "id": "NHBEfKTiOTJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get available raster link files\n",
        "print(\"# Select raster downloads\")\n",
        "print(\"raster_downloads = [\")\n",
        "for file in sorted(os.listdir(gedi_links_dir)):\n",
        "    if file in [\"GEDI04_B_links.txt\", \"GEDI04_D_links.txt\"]:\n",
        "        print(f'  \"{file}\",')\n",
        "print(\"]\")"
      ],
      "metadata": {
        "id": "AIlbqj2B4yUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select raster downloads\n",
        "raster_downloads = [\n",
        "    \"GEDI04_B_links.txt\",\n",
        "    \"GEDI04_D_links.txt\",\n",
        "]\n",
        "\n",
        "# Print available raster types per product\n",
        "for text_file in [\"GEDI04_B_links.txt\", \"GEDI04_D_links.txt\"]:\n",
        "    text_file_path = join(gedi_links_dir, text_file)\n",
        "    if not exists(text_file_path):\n",
        "        continue\n",
        "    product = text_file[:8]\n",
        "    with open(text_file_path, 'r') as file:\n",
        "        url_list = [url.strip() for url in file.readlines()]\n",
        "\n",
        "    raster_types = set()\n",
        "    for url in url_list:\n",
        "        filename = url.split('/')[-1].replace('.tif', '')\n",
        "        raster_type = '_'.join(filename.split('_')[6:])\n",
        "        raster_types.add(raster_type)\n",
        "\n",
        "    print(f\"{product.lower()}_rasters = [\")\n",
        "    for r in sorted(raster_types):\n",
        "        print(f'  \"{r}\",')\n",
        "    print(\"]\\n\")"
      ],
      "metadata": {
        "id": "pOlYlUpI43vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gedi04_b_rasters = [\n",
        "  # \"R01000M_MI\",\n",
        "  \"R01000M_MU\",\n",
        "  # \"R01000M_NC\",\n",
        "  # \"R01000M_NS\",\n",
        "  \"R01000M_PE\",\n",
        "  # \"R01000M_PS\",\n",
        "  # \"R01000M_QF\",\n",
        "  \"R01000M_SE\",\n",
        "  # \"R01000M_V1\",\n",
        "  # \"R01000M_V2\",\n",
        "]\n",
        "\n",
        "gedi04_d_rasters = [\n",
        "  # \"QA\",\n",
        "  \"agbd\",\n",
        "  # \"cover_z_000\",\n",
        "  # \"rh_010\",\n",
        "  # \"rh_020\",\n",
        "  # \"rh_030\",\n",
        "  # \"rh_040\",\n",
        "  # \"rh_050\",\n",
        "  # \"rh_060\",\n",
        "  # \"rh_070\",\n",
        "  # \"rh_080\",\n",
        "  # \"rh_090\",\n",
        "  # \"rh_095\",\n",
        "  # \"rh_098\",\n",
        "  # \"sensitivity_a2\",\n",
        "  # \"shot_number\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "C1z4or8q41mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download rasters\n",
        "for text_file in raster_downloads:\n",
        "    product = text_file[:8]\n",
        "    text_file_path = join(gedi_links_dir, text_file)\n",
        "    with open(text_file_path, 'r') as file:\n",
        "        url_list = [url.strip() for url in file.readlines()]\n",
        "\n",
        "    selected_types = gedi04_b_rasters if product == \"GEDI04_B\" else gedi04_d_rasters\n",
        "    filtered_urls = [url for url in url_list if any(f\"_{r}.tif\" in url for r in selected_types)]\n",
        "\n",
        "    product_dir = join(gedi_raster_downloads_dir, product)\n",
        "    makedirs(product_dir, exist_ok=True)\n",
        "\n",
        "    index = 0\n",
        "    progress_label = widgets.Label(value=f\"{product} progress: {index}/{len(filtered_urls)} files downloaded.\")\n",
        "    display(progress_label)\n",
        "\n",
        "    for url in filtered_urls:\n",
        "        product_filename = url.split('/')[-1]\n",
        "        product_file_path = join(product_dir, product_filename)\n",
        "        if not exists(product_file_path):\n",
        "            try:\n",
        "                request = urllib.request.Request(url)\n",
        "                response = urllib.request.urlopen(request)\n",
        "                body = response.read()\n",
        "                open(product_file_path, 'wb').write(body)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed: {url}\\nError: {e}\")\n",
        "        index += 1\n",
        "        progress_label.value = f\"{product} progress: {index}/{len(filtered_urls)} files downloaded.\""
      ],
      "metadata": {
        "id": "PB93CUxXMyDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finalise GEDI rasters"
      ],
      "metadata": {
        "id": "kDTmlKf_C9iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clip GEDI rasters, output original and reprojected versions\n",
        "nodatavalue = -11111\n",
        "\n",
        "# Define template area polygon\n",
        "template_area = gpd.read_file(join(polygons_dir, \"template.gpkg\"))\n",
        "bbox_4326 = list(template_area.total_bounds)\n",
        "\n",
        "# Determine centroid UTM zone\n",
        "centroid = template_area.union_all().centroid\n",
        "utm_zone = int((centroid.x + 180) / 6) + 1\n",
        "centroid_utm_epsg = 32600 + utm_zone if centroid.y >= 0 else 32700 + utm_zone\n",
        "centroid_utm_name = f\"UTM{utm_zone}_{'North' if centroid.y >= 0 else 'South'}\"\n",
        "print(f\"Centroid UTM: {centroid_utm_name}, EPSG:{centroid_utm_epsg}\")\n",
        "\n",
        "# Pre-compute transformed bounds\n",
        "bbox_6933 = list(template_area.to_crs(epsg=6933).total_bounds)\n",
        "bbox_centroid_utm = list(template_area.to_crs(epsg=centroid_utm_epsg).total_bounds)\n",
        "\n",
        "# Process GEDI04_B\n",
        "gedi04b_dir = join(gedi_raster_downloads_dir, \"GEDI04_B\")\n",
        "if exists(gedi04b_dir) and os.listdir(gedi04b_dir):\n",
        "    print(\"\\nGEDI04_B detected\")\n",
        "    gedi04b_final_dir = join(gedi_raster_final_dir, \"GEDI04_B\")\n",
        "    raster_groups = {}\n",
        "    for file in os.listdir(gedi04b_dir):\n",
        "        if file.endswith(\".tif\"):\n",
        "            raster_type = '_'.join(file.replace('.tif', '').split('_')[6:]).replace('R01000M_', '')\n",
        "            raster_groups.setdefault(raster_type, []).append(join(gedi04b_dir, file))\n",
        "\n",
        "    for raster_type, tiles in raster_groups.items():\n",
        "        # Original EPSG:6933\n",
        "        output_original = join(gedi04b_final_dir, f\"GEDI04_B_original_epsg6933_EASE-Grid_{raster_type}.tif\")\n",
        "        if not exists(output_original):\n",
        "            makedirs(gedi04b_final_dir, exist_ok=True)\n",
        "            print(f\"  {raster_type}: clipping in native CRS (EPSG:6933 EASE-Grid)\")\n",
        "            gdal.Warp(output_original, tiles, options=gdal.WarpOptions(\n",
        "                outputBounds=bbox_6933, dstNodata=nodatavalue,\n",
        "                format='GTiff', creationOptions=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1']))\n",
        "        else: print(f\"  {raster_type}: original exists\")\n",
        "\n",
        "        # Reprojected to WGS84\n",
        "        output_reproj = join(gedi04b_final_dir, f\"GEDI04_B_reprojected_epsg4326_WGS84_{raster_type}.tif\")\n",
        "        if not exists(output_reproj):\n",
        "            makedirs(gedi04b_final_dir, exist_ok=True)\n",
        "            print(f\"  {raster_type}: reprojecting to EPSG:4326 WGS84\")\n",
        "            gdal.Warp(output_reproj, tiles, options=gdal.WarpOptions(\n",
        "                dstSRS='EPSG:4326', outputBounds=bbox_4326, dstNodata=nodatavalue,\n",
        "                format='GTiff', creationOptions=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1']))\n",
        "        else: print(f\"  {raster_type}: WGS84 exists\")\n",
        "else: print(\"\\nGEDI04_B: not detected\")\n",
        "\n",
        "# Process GEDI04_D\n",
        "gedi04d_dir = join(gedi_raster_downloads_dir, \"GEDI04_D\")\n",
        "if exists(gedi04d_dir) and os.listdir(gedi04d_dir):\n",
        "    print(\"\\nGEDI04_D detected\")\n",
        "    gedi04d_final_dir = join(gedi_raster_final_dir, \"GEDI04_D\")\n",
        "\n",
        "    # Group by raster type and UTM zone\n",
        "    raster_groups = {}\n",
        "    all_by_type = {}\n",
        "    for file in os.listdir(gedi04d_dir):\n",
        "        if file.endswith(\".tif\"):\n",
        "            raster_type = '_'.join(file.replace('.tif', '').split('_')[6:])\n",
        "            match = re.search(r'(UTM\\d+)_(North|South)', file)\n",
        "            if match:\n",
        "                utm_name = f\"{match.group(1)}_{match.group(2)}\"\n",
        "                zone_num = int(match.group(1).replace('UTM', ''))\n",
        "                utm_epsg = 32600 + zone_num if match.group(2) == 'North' else 32700 + zone_num\n",
        "            else: utm_name, utm_epsg = None, None\n",
        "            filepath = join(gedi04d_dir, file)\n",
        "            raster_groups.setdefault((raster_type, utm_name, utm_epsg), []).append(filepath)\n",
        "            all_by_type.setdefault(raster_type, []).append(filepath)\n",
        "\n",
        "    # Original: clip each UTM zone separately, track which have data\n",
        "    zones_with_data = {}\n",
        "    for (raster_type, utm_name, utm_epsg), tiles in raster_groups.items():\n",
        "        output_original = join(gedi04d_final_dir, f\"GEDI04_D_original_epsg{utm_epsg}_{utm_name}_{raster_type}.tif\")\n",
        "        if not exists(output_original):\n",
        "            makedirs(gedi04d_final_dir, exist_ok=True)\n",
        "            print(f\"  {raster_type}: clipping in native CRS (EPSG:{utm_epsg} {utm_name})\")\n",
        "            bbox_utm = list(template_area.to_crs(epsg=utm_epsg).total_bounds)\n",
        "            gdal.Warp(output_original, tiles, options=gdal.WarpOptions(\n",
        "                outputBounds=bbox_utm, dstNodata=nodatavalue,\n",
        "                format='GTiff', creationOptions=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1']))\n",
        "            # Check if output has data, delete if empty\n",
        "            ds = gdal.Open(output_original)\n",
        "            arr = ds.GetRasterBand(1).ReadAsArray()\n",
        "            ds = None\n",
        "            if (arr == nodatavalue).all():\n",
        "                os.remove(output_original)\n",
        "                print(f\"    {utm_name}: no data in extent, removed\")\n",
        "            else: zones_with_data.setdefault(raster_type, []).append((utm_name, utm_epsg))\n",
        "        else:\n",
        "            print(f\"  {raster_type} ({utm_name}): original exists\")\n",
        "            # Check existing file for data\n",
        "            ds = gdal.Open(output_original)\n",
        "            arr = ds.GetRasterBand(1).ReadAsArray()\n",
        "            ds = None\n",
        "            if not (arr == nodatavalue).all(): zones_with_data.setdefault(raster_type, []).append((utm_name, utm_epsg))\n",
        "\n",
        "    # Merged reprojections\n",
        "    for raster_type, tiles in all_by_type.items():\n",
        "        valid_zones = zones_with_data.get(raster_type, [])\n",
        "        # Reprojected to centroid UTM (skip if only 1 zone with data)\n",
        "        if len(valid_zones) > 1:\n",
        "            output_utm = join(gedi04d_final_dir, f\"GEDI04_D_reprojected_epsg{centroid_utm_epsg}_{centroid_utm_name}_{raster_type}.tif\")\n",
        "            if not exists(output_utm):\n",
        "                makedirs(gedi04d_final_dir, exist_ok=True)\n",
        "                print(f\"  {raster_type}: merging {len(valid_zones)} zones and reprojecting to EPSG:{centroid_utm_epsg} {centroid_utm_name}\")\n",
        "                gdal.Warp(output_utm, tiles, options=gdal.WarpOptions(\n",
        "                    dstSRS=f'EPSG:{centroid_utm_epsg}', outputBounds=bbox_centroid_utm, dstNodata=nodatavalue,\n",
        "                    format='GTiff', creationOptions=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1']))\n",
        "            else: print(f\"  {raster_type}: {centroid_utm_name} exists\")\n",
        "        elif len(valid_zones) == 1:\n",
        "            print(f\"  {raster_type}: only 1 zone with data ({valid_zones[0][0]}), skipping centroid UTM reprojection\")\n",
        "\n",
        "        # Reprojected to EPSG:6933\n",
        "        output_6933 = join(gedi04d_final_dir, f\"GEDI04_D_reprojected_epsg6933_EASE-Grid_{raster_type}.tif\")\n",
        "        if not exists(output_6933):\n",
        "            makedirs(gedi04d_final_dir, exist_ok=True)\n",
        "            print(f\"  {raster_type}: merging and reprojecting to EPSG:6933 EASE-Grid\")\n",
        "            gdal.Warp(output_6933, tiles, options=gdal.WarpOptions(\n",
        "                dstSRS='EPSG:6933', outputBounds=bbox_6933, dstNodata=nodatavalue,\n",
        "                format='GTiff', creationOptions=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1']))\n",
        "        else: print(f\"  {raster_type}: EASE-Grid exists\")\n",
        "\n",
        "        # Reprojected to WGS84\n",
        "        output_4326 = join(gedi04d_final_dir, f\"GEDI04_D_reprojected_epsg4326_WGS84_{raster_type}.tif\")\n",
        "        if not exists(output_4326):\n",
        "            makedirs(gedi04d_final_dir, exist_ok=True)\n",
        "            print(f\"  {raster_type}: merging and reprojecting to EPSG:4326 WGS84\")\n",
        "            gdal.Warp(output_4326, tiles, options=gdal.WarpOptions(\n",
        "                dstSRS='EPSG:4326', outputBounds=bbox_4326, dstNodata=nodatavalue,\n",
        "                format='GTiff', creationOptions=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1']))\n",
        "        else: print(f\"  {raster_type}: WGS84 exists\")\n",
        "else: print(\"\\nGEDI04_D: not detected\")"
      ],
      "metadata": {
        "id": "qbt8hCRBDCN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cakvw5x97t-"
      },
      "source": [
        "# Convert GEDI .h5 to .pkl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vP2jARV-M2O"
      },
      "outputs": [],
      "source": [
        "# Load project and GEDI area polygons (filters to extent)\n",
        "project_area_polygon_path = join(polygons_dir, 'project_area.gpkg')\n",
        "project_area = gpd.read_file(project_area_polygon_path)\n",
        "print(\"Project area polygon:\")\n",
        "display(project_area[\"geometry\"].iloc[0])\n",
        "\n",
        "prediction_area_polygon_path = join(polygons_dir, \"prediction_area.gpkg\")\n",
        "prediction_area = gpd.read_file(prediction_area_polygon_path)[\"geometry\"].iloc[0]\n",
        "prediction_area_nw_lon, prediction_area_se_lat, prediction_area_se_lon, prediction_area_nw_lat = prediction_area.bounds\n",
        "print(f\"GEDI area polygon:\")\n",
        "display(prediction_area)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3NTdTt1-P85"
      },
      "outputs": [],
      "source": [
        "# Print GEDI products in the 'downloads' directory\n",
        "gedi_products = sorted(listdir(gedi_h5_downloads_dir))\n",
        "print(f'Found the following GEDI gedi products in the GEDI downloads directory:\\n {gedi_products}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D1RmrHS-Qq7"
      },
      "outputs": [],
      "source": [
        "# Print GEDI product parameters and data types\n",
        "\n",
        "# Simplified function from pyGEDI\n",
        "def getLayer(layer, files):\n",
        "    dictionary = {}\n",
        "    for h5_file in files:\n",
        "        layers , stack = [] , [h5_file['BEAM0000']]\n",
        "        while stack:\n",
        "            item = stack.pop()\n",
        "            if isinstance(item, h5py.Dataset):\n",
        "                layers.append(item.name.replace('/BEAM0000/', ''))\n",
        "            elif isinstance(item, h5py.Group):\n",
        "                stack.extend(item.values())\n",
        "        filtered_layers = [l for l in layers if layer in l]\n",
        "        if filtered_layers:\n",
        "            dictionary[h5_file.filename] = filtered_layers\n",
        "    return dictionary\n",
        "\n",
        "dict_gedi_parameters = {}\n",
        "for product in gedi_products:\n",
        "  product_dir = join(gedi_h5_downloads_dir, product)\n",
        "  # Sample a single .H5 file\n",
        "  sample_file_path = listdir(product_dir)[0]\n",
        "  h5_file = h5py.File(join(product_dir, sample_file_path), 'r')\n",
        "  layers = list(getLayer('', [h5_file]).values())[0]\n",
        "  # Sample from beam '0000'\n",
        "  compatible_shape = (len(h5_file['BEAM0000']['shot_number']), )\n",
        "  dict_layer = {}\n",
        "  for layer in layers:\n",
        "    layer_shape = h5_file['BEAM0000'][layer].shape\n",
        "    if layer_shape == compatible_shape:\n",
        "      # Get data types\n",
        "      df_layer = pd.DataFrame()\n",
        "      df_layer[layer] = h5_file['BEAM0000'][layer]\n",
        "      # Add parameter name and data type to dictionary\n",
        "      dict_layer[layer] = str(df_layer[layer].dtype)\n",
        "  dict_gedi_parameters[product] = dict_layer\n",
        "# Print config\n",
        "print(\"Copy selected targets into the next cell.\\n\")\n",
        "pp = pprint.PrettyPrinter(indent=1)\n",
        "pp.pprint(dict_gedi_parameters)\n",
        "# Close files\n",
        "h5_file.close()\n",
        "df_layer = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPhNthZN-Tbp"
      },
      "outputs": [],
      "source": [
        "# Copy and paste GEDI parameters for inclusion in .pkl outputs.\n",
        "# Each parameter requires additional memory and time to process.\n",
        "# Changing the data type (e.g. float64 -> float32) may increase performance,\n",
        "# but can reduce precision and cause other issues. Fot example, Float32 rounds to\n",
        "# 7 significant digits, imprecise for coordinates (lat and lon lowest mode).\n",
        "# Include at least beam, lat_lowestmode, lon_lowestmode, shot_number\n",
        "# and ONE quality flag for each product.\n",
        "\n",
        "supported_data_types = [\"int8\",\"int16\",\"int32\",\"int64\",\"uint8\",\"uint16\",\"uint32\",\"uint64\",\"float16\",\"float32\",\"float64\",\"str\"]\n",
        "\n",
        "selected_parameters = {\n",
        "\n",
        "  'GEDI04_A': {\n",
        "    'agbd': 'float32',\n",
        "    'agbd_se': 'float32',\n",
        "    'beam': 'str',\n",
        "    'elev_lowestmode': 'float32',\n",
        "    'lat_lowestmode': 'float64',\n",
        "    'lon_lowestmode': 'float64',\n",
        "    'l4_quality_flag': 'uint8',\n",
        "    'sensitivity': 'float32',\n",
        "    'shot_number': 'str',\n",
        "  },\n",
        "\n",
        "  # 'GEDI02_A': {\n",
        "  #   'rh_parameters': 'float16', # e.g. rh25, rh50, rh75, rh95, rh98, modify in block below\n",
        "  #   'beam': 'object',\n",
        "  #   # 'elev_highestreturn': 'float32',\n",
        "  #   'elev_lowestmode': 'float32',\n",
        "  #   'lat_lowestmode': 'float64',\n",
        "  #   'lon_lowestmode': 'float64',\n",
        "  #   'quality_flag': 'uint8',\n",
        "  #   'sensitivity': 'float32',\n",
        "  #   'shot_number': 'str',\n",
        "  # },\n",
        "\n",
        "  # 'GEDI02_B': {\n",
        "  #   'beam': 'object',\n",
        "  #   'cover': 'float32',\n",
        "  #   'fhd_normal': 'float32',\n",
        "  #   'omega': 'float32',\n",
        "  #   'pai': 'float32',\n",
        "  #   'geolocation/lat_lowestmode': 'float64',\n",
        "  #   'geolocation/lon_lowestmode': 'float64',\n",
        "  #   'l2b_quality_flag': 'uint8',\n",
        "  #   'sensitivity': 'float32',\n",
        "  #   'shot_number': 'str',\n",
        "  # },\n",
        "\n",
        "}\n",
        "\n",
        "# Check if GEDI products in the selected parameters match those in the downloads directory\n",
        "assert set(selected_parameters.keys()).issubset(set(gedi_products)), f\"GEDI products in selected parameters do not match those in {gedi_h5_downloads_dir}\"\n",
        "# Check if parameters match those in the selected_parameters dictionary\n",
        "for product in selected_parameters.keys():\n",
        "  parameter_list = list(dict_gedi_parameters[product].keys())\n",
        "  if product == 'GEDI02_A': parameter_list.append(\"rh_parameters\") # These are custom parameters added later\n",
        "  assert set(selected_parameters[product]).issubset(set(parameter_list)), f\"{product}'s selected parameters are not all available\"\n",
        "# Check that data types are supported\n",
        "data_types_list = []\n",
        "for key in selected_parameters.keys():\n",
        "  for data_type in list(selected_parameters[key].values()):\n",
        "    data_types_list.append(data_type)\n",
        "# Remove duplicates for assert\n",
        "data_types_list = list(dict.fromkeys(data_types_list))\n",
        "assert set(data_types_list).issubset(set(supported_data_types)), \"An unsupported data type has been selected.\"\n",
        "print(\"All selected GEDI products, parameters and data types successfully applied for conversion from .h5 to .pkl format.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9MrbXo5QLdL"
      },
      "outputs": [],
      "source": [
        "# Convert .h5 files to .pkl\n",
        "\n",
        "use_project_area = False  # Toggle to use complex project area bounds, will impact performance.\n",
        "test_processing = False  # Toggle for testing mode to limit the number of files processed.\n",
        "test_number = 20  # Number of files to process in test mode.\n",
        "sensitivity_threshold = 0.95  # Sensitivity threshold for data filtering.\n",
        "print_filename = True  # Toggle to print the filename being processed.\n",
        "max_attempts = 3 # At trying to open an .h5 file if it initially fails\n",
        "delay = 5 # Waiting between attempts\n",
        "\n",
        "# Loop through products and their parameters.\n",
        "for product, parameters in selected_parameters.items():\n",
        "    # Set up directories for caching and downloading.\n",
        "    gedi_h5_pkl_cache_product_dir = join(h5_pkl_cache_dir, product)\n",
        "    makedirs(gedi_h5_pkl_cache_product_dir, exist_ok=True)\n",
        "    gedi_downloads_product_dir = join(gedi_h5_downloads_dir, product)\n",
        "\n",
        "    # List .h5 files to be processed.\n",
        "    h5_data = [[join(gedi_downloads_product_dir, h5_file_dir), parameters, gedi_h5_pkl_cache_product_dir] for h5_file_dir in listdir(gedi_downloads_product_dir)]\n",
        "\n",
        "    # Initialize progress display.\n",
        "    index = 0\n",
        "    progress_label = widgets.Label(value=f\"{product} progress: {index}/{len(h5_data)} H5 files converted to a .pkl cache.\")\n",
        "    display(progress_label)\n",
        "\n",
        "    # Processing loop for each file.\n",
        "    for filename, parameters_dict, gedi_h5_pkl_cache_product_dir in h5_data:\n",
        "\n",
        "        # Construct the destination filename for the .pkl file\n",
        "        dst_filename = join(gedi_h5_pkl_cache_product_dir, path.splitext(path.split(filename)[-1])[0]) + '.pkl'\n",
        "\n",
        "        # Check if .pkl file already exists to skip processing\n",
        "        if not exists(dst_filename):\n",
        "            if print_filename: print(f\"Processing started: {filename.split('/')[-1]}\")\n",
        "\n",
        "            # Extract data from h5 first (keeping it open is unstable on Google Drive)\n",
        "            h5_file = {}\n",
        "            for attempt in range(max_attempts):\n",
        "                try:\n",
        "                    with h5py.File(filename, 'r') as h5:\n",
        "                        for beam in ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011']:\n",
        "                            if beam not in h5: continue  # Skip missing beams.\n",
        "                            h5_file[beam] = {}\n",
        "                            latlayer, lonlayer = ('lat_lowestmode', 'lon_lowestmode') if 'lat_lowestmode' in h5[beam] else ('geolocation/lat_lowestmode', 'geolocation/lon_lowestmode')\n",
        "                            h5_file[beam][latlayer] = h5[beam][latlayer][:]\n",
        "                            h5_file[beam][lonlayer] = h5[beam][lonlayer][:]\n",
        "                            h5_file[beam]['shot_number'] = h5[beam]['shot_number'][:].astype(parameters_dict['shot_number'])\n",
        "                            for layer in parameters_dict.keys():\n",
        "                                if layer in h5[beam]: h5_file[beam][layer] = h5[beam][layer][:]\n",
        "                                elif layer == 'rh_parameters': h5_file[beam]['rh'] = h5[beam]['rh'][:, :100]\n",
        "                    break  # If successful, break the retry loop\n",
        "                except Exception as e:\n",
        "                    if attempt < max_attempts - 1:\n",
        "                        print(f\"Error opening file (attempt {attempt + 1}/{max_attempts}): {str(e)}\")\n",
        "                        print(f\"Retrying in {delay} seconds\")\n",
        "                        sleep(delay)\n",
        "                    else:\n",
        "                        print(f\"Failed to open file after {max_attempts} attempts. Skipping.\")\n",
        "                        continue  # Skip to the next file in the outer loop\n",
        "            if attempt == max_attempts - 1: continue  # Skip to the next file if all attempts failed\n",
        "\n",
        "            layer_df_list = []  # List to store data frames for each layer and beam combination.\n",
        "            first_layer = True # For shot_number and beam to be added\n",
        "\n",
        "            # Iterate through each parameter layer 'beam' and 'shot_number'.\n",
        "            for layer in [layer for layer in parameters_dict.keys() if layer not in ('beam', 'shot_number')]:\n",
        "                beam_df_list = []\n",
        "                # Iterate through each beam.\n",
        "                for beam in ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011']:\n",
        "                    if beam not in h5_file: continue  # Skip missing beams.\n",
        "                    # Define geospatial extent.\n",
        "                    latlayer, lonlayer = 'lat_lowestmode', 'lon_lowestmode'\n",
        "                    if latlayer not in h5_file[beam]:\n",
        "                        latlayer, lonlayer = 'geolocation/lat_lowestmode', 'geolocation/lon_lowestmode'\n",
        "                    lat_array = h5_file[beam][latlayer]\n",
        "                    lon_array = h5_file[beam][lonlayer]\n",
        "\n",
        "                    # Calculate index based on geographic filters.\n",
        "                    if use_project_area:\n",
        "                        project_polygon = project_area.geometry.iloc[0]\n",
        "                        gedi_points = gpd.GeoDataFrame(geometry=gpd.points_from_xy(lon_array, lat_array), crs=\"EPSG:4326\")\n",
        "                        points_tree = STRtree(gedi_points.geometry.values)\n",
        "                        points_tree_indices = points_tree.query(project_polygon, predicate='contains')\n",
        "                        if len(points_tree_indices) > 0: geo_index = points_tree_indices\n",
        "                        else: geo_index = np.array([], dtype=int)\n",
        "                    else: geo_index = np.where((lat_array > prediction_area_se_lat) & (lat_array < prediction_area_nw_lat) & (lon_array > prediction_area_nw_lon) & (lon_array < prediction_area_se_lon))[0]\n",
        "\n",
        "                    # Collect data for each layer and beam, handling special cases.\n",
        "                    beam_df = pd.DataFrame()\n",
        "                    if layer == 'rh_parameters':\n",
        "                        # All rh values\n",
        "                        data = h5_file[beam]['rh'][geo_index][:, :100]\n",
        "                        columns = [f'rh{i+1}' for i in range(100)]\n",
        "                        # rh values in intervals of 5\n",
        "                        # data = h5_file[beam]['rh'][geo_index][:, [0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 74, 79, 84, 89, 94, 97, 99]]\n",
        "                        # columns = ['rh1', 'rh5', 'rh10', 'rh15', 'rh20', 'rh25', 'rh30', 'rh35', 'rh40', 'rh45', 'rh50', 'rh55', 'rh60', 'rh65', 'rh70', 'rh75', 'rh80', 'rh85', 'rh90', 'rh95', 'rh98', 'rh100']\n",
        "                        beam_df = pd.DataFrame(data, columns=columns)\n",
        "                    elif h5_file[beam][layer].shape == (len(lon_array), ):\n",
        "                        beam_df[layer] = h5_file[beam][layer][geo_index].astype(parameters_dict[layer])\n",
        "\n",
        "                    if len(beam_df) > 0:\n",
        "                      if first_layer:\n",
        "                        beam_df.insert(0, 'shot_number', h5_file[beam]['shot_number'][geo_index].astype(parameters_dict['shot_number']))\n",
        "                        beam_df.insert(1, 'beam', beam)\n",
        "\n",
        "                    beam_df_list.append(beam_df)\n",
        "\n",
        "                # Concatenate data frames for all beams within the same layer.\n",
        "                if len(beam_df_list) > 0:\n",
        "                    layer_df = pd.concat(beam_df_list)\n",
        "                    layer_df_list.append(layer_df)\n",
        "                    first_layer = False\n",
        "\n",
        "            first_layer = True # Restart first layer for adding shot_number and beam\n",
        "\n",
        "            # Concatenate all layer data frames for the file.\n",
        "            if len(layer_df_list) > 0:\n",
        "                h5_df = pd.concat(layer_df_list, axis=1)\n",
        "                # Define timestamp and convert geodataframe.\n",
        "                for part in filename.split('_'):\n",
        "                  try: timestamp = pd.to_datetime(part, format='%Y%j%H%M%S', utc=True)\n",
        "                  except: continue\n",
        "                h5_df['timestamp'] = timestamp\n",
        "                # Apply quality and sensitivity filters.\n",
        "                quality_flag = [key for key in parameters_dict.keys() if 'quality_flag' in key][0]\n",
        "                h5_df = h5_df.loc[(h5_df[quality_flag] == 1) & (h5_df['sensitivity'] >= sensitivity_threshold)]\n",
        "                h5_df = h5_df.drop(columns=[quality_flag])\n",
        "                # Final type check and conversion if necessary\n",
        "                for column, dtype in parameters_dict.items():\n",
        "                    if column in h5_df.columns:\n",
        "                        h5_df[column] = h5_df[column].astype(dtype)\n",
        "                    elif column == 'rh_parameters':\n",
        "                        rh_columns = [col for col in h5_df.columns if col.startswith('rh')]\n",
        "                        h5_df[rh_columns] = h5_df[rh_columns].astype(dtype)\n",
        "                # Export to pickle\n",
        "                h5_df.to_pickle(dst_filename)\n",
        "                if print_filename: print(f\"Processing complete: {filename.split('/')[-1]}\")\n",
        "\n",
        "        # Update progress display.\n",
        "        index += 1\n",
        "        progress_label.value = f\"{product} progress: {index}/{len(h5_data)} H5 files converted to a .pkl cache.\"\n",
        "        if test_processing and index == test_number: break  # Break after processing specified number of files if in test mode."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert .h5 files to .pkl\n",
        "\n",
        "use_project_area = False  # Toggle to use complex project area bounds, will impact performance.\n",
        "test_processing = False  # Toggle for testing mode to limit the number of files processed.\n",
        "test_number = 20  # Number of files to process in test mode.\n",
        "sensitivity_threshold = 0.95  # Sensitivity threshold for data filtering.\n",
        "print_filename = True  # Toggle to print the filename being processed.\n",
        "max_attempts = 3  # Attempts at opening an .h5 file if it initially fails.\n",
        "delay = 5  # Seconds between attempts.\n",
        "\n",
        "# Loop through products and their parameters.\n",
        "for product, parameters in selected_parameters.items():\n",
        "    # Set up directories for caching and downloading.\n",
        "    gedi_h5_pkl_cache_product_dir = join(h5_pkl_cache_dir, product)\n",
        "    makedirs(gedi_h5_pkl_cache_product_dir, exist_ok=True)\n",
        "    gedi_downloads_product_dir = join(gedi_h5_downloads_dir, product)\n",
        "\n",
        "    # Validate quality_flag parameter exists.\n",
        "    quality_flags = [key for key in parameters.keys() if 'quality_flag' in key]\n",
        "    if not quality_flags:\n",
        "        print(f\"No quality_flag parameter found in {product} parameters. Skipping product.\")\n",
        "        continue\n",
        "    quality_flag = quality_flags[0]\n",
        "\n",
        "    # List .h5 files to be processed.\n",
        "    h5_data = [[join(gedi_downloads_product_dir, h5_file_dir), parameters, gedi_h5_pkl_cache_product_dir] for h5_file_dir in listdir(gedi_downloads_product_dir)]\n",
        "\n",
        "    # Initialize progress display.\n",
        "    index = 0\n",
        "    progress_label = widgets.Label(value=f\"{product} progress: {index}/{len(h5_data)} H5 files converted to a .pkl cache.\")\n",
        "    display(progress_label)\n",
        "\n",
        "    # Processing loop for each file.\n",
        "    for filename, parameters_dict, gedi_h5_pkl_cache_product_dir in h5_data:\n",
        "\n",
        "        # Construct the destination filename for the .pkl file.\n",
        "        dst_filename = join(gedi_h5_pkl_cache_product_dir, path.splitext(path.split(filename)[-1])[0]) + '.pkl'\n",
        "\n",
        "        # Check if .pkl file already exists to skip processing.\n",
        "        if not exists(dst_filename):\n",
        "            if print_filename: print(f\"Processing started: {filename.split('/')[-1]}\")\n",
        "\n",
        "            # Extract data from h5 first (keeping it open is unstable on Google Drive).\n",
        "            h5_file = {}\n",
        "            file_opened = False\n",
        "            for attempt in range(max_attempts):\n",
        "                try:\n",
        "                    with h5py.File(filename, 'r') as h5:\n",
        "                        for beam in ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011']:\n",
        "                            if beam not in h5: continue  # Skip missing beams.\n",
        "                            h5_file[beam] = {}\n",
        "                            latlayer, lonlayer = ('lat_lowestmode', 'lon_lowestmode') if 'lat_lowestmode' in h5[beam] else ('geolocation/lat_lowestmode', 'geolocation/lon_lowestmode')\n",
        "                            h5_file[beam][latlayer] = h5[beam][latlayer][:]\n",
        "                            h5_file[beam][lonlayer] = h5[beam][lonlayer][:]\n",
        "                            h5_file[beam]['shot_number'] = h5[beam]['shot_number'][:].astype(parameters_dict['shot_number'])\n",
        "                            for layer in parameters_dict.keys():\n",
        "                                if layer in h5[beam]: h5_file[beam][layer] = h5[beam][layer][:]\n",
        "                                elif layer == 'rh_parameters': h5_file[beam]['rh'] = h5[beam]['rh'][:, :100]\n",
        "                    file_opened = True\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    if attempt < max_attempts - 1:\n",
        "                        print(f\"Error opening file (attempt {attempt + 1}/{max_attempts}): {str(e)}\")\n",
        "                        print(f\"Retrying in {delay} seconds\")\n",
        "                        sleep(delay)\n",
        "                    else:\n",
        "                        print(f\"Failed to open file after {max_attempts} attempts. Skipping.\")\n",
        "\n",
        "            if not file_opened:\n",
        "                index += 1\n",
        "                progress_label.value = f\"{product} progress: {index}/{len(h5_data)} H5 files converted to a .pkl cache.\"\n",
        "                continue\n",
        "\n",
        "            layer_df_list = []  # List to store data frames for each layer and beam combination.\n",
        "            first_layer = True  # For shot_number and beam to be added.\n",
        "\n",
        "            # Iterate through each parameter layer 'beam' and 'shot_number'.\n",
        "            for layer in [layer for layer in parameters_dict.keys() if layer not in ('beam', 'shot_number')]:\n",
        "                beam_df_list = []\n",
        "                # Iterate through each beam.\n",
        "                for beam in ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011']:\n",
        "                    if beam not in h5_file: continue  # Skip missing beams.\n",
        "                    # Define geospatial extent.\n",
        "                    latlayer, lonlayer = 'lat_lowestmode', 'lon_lowestmode'\n",
        "                    if latlayer not in h5_file[beam]:\n",
        "                        latlayer, lonlayer = 'geolocation/lat_lowestmode', 'geolocation/lon_lowestmode'\n",
        "                    lat_array = h5_file[beam][latlayer]\n",
        "                    lon_array = h5_file[beam][lonlayer]\n",
        "\n",
        "                    # Calculate index based on geographic filters.\n",
        "                    if use_project_area:\n",
        "                        project_polygon = project_area.geometry.iloc[0]\n",
        "                        gedi_points = gpd.GeoDataFrame(geometry=gpd.points_from_xy(lon_array, lat_array), crs=\"EPSG:4326\")\n",
        "                        points_tree = STRtree(gedi_points.geometry.values)\n",
        "                        points_tree_indices = points_tree.query(project_polygon, predicate='contains')\n",
        "                        if len(points_tree_indices) > 0: geo_index = points_tree_indices\n",
        "                        else: geo_index = np.array([], dtype=int)\n",
        "                    else: geo_index = np.where((lat_array > prediction_area_se_lat) & (lat_array < prediction_area_nw_lat) & (lon_array > prediction_area_nw_lon) & (lon_array < prediction_area_se_lon))[0]\n",
        "\n",
        "                    # Collect data for each layer and beam, handling special cases.\n",
        "                    beam_df = pd.DataFrame()\n",
        "                    if layer == 'rh_parameters':\n",
        "                        # All rh values.\n",
        "                        data = h5_file[beam]['rh'][geo_index][:, :100]\n",
        "                        columns = [f'rh{i+1}' for i in range(100)]\n",
        "                        # rh values in intervals of 5.\n",
        "                        # data = h5_file[beam]['rh'][geo_index][:, [0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 74, 79, 84, 89, 94, 97, 99]]\n",
        "                        # columns = ['rh1', 'rh5', 'rh10', 'rh15', 'rh20', 'rh25', 'rh30', 'rh35', 'rh40', 'rh45', 'rh50', 'rh55', 'rh60', 'rh65', 'rh70', 'rh75', 'rh80', 'rh85', 'rh90', 'rh95', 'rh98', 'rh100']\n",
        "                        beam_df = pd.DataFrame(data, columns=columns)\n",
        "                    elif h5_file[beam][layer].shape == (len(lon_array), ):\n",
        "                        beam_df[layer] = h5_file[beam][layer][geo_index].astype(parameters_dict[layer])\n",
        "\n",
        "                    if len(beam_df) > 0:\n",
        "                      if first_layer:\n",
        "                        beam_df.insert(0, 'shot_number', h5_file[beam]['shot_number'][geo_index].astype(parameters_dict['shot_number']))\n",
        "                        beam_df.insert(1, 'beam', beam)\n",
        "\n",
        "                    beam_df_list.append(beam_df)\n",
        "\n",
        "                # Concatenate data frames for all beams within the same layer.\n",
        "                if len(beam_df_list) > 0:\n",
        "                    layer_df = pd.concat(beam_df_list)\n",
        "                    layer_df_list.append(layer_df)\n",
        "                    first_layer = False\n",
        "\n",
        "            # Concatenate all layer data frames for the file.\n",
        "            if len(layer_df_list) > 0:\n",
        "                h5_df = pd.concat(layer_df_list, axis=1)\n",
        "                # Define timestamp and convert geodataframe.\n",
        "                for part in filename.split('_'):\n",
        "                  try: timestamp = pd.to_datetime(part, format='%Y%j%H%M%S', utc=True)\n",
        "                  except: continue\n",
        "                h5_df['timestamp'] = timestamp\n",
        "                # Apply quality and sensitivity filters.\n",
        "                h5_df = h5_df.loc[(h5_df[quality_flag] == 1) & (h5_df['sensitivity'] >= sensitivity_threshold)]\n",
        "                h5_df = h5_df.drop(columns=[quality_flag])\n",
        "                # Final type check and conversion if necessary.\n",
        "                for column, dtype in parameters_dict.items():\n",
        "                    if column in h5_df.columns:\n",
        "                        h5_df[column] = h5_df[column].astype(dtype)\n",
        "                    elif column == 'rh_parameters':\n",
        "                        rh_columns = [col for col in h5_df.columns if col.startswith('rh')]\n",
        "                        h5_df[rh_columns] = h5_df[rh_columns].astype(dtype)\n",
        "                # Export to pickle.\n",
        "                h5_df.to_pickle(dst_filename)\n",
        "                if print_filename: print(f\"Processing complete: {filename.split('/')[-1]}\")\n",
        "\n",
        "        # Update progress display.\n",
        "        index += 1\n",
        "        progress_label.value = f\"{product} progress: {index}/{len(h5_data)} H5 files converted to a .pkl cache.\"\n",
        "        if test_processing and index == test_number: break  # Break after processing specified number of files if in test mode."
      ],
      "metadata": {
        "id": "857a3q4NVCxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WHSbQ-z9EH9"
      },
      "outputs": [],
      "source": [
        "# Get list of proucts with cache files\n",
        "print('products_to_finalise = [')\n",
        "for product in os.listdir(h5_pkl_cache_dir):\n",
        "  print(f\"'{product}',\")\n",
        "print(']')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcQXeovt9DAt"
      },
      "outputs": [],
      "source": [
        "products_to_finalise = [\n",
        "'GEDI04_A',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8WL0qTcUwAV"
      },
      "outputs": [],
      "source": [
        "for product in products_to_finalise:\n",
        "  final_pkl_path = join(pkl_final_dir, f\"{product}.pkl\")\n",
        "  product_cache_dir = join(h5_pkl_cache_dir, product)\n",
        "  # List prevents duplicates (e.g. ' (1)' from being added).\n",
        "  cache_files = [f for f in os.listdir(product_cache_dir) if ' (' not in f]\n",
        "  if len(cache_files) > 0:\n",
        "    # Progress.\n",
        "    final_pkl_index = 0\n",
        "    final_pkl_progress_label = widgets.Label(value=f\"{product} progress: {final_pkl_index}/{len(cache_files)} .pkl caches appended to list.\")\n",
        "    display(final_pkl_progress_label)\n",
        "    # Initiate cache dataframe list.\n",
        "    cache_dataframe_list = []\n",
        "    # Loop through cache files, appending each to the list.\n",
        "    for cache in cache_files:\n",
        "      chunk_cache = pd.read_pickle(join(product_cache_dir, cache))\n",
        "      if len(chunk_cache) > 0:\n",
        "          cache_dataframe_list.append(chunk_cache)\n",
        "      final_pkl_index += 1\n",
        "      final_pkl_progress_label.value = f\"{product} progress: {final_pkl_index}/{len(cache_files)} .pkl caches appended to list.\"\n",
        "    # Check cache list is not empty.\n",
        "    if len(cache_dataframe_list) == 0:\n",
        "      print(f\"All cache files for {product} were empty.\")\n",
        "      continue\n",
        "    # Concatenate and save final .pkl file.\n",
        "    print(\"Concatenating list of cache dataframes.\")\n",
        "    final_pkl_df_concat = pd.concat(cache_dataframe_list)\n",
        "    duplicate_count = final_pkl_df_concat.duplicated(subset=['shot_number']).sum()\n",
        "    if duplicate_count > 0:\n",
        "        print(f\"Removing {duplicate_count} duplicate shot_numbers.\")\n",
        "        final_pkl_df_concat = final_pkl_df_concat.drop_duplicates(subset=['shot_number'], keep='first')\n",
        "    cache_dataframe_list = []\n",
        "    # Detect lat/lon column names.\n",
        "    if 'lat_lowestmode' in final_pkl_df_concat.columns:\n",
        "        lat_col, lon_col = 'lat_lowestmode', 'lon_lowestmode'\n",
        "    else:\n",
        "        lat_col, lon_col = 'geolocation/lat_lowestmode', 'geolocation/lon_lowestmode'\n",
        "    # Make geodataframe.\n",
        "    print(\"Converting to a geodataframe.\")\n",
        "    final_pkl_gdf_concat = gpd.GeoDataFrame(final_pkl_df_concat,\n",
        "        geometry=gpd.points_from_xy(final_pkl_df_concat[lon_col],\n",
        "                                    final_pkl_df_concat[lat_col], crs=\"EPSG:4326\"))\n",
        "    final_pkl_gdf_concat = final_pkl_gdf_concat.drop(columns=[lat_col, lon_col])\n",
        "    # Save the final converted dataframe.\n",
        "    final_pkl_gdf_concat = final_pkl_gdf_concat.reset_index(drop=True)\n",
        "    final_pkl_gdf_concat.to_pickle(f\"{final_pkl_path}\")\n",
        "    print(f\"{product} final .pkl complete.\")\n",
        "  else: print(f\"No cache data found for {product}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktTAxR_e-YBS"
      },
      "source": [
        "# Convert GEDI .pkl to .gpkg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPjknGwnmDpq"
      },
      "outputs": [],
      "source": [
        "# For verification and visualisation in GIS software\n",
        "# Get list of .txt files in the 'links' directory\n",
        "pkl_final_list = []\n",
        "for file in os.listdir(pkl_final_dir):\n",
        "  pkl_final_list.append(file)\n",
        "\n",
        "# Select final .pkl file to convert to .gpkg\n",
        "for pkl_final in sorted(pkl_final_list):\n",
        "  print(f\"pkl_to_convert = '{pkl_final}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9fChPk4nL1V"
      },
      "outputs": [],
      "source": [
        "pkl_to_convert = 'GEDI04_A.pkl'\n",
        "\n",
        "# Load final .pkl\n",
        "pkl_to_convert_dir = join(pkl_final_dir, pkl_to_convert)\n",
        "pkl_to_convert_df = pd.read_pickle(pkl_to_convert_dir)\n",
        "print(f\"Final .pkl has {len(pkl_to_convert_df)} rows.\\n\\n\")\n",
        "\n",
        "# Print available parameters in final .pkl for selection\n",
        "print(f'selected_parameters = [')\n",
        "for column in pkl_to_convert_df.columns:\n",
        "    if column in ('geometry'):\n",
        "        continue\n",
        "    print(f\"\\t'{column}',\")\n",
        "print(f']')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt0gwgr6-bmA"
      },
      "outputs": [],
      "source": [
        "# Select from available parameters in .pkl\n",
        "\n",
        "selected_parameters = [\n",
        "\t'shot_number',\n",
        "\t'beam',\n",
        "\t'agbd',\n",
        "\t'agbd_se',\n",
        "\t'elev_lowestmode',\n",
        "\t'sensitivity',\n",
        "\t'timestamp',\n",
        "]\n",
        "\n",
        "assert all([selected_parameter in pkl_to_convert_df.columns for selected_parameter in selected_parameters])\n",
        "final_gpkg_file_dir = join(targets_gpkg_dir, f'{pkl_to_convert[:-4]}.gpkg')\n",
        "if exists(final_gpkg_file_dir):\n",
        "  print(f'File {final_gpkg_file_dir} already exists.')\n",
        "else:\n",
        "  print(f'Converting final {pkl_to_convert} to .gpkg.')\n",
        "  df_selected_parameters = pkl_to_convert_df[['geometry', *selected_parameters]]\n",
        "  gpkg_gdf = gpd.GeoDataFrame(df_selected_parameters, geometry='geometry')\n",
        "  gpkg_gdf = gpkg_gdf.set_crs(4326) # WGS84\n",
        "  gpkg_gdf.to_file(final_gpkg_file_dir, driver=\"GPKG\")\n",
        "  print(f'Saved to {final_gpkg_file_dir}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BgtsxUKo8NT"
      },
      "source": [
        "# User uploaded targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YldhycmXpCrB"
      },
      "outputs": [],
      "source": [
        "# Upload of any spatial data for XGBoost prediction.\n",
        "# Place .csv(s) with 'x' and 'y' columns in '2_targets/csv'.\n",
        "\n",
        "print(\"Select 'user targets' csv to be compiled\\n\")\n",
        "for file in os.listdir(targets_user_csv_dir):\n",
        "  if file.endswith(\".csv\"):\n",
        "    print(f'user_uploaded_targets = \"{file}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlWNwcSMVz3P"
      },
      "outputs": [],
      "source": [
        "user_uploaded_targets = \"user_uploaded.csv\"\n",
        "\n",
        "user_targets_csv = pd.read_csv(join(targets_user_csv_dir, user_uploaded_targets))\n",
        "\n",
        "print(\"Select targets columns to transfer to the dataset\\n\")\n",
        "print(\"targets_columns = [\")\n",
        "for column in list(user_targets_csv.columns):\n",
        "  print(f'  \"{column}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh1w1kHu9qP2"
      },
      "outputs": [],
      "source": [
        "targets_columns = [\n",
        "\n",
        "]\n",
        "\n",
        "export_gpkg = False\n",
        "\n",
        "# Add column flags.\n",
        "dataset_targets_columns = [(targets_column, f\"tar_{targets_column}\") for targets_column in targets_columns]\n",
        "dataset_targets_columns_dict = dict(dataset_targets_columns)\n",
        "\n",
        "# Detect coordinate columns (case-insensitive).\n",
        "cols_lower = {col.lower(): col for col in user_targets_csv.columns}\n",
        "lon_variants = ['x', 'lon', 'long', 'longitude', 'lon_lowestmode']\n",
        "lat_variants = ['y', 'lat', 'latitude', 'lat_lowestmode']\n",
        "\n",
        "lon_col, lat_col = None, None\n",
        "for lon_var in lon_variants:\n",
        "    if lon_var in cols_lower:\n",
        "        lon_col = cols_lower[lon_var]\n",
        "        break\n",
        "for lat_var in lat_variants:\n",
        "    if lat_var in cols_lower:\n",
        "        lat_col = cols_lower[lat_var]\n",
        "        break\n",
        "\n",
        "if lon_col is None or lat_col is None:\n",
        "    raise ValueError(f\"CSV must contain coordinate columns. Supported: {lon_variants} and {lat_variants} (case-insensitive).\")\n",
        "\n",
        "coord_cols = [lon_col, lat_col]\n",
        "\n",
        "# Drop any columns which aren't selected (keep coordinate columns).\n",
        "cols_to_keep = list(dataset_targets_columns_dict.keys()) + coord_cols\n",
        "user_targets_csv = user_targets_csv.drop(columns=[col for col in user_targets_csv.columns if col not in cols_to_keep])\n",
        "\n",
        "# Rename target columns with tar_ prefix.\n",
        "user_targets_csv = user_targets_csv.rename(columns=dataset_targets_columns_dict)\n",
        "\n",
        "# Create geometry.\n",
        "user_targets_geometry = gpd.GeoDataFrame(\n",
        "    user_targets_csv,\n",
        "    geometry=gpd.points_from_xy(user_targets_csv[lon_col], user_targets_csv[lat_col], crs=\"EPSG:4326\")\n",
        ")\n",
        "user_targets_geometry = user_targets_geometry.drop(columns=coord_cols)\n",
        "\n",
        "user_targets_final = f\"user_{user_uploaded_targets[:-4]}_{datetime.utcnow().strftime('%y%m%d_%H%M%S')}\"\n",
        "\n",
        "# Export to .pkl.\n",
        "user_targets_geometry.to_pickle(join(pkl_final_dir, f\"{user_targets_final}.pkl\"))\n",
        "\n",
        "# Check .pkl.\n",
        "display(pd.read_pickle(join(pkl_final_dir, f\"{user_targets_final}.pkl\")))\n",
        "\n",
        "# Export to .gpkg.\n",
        "if export_gpkg:\n",
        "  user_targets_gpkg = join(targets_gpkg_dir, f\"{user_targets_final}.gpkg\")\n",
        "  user_targets_geometry.to_file(user_targets_gpkg, driver=\"GPKG\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVQP-Oy3i_jC"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R4NjWtVhQLL"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "8Cakvw5x97t-",
        "ktTAxR_e-YBS"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}