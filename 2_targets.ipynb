{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/dev/2_targets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUfm-wkD9x1i"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gUXicFlGp1W"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/gdrive/Shareddrives/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHh9EmwkDtUb"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlKpdcKG90gB"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "import geopandas as gpd\n",
        "import h5py\n",
        "import ipywidgets as widgets\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import re\n",
        "import requests\n",
        "from time import sleep\n",
        "import urllib\n",
        "from datetime import datetime\n",
        "from getpass import getpass\n",
        "from google.colab import runtime\n",
        "from http.cookiejar import CookieJar\n",
        "from os import listdir, makedirs, path\n",
        "from os.path import exists, join\n",
        "from requests.auth import HTTPBasicAuth\n",
        "from shapely.geometry import MultiPolygon, Polygon, box\n",
        "from shapely.ops import unary_union, orient\n",
        "from shapely.strtree import STRtree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0YGFT7P99xX"
      },
      "outputs": [],
      "source": [
        "# 1_areas directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(base_dir, \"1_areas/polygons\")\n",
        "masks_dir = join(base_dir, \"1_areas/masks\")\n",
        "\n",
        "# 2_targets directories\n",
        "targets_dir = join(base_dir, \"2_targets\")\n",
        "gedi_links_dir = join(targets_dir, \"gedi_h5_links\")\n",
        "gedi_downloads_dir = join(targets_dir, \"gedi_h5_downloads\")\n",
        "h5_pkl_cache_dir = join(targets_dir, \"gedi_h5_pkl_cache\")\n",
        "pkl_final_dir = join(targets_dir, \"pkl_final\")\n",
        "targets_gpkg_dir = join(targets_dir, \"gpkg_final\")\n",
        "targets_user_csv_dir = join(targets_dir, \"csv\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(targets_dir, exist_ok=True)\n",
        "makedirs(targets_user_csv_dir, exist_ok=True)\n",
        "makedirs(pkl_final_dir, exist_ok=True)\n",
        "makedirs(targets_gpkg_dir, exist_ok=True)\n",
        "makedirs(gedi_links_dir, exist_ok=True)\n",
        "makedirs(gedi_downloads_dir, exist_ok=True)\n",
        "makedirs(h5_pkl_cache_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56r29iR6l6Pc"
      },
      "outputs": [],
      "source": [
        "# Global function from pyGEDI (simplified)\n",
        "def getLayer(layer, files):\n",
        "    dictionary = {}\n",
        "    for h5_file in files:\n",
        "        layers , stack = [] , [h5_file['BEAM0000']]\n",
        "        while stack:\n",
        "            item = stack.pop()\n",
        "            if isinstance(item, h5py.Dataset):\n",
        "                layers.append(item.name.replace('/BEAM0000/', ''))\n",
        "            elif isinstance(item, h5py.Group):\n",
        "                stack.extend(item.values())\n",
        "        filtered_layers = [l for l in layers if layer in l]\n",
        "        if filtered_layers:\n",
        "            dictionary[h5_file.filename] = filtered_layers\n",
        "    return dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwQ99PbzH3o"
      },
      "source": [
        "#GEDI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_EYCjhSPKoK"
      },
      "source": [
        "## Get GEDI download links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96kV5Y_Y69KX"
      },
      "outputs": [],
      "source": [
        "# Use the rectangular extent of the project area to create a GEDI download area\n",
        "\n",
        "crs_epsg = 4326\n",
        "project_area_path = join(polygons_dir, 'project_area.gpkg')\n",
        "if exists(project_area_path):\n",
        "  print(\"Project polygon found:\\n\")\n",
        "  # Read project polygon\n",
        "  project_area_read = gpd.read_file(join(polygons_dir, 'project_area.gpkg'))\n",
        "  display(project_area_read[\"geometry\"].iloc[0])\n",
        "  if project_area_read.crs.to_epsg() == crs_epsg:\n",
        "    print(f\"The project polygon has the correct CRS (EPSG:{crs_epsg})\")\n",
        "    # Create the GEDI area polygon\n",
        "    gedi_area_path = join(polygons_dir, 'gedi_area.gpkg')\n",
        "    if not exists (gedi_area_path):\n",
        "      gedi_area = box(*project_area_read.total_bounds)\n",
        "      gdf = gpd.GeoDataFrame(geometry=[gedi_area], crs=f\"EPSG:{crs_epsg}\")\n",
        "      gdf.to_file(gedi_area_path, driver='GPKG')\n",
        "      print(f\"Created a bounding box for the GEDI area polygon: {gedi_area_path}\")\n",
        "    else: print(f\"Project area has already been bound to a box for GEDI downloads: {gedi_area_path}\")\n",
        "    # Create an inverse GEDI area polygon for potential masking\n",
        "    inverse_gedi_area_path = join(polygons_dir, \"gedi_area_inverse.gpkg\")\n",
        "    if not exists(inverse_gedi_area_path):\n",
        "      template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "      template_polygon = gpd.read_file(template_polygon_path)\n",
        "      gedi_area_polygon = gpd.read_file(gedi_area_path)\n",
        "      inverse_gedi_area_polygon = template_polygon['geometry'].difference(gedi_area_polygon['geometry']).iloc[0]\n",
        "      inverse_gedi_area_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_gedi_area_polygon]}, crs=f\"EPSG:{crs_epsg}\")\n",
        "      inverse_gedi_area_polygon_gdf.to_file(inverse_gedi_area_path, driver=\"GPKG\")\n",
        "      print(\"An inversed GEDI polygon for masking has been created.\")\n",
        "    else: print(\"An inversed GEDI polygon for masking already exists.\")\n",
        "\n",
        "else: print(\"Create 'project_area.gpkg' and upload to 1_areas/polygons\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU5rP8U6PMra"
      },
      "outputs": [],
      "source": [
        "# Define GEDI area in which to download GEDI data\n",
        "gedi_area_path = join(polygons_dir, \"gedi_area.gpkg\")\n",
        "if not exists(gedi_area_path):\n",
        "  print(f\"Upload 'gedi_area.gpkg' to {polygons_dir}.\")\n",
        "else:\n",
        "  gedi_area = gpd.read_file(gedi_area_path)\n",
        "  gedi_area.geometry = gedi_area.geometry.apply(orient, args=(1,))\n",
        "\n",
        "  # https://github.com/ornldaac/gedi_tutorials/blob/main/1_gedi_l4a_search_download.ipynb\n",
        "  # CMR API base url\n",
        "  cmrurl='https://cmr.earthdata.nasa.gov/search/'\n",
        "\n",
        "  # GEDI product DOI list\n",
        "  gedi_doi_list = {\n",
        "  'GEDI01_B': '10.5067/GEDI/GEDI01_B.002', # GEDI L1B\n",
        "  'GEDI02_A': '10.5067/GEDI/GEDI02_A.002', # GEDI L2A\n",
        "  'GEDI02_B': '10.5067/GEDI/GEDI02_B.002', # GEDI L2B\n",
        "  'GEDI04_A': '10.3334/ORNLDAAC/2056', # GEDI L4A\n",
        "  }\n",
        "\n",
        "  # Hardcoded concept IDs for reliability\n",
        "  gedi_concept_ids = {\n",
        "    'GEDI01_B': 'C2142749196-LPCLOUD',\n",
        "    'GEDI02_A': 'C2142771958-LPCLOUD',\n",
        "    'GEDI02_B': 'C2142776747-LPCLOUD',\n",
        "    'GEDI04_A': 'C2237824918-ORNL_CLOUD'\n",
        "  }\n",
        "\n",
        "  # Provider mapping for different GEDI products\n",
        "  gedi_providers = {\n",
        "    'GEDI01_B': 'LPCLOUD',\n",
        "    'GEDI02_A': 'LPCLOUD',\n",
        "    'GEDI02_B': 'LPCLOUD',\n",
        "    'GEDI04_A': 'ORNL_CLOUD'\n",
        "  }\n",
        "\n",
        "  for gedi_product, doi in gedi_doi_list.items():\n",
        "    # Use hardcoded concept_id if available, otherwise retrieve from DOI\n",
        "    if gedi_concept_ids[gedi_product]:\n",
        "      concept_id = gedi_concept_ids[gedi_product]\n",
        "    else:\n",
        "      response = requests.get(cmrurl + 'collections.json?doi=' + doi)\n",
        "      response.raise_for_status()\n",
        "      concept_id = response.json()['feed']['entry'][0]['id']\n",
        "\n",
        "    geojson = {\"shapefile\": (\"gedi_area.geojson\", gedi_area.geometry.to_json(), \"application/geo+json\")}\n",
        "    page_num, page_size = 1, 2000 # CMR page size limit\n",
        "    granule_arr = []\n",
        "    while True:\n",
        "        # Set up provider-specific parameters\n",
        "        if gedi_providers[gedi_product]:\n",
        "            cmr_param = {\"collection_concept_id\": concept_id, \"page_size\": page_size, \"page_num\": page_num,\n",
        "                \"simplify-shapefile\": 'true', # Needed to bypass 5000 coordinates limit of CMR\n",
        "                \"provider\": gedi_providers[gedi_product]  # Set correct provider\n",
        "            }\n",
        "        else:\n",
        "            # Use default parameters when provider not specified\n",
        "            cmr_param = {\"collection_concept_id\": concept_id, \"page_size\": page_size, \"page_num\": page_num,\n",
        "                \"simplify-shapefile\": 'true' # Needed to bypass 5000 coordinates limit of CMR\n",
        "            }\n",
        "\n",
        "        granulesearch = cmrurl + 'granules.json'\n",
        "        response = requests.post(granulesearch, data=cmr_param, files=geojson)\n",
        "        granules = response.json()['feed']['entry']\n",
        "        if granules:\n",
        "            for g in granules:\n",
        "                granule_url, granule_size, granule_poly = None, None, None\n",
        "                for links in g['links']:\n",
        "                  if links['href'].endswith('.h5') and not links['href'].startswith('https://opendap') and not links['href'].startswith('s3'):\n",
        "                      granule_url = links['href']\n",
        "                if granule_url != None: # Some GEDI2A granules do not have links in the metadata\n",
        "                  # read file size\n",
        "                  granule_size = float(g['granule_size'])\n",
        "                  # reading bounding geometries\n",
        "                  if 'polygons' in g:\n",
        "                      polygons= g['polygons']\n",
        "                      multipolygons = []\n",
        "                      for poly in polygons:\n",
        "                          i=iter(poly[0].split (\" \"))\n",
        "                          ltln = list(map(\" \".join,zip(i,i)))\n",
        "                          multipolygons.append(Polygon([[float(p.split(\" \")[1]), float(p.split(\" \")[0])] for p in ltln]))\n",
        "                      granule_poly = MultiPolygon(multipolygons)\n",
        "                  # Get URL to HDF5 files\n",
        "                  granule_arr.append([granule_url, granule_size, granule_poly])\n",
        "            page_num += 1\n",
        "        else: break\n",
        "\n",
        "    # Create pandas dataframe\n",
        "    doi_df = pd.DataFrame(granule_arr, columns=[\"granule_url\", \"granule_size\", \"granule_poly\"])\n",
        "    # Drop granules with empty geometry\n",
        "    gedi_product_granules = doi_df[doi_df['granule_poly'] != \"\"]\n",
        "    print(f\"Total granules found for {gedi_product}: {len(doi_df.index)-1}\")\n",
        "    print(f\"Total file size for {gedi_product}: {round(doi_df['granule_size'].sum() / 1024, 3)}GB\\n\")\n",
        "\n",
        "    # Export links list\n",
        "    gedi_product_links_path = join(gedi_links_dir, f\"{gedi_product}_links.txt\")\n",
        "    gedi_product_granules.to_csv(gedi_product_links_path, columns = ['granule_url'], index=False, header = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdXO-FMQ-E0w"
      },
      "source": [
        "## Download GEDI .h5 files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEHuWS3R9_Eu"
      },
      "outputs": [],
      "source": [
        "# earthdata.nasa.gov server credentials\n",
        "print(\"Enter your Earthdata login details...\")\n",
        "username = input(\"Username: \")\n",
        "password = getpass(\"Password: \")\n",
        "manager = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
        "manager.add_password(None, \"https://urs.earthdata.nasa.gov\", username, password)\n",
        "auth = urllib.request.HTTPBasicAuthHandler(manager)\n",
        "cookie_jar = CookieJar()\n",
        "opener = urllib.request.build_opener(\n",
        "    urllib.request.HTTPBasicAuthHandler(manager),\n",
        "    urllib.request.HTTPCookieProcessor(cookie_jar))\n",
        "urllib.request.install_opener(opener)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKVd_Ks_-KOp"
      },
      "outputs": [],
      "source": [
        "# Get list of .txt files in the 'links' directory\n",
        "link_files = []\n",
        "for file in os.listdir(gedi_links_dir):\n",
        "  link_files.append(file)\n",
        "\n",
        "print('# Select downloads')\n",
        "print(\"downloads = [\")\n",
        "for link_file in sorted(link_files):\n",
        "  print(f'  \"{link_file}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcBdA4URR0C9"
      },
      "outputs": [],
      "source": [
        "# Select downloads\n",
        "downloads = [\n",
        "  # \"GEDI01_B_links.txt\",\n",
        "  # \"GEDI02_A_links.txt\",\n",
        "  # \"GEDI02_B_links.txt\",\n",
        "  \"GEDI04_A_links.txt\",\n",
        "]\n",
        "\n",
        "# Process URLs\n",
        "for text_file in downloads:\n",
        "  text_file_path = join(gedi_links_dir, text_file)\n",
        "  with open(text_file_path, 'r') as file:\n",
        "    url_list = file.readlines()\n",
        "  url_list = [url.strip() for url in url_list] # Remove any white space\n",
        "  product = text_file[:8]\n",
        "  # Display progress\n",
        "  index = 0\n",
        "  progress_label = widgets.Label(value=f\"{product} progress: {index}/{len(url_list)} files downloaded.\")\n",
        "  display(progress_label)\n",
        "  for url in url_list:\n",
        "    product = re.search('GEDI0.*?_.*?\\w', url).group()\n",
        "    product_dir = join(gedi_downloads_dir, product)\n",
        "    makedirs(product_dir, exist_ok=True)\n",
        "    product_filename = url.split(path.sep)[-1]\n",
        "    product_file_path = join(product_dir, product_filename)\n",
        "    if not exists(product_file_path):\n",
        "      try:\n",
        "        request = urllib.request.Request(url)\n",
        "        response = urllib.request.urlopen(request)\n",
        "        body = response.read()\n",
        "        open(product_file_path, 'wb').write(body)\n",
        "      except Exception as e:\n",
        "        print(f\"Failed URL in {product}: {url}\")\n",
        "        print(f\"Error: {e}\")\n",
        "    # Update progress\n",
        "    index += 1\n",
        "    progress_label.value = f\"{product} progress: {index}/{len(url_list)} files downloaded.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cakvw5x97t-"
      },
      "source": [
        "## Convert GEDI from .h5 to .pkl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vP2jARV-M2O"
      },
      "outputs": [],
      "source": [
        "# Load project and GEDI area polygons (filters to extent)\n",
        "project_area_polygon_path = join(polygons_dir, 'project_area.gpkg')\n",
        "project_area = gpd.read_file(project_area_polygon_path)\n",
        "print(\"Project area polygon:\")\n",
        "display(project_area[\"geometry\"].iloc[0])\n",
        "\n",
        "gedi_area_polygon_path = join(polygons_dir, \"gedi_area.gpkg\")\n",
        "gedi_area = gpd.read_file(gedi_area_polygon_path)[\"geometry\"].iloc[0]\n",
        "gedi_area_nw_lon, gedi_area_se_lat, gedi_area_se_lon, gedi_area_nw_lat = gedi_area.bounds\n",
        "print(f\"GEDI area polygon:\")\n",
        "display(gedi_area)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3NTdTt1-P85"
      },
      "outputs": [],
      "source": [
        "# Print GEDI products in the 'downloads' directory\n",
        "gedi_products = sorted(listdir(gedi_downloads_dir))\n",
        "print(f'Found the following GEDI gedi products in the GEDI downloads directory:\\n {gedi_products}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D1RmrHS-Qq7"
      },
      "outputs": [],
      "source": [
        "# Print GEDI product parameters and data types\n",
        "dict_gedi_parameters = {}\n",
        "for product in gedi_products:\n",
        "  product_dir = join(gedi_downloads_dir, product)\n",
        "  # Sample a single .H5 file\n",
        "  sample_file_path = listdir(product_dir)[0]\n",
        "  h5_file = h5py.File(join(product_dir, sample_file_path), 'r')\n",
        "  layers = list(getLayer('', [h5_file]).values())[0]\n",
        "  # Sample from beam '0000'\n",
        "  compatible_shape = (len(h5_file['BEAM0000']['shot_number']), )\n",
        "  dict_layer = {}\n",
        "  for layer in layers:\n",
        "    layer_shape = h5_file['BEAM0000'][layer].shape\n",
        "    if layer_shape == compatible_shape:\n",
        "      # Get data types\n",
        "      df_layer = pd.DataFrame()\n",
        "      df_layer[layer] = h5_file['BEAM0000'][layer]\n",
        "      # Add parameter name and data type to dictionary\n",
        "      dict_layer[layer] = str(df_layer[layer].dtype)\n",
        "  dict_gedi_parameters[product] = dict_layer\n",
        "# Print config\n",
        "print(\"Copy selected targets into the next cell.\\n\")\n",
        "pp = pprint.PrettyPrinter(indent=1)\n",
        "pp.pprint(dict_gedi_parameters)\n",
        "# Close files\n",
        "h5_file.close\n",
        "df_layer = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPhNthZN-Tbp"
      },
      "outputs": [],
      "source": [
        "# Copy and paste GEDI parameters for inclusion in .pkl outputs.\n",
        "# Each parameter requires additional memory and time to process.\n",
        "# Changing the data type (e.g. float32 -> float16) may increase performance,\n",
        "# but can reduce precision and cause other issues.\n",
        "# Include at least beam, lat_lowestmode, lon_lowestmode, shot_number\n",
        "# and ONE quality flag for each product.\n",
        "\n",
        "supported_data_types = [\"int8\",\"int16\",\"int32\",\"int64\",\"uint8\",\"uint16\",\"uint32\",\"uint64\",\"float16\",\"float32\",\"float64\",\"str\"]\n",
        "\n",
        "selected_parameters = {\n",
        "\n",
        "  'GEDI04_A': {\n",
        "    'agbd': 'float32',\n",
        "    'agbd_se': 'float32',\n",
        "    'beam': 'str',\n",
        "    'elev_lowestmode': 'float32',\n",
        "    'lat_lowestmode': 'float32',\n",
        "    'lon_lowestmode': 'float32',\n",
        "    'l4_quality_flag': 'uint8',\n",
        "    'sensitivity': 'float32',\n",
        "    'shot_number': 'str',\n",
        "  },\n",
        "\n",
        "  # 'GEDI02_A': {\n",
        "  #   'rh_parameters': 'float16', # e.g. rh25, rh50, rh75, rh95, rh98, modify in block below\n",
        "  #   'beam': 'object',\n",
        "  #   # 'elev_highestreturn': 'float32',\n",
        "  #   'elev_lowestmode': 'float32',\n",
        "  #   'lat_lowestmode': 'float32',\n",
        "  #   'lon_lowestmode': 'float32',\n",
        "  #   'quality_flag': 'uint8',\n",
        "  #   'sensitivity': 'float32',\n",
        "  #   'shot_number': 'str',\n",
        "  # },\n",
        "\n",
        "  # 'GEDI02_B': {\n",
        "  #   'beam': 'object',\n",
        "  #   'cover': 'float32',\n",
        "  #   'fhd_normal': 'float32',\n",
        "  #   'omega': 'float32',\n",
        "  #   'pai': 'float32',\n",
        "  #   'geolocation/lat_lowestmode': 'float32',\n",
        "  #   'geolocation/lon_lowestmode': 'float32',\n",
        "  #   'l2b_quality_flag': 'uint8',\n",
        "  #   'sensitivity': 'float32',\n",
        "  #   'shot_number': 'str',\n",
        "  # },\n",
        "\n",
        "}\n",
        "\n",
        "# Check if GEDI products in the selected parameters match those in the downloads directory\n",
        "assert set(selected_parameters.keys()).issubset(set(gedi_products)), f\"GEDI products in selected parameters do not match those in {gedi_downloads_dir}\"\n",
        "# Check if parameters match those in the selected_parameters dictionary\n",
        "for product in selected_parameters.keys():\n",
        "  parameter_list = list(dict_gedi_parameters[product].keys())\n",
        "  if product == 'GEDI02_A': parameter_list.append(\"rh_parameters\") # These are custom parameters added later\n",
        "  assert set(selected_parameters[product]).issubset(set(parameter_list)), f\"{product}'s selected parameters are not all available\"\n",
        "# Check that data types are supported\n",
        "data_types_list = []\n",
        "for key in selected_parameters.keys():\n",
        "  for data_type in list(selected_parameters[key].values()):\n",
        "    data_types_list.append(data_type)\n",
        "# Remove duplicates for assert\n",
        "data_types_list = list(dict.fromkeys(data_types_list))\n",
        "assert set(data_types_list).issubset(set(supported_data_types)), \"An unsupported data type has been selected.\"\n",
        "print(\"All selected GEDI products, parameters and data types successfully applied for conversion from .h5 to .pkl format.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G9MrbXo5QLdL"
      },
      "outputs": [],
      "source": [
        "# Convert .h5 files to .pkl\n",
        "\n",
        "use_project_area = False  # Toggle to use complex project area bounds, will impact performance.\n",
        "test_processing = False  # Toggle for testing mode to limit the number of files processed.\n",
        "test_number = 20  # Number of files to process in test mode.\n",
        "sensitivity_threshold = 0.95  # Sensitivity threshold for data filtering.\n",
        "print_filename = True  # Toggle to print the filename being processed.\n",
        "max_attempts = 3 # At trying to open an .h5 file if it initially fails\n",
        "delay = 5 # Waiting between attempts\n",
        "\n",
        "# Loop through products and their parameters.\n",
        "for product, parameters in selected_parameters.items():\n",
        "    # Set up directories for caching and downloading.\n",
        "    gedi_h5_pkl_cache_product_dir = join(h5_pkl_cache_dir, product)\n",
        "    makedirs(gedi_h5_pkl_cache_product_dir, exist_ok=True)\n",
        "    gedi_downloads_product_dir = join(gedi_downloads_dir, product)\n",
        "\n",
        "    # List .h5 files to be processed.\n",
        "    h5_data = [[join(gedi_downloads_product_dir, h5_file_dir), parameters, gedi_h5_pkl_cache_product_dir] for h5_file_dir in listdir(gedi_downloads_product_dir)]\n",
        "\n",
        "    # Initialize progress display.\n",
        "    index = 0\n",
        "    progress_label = widgets.Label(value=f\"{product} progress: {index}/{len(h5_data)} H5 files converted to a .pkl cache.\")\n",
        "    display(progress_label)\n",
        "\n",
        "    # Processing loop for each file.\n",
        "    for filename, parameters_dict, gedi_h5_pkl_cache_product_dir in h5_data:\n",
        "\n",
        "        # Construct the destination filename for the .pkl file\n",
        "        dst_filename = join(gedi_h5_pkl_cache_product_dir, path.splitext(path.split(filename)[-1])[0]) + '.pkl'\n",
        "\n",
        "        # Check if .pkl file already exists to skip processing\n",
        "        if not exists(dst_filename):\n",
        "            if print_filename: print(f\"Processing started: {filename.split('/')[-1]}\")\n",
        "\n",
        "            # Extract data from h5 first (keeping it open is unstable on Google Drive)\n",
        "            h5_file = {}\n",
        "            for attempt in range(max_attempts):\n",
        "                try:\n",
        "                    with h5py.File(filename, 'r') as h5:\n",
        "                        for beam in ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011']:\n",
        "                            h5_file[beam] = {}\n",
        "                            latlayer, lonlayer = ('lat_lowestmode', 'lon_lowestmode') if 'lat_lowestmode' in h5[beam] else ('geolocation/lat_lowestmode', 'geolocation/lon_lowestmode')\n",
        "                            h5_file[beam][latlayer] = h5[beam][latlayer][:]\n",
        "                            h5_file[beam][lonlayer] = h5[beam][lonlayer][:]\n",
        "                            h5_file[beam]['shot_number'] = h5[beam]['shot_number'][:].astype(parameters_dict['shot_number'])\n",
        "                            for layer in parameters_dict.keys():\n",
        "                                if layer in h5[beam]: h5_file[beam][layer] = h5[beam][layer][:]\n",
        "                                elif layer == 'rh_parameters': h5_file[beam]['rh'] = h5[beam]['rh'][:, :100]\n",
        "                    break  # If successful, break the retry loop\n",
        "                except Exception as e:\n",
        "                    if attempt < max_attempts - 1:\n",
        "                        print(f\"Error opening file (attempt {attempt + 1}/{max_attempts}): {str(e)}\")\n",
        "                        print(f\"Retrying in {delay} seconds...\")\n",
        "                        sleep(delay)\n",
        "                    else:\n",
        "                        print(f\"Failed to open file after {max_attempts} attempts. Skipping.\")\n",
        "                        continue  # Skip to the next file in the outer loop\n",
        "            if attempt == max_attempts - 1: continue  # Skip to the next file if all attempts failed\n",
        "\n",
        "            layer_df_list = []  # List to store data frames for each layer and beam combination.\n",
        "            first_layer = True # For shot_number and beam to be added\n",
        "\n",
        "            # Iterate through each parameter layer, skipping 'beam' and 'shot_number'.\n",
        "            for layer in [layer for layer in parameters_dict.keys() if layer not in ('beam', 'shot_number')]:\n",
        "                beam_df_list = []\n",
        "                # Iterate through each beam.\n",
        "                for beam in ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011', 'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011']:\n",
        "                    # Define geospatial extent.\n",
        "                    latlayer, lonlayer = 'lat_lowestmode', 'lon_lowestmode'\n",
        "                    if latlayer not in h5_file[beam]:\n",
        "                        latlayer, lonlayer = 'geolocation/lat_lowestmode', 'geolocation/lon_lowestmode'\n",
        "                    lat_array = h5_file[beam][latlayer]\n",
        "                    lon_array = h5_file[beam][lonlayer]\n",
        "\n",
        "                    # Calculate index based on geographic filters.\n",
        "                    if use_project_area:\n",
        "                        project_polygon = project_area.geometry.iloc[0]\n",
        "                        gedi_points = gpd.GeoDataFrame(geometry=gpd.points_from_xy(lon_array, lat_array), crs=\"EPSG:4326\")\n",
        "                        points_tree = STRtree(gedi_points.geometry.values)\n",
        "                        points_tree_indices = points_tree.query(project_polygon, predicate='contains')\n",
        "                        if len(points_tree_indices) > 0: geo_index = points_tree_indices\n",
        "                        else: geo_index = np.array([], dtype=int)\n",
        "                    else: geo_index = np.where((lat_array > gedi_area_se_lat) & (lat_array < gedi_area_nw_lat) & (lon_array > gedi_area_nw_lon) & (lon_array < gedi_area_se_lon))[0]\n",
        "\n",
        "                    # Collect data for each layer and beam, handling special cases.\n",
        "                    beam_df = pd.DataFrame()\n",
        "                    if layer == 'rh_parameters':\n",
        "                        # All rh values\n",
        "                        data = h5_file[beam]['rh'][geo_index][:, :100]\n",
        "                        columns = [f'rh{i+1}' for i in range(100)]\n",
        "                        # rh values in intervals of 5\n",
        "                        # data = h5_file[beam]['rh'][geo_index][:, [0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 74, 79, 84, 89, 94, 97, 99]]\n",
        "                        # columns = ['rh1', 'rh5', 'rh10', 'rh15', 'rh20', 'rh25', 'rh30', 'rh35', 'rh40', 'rh45', 'rh50', 'rh55', 'rh60', 'rh65', 'rh70', 'rh75', 'rh80', 'rh85', 'rh90', 'rh95', 'rh98', 'rh100']\n",
        "                        beam_df = pd.DataFrame(data, columns=columns)\n",
        "                    elif h5_file[beam][layer].shape == (len(lon_array), ):\n",
        "                        beam_df[layer] = h5_file[beam][layer][geo_index].astype(parameters_dict[layer])\n",
        "\n",
        "                    if len(beam_df) > 0:\n",
        "                      if first_layer:\n",
        "                        beam_df.insert(0, 'shot_number', h5_file[beam]['shot_number'][geo_index].astype(parameters_dict['shot_number']))\n",
        "                        beam_df.insert(1, 'beam', beam)\n",
        "\n",
        "                    beam_df_list.append(beam_df)\n",
        "\n",
        "                # Concatenate data frames for all beams within the same layer.\n",
        "                if len(beam_df_list) > 0:\n",
        "                    layer_df = pd.concat(beam_df_list)\n",
        "                    layer_df_list.append(layer_df)\n",
        "                    first_layer = False\n",
        "\n",
        "            first_layer = True # Restart first layer for adding shot_number and beam\n",
        "\n",
        "            # Concatenate all layer data frames for the file.\n",
        "            if len(layer_df_list) > 0:\n",
        "                h5_df = pd.concat(layer_df_list, axis=1)\n",
        "                # Define timestamp and convert geodataframe.\n",
        "                for part in filename.split('_'):\n",
        "                  try: timestamp = pd.to_datetime(part, format='%Y%j%H%M%S', utc=True)\n",
        "                  except: continue\n",
        "                h5_df['timestamp'] = timestamp\n",
        "                # Apply quality and sensitivity filters.\n",
        "                quality_flag = [key for key in parameters_dict.keys() if 'quality_flag' in key][0]\n",
        "                h5_df = h5_df.loc[(h5_df[quality_flag] == 1) & (h5_df['sensitivity'] >= sensitivity_threshold)]\n",
        "                h5_df = h5_df.drop(columns=[quality_flag])\n",
        "                # Final type check and conversion if necessary\n",
        "                for column, dtype in parameters_dict.items():\n",
        "                    if column in h5_df.columns:\n",
        "                        h5_df[column] = h5_df[column].astype(dtype)\n",
        "                    elif column == 'rh_parameters':\n",
        "                        rh_columns = [col for col in h5_df.columns if col.startswith('rh')]\n",
        "                        h5_df[rh_columns] = h5_df[rh_columns].astype(dtype)\n",
        "                # Export to pickle\n",
        "                h5_df.to_pickle(dst_filename)\n",
        "                if print_filename: print(f\"Processing complete: {filename.split('/')[-1]}\")\n",
        "\n",
        "        # Update progress display.\n",
        "        index += 1\n",
        "        progress_label.value = f\"{product} progress: {index}/{len(h5_data)} H5 files converted to a .pkl cache.\"\n",
        "        if test_processing and index == test_number: break  # Break after processing specified number of files if in test mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_WHSbQ-z9EH9"
      },
      "outputs": [],
      "source": [
        "# Get list of proucts with cache files\n",
        "print('products_to_finalise = [')\n",
        "for product in os.listdir(h5_pkl_cache_dir):\n",
        "  print(f\"'{product}',\")\n",
        "print(']')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DcQXeovt9DAt"
      },
      "outputs": [],
      "source": [
        "products_to_finalise = [\n",
        "'GEDI04_A',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m8WL0qTcUwAV"
      },
      "outputs": [],
      "source": [
        "for product in products_to_finalise:\n",
        "  final_pkl_path = join(pkl_final_dir, f\"{product}.pkl\")\n",
        "  product_cache_dir = join(h5_pkl_cache_dir, product)\n",
        "  # List prevents duplicates (e.g. ' (1)' from being added)\n",
        "  cache_files = [f for f in os.listdir(product_cache_dir) if ' (' not in f]\n",
        "  if len(cache_files) > 0:\n",
        "    # Progress\n",
        "    final_pkl_index = 0\n",
        "    final_pkl_progress_label = widgets.Label(value=f\"{product} progress: {final_pkl_index}/{len(cache_files)} .pkl caches appended to list.\")\n",
        "    display(final_pkl_progress_label)\n",
        "    # Initiate cache dataframe list\n",
        "    cache_dataframe_list = []\n",
        "    # Loop through cache files, appending each to the list\n",
        "    for cache in cache_files:\n",
        "      chunk_cache = pd.read_pickle(join(product_cache_dir, cache))\n",
        "      if len(chunk_cache) > 0: # Check the chunk isn't empty\n",
        "          cache_dataframe_list.append(chunk_cache)\n",
        "      final_pkl_index += 1\n",
        "      final_pkl_progress_label.value = f\"{product} progress: {final_pkl_index}/{len(cache_files)} .pkl caches appended to list.\"\n",
        "    # Concatenate and save final .pkl file\n",
        "    print(\"Concatenating list of cache dataframes.\")\n",
        "    final_pkl_df_concat = pd.concat(cache_dataframe_list)\n",
        "    cache_dataframe_list = [] # Clear list\n",
        "    # Make geodataframe\n",
        "    print(\"Converting to a geodataframe.\")\n",
        "    final_pkl_gdf_concat = gpd.GeoDataFrame(final_pkl_df_concat,\n",
        "        geometry=gpd.points_from_xy(final_pkl_df_concat['lon_lowestmode'],\n",
        "                                    final_pkl_df_concat['lat_lowestmode'], crs=\"EPSG:4326\"))\n",
        "    final_pkl_gdf_concat = final_pkl_gdf_concat.drop(columns=['lat_lowestmode', 'lon_lowestmode'])\n",
        "    # Save the final converted dataframe\n",
        "    final_pkl_gdf_concat = final_pkl_gdf_concat.reset_index(drop=True)\n",
        "    final_pkl_gdf_concat.to_pickle(f\"{final_pkl_path}\")\n",
        "    print(f\"{product} final .pkl complete.\")\n",
        "  else: print(f\"No cache data found for {product}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktTAxR_e-YBS"
      },
      "source": [
        "## Convert GEDI from .pkl to .gpkg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPjknGwnmDpq"
      },
      "outputs": [],
      "source": [
        "# For verification and visualisation in GIS software\n",
        "# Get list of .txt files in the 'links' directory\n",
        "pkl_final_list = []\n",
        "for file in os.listdir(pkl_final_dir):\n",
        "  pkl_final_list.append(file)\n",
        "\n",
        "# Select final .pkl file to convert to .gpkg\n",
        "for pkl_final in sorted(pkl_final_list):\n",
        "  print(f\"pkl_to_convert = '{pkl_final}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9fChPk4nL1V"
      },
      "outputs": [],
      "source": [
        "pkl_to_convert = 'GEDI04_A.pkl'\n",
        "\n",
        "# Load final .pkl\n",
        "pkl_to_convert_dir = join(pkl_final_dir, pkl_to_convert)\n",
        "pkl_to_convert_df = pd.read_pickle(pkl_to_convert_dir)\n",
        "print(f\"Final .pkl has {len(pkl_to_convert_df)} rows.\\n\\n\")\n",
        "\n",
        "# Print available parameters in final .pkl for selection\n",
        "print(f'selected_parameters = [')\n",
        "for column in pkl_to_convert_df.columns:\n",
        "    if column in ('geometry'):\n",
        "        continue\n",
        "    print(f\"\\t'{column}',\")\n",
        "print(f']')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt0gwgr6-bmA"
      },
      "outputs": [],
      "source": [
        "# Select from available parameters in .pkl\n",
        "\n",
        "selected_parameters = [\n",
        "\t'shot_number',\n",
        "\t'beam',\n",
        "\t'agbd',\n",
        "\t'agbd_se',\n",
        "\t'elev_lowestmode',\n",
        "\t'sensitivity',\n",
        "\t'timestamp',\n",
        "]\n",
        "\n",
        "assert all([selected_parameter in pkl_to_convert_df.columns for selected_parameter in selected_parameters])\n",
        "final_gpkg_file_dir = join(targets_gpkg_dir, f'{pkl_to_convert[:-4]}.gpkg')\n",
        "if exists(final_gpkg_file_dir):\n",
        "  print(f'File {final_gpkg_file_dir} already exists.')\n",
        "else:\n",
        "  print(f'Converting final {pkl_to_convert} to .gpkg.')\n",
        "  df_selected_parameters = pkl_to_convert_df[['geometry', *selected_parameters]]\n",
        "  gpkg_gdf = gpd.GeoDataFrame(df_selected_parameters, geometry='geometry')\n",
        "  gpkg_gdf = gpkg_gdf.set_crs(4326) # WGS84\n",
        "  gpkg_gdf.to_file(final_gpkg_file_dir, driver=\"GPKG\")\n",
        "  print(f'Saved to {final_gpkg_file_dir}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BgtsxUKo8NT"
      },
      "source": [
        "# User uploaded target data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YldhycmXpCrB"
      },
      "outputs": [],
      "source": [
        "# Upload of any spatial data for XGBoost prediction.\n",
        "# Place .csv(s) with 'x' and 'y' columns in '2_targets/csv'.\n",
        "\n",
        "print(\"Select 'user targets' csv to be compiled...\\n\")\n",
        "for file in os.listdir(targets_user_csv_dir):\n",
        "  if file.endswith(\".csv\"):\n",
        "    print(f'user_uploaded_targets = \"{file}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlWNwcSMVz3P"
      },
      "outputs": [],
      "source": [
        "user_uploaded_targets = \"\"\n",
        "\n",
        "user_targets_csv = pd.read_csv(join(targets_user_csv_dir, user_uploaded_targets))\n",
        "\n",
        "print(\"Select targets columns to transfer to the dataset...\\n\")\n",
        "print(\"targets_columns = [\")\n",
        "for column in list(user_targets_csv.columns):\n",
        "  print(f'  \"{column}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh1w1kHu9qP2"
      },
      "outputs": [],
      "source": [
        "targets_columns = [\n",
        "\n",
        "]\n",
        "\n",
        "export_gpkg = True\n",
        "\n",
        "# Add column flags\n",
        "dataset_targets_columns = [(targets_column, f\"tar_{targets_column}\") for targets_column in targets_columns]\n",
        "dataset_targets_columns_dict = dict(dataset_targets_columns)\n",
        "\n",
        "# Drop any columns which aren't selected (keep 'x' and 'y')\n",
        "user_targets_csv.drop(columns=[col for col in user_targets_csv.columns.values if col not in list(dataset_targets_columns_dict.keys()) and col != \"x\" and col != \"y\"], inplace=True)\n",
        "\n",
        "# Create geometry\n",
        "try:\n",
        "  user_uploaded_coord_cols = [\"x\", \"y\"]\n",
        "  user_targets_geometry = gpd.GeoDataFrame(user_targets_csv, geometry=gpd.points_from_xy(user_targets_csv.x, user_targets_csv.y,  crs=\"EPSG:4326\"))\n",
        "  user_targets_geometry.drop(user_uploaded_coord_cols, axis=1, inplace=True)\n",
        "except:\n",
        "  user_uploaded_coord_cols = [\"lon_lowestmode\", \"lat_lowestmode\"]\n",
        "  user_targets_geometry = gpd.GeoDataFrame(user_targets_csv, geometry=gpd.points_from_xy(user_targets_csv.lon_lowestmode, user_targets_csv.lat_lowestmode,  crs=\"EPSG:4326\"))\n",
        "  user_targets_geometry.drop(user_uploaded_coord_cols, axis=1, inplace=True)\n",
        "\n",
        "user_targets_final = f\"user_{user_uploaded_targets[:-4]}_{datetime.utcnow().strftime('%y%m%d_%H%M%S')}\"\n",
        "\n",
        "# Export to .pkl\n",
        "user_targets_geometry.to_pickle(join(pkl_final_dir, f\"{user_targets_final}.pkl\"))\n",
        "\n",
        "# Check .pkl\n",
        "pd.read_pickle(join(pkl_final_dir, f\"{user_targets_final}.pkl\"))\n",
        "\n",
        "# Export to .gpkg\n",
        "if export_gpkg:\n",
        "  user_targets_gpkg = join(targets_gpkg_dir, f\"{user_targets_final}.gpkg\")\n",
        "  user_targets_geometry.to_file(user_targets_gpkg, driver=\"GPKG\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVQP-Oy3i_jC"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R4NjWtVhQLL"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "fUfm-wkD9x1i"
      ],
      "gpuType": "V5E1",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}