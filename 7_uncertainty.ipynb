{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/7_uncertainty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhx977-WIaWJ"
      },
      "source": [
        "# Imports and directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRnNty96IThZ"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4n2MGF0aZtJ"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpmgsb8KKMGN"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "try: import cupy # Only works on GPU runtime\n",
        "except: None\n",
        "from contextlib import contextmanager\n",
        "import gc\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import ipywidgets as widgets\n",
        "import json\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy.random import normal\n",
        "from os import makedirs\n",
        "from os.path import exists, join\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "import scipy.stats as st\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQwNO9T0_HPH"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "feature_dir = join(base_dir, \"3_features\")\n",
        "feature_final_dir = join(feature_dir, \"final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "masks_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(uncertainty_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koHn-AiGG7KM"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = [\"COMPRESS=DEFLATE\", \"PREDICTOR=3\", \"ZLEVEL=9\"]\n",
        "    else: options = []\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = vector = None\n",
        "    try:\n",
        "        raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "        vector = ogr.Open(polygon_path)\n",
        "        if not raster or not vector:\n",
        "            raise ValueError(\"Cannot open input files\")\n",
        "        layer = vector.GetLayer()\n",
        "        options = [\"ALL_TOUCHED=TRUE\"] if all_touched else []\n",
        "        if fixed:\n",
        "            gdal.RasterizeLayer(raster, [1], layer, burn_values=[fixed_value], options=options)\n",
        "        else:\n",
        "            attr_name = column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()\n",
        "            options.append(f\"ATTRIBUTE={attr_name}\")\n",
        "            gdal.RasterizeLayer(raster, [1], layer, options=options)\n",
        "    finally:\n",
        "        if raster: raster.FlushCache()\n",
        "        raster = vector = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lqjsLKZaQXo"
      },
      "source": [
        "# Select model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SqkKHKnaV-8"
      },
      "outputs": [],
      "source": [
        "# Select a baseline model, tested and trained in advance.\n",
        "# The target must have an uncertainty metric - otherwise\n",
        "# skip to the next notebook '8_statistics' and use the outputs\n",
        "# of the '6_scenarios' notebook.\n",
        "model_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file == 'model.json':\n",
        "      print(f'selected_model = \"{subdir.split(f\"{models_dir}/\",1)[1]}\"')\n",
        "      model_exists = True\n",
        "if not model_exists:\n",
        "  print(\"No model exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnerzp-gbapN"
      },
      "outputs": [],
      "source": [
        "selected_model = \"agbd_tekai_250625_003858\"\n",
        "\n",
        "# Define model directories\n",
        "selected_model_dir = join(models_dir,selected_model)\n",
        "selected_model_json = join(selected_model_dir, \"model.json\")\n",
        "selected_model_descr_dir = join(selected_model_dir, \"model_description.json\")\n",
        "selected_model_dataset_path = join(selected_model_dir, f\"{selected_model}.pkl\")\n",
        "selected_model_dataset = pd.read_pickle(selected_model_dataset_path)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(selected_model_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_target = model_dataset_description[\"selected_target\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "covariates_categorised = model_dataset_description[\"covariates_categorised\"]\n",
        "selected_features = model_dataset_description[\"selected_features\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "filter_parameter = model_dataset_description[\"filter_parameter\"]\n",
        "filter_values_to_include = model_dataset_description[\"filter_values_to_include\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "# Reload hyperparameters\n",
        "with open(selected_model_descr_dir) as model_description_json:\n",
        "  model_description = json.load(model_description_json)\n",
        "final_hyperparameters = eval(model_description[\"hyperparameters\"])\n",
        "\n",
        "# Remove early stopping and replace with mean n_estimators\n",
        "if \"early_stopping_rounds\" in final_hyperparameters:\n",
        "  final_hyperparameters = {k:v for k, v in final_hyperparameters.items() if k != \"early_stopping_rounds\"}\n",
        "  final_hyperparameters[\"n_estimators\"] = round(model_description[\"n_estimators mean\"])\n",
        "\n",
        "# Define directories\n",
        "scenarios_model_dir = join(scenarios_dir,selected_model)\n",
        "features_dir = join(scenarios_model_dir, \"features\")\n",
        "tile_templates_dir = join(scenarios_model_dir, 'tile_templates')\n",
        "tile_feature_stacks_dir = join(scenarios_model_dir, 'tile_feature_stacks')\n",
        "uncertainty_selected_model_dir = join(uncertainty_dir, selected_model)\n",
        "model_iterations_dir = join(uncertainty_selected_model_dir, \"model_iterations\")\n",
        "tile_prediction_cache_dir = join(uncertainty_selected_model_dir, \"tile_prediction_cache\")\n",
        "uncertainty_predictions_unmasked_dir = join(uncertainty_selected_model_dir, \"uncertainty_predictions_unmasked\")\n",
        "predictions_dir = join(uncertainty_selected_model_dir, \"uncertainty_predictions\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(uncertainty_selected_model_dir, exist_ok=True)\n",
        "makedirs(model_iterations_dir, exist_ok=True)\n",
        "makedirs(tile_prediction_cache_dir, exist_ok=True)\n",
        "makedirs(uncertainty_predictions_unmasked_dir, exist_ok=True)\n",
        "makedirs(predictions_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9-hQ7G37uZi"
      },
      "source": [
        "# Model iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWymSi4ndKUA"
      },
      "outputs": [],
      "source": [
        "# Verify that the target is equal to the mean\n",
        "print(f'mean = \"{selected_target}\"')\n",
        "\n",
        "# Calculate se from columns flagged 'uncertainty'\n",
        "if len(uncertainty)==0:\n",
        "  print(\"There are no flagged uncertainty columns to calculate SE from.\")\n",
        "  print(\"Manually create the metric from the available columns.\")\n",
        "  for col in selected_model_dataset.columns:\n",
        "    print(f\"{col}\")\n",
        "else:\n",
        "  for col in selected_model_dataset.columns:\n",
        "    if col in uncertainty and col not in selected_target:\n",
        "      print(f'se = \"{col}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBalj00xwmRX"
      },
      "outputs": [],
      "source": [
        "mean = \"tar_agbd\"\n",
        "se = \"tar_agbd_se\"\n",
        "# GEDI L4A provides standard errors (agbd_se) for each footprint prediction that incorporate\n",
        "# both model residual error and prediction uncertainty (GEDI L4A ATBD, Eq. 9, Kellner et al. 2021)\n",
        "# Following Liang et al. (2023), these standard errors are used as standard deviations for Monte Carlo sampling\n",
        "\n",
        "# Set model iterations\n",
        "model_iterations = 100\n",
        "\n",
        "# Define model (y axis changes for each iteration based on mean and se arrays)\n",
        "model_dataset_x = selected_model_dataset[selected_features].copy()\n",
        "for col in categorical_columns:\n",
        "    if col in model_dataset_x.columns:\n",
        "        model_dataset_x[col] = model_dataset_x[col].astype('category')\n",
        "mean_array = selected_model_dataset[mean].values\n",
        "se_array = selected_model_dataset[se].values\n",
        "\n",
        "# Detect model type from existing model or determine from target\n",
        "existing_model_path = join(model_iterations_dir, \"model_iteration_1.json\")\n",
        "if exists(existing_model_path):\n",
        "    # Load existing model and detect type\n",
        "    temp_booster = xgb.Booster()\n",
        "    temp_booster.load_model(existing_model_path)\n",
        "    model_config = json.loads(temp_booster.save_config())\n",
        "\n",
        "    objective_name = model_config['learner']['objective']['name']\n",
        "    num_class = int(model_config['learner']['learner_model_param'].get('num_class', '0'))\n",
        "    classification = any(obj_type in objective_name for obj_type in ['logistic', 'softprob', 'softmax'])\n",
        "    multiclass = classification and num_class > 2\n",
        "else:\n",
        "    # Determine from target variable characteristics\n",
        "    unique_values = len(np.unique(mean_array))\n",
        "    if unique_values <= 10 and all(val == int(val) for val in np.unique(mean_array)):\n",
        "        classification = True\n",
        "        multiclass = unique_values > 2\n",
        "        num_class = unique_values if multiclass else 0\n",
        "    else:\n",
        "        classification = False\n",
        "        multiclass = False\n",
        "        num_class = 0\n",
        "\n",
        "# Set model type\n",
        "if classification:\n",
        "    XGBPredictor = xgb.XGBClassifier(**final_hyperparameters)\n",
        "    if multiclass: print(f\"Model type: Multiclass classification ({num_class} classes)\")\n",
        "    else: print(\"Model type: Binary classification\")\n",
        "else:\n",
        "    XGBPredictor = xgb.XGBRegressor(**final_hyperparameters)\n",
        "    print(\"Model type: Regression\")\n",
        "\n",
        "model_params = XGBPredictor.get_params()\n",
        "model_params['eval_metric'] = model_description['metric_used_for_training']\n",
        "# Default fix for new XGBoost version\n",
        "[model_params.pop(key, None) for key in ['n_estimators', 'enable_categorical', 'missing']]\n",
        "\n",
        "# Progress label\n",
        "model_progress_index = 0\n",
        "model_progress_label = widgets.Label(f\"Model iteration: {model_progress_index}/{model_iterations}\")\n",
        "display(model_progress_label)\n",
        "\n",
        "for model_iteration in range(1,model_iterations+1):\n",
        "  # Set model iteration filename and check if already exists\n",
        "  model_iteration_filename = f\"model_iteration_{model_iteration}.json\"\n",
        "  model_iteration_path = join(model_iterations_dir, model_iteration_filename)\n",
        "  # If model iteration does not exist...\n",
        "  if not exists(model_iteration_path):\n",
        "    # Set the random seed based on iteration for replicability\n",
        "    np.random.seed(model_iteration)\n",
        "    # Set a normal distribution sample as the y for this iteration\n",
        "    model_dataset_y = normal(mean_array, se_array)\n",
        "    # Create DMatrix objects\n",
        "    model_dtrain = xgb.DMatrix(model_dataset_x, model_dataset_y, enable_categorical=True)\n",
        "    # Train the model iteration using the tested hyperparameters\n",
        "    model = xgb.train(model_params,\n",
        "                        model_dtrain,\n",
        "                        num_boost_round=final_hyperparameters['n_estimators'],\n",
        "                        verbose_eval=True)\n",
        "    # Save the model iteration\n",
        "    model.save_model(model_iteration_path)\n",
        "  # Update progress\n",
        "  model_progress_index += 1\n",
        "  model_progress_label.value = f\"Model iteration: {model_progress_index}/{model_iterations}\"\n",
        "print(\"All model iterations have been trained and saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1UxS-kO8_1d"
      },
      "source": [
        "# Scenario iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYxqe0eeKmKi"
      },
      "outputs": [],
      "source": [
        "# Scenarios must be designed and tested using 06_scenarios first.\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "assert n_tiles > 0, \"# There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "if n_tiles == 1: print(f\"# There is 1 template tile.\\n\")\n",
        "if n_tiles > 1: print(f\"# There are {n_tiles} template tiles.\\n\")\n",
        "\n",
        "# Collect available scenarios from the feature stack tiles directory\n",
        "scenario_stacks_list = []\n",
        "for scenario in os.listdir(tile_feature_stacks_dir):\n",
        "    if len(os.listdir(join(tile_feature_stacks_dir, scenario))) == n_tiles:\n",
        "        scenario_stacks_list.append(scenario)\n",
        "\n",
        "# Select scenarios to predict\n",
        "print(\"scenarios_to_predict = [\")\n",
        "for scenario in sorted(scenario_stacks_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE-bIxq9F3Vy"
      },
      "outputs": [],
      "source": [
        "# There is 1 template tile.\n",
        "\n",
        "scenarios_to_predict = [\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_no_degradation_since_1993\",\n",
        "  \"2021_no_disturbance_since_1993\",\n",
        "  \"2021_oldgrowth_1\",\n",
        "  \"2021_oldgrowth_2\",\n",
        "  \"2021_oldgrowth_all_land_1\",\n",
        "  \"2021_oldgrowth_all_land_2\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_no_degradation_since_1996\",\n",
        "  \"2024_no_disturbance_since_1996\",\n",
        "  \"2024_no_disturbance_since_1997\",\n",
        "  \"2024_no_disturbance_since_1998\",\n",
        "  \"2024_no_disturbance_since_1999\",\n",
        "  \"2024_no_disturbance_since_2000\",\n",
        "  \"2024_no_disturbance_since_2001\",\n",
        "  \"2024_no_disturbance_since_2002\",\n",
        "  \"2024_no_disturbance_since_2003\",\n",
        "  \"2024_no_disturbance_since_2004\",\n",
        "  \"2024_no_disturbance_since_2005\",\n",
        "  \"2024_no_disturbance_since_2006\",\n",
        "  \"2024_no_disturbance_since_2007\",\n",
        "  \"2024_no_disturbance_since_2008\",\n",
        "  \"2024_no_disturbance_since_2009\",\n",
        "  \"2024_no_disturbance_since_2010\",\n",
        "  \"2024_no_disturbance_since_2011\",\n",
        "  \"2024_no_disturbance_since_2012\",\n",
        "  \"2024_no_disturbance_since_2013\",\n",
        "  \"2024_no_disturbance_since_2014\",\n",
        "  \"2024_no_disturbance_since_2015\",\n",
        "  \"2024_no_disturbance_since_2016\",\n",
        "  \"2024_no_disturbance_since_2017\",\n",
        "  \"2024_no_disturbance_since_2018\",\n",
        "  \"2024_no_disturbance_since_2019\",\n",
        "  \"2024_no_disturbance_since_2020\",\n",
        "  \"2024_no_disturbance_since_2021\",\n",
        "  \"2024_no_disturbance_since_2022\",\n",
        "  \"2024_no_disturbance_since_2023\",\n",
        "  \"2024_no_disturbance_since_2024\",\n",
        "  \"2024_oldgrowth_1\",\n",
        "  \"2024_oldgrowth_2\",\n",
        "  \"2024_oldgrowth_all_land_1\",\n",
        "  \"2024_oldgrowth_all_land_2\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]\n",
        "\n",
        "# Check the number of model iterations available\n",
        "model_iterations_available = len(os.listdir(model_iterations_dir))\n",
        "print(f\"\\nThere are {len(os.listdir(model_iterations_dir))} model iterations available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqP1L8LtFcph"
      },
      "outputs": [],
      "source": [
        "# Set the number of scenario iterations. It must be <= the number of model iterations available.\n",
        "scenario_iterations = 100\n",
        "\n",
        "assert scenario_iterations <= model_iterations_available, f\"Reduce the number of scenario iterations to <= {model_iterations_available}.\"\n",
        "\n",
        "# GEDI covariates need to be changed to a set value for all predictions.\n",
        "add_covariates = True # Adds a selected covariate value as the feature\n",
        "sensitivity_value = 0.99\n",
        "# Higher sensitivity indicative of GEDI footprint 'quality'\n",
        "# However it may also overestimate vegetation metrics like AGBD.\n",
        "# If predictions appear to have a positive bias, lower this and run again.\n",
        "beam_value = 5\n",
        "# 5 is the first of the full beams, which appears to have the least bias on vegetation metrics.\n",
        "# Cover beams 1 - 4 underestimate. Full beams 7 - 8 overestimate. 5 - 6 tend give average values.\n",
        "\n",
        "# Probabilities instead of classes IF binary classification\n",
        "predict_probabilities = False\n",
        "\n",
        "# Classification threshold IF binary classification\n",
        "classification_threshold = 0.5\n",
        "\n",
        "# Detect GPU availability and set predictor type\n",
        "try:\n",
        "    test_array = cupy.array([1, 2, 3])\n",
        "    del test_array\n",
        "    predictor_type = 'gpu_predictor'\n",
        "    gpu_id, use_gpu = 0, True\n",
        "    print(\"GPU detected and accessible - using GPU to load the feature stack and to predict.\")\n",
        "    @contextmanager\n",
        "    def gpu_memory_context():\n",
        "        try: yield\n",
        "        finally:\n",
        "            cupy.cuda.Device().synchronize()\n",
        "            gc.collect()\n",
        "            cupy.get_default_memory_pool().free_all_blocks()\n",
        "            cupy.get_default_pinned_memory_pool().free_all_blocks()\n",
        "except:\n",
        "    predictor_type = 'cpu_predictor'\n",
        "    gpu_id, use_gpu = -1, False\n",
        "    print(\"GPU not accessible - using CPU prediction\")\n",
        "xgb.set_config(verbosity=0, use_rmm=use_gpu)\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "if n_tiles < 1: print(\"There are currently no template tiles.\")\n",
        "if n_tiles >= 1:\n",
        "  template_tile = gdal.Open(join(tile_templates_dir,'template_tile_1.tif'))\n",
        "  template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "  print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "# Tile progress\n",
        "if n_tiles > 1:\n",
        "  tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "  display(tile_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_predict:\n",
        "  scenario_feature_stack_dir = join(tile_feature_stacks_dir, scenario)\n",
        "  # Create scenario iterations directory\n",
        "  scenario_iterations_dir = join(uncertainty_selected_model_dir,f\"{scenario}_iterations\")\n",
        "  makedirs(scenario_iterations_dir, exist_ok=True)\n",
        "  # Iteration progress\n",
        "  iteration_progress_index = 0\n",
        "  iteration_progress_label = widgets.Label(f\"{scenario} iteration progress: {iteration_progress_index} / {scenario_iterations}\")\n",
        "  display(iteration_progress_label)\n",
        "\n",
        "  # Check if all scenario iterations already exist, if not then load feature stack\n",
        "  scenario_iteration_list = []\n",
        "  for model_iteration in range(1,scenario_iterations+1):\n",
        "    prediction_iteration_filename = f\"{scenario}__{selected_model}_unmasked_iteration_{model_iteration}.tif\"\n",
        "    prediction_iteration_path = join(scenario_iterations_dir, prediction_iteration_filename)\n",
        "    scenario_iteration_list.append(prediction_iteration_path)\n",
        "  all_scenario_iterations_exist = True\n",
        "  for scenario_iteration in scenario_iteration_list:\n",
        "    if not exists(scenario_iteration): all_scenario_iterations_exist = False\n",
        "  if not all_scenario_iterations_exist:\n",
        "    if n_tiles == 1:\n",
        "    # Load template parameters\n",
        "      template_tile_dir = join(tile_templates_dir, f\"template_tile_1.tif\")\n",
        "      template_tile = gdal.Open(template_tile_dir)\n",
        "      template_tile_y = template_tile.GetRasterBand(1).YSize\n",
        "      template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "      # Load feature tile stack with GPU/CPU fallback - use memory mapping to avoid loading entire file\n",
        "      stack_filename = f\"feature_stack_{scenario}_1.npy\"\n",
        "      stack_path = join(scenario_feature_stack_dir, stack_filename)\n",
        "      # Load with memory mapping first to check dimensions without full load\n",
        "      temp_stack = np.load(stack_path, mmap_mode='r')\n",
        "      n_rows, n_cols = temp_stack.shape\n",
        "      final_n_cols = n_cols + 2 if add_covariates else n_cols\n",
        "      feature_stack_on_gpu = False # Track if feature stack loaded to GPU\n",
        "      if use_gpu:\n",
        "        try:\n",
        "          # Pre-allocate final array size to avoid hstack copy\n",
        "          feature_stack = cupy.empty((n_rows, final_n_cols), dtype=temp_stack.dtype)\n",
        "          feature_stack[:, :n_cols] = cupy.asarray(temp_stack)\n",
        "          if add_covariates:\n",
        "            feature_stack[:, -2] = beam_value\n",
        "            feature_stack[:, -1] = sensitivity_value\n",
        "          feature_stack_on_gpu = True  # Only set to True after successful completion\n",
        "        except Exception as e:\n",
        "          if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "            print(\"GPU memory insufficient for feature stack, loading with CPU.\")\n",
        "            feature_stack = np.empty((n_rows, final_n_cols), dtype=temp_stack.dtype)\n",
        "            feature_stack[:, :n_cols] = temp_stack\n",
        "            if add_covariates:\n",
        "              feature_stack[:, -2] = beam_value\n",
        "              feature_stack[:, -1] = sensitivity_value\n",
        "          else: raise\n",
        "      else:\n",
        "        feature_stack = np.empty((n_rows, final_n_cols), dtype=temp_stack.dtype)\n",
        "        feature_stack[:, :n_cols] = temp_stack\n",
        "        if add_covariates:\n",
        "          feature_stack[:, -2] = beam_value\n",
        "          feature_stack[:, -1] = sensitivity_value\n",
        "      temp_stack = None # Flush memory mapped array\n",
        "    # Predict scenario for each model iteration\n",
        "    for model_iteration in range(1,scenario_iterations+1):\n",
        "      # Clear GPU memory if feature stack not on GPU\n",
        "      if use_gpu and not feature_stack_on_gpu:\n",
        "        with gpu_memory_context(): pass\n",
        "      # Define the model\n",
        "      model_dir = join(model_iterations_dir,f\"model_iteration_{model_iteration}.json\")\n",
        "      prediction_iteration_filename = f\"{scenario}__{selected_model}_unmasked_iteration_{model_iteration}.tif\"\n",
        "      prediction_iteration_path = join(scenario_iterations_dir, prediction_iteration_filename)\n",
        "      # If scenario iteration does not exist:\n",
        "      if not exists(prediction_iteration_path):\n",
        "        # Load model and detect type\n",
        "        booster = xgb.Booster()\n",
        "        booster.load_model(selected_model_json)\n",
        "        model_config = json.loads(booster.save_config())\n",
        "\n",
        "        objective_name = model_config['learner']['objective']['name']\n",
        "        num_class = int(model_config['learner']['learner_model_param'].get('num_class', '0'))\n",
        "        classification = any(obj_type in objective_name for obj_type in ['logistic', 'softprob', 'softmax'])\n",
        "        multiclass = classification and num_class > 2\n",
        "        if classification and multiclass: print(f\"Model type: Multiclass classification ({num_class} classes)\")\n",
        "        elif classification: print(\"Model type: Binary classification\")\n",
        "        else: print(\"Model type: Regression\")\n",
        "\n",
        "        # Select appropriate predictor type\n",
        "        if classification:\n",
        "            XGBPredictor = xgb.XGBClassifier()\n",
        "            XGBPredictor.load_model(selected_model_json)\n",
        "            XGBPredictor.set_params(predictor=predictor_type)\n",
        "            if use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "        else:\n",
        "            XGBPredictor = xgb.XGBRegressor()\n",
        "            XGBPredictor.load_model(selected_model_json)\n",
        "            XGBPredictor.set_params(predictor=predictor_type)\n",
        "            if use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "\n",
        "        # Get number of stacks\n",
        "        n_stacks = len(os.listdir(scenario_feature_stack_dir))\n",
        "        if n_stacks == 1:\n",
        "          # Predict - terminate runtime if GPU prediction fails\n",
        "          # Predict - terminate runtime if GPU prediction fails\n",
        "          try:\n",
        "              if classification and predict_probabilities and not multiclass:\n",
        "                  # Get probability of class 1 for binary classification\n",
        "                  prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                  prediction = prediction_proba[:, 1] # Probability of class 1\n",
        "              else:\n",
        "                  if classification and not multiclass:\n",
        "                      # Use predict_proba for better accuracy in binary classification\n",
        "                      prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                      prediction = (prediction_proba[:, 1] > classification_threshold).astype(int)\n",
        "                  else:\n",
        "                      prediction = XGBPredictor.predict(feature_stack)\n",
        "                      if classification:\n",
        "                          # Check if prediction is 2D (probabilities) and convert to class labels\n",
        "                          if prediction.ndim > 1 and prediction.shape[1] > 1: prediction = np.argmax(prediction, axis=1)\n",
        "                          # Ensure prediction is integer type for classification\n",
        "                          prediction = prediction.astype(int)\n",
        "          except Exception as e:\n",
        "            if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "              print(\"GPU memory insufficient for prediction. Terminating runtime to save compute units, restart with TPU.\")\n",
        "              runtime.unassign()\n",
        "            else: raise\n",
        "          prediction_array = prediction.reshape((template_tile_y, template_tile_x))\n",
        "          prediction = None # Flush prediction\n",
        "\n",
        "        # Tiling for if feature stacks and separated into chunks\n",
        "        if n_stacks > 1:\n",
        "          # Create a tile cache directory for the prediction\n",
        "          tile_cache_iteration_dir = join(tile_prediction_cache_dir, prediction_iteration_filename[:-4])\n",
        "          makedirs(tile_cache_iteration_dir, exist_ok=True)\n",
        "          # Create a tile count to match the feature stack chunk\n",
        "          for stack in range(1, n_stacks+1):\n",
        "            iteration_tile_filename = f\"scenario_tile_{stack}.tif\"\n",
        "            # Check if tile already exists\n",
        "            scenario_tile_exists = False\n",
        "            for scenario_tile in os.listdir(tile_cache_iteration_dir):\n",
        "              if scenario_tile == iteration_tile_filename: scenario_tile_exists=True\n",
        "            # If scenario prediction tile does not exist:\n",
        "            if scenario_tile_exists == False:\n",
        "              # Clear GPU memory before new stack\n",
        "              if use_gpu:\n",
        "                with gpu_memory_context(): pass\n",
        "              # Load template tile parameters\n",
        "              template_tile_dir = join(tile_templates_dir, f\"template_tile_{stack}.tif\")\n",
        "              template_tile = gdal.Open(template_tile_dir)\n",
        "              template_tile_y = template_tile.GetRasterBand(1).YSize\n",
        "              template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "              # Load feature tile stack with GPU/CPU fallback - use memory mapping to avoid loading entire file\n",
        "              stack_filename = f\"feature_stack_{scenario}_{stack}.npy\"\n",
        "              stack_path = join(scenario_feature_stack_dir, stack_filename)\n",
        "              # Check if the .npy file actually exists\n",
        "              if not os.path.exists(stack_path):\n",
        "                  print(f\"Warning: {stack_filename} not found in {scenario_feature_stack_dir}. Skipping tile {stack}.\")\n",
        "                  continue\n",
        "              # Load with memory mapping first to check dimensions without full load\n",
        "              temp_stack = np.load(stack_path, mmap_mode='r')\n",
        "              n_rows, n_cols = temp_stack.shape\n",
        "              final_n_cols = n_cols + 2 if add_covariates else n_cols\n",
        "              if use_gpu:\n",
        "                try:\n",
        "                  # Pre-allocate final array size to avoid hstack copy\n",
        "                  feature_stack = cupy.empty((n_rows, final_n_cols), dtype=temp_stack.dtype)\n",
        "                  feature_stack[:, :n_cols] = cupy.asarray(temp_stack)\n",
        "                  if add_covariates:\n",
        "                    feature_stack[:, -2] = beam_value\n",
        "                    feature_stack[:, -1] = sensitivity_value\n",
        "                except Exception as e:\n",
        "                  if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "                    print(\"GPU memory insufficient for feature stack, loading stack with CPU.\")\n",
        "                    feature_stack = np.empty((n_rows, final_n_cols), dtype=temp_stack.dtype)\n",
        "                    feature_stack[:, :n_cols] = temp_stack\n",
        "                    if add_covariates:\n",
        "                      feature_stack[:, -2] = beam_value\n",
        "                      feature_stack[:, -1] = sensitivity_value\n",
        "                    with gpu_memory_context(): pass\n",
        "                  else: raise\n",
        "              else:\n",
        "                feature_stack = np.empty((n_rows, final_n_cols), dtype=temp_stack.dtype)\n",
        "                feature_stack[:, :n_cols] = temp_stack\n",
        "                # Add covariates (sensitivity and BEAM)\n",
        "                if add_covariates:\n",
        "                  feature_stack[:, -2] = beam_value\n",
        "                  feature_stack[:, -1] = sensitivity_value\n",
        "              temp_stack = None # Flush memory mapped array\n",
        "              # Predict - terminate runtime if GPU prediction fails\n",
        "              try:\n",
        "                  if classification and predict_probabilities and not multiclass:\n",
        "                      # Get probability of class 1 for binary classification\n",
        "                      prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                      prediction = prediction_proba[:, 1] # Probability of class 1\n",
        "                  else:\n",
        "                      if classification and not multiclass:\n",
        "                          # Use predict_proba for better accuracy in binary classification\n",
        "                          prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                          prediction = (prediction_proba[:, 1] > classification_threshold).astype(int)\n",
        "                      else:\n",
        "                          prediction = XGBPredictor.predict(feature_stack)\n",
        "                          if classification:\n",
        "                              # Check if prediction is 2D (probabilities) and convert to class labels\n",
        "                              if prediction.ndim > 1 and prediction.shape[1] > 1: prediction = np.argmax(prediction, axis=1)\n",
        "                              # Ensure prediction is integer type for classification\n",
        "                              prediction = prediction.astype(int)\n",
        "              except Exception as e:\n",
        "                if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "                  print(\"GPU memory insufficient for prediction. Terminating runtime to save compute units, restart with TPU.\")\n",
        "                  runtime.unassign()\n",
        "                else: raise\n",
        "              feature_stack = None # Flush feature stack\n",
        "              prediction_tile = prediction.reshape((template_tile_y, template_tile_x))\n",
        "              prediction = None # Flush prediction\n",
        "              # Export prediction array as .tif\n",
        "              export_array_as_tif(prediction_tile, join(tile_cache_iteration_dir, iteration_tile_filename), template = template_tile_dir, compress = False)\n",
        "              prediction_tile = None # Flush prediction tile\n",
        "              # Update progress\n",
        "            tile_progress_index += 1\n",
        "            tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_stacks}\"\n",
        "          # Prepare empty array for merging tiles\n",
        "          prediction_array = np.empty((0,template_tile_x))\n",
        "          # Read each tile .tif as an array, stack, then export as a .tif\n",
        "          for subdir in os.listdir(tile_cache_iteration_dir):\n",
        "            if subdir.endswith('.tif'):\n",
        "              tile_dir = join(tile_cache_iteration_dir, subdir)\n",
        "              prediction_array = np.vstack((prediction_array, gdal.Open(tile_dir).ReadAsArray()))\n",
        "          # Delete scenario tile cache directory and reset index\n",
        "          shutil.rmtree(tile_cache_iteration_dir)\n",
        "          tile_progress_index = 0\n",
        "          tile_progress_label.value = f\"Tile progress: 0 / {n_tiles}\"\n",
        "\n",
        "        # Define scenario template\n",
        "        scenario_template = join(features_dir, os.listdir(features_dir)[0])\n",
        "        export_array_as_tif(prediction_array, prediction_iteration_path, template = scenario_template, compress = True)\n",
        "\n",
        "      iteration_progress_index += 1\n",
        "      iteration_progress_label.value = f\"{scenario} iteration progress: {iteration_progress_index} / {scenario_iterations}\"\n",
        "  else:\n",
        "    iteration_progress_label.value = f\"{scenario} iteration progress: 100 / {scenario_iterations}\"\n",
        "print(\"\\nScenario iterations complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIWo8ATZs-Ny"
      },
      "source": [
        "# Scenario statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9ZTJoAmSLLCM"
      },
      "outputs": [],
      "source": [
        "# Collect scenarios with iterations\n",
        "scenarios_iterations_list = []\n",
        "for subdir in os.listdir(uncertainty_selected_model_dir):\n",
        "  if subdir.endswith(\"_iterations\"):\n",
        "    if \"model_iterations\" not in subdir:\n",
        "      scenarios_iterations_list.append(subdir[:-11])\n",
        "# Select scenarios to calculate mean and standard deviation\n",
        "print(\"scenarios_to_calculate = [\")\n",
        "for scenario in sorted(scenarios_iterations_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6B0Por9lPQlA"
      },
      "outputs": [],
      "source": [
        "scenarios_to_calculate = [\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_no_degradation_since_1993\",\n",
        "  \"2021_no_disturbance_since_1993\",\n",
        "  \"2021_oldgrowth_1\",\n",
        "  \"2021_oldgrowth_2\",\n",
        "  \"2021_oldgrowth_all_land_1\",\n",
        "  \"2021_oldgrowth_all_land_2\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_no_degradation_since_1996\",\n",
        "  \"2024_no_disturbance_since_1996\",\n",
        "  \"2024_no_disturbance_since_1997\",\n",
        "  \"2024_no_disturbance_since_1998\",\n",
        "  \"2024_no_disturbance_since_1999\",\n",
        "  \"2024_no_disturbance_since_2000\",\n",
        "  \"2024_no_disturbance_since_2001\",\n",
        "  \"2024_no_disturbance_since_2002\",\n",
        "  \"2024_no_disturbance_since_2003\",\n",
        "  \"2024_no_disturbance_since_2004\",\n",
        "  \"2024_no_disturbance_since_2005\",\n",
        "  \"2024_no_disturbance_since_2006\",\n",
        "  \"2024_no_disturbance_since_2007\",\n",
        "  \"2024_no_disturbance_since_2008\",\n",
        "  \"2024_no_disturbance_since_2009\",\n",
        "  \"2024_no_disturbance_since_2010\",\n",
        "  \"2024_no_disturbance_since_2011\",\n",
        "  \"2024_no_disturbance_since_2012\",\n",
        "  \"2024_no_disturbance_since_2013\",\n",
        "  \"2024_no_disturbance_since_2014\",\n",
        "  \"2024_no_disturbance_since_2015\",\n",
        "  \"2024_no_disturbance_since_2016\",\n",
        "  \"2024_no_disturbance_since_2017\",\n",
        "  \"2024_no_disturbance_since_2018\",\n",
        "  \"2024_no_disturbance_since_2019\",\n",
        "  \"2024_no_disturbance_since_2020\",\n",
        "  \"2024_no_disturbance_since_2021\",\n",
        "  \"2024_no_disturbance_since_2022\",\n",
        "  \"2024_no_disturbance_since_2023\",\n",
        "  \"2024_no_disturbance_since_2024\",\n",
        "  \"2024_oldgrowth_1\",\n",
        "  \"2024_oldgrowth_2\",\n",
        "  \"2024_oldgrowth_all_land_1\",\n",
        "  \"2024_oldgrowth_all_land_2\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IFiOCz6mXaXm"
      },
      "outputs": [],
      "source": [
        "# Check iteration number\n",
        "\n",
        "# Exact '0' pixels without decimals can be an indicator that an iteration was incorrectly generated.\n",
        "# These must be deleted and predicted again to avoid incorrect statistics.\n",
        "check_problems = False\n",
        "\n",
        "# Exact '0' pixels might have been genuinely predicted (though unlikely in a regression),\n",
        "# In which case set fix_problems to True and add these iterations to problem_arrays list.\n",
        "# The code below will add 0.001 so they won't trigger the problem checker again.\n",
        "fix_problems = False\n",
        "problem_rasters = [\n",
        "\n",
        "]\n",
        "\n",
        "if len(problem_rasters) > 0:\n",
        "  for problem_raster in problem_rasters:\n",
        "    problem_raster_path = join(uncertainty_selected_model_dir, f\"{scenario}_iterations\", problem_raster)\n",
        "    problem_raster_array = gdal.Open(problem_raster_path).ReadAsArray()\n",
        "    problem_raster_array[problem_raster_array == 0] = 0.001\n",
        "    export_array_as_tif(problem_raster_array, problem_raster_path, template = problem_raster_path, compress=True)\n",
        "\n",
        "# Check the number of prediction iterations\n",
        "for scenario in scenarios_to_calculate:\n",
        "  scenario_iterations_dir = join(uncertainty_selected_model_dir,f\"{scenario}_iterations\")\n",
        "  iterations = 0\n",
        "  for subdir in os.listdir(scenario_iterations_dir):\n",
        "    if subdir.endswith(\".tif\"):\n",
        "      # Check whether the prediction iteration is valid\n",
        "      if check_problems:\n",
        "        iteration = join(scenario_iterations_dir,subdir)\n",
        "        iteration_array = gdal.Open(iteration).ReadAsArray()\n",
        "        assert np.count_nonzero(iteration_array==0) == 0, f\"{subdir} contains 0 values, so the iteration may not have predicted correctly.\\n Check the file, delete and repredict if necessary.\\n If they are valid 0 values, run the cell below on:\\n {iteration}.\"\n",
        "      iterations += 1\n",
        "  print(f\"There are {iterations} prediction iterations for scenario {scenario} statistics.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3ekgu_-LLOY_"
      },
      "outputs": [],
      "source": [
        "confidence_interval = 0.95\n",
        "\n",
        "# Statistics progress\n",
        "stats_progress_index = 0\n",
        "stats_progress_label = widgets.Label(f\"Scenario stats progress: {stats_progress_index}/{len(scenarios_to_calculate)}\")\n",
        "display(stats_progress_label)\n",
        "\n",
        "for scenario in scenarios_to_calculate:\n",
        "    stat_base_filename = f\"{scenario}__{selected_model}\"\n",
        "    scenario_iterations_dir = join(uncertainty_selected_model_dir,f\"{scenario}_iterations\")\n",
        "\n",
        "    # Define statistics raster directories\n",
        "    stat_mean_filename = f\"mean__{stat_base_filename}_unmasked.tif\"\n",
        "    stat_mean_dir = join(uncertainty_predictions_unmasked_dir,stat_mean_filename)\n",
        "    stat_uncertainty_filename = f\"uncertainty__{stat_base_filename}_unmasked.tif\"\n",
        "    stat_uncertainty_dir = join(uncertainty_predictions_unmasked_dir,stat_uncertainty_filename)\n",
        "\n",
        "    # Check whether statistics rasters already exist\n",
        "    stat_mean_tif_exists, stat_uncertainty_tif_exists = False, False\n",
        "    for stat_tif in os.listdir(uncertainty_predictions_unmasked_dir):\n",
        "        if stat_tif == stat_mean_filename: stat_mean_tif_exists = True\n",
        "        if stat_tif == stat_uncertainty_filename: stat_uncertainty_tif_exists = True\n",
        "\n",
        "    # If either mean or uncertainty do not exist\n",
        "    if stat_mean_tif_exists == False or stat_uncertainty_tif_exists == False:\n",
        "        stat_sum = None\n",
        "        stat_sum_sq = None\n",
        "        iteration_n = 0\n",
        "\n",
        "        for subdir in os.listdir(scenario_iterations_dir):\n",
        "            if subdir.endswith(\".tif\"):\n",
        "                iteration = os.path.join(scenario_iterations_dir, subdir)\n",
        "                # Create float64 array for precise calculations\n",
        "                iteration_array = gdal.Open(iteration).ReadAsArray().astype(np.float64)\n",
        "                if stat_sum is None:\n",
        "                    stat_sum = np.zeros_like(iteration_array, dtype=np.float64)\n",
        "                    stat_sum_sq = np.zeros_like(iteration_array, dtype=np.float64)\n",
        "                # Sum and sum of squares\n",
        "                stat_sum += iteration_array\n",
        "                stat_sum_sq += iteration_array ** 2\n",
        "                iteration_n += 1\n",
        "\n",
        "        # Calculate mean: sum / count\n",
        "        stat_mean = stat_sum / iteration_n\n",
        "        if stat_mean_tif_exists == False:\n",
        "            export_array_as_tif(stat_mean, stat_mean_dir, template = iteration)\n",
        "            print(f\"{stat_mean_filename} has been exported.\")\n",
        "        else: print(f\"{stat_mean_filename} already exists.\")\n",
        "\n",
        "        if stat_uncertainty_tif_exists == False:\n",
        "            # Calculate variance: E[X^2] - (E[X])^2\n",
        "            stat_variance = np.maximum(0, (stat_sum_sq - stat_sum ** 2 / iteration_n) / (iteration_n - 1))\n",
        "            # Standard error: σ / sqrt(n)\n",
        "            stat_se = np.sqrt(stat_variance) / np.sqrt(iteration_n)\n",
        "            # Handle zero SE to avoid scipy warnings\n",
        "            min_se_threshold = 1e-15\n",
        "            stat_se_clean = np.maximum(stat_se, min_se_threshold)\n",
        "            # Calculate confidence intervals using t-distribution\n",
        "            stat_ci_lower, stat_ci_upper = st.t.interval(confidence_interval, iteration_n - 1, loc=stat_mean, scale=stat_se_clean)\n",
        "            # Reset CI bounds to mean for pixels that originally had zero SE\n",
        "            zero_se_mask = (stat_se == 0.0)\n",
        "            stat_ci_lower[zero_se_mask] = stat_mean[zero_se_mask]\n",
        "            stat_ci_upper[zero_se_mask] = stat_mean[zero_se_mask]\n",
        "            # CI width: (upper - lower) / 2\n",
        "            stat_ci = (stat_ci_upper - stat_ci_lower) / 2\n",
        "            # Uncertainty: (CI / mean) * 100%\n",
        "            stat_uncertainty = np.zeros_like(stat_mean)\n",
        "            valid_mask = np.abs(stat_mean) > 1e-15\n",
        "            stat_uncertainty[valid_mask] = (stat_ci[valid_mask] / stat_mean[valid_mask]) * 100\n",
        "\n",
        "            # Export statistics arrays as rasters\n",
        "            export_array_as_tif(stat_se, join(uncertainty_predictions_unmasked_dir,f\"se__{stat_base_filename}_unmasked.tif\"), template = iteration)\n",
        "            print(f\"se__{stat_base_filename}_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_ci_lower, join(uncertainty_predictions_unmasked_dir,f\"ci_lower__{stat_base_filename}_unmasked.tif\"), template = iteration)\n",
        "            print(f\"ci_lower__{stat_base_filename}_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_ci_upper, join(uncertainty_predictions_unmasked_dir,f\"ci_upper__{stat_base_filename}_unmasked.tif\"), template = iteration)\n",
        "            print(f\"ci_upper__{stat_base_filename}_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_ci, join(uncertainty_predictions_unmasked_dir,f\"ci__{stat_base_filename}_unmasked.tif\"), template = iteration)\n",
        "            print(f\"ci__{stat_base_filename}_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_uncertainty, stat_uncertainty_dir, template = iteration)\n",
        "            print(f\"{stat_uncertainty_filename} has been exported.\")\n",
        "        else: print(f\"{stat_uncertainty_filename} already exists.\")\n",
        "\n",
        "    else: print(f\"{stat_mean_filename} and {stat_uncertainty_filename} already exist.\")\n",
        "    stats_progress_index += 1\n",
        "    stats_progress_label.value = (f\"Scenario stats progress: {stats_progress_index}/{len(scenarios_to_calculate)}\")\n",
        "\n",
        "print(\"Statistics calculations and .tif exports complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbqtNygUAboh"
      },
      "source": [
        "# Mask scenario statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TgjqruNBu_H"
      },
      "outputs": [],
      "source": [
        "# Use polygons for masking, only areas inside the polygons will be included\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "print(\"mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"  '{polygon[:-5]}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVMXY7mNAZFi"
      },
      "outputs": [],
      "source": [
        "mask_polygons = [\n",
        "  # 'project_area',\n",
        "  'gedi_area',\n",
        "  # 'peninsular_malaysia',\n",
        "  # 'pa_taman_krau',\n",
        "  # 'pa_ais',\n",
        "]\n",
        "\n",
        "# GEDI L4A AGBD precision is 0\n",
        "mean_precision = 0\n",
        "# Non-negligible percentage uncertainty sometimes <1\n",
        "uncertainty_precision = 1\n",
        "\n",
        "# Create an inverse project area path for masking\n",
        "template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "for polygon in mask_polygons:\n",
        "  inverse_polygon_path = join(polygons_dir, f\"{polygon}_inverse.gpkg\")\n",
        "  if not exists(inverse_polygon_path):\n",
        "    polygon_path = join(polygons_dir, f\"{polygon}.gpkg\")\n",
        "    template_polygon = gpd.read_file(template_polygon_path)\n",
        "    polygon_read = gpd.read_file(polygon_path)\n",
        "    polygon_crs = polygon_read.crs.to_epsg()\n",
        "    inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "    inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "    inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "    print(f\"An inverse masking polygon for {polygon} has been created in {polygons_dir}.\")\n",
        "  else: print(f\"An inverse masking polygon for {polygon} already exists.\")\n",
        "\n",
        "\n",
        "# If only [scenario]_oldgrowth_1 exists, simply all disturbance from all disturbance features\n",
        "# This will be masked to [scenario]_oldgrowth.\n",
        "\n",
        "# If both oldgrowth_1 and oldgrowth_2 exist,\n",
        "# oldgrowth_1 uses an area-based proxy for pre-Landsat undisturbed forest\n",
        "# oldgrowth_2 simply removes all disturbance from all disturbance features\n",
        "# The final masked [scenario]_oldgrowth chooses the maximum pixel values from comparing each.\n",
        "\n",
        "# Identify all oldgrowth 1 files (both mean and uncertainty)\n",
        "oldgrowth_oldgrowth_1_files = [f for f in os.listdir(uncertainty_predictions_unmasked_dir)\n",
        "                     if ('_1__' in f) and\n",
        "                        (f.startswith('mean__') or f.startswith('uncertainty__')) and\n",
        "                        f.endswith('_unmasked.tif')]\n",
        "\n",
        "# First find all mean oldgrowth_1 files\n",
        "mean_oldgrowth_1_files = [f for f in os.listdir(uncertainty_predictions_unmasked_dir)\n",
        "                if f.startswith('mean__') and '_1__' in f and f.endswith('_unmasked.tif')]\n",
        "\n",
        "for mean_oldgrowth_1_file in mean_oldgrowth_1_files:\n",
        "    # Get corresponding uncertainty oldgrowth_1 file\n",
        "    uncertainty_oldgrowth_1_file = mean_oldgrowth_1_file.replace('mean__', 'uncertainty__')\n",
        "    # Get oldgrowth_2 files\n",
        "    mean_oldgrowth_2_file = mean_oldgrowth_1_file.replace('_1__', '_2__')\n",
        "    uncertainty_oldgrowth_2_file = uncertainty_oldgrowth_1_file.replace('_1__', '_2__')\n",
        "    # Create merged filenames\n",
        "    mean_merged_file = mean_oldgrowth_1_file.replace('_1__', '__')\n",
        "    uncertainty_merged_file = uncertainty_oldgrowth_1_file.replace('_1__', '__')\n",
        "    # Paths\n",
        "    mean_merged_path = join(uncertainty_predictions_unmasked_dir, mean_merged_file)\n",
        "    uncertainty_merged_path = join(uncertainty_predictions_unmasked_dir, uncertainty_merged_file)\n",
        "    # Skip if merged files already exist\n",
        "    if exists(mean_merged_path) and exists(uncertainty_merged_path):\n",
        "        print(f\"Merged files already exist for {mean_oldgrowth_1_file}\")\n",
        "        continue\n",
        "    print(f\"Processing {mean_oldgrowth_1_file}\")\n",
        "\n",
        "    # Check if oldgrowth_2 exists\n",
        "    if exists(join(uncertainty_predictions_unmasked_dir, mean_oldgrowth_2_file)):\n",
        "        # Process with oldgrowth_2\n",
        "        mean_oldgrowth_1_array = gdal.Open(join(uncertainty_predictions_unmasked_dir, mean_oldgrowth_1_file)).ReadAsArray()\n",
        "        mean_oldgrowth_2_array = gdal.Open(join(uncertainty_predictions_unmasked_dir, mean_oldgrowth_2_file)).ReadAsArray()\n",
        "        # Which one is greater?\n",
        "        oldgrowth_1_is_greater = mean_oldgrowth_1_array > mean_oldgrowth_2_array\n",
        "        # Take maximum value\n",
        "        merged_mean = np.maximum(mean_oldgrowth_1_array, mean_oldgrowth_2_array)\n",
        "        # Save merged mean\n",
        "        export_array_as_tif(merged_mean, mean_merged_path, compress=True)\n",
        "        print(f\"Saved merged mean: {mean_merged_file}\")\n",
        "        # Process uncertainty if oldgrowth_1 exists\n",
        "        if exists(join(uncertainty_predictions_unmasked_dir, uncertainty_oldgrowth_1_file)):\n",
        "            uncertainty_oldgrowth_1_array = gdal.Open(join(uncertainty_predictions_unmasked_dir, uncertainty_oldgrowth_1_file)).ReadAsArray()\n",
        "            # If oldgrowth_2 uncertainty exists, use it where appropriate\n",
        "            if exists(join(uncertainty_predictions_unmasked_dir, uncertainty_oldgrowth_2_file)):\n",
        "                uncertainty_oldgrowth_2_array = gdal.Open(join(uncertainty_predictions_unmasked_dir, uncertainty_oldgrowth_2_file)).ReadAsArray()\n",
        "                merged_uncertainty = np.where(oldgrowth_1_is_greater, uncertainty_oldgrowth_1_array, uncertainty_oldgrowth_2_array)\n",
        "            else: merged_uncertainty = uncertainty_oldgrowth_1_array\n",
        "            # Save merged uncertainty\n",
        "            export_array_as_tif(merged_uncertainty, uncertainty_merged_path, compress=True)\n",
        "            print(f\"Saved merged uncertainty: {uncertainty_merged_file}\")\n",
        "    else: # Just use '_1'\n",
        "        if not exists(mean_merged_path):\n",
        "            shutil.copy2(join(uncertainty_predictions_unmasked_dir, mean_oldgrowth_1_file), mean_merged_path)\n",
        "            print(f\"Copied oldgrowth_1 mean to {mean_merged_file}\")\n",
        "        if exists(join(uncertainty_predictions_unmasked_dir, uncertainty_oldgrowth_1_file)) and not exists(uncertainty_merged_path):\n",
        "            shutil.copy2(join(uncertainty_predictions_unmasked_dir, uncertainty_oldgrowth_1_file), uncertainty_merged_path)\n",
        "            print(f\"Copied oldgrowth_1 uncertainty to {uncertainty_merged_file}\")\n",
        "\n",
        "\n",
        "unmasked_predictions = []\n",
        "for scenario_prediction in os.listdir(uncertainty_predictions_unmasked_dir):\n",
        "  # Only mask mean, uncertainty and ci for visualisation and calculating statistics\n",
        "  if ('mean__' in scenario_prediction) or ('uncertainty__' in scenario_prediction):\n",
        "      # Skip oldgrowth version files\n",
        "      if scenario_prediction.split('__')[0].endswith(('_1__', '_2__')):\n",
        "        continue\n",
        "      unmasked_predictions.append(scenario_prediction)\n",
        "\n",
        "# Determine last feature year for masking future scenarios\n",
        "final_feature_years = []\n",
        "for final_feature in os.listdir(feature_final_dir):\n",
        "  if final_feature.endswith('.tif') and final_feature[-9] == '_':\n",
        "    try: final_feature_years.append(int(final_feature[-8:-4]))\n",
        "    except: continue\n",
        "last_feature_year = max(final_feature_years)\n",
        "\n",
        "# Masking progress\n",
        "masking_progress_index = 0\n",
        "masking_progress_label = widgets.Label(f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\")\n",
        "display(masking_progress_label)\n",
        "\n",
        "# Mask scenario statistics with the relevant mask\n",
        "for scenario_prediction in unmasked_predictions: # Loop through each unmasked scenario\n",
        "  scenario_masked_filename = f\"{scenario_prediction[:-13]}.tif\"\n",
        "  scenario_masked_dir = join(predictions_dir, scenario_masked_filename)\n",
        "  scenario_year = int(scenario_prediction.split('__')[1][:4])\n",
        "  if not exists(scenario_masked_dir):\n",
        "    mask_exists = False\n",
        "    for mask in os.listdir(masks_dir):\n",
        "      mask_year = int(mask[12:16])\n",
        "\n",
        "      # Match 'no disturbance since' scenarios (use mask from last year of disturbance)\n",
        "      if 'no_disturbance_since' in scenario_prediction:\n",
        "        disturbance_since_year = int(scenario_prediction.split('__')[1][-4:])\n",
        "        if disturbance_since_year == mask_year:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "          break\n",
        "\n",
        "      # Match 'all land' old-growth scenarios\n",
        "      elif 'oldgrowth_all_land' in scenario_prediction:\n",
        "        if 'oldgrowth_all_land' in mask and f'{scenario_year}_oldgrowth_all_land' in mask:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "          break\n",
        "\n",
        "      # Match area-based deforestation scenarios\n",
        "      elif 'deforestation' in scenario_prediction:\n",
        "        if 'deforestation' in mask:\n",
        "          mask_middle = mask[12:-4]  # Remove \"mask_forest_\" and \".tif\"\n",
        "          if scenario_prediction.split('__')[1].startswith(mask_middle):\n",
        "            selected_mask_filename = mask\n",
        "            selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "            mask_exists = True\n",
        "            break\n",
        "\n",
        "      # Match future scenarios with most recent forest mask\n",
        "      elif scenario_year > last_feature_year:\n",
        "        if last_feature_year == mask_year and 'oldgrowth_all_land' not in mask:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "          break\n",
        "\n",
        "      # Match all other historic scenarios (exclude specialised masks)\n",
        "      elif scenario_year == mask_year and 'oldgrowth_all_land' not in mask:\n",
        "        selected_mask_filename = mask\n",
        "        selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "        mask_exists = True\n",
        "        break\n",
        "\n",
        "    if mask_exists == False: print(f\"A suitable mask for {scenario_prediction} does not exist.\\n\")\n",
        "    else: # Mask the scenario prediction\n",
        "      print(f\"Masking {scenario_prediction} with {selected_mask_filename}...\")\n",
        "      mask_array = gdal.Open(selected_mask_dir).ReadAsArray()\n",
        "      scenario_prediction_unmasked_dir = join(uncertainty_predictions_unmasked_dir, scenario_prediction)\n",
        "      scenario_prediction_array = gdal.Open(scenario_prediction_unmasked_dir).ReadAsArray()\n",
        "      if 'mean__' in scenario_prediction: scenario_prediction_array = np.round(scenario_prediction_array, mean_precision)\n",
        "      if 'uncertainty__' in scenario_prediction: scenario_prediction_array = np.round(scenario_prediction_array, uncertainty_precision)\n",
        "      # Mask where the mask array is not 1\n",
        "      scenario_masked_array = np.where(mask_array != 1, nodatavalue, scenario_prediction_array)\n",
        "      export_array_as_tif(scenario_masked_array, scenario_masked_dir, compress = True)\n",
        "      if len(mask_polygons) > 0:\n",
        "        for polygon_mask in mask_polygons:\n",
        "          inverse_gedi_area_path = join(polygons_dir, f\"{polygon_mask}_inverse.gpkg\")\n",
        "          print(f\"Masking {scenario_prediction} with {polygon_mask}...\")\n",
        "          burn_polygon_to_raster(scenario_masked_dir, inverse_gedi_area_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        # Recompress the prediction after burning the polygon masks\n",
        "        scenario_masked_array_2 = gdal.Open(scenario_masked_dir).ReadAsArray()\n",
        "        export_array_as_tif(scenario_masked_array_2, scenario_masked_dir, compress = True)\n",
        "      print(f\"{scenario_masked_filename} exported.\")\n",
        "  # Update masking progress\n",
        "  masking_progress_index += 1\n",
        "  masking_progress_label.value = f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nezNm_Gz3cnz"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kki0NPO3FKOw"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution upon completion\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "Xhx977-WIaWJ",
        "5lqjsLKZaQXo",
        "C9-hQ7G37uZi"
      ],
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}