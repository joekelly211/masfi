{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/7_uncertainty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhx977-WIaWJ"
      },
      "source": [
        "# Imports and directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRnNty96IThZ"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "\n",
        "# Mount Google Drive and set base directory\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4n2MGF0aZtJ"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install xgboost --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpmgsb8KKMGN"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "import json\n",
        "from google.colab import runtime\n",
        "import ipywidgets as widgets\n",
        "import itertools\n",
        "import joblib\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy.random import normal\n",
        "from os import makedirs\n",
        "from os.path import exists, join\n",
        "from osgeo import gdal\n",
        "import pandas as pd\n",
        "import scipy.stats as st\n",
        "import shutil\n",
        "import xgboost as xgb\n",
        "\n",
        "# Define GPU\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found.')\n",
        "else:\n",
        "  print(f\"Found GPU at: {device_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQwNO9T0_HPH"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "masks_dir = join(areas_dir, \"masks\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(uncertainty_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koHn-AiGG7KM"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lqjsLKZaQXo"
      },
      "source": [
        "# Select model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SqkKHKnaV-8"
      },
      "outputs": [],
      "source": [
        "# Select a baseline model, tested and trained in advance.\n",
        "model_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file == 'model.json':\n",
        "      print(f'selected_model = \"{subdir.split(f\"{models_dir}/\",1)[1]}\"')\n",
        "      model_exists = True\n",
        "if not model_exists:\n",
        "  print(\"No model exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnerzp-gbapN"
      },
      "outputs": [],
      "source": [
        "selected_model = \"agbd_240819_091905\"\n",
        "categorise_variate = False # If the variate was categorised in 5_models\n",
        "\n",
        "# Define model directories\n",
        "selected_model_dir = join(models_dir,selected_model)\n",
        "selected_model_json = join(selected_model_dir, \"model.json\")\n",
        "selected_model_descr_dir = join(selected_model_dir, \"model_description.json\")\n",
        "selected_model_dataset_path = join(selected_model_dir, f\"{selected_model}.pkl\")\n",
        "selected_model_dataset = pd.read_pickle(selected_model_dataset_path)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(selected_model_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_variate = model_dataset_description[\"selected_variate\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "covariates_categorised = model_dataset_description[\"covariates_categorised\"]\n",
        "selected_predictors = model_dataset_description[\"selected_predictors\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "filter_parameter = model_dataset_description[\"filter_parameter\"]\n",
        "filter_values_to_include = model_dataset_description[\"filter_values_to_include\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "# Reload hyperparameters\n",
        "with open(selected_model_descr_dir) as model_description_json:\n",
        "  model_description = json.load(model_description_json)\n",
        "final_hyperparameters = eval(model_description[\"hyperparameters\"])\n",
        "\n",
        "# Remove early stopping and replace with mean n_estimators\n",
        "if \"early_stopping_rounds\" in final_hyperparameters:\n",
        "  final_hyperparameters = {k:v for k, v in final_hyperparameters.items() if k != \"early_stopping_rounds\"}\n",
        "  final_hyperparameters[\"n_estimators\"] = round(model_description[\"n_estimators mean\"])\n",
        "\n",
        "# Define directories\n",
        "scenarios_model_dir = join(scenarios_dir,selected_model)\n",
        "uncertainty_selected_model_dir = join(uncertainty_dir, selected_model)\n",
        "model_iterations_dir = join(uncertainty_selected_model_dir, \"model_iterations\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(uncertainty_selected_model_dir, exist_ok=True)\n",
        "makedirs(model_iterations_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9-hQ7G37uZi"
      },
      "source": [
        "# Model iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWymSi4ndKUA"
      },
      "outputs": [],
      "source": [
        "# Verify that the variate is equal to the mean\n",
        "print(f'mean = \"{selected_variate}\"')\n",
        "\n",
        "# Calculate se from columns flagged 'uncertainty'\n",
        "if len(uncertainty)==0:\n",
        "  print(\"There are no flagged uncertainty columns to calculate SE from.\")\n",
        "  print(\"Manually create the metric from the available columns.\")\n",
        "  for col in selected_model_dataset.columns:\n",
        "    print(f\"{col}\")\n",
        "else:\n",
        "  for col in selected_model_dataset.columns:\n",
        "    if col in uncertainty and col not in selected_variate:\n",
        "      print(f'se = \"{col}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QBalj00xwmRX"
      },
      "outputs": [],
      "source": [
        "mean = \"var_agbd\"\n",
        "se = \"var_agbd_se\"\n",
        "# Liang et al. (2023) use SE as a proxy for STDEV\n",
        "\n",
        "# Set model iterations\n",
        "model_iterations = 100\n",
        "\n",
        "# Define model (y axis changes for each iteration based on mean and se arrays)\n",
        "model_dataset_x = selected_model_dataset[selected_predictors]\n",
        "mean_array = selected_model_dataset[mean].values\n",
        "se_array = selected_model_dataset[se].values\n",
        "if categorise_variate: XGBPredictor = xgb.XGBClassifier(**final_hyperparameters)\n",
        "else: XGBPredictor = xgb.XGBRegressor(**final_hyperparameters)\n",
        "model_params = XGBPredictor.get_params()\n",
        "model_params['eval_metric'] = model_description['metric_used_for_training']\n",
        "# Default fix for new XGBoost version\n",
        "[model_params.pop(key, None) for key in ['n_estimators', 'enable_categorical', 'missing']]\n",
        "\n",
        "# Progress label\n",
        "model_progress_index = 0\n",
        "model_progress_label = widgets.Label(f\"Model iteration: {model_progress_index}/{model_iterations}\")\n",
        "display(model_progress_label)\n",
        "\n",
        "for model_iteration in range(1,model_iterations+1):\n",
        "  # Set model iteration filename and check if already exists\n",
        "  model_iteration_filename = f\"model_iteration_{model_iteration}.json\"\n",
        "  model_iteration_path = join(model_iterations_dir, model_iteration_filename)\n",
        "  # If model iteration does not exist...\n",
        "  if not exists(model_iteration_path):\n",
        "    # Set the random seed based on iteration for replicability\n",
        "    np.random.seed(model_iteration)\n",
        "    # Set a normal distribution sample as the y for this iteration\n",
        "    model_dataset_y = normal(mean_array, se_array)\n",
        "    # Create DMatrix objects\n",
        "    model_dtrain = xgb.DMatrix(model_dataset_x, model_dataset_y, enable_categorical=True)\n",
        "    # Train the model iteration using the tested hyperparameters\n",
        "    model = xgb.train(model_params,\n",
        "                        model_dtrain,\n",
        "                        num_boost_round=final_hyperparameters['n_estimators'],\n",
        "                        verbose_eval=True)\n",
        "    # Save the model iteration\n",
        "    model.save_model(model_iteration_path)\n",
        "  # Update progress\n",
        "  model_progress_index += 1\n",
        "  model_progress_label.value = f\"Model iteration: {model_progress_index}/{model_iterations}\"\n",
        "print(\"All model iterations have been trained and saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1UxS-kO8_1d"
      },
      "source": [
        "# Scenario iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UL55gK2y6Ye"
      },
      "outputs": [],
      "source": [
        "# Scenarios must be designed and tested using 06_scenarios first.\n",
        "# Predictors in the 'constant' and scenario subdirs should not be moved.\n",
        "\n",
        "# Select a scenario area\n",
        "scenario_area_exists = False\n",
        "for subdir in os.listdir(scenarios_model_dir):\n",
        "  if not subdir.endswith('.json') and not subdir.endswith('.csv'):\n",
        "    print(f'selected_scenario_area = \"{subdir}\"')\n",
        "    scenario_area_exists = True\n",
        "if not scenario_area_exists:\n",
        "  print(f\"Create a scenario area directory in {scenarios_model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYxqe0eeKmKi"
      },
      "outputs": [],
      "source": [
        "selected_scenario_area = \"taman\"\n",
        "\n",
        "# Locate scenario area directories\n",
        "selected_scenario_area_dir = join(scenarios_model_dir, selected_scenario_area)\n",
        "predictors_dir = join(selected_scenario_area_dir, \"predictors\")\n",
        "tile_templates_dir = join(selected_scenario_area_dir, 'tile_templates')\n",
        "tile_predictor_stacks_dir = join(selected_scenario_area_dir, 'tile_predictor_stacks')\n",
        "\n",
        "# Define uncertainty scenario area directory\n",
        "uncertainty_scenario_area_dir = join(uncertainty_selected_model_dir,f\"scenarios_{selected_scenario_area}\")\n",
        "tile_prediction_cache_dir = join(uncertainty_scenario_area_dir, \"tile_prediction_cache\")\n",
        "makedirs(uncertainty_scenario_area_dir, exist_ok=True)\n",
        "makedirs(tile_prediction_cache_dir, exist_ok=True)\n",
        "\n",
        "# Collect available scenarios from the predictor stack tiles directory\n",
        "scenario_stacks_list = []\n",
        "for scenario in os.listdir(tile_predictor_stacks_dir):\n",
        "    scenario_stacks_list.append(scenario)\n",
        "\n",
        "# Select scenarios to predict\n",
        "print(\"scenarios_to_predict = [\")\n",
        "for scenario in sorted(scenario_stacks_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE-bIxq9F3Vy"
      },
      "outputs": [],
      "source": [
        "scenarios_to_predict = [\n",
        "  # \"2019\",\n",
        "  # \"2020\",\n",
        "  # \"2021\",\n",
        "  # \"2022\",\n",
        "  # \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_nodef_historic\",\n",
        "  \"2024_nodeg_historic\",\n",
        "  \"9999_comrec\",\n",
        "  \"9999_comrest\",\n",
        "  \"9999_comrest_first_historic\",\n",
        "]\n",
        "\n",
        "# Check the number of model iterations available\n",
        "model_iterations_available = len(os.listdir(model_iterations_dir))\n",
        "print(f\"\\nThere are {len(os.listdir(model_iterations_dir))} model iterations available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aqP1L8LtFcph"
      },
      "outputs": [],
      "source": [
        "# Set the number of scenario iterations. It must be <= the number of model iterations available.\n",
        "scenario_iterations = 100\n",
        "\n",
        "assert scenario_iterations <= model_iterations_available, f\"Reduce the number of scenario iterations to <= {model_iterations_available}.\"\n",
        "\n",
        "# Change this and the code within the block accordingly.\n",
        "add_covariates = True # Adds a selected covariate value as the predictor\n",
        "sensitivity_value = 0.99\n",
        "beam_value = 5\n",
        "\n",
        "# Check for GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0': print('GPU device not found')\n",
        "else: print(f\"Found GPU at: {device_name}\")\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "if n_tiles < 1: print(\"There are currently no template tiles.\")\n",
        "if n_tiles >= 1:\n",
        "  template_tile_x = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).XSize\n",
        "  print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "# Tile progress\n",
        "if n_tiles > 1:\n",
        "  tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "  display(tile_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_predict:\n",
        "  scenario_predictor_stack_dir = join(tile_predictor_stacks_dir, scenario)\n",
        "  # Create scenario iterations directory\n",
        "  scenario_iterations_dir = join(uncertainty_scenario_area_dir,f\"{scenario}_iterations\")\n",
        "  makedirs(scenario_iterations_dir, exist_ok=True)\n",
        "  # Iteration progress\n",
        "  iteration_progress_index = 0\n",
        "  iteration_progress_label = widgets.Label(f\"{scenario} iteration progress: {iteration_progress_index} / {scenario_iterations}\")\n",
        "  display(iteration_progress_label)\n",
        "\n",
        "  if n_tiles == 1:\n",
        "    # Load template parameters\n",
        "    template_tile_dir = join(tile_templates_dir, f\"template_tile_1.tif\")\n",
        "    template_tile_y = gdal.Open(template_tile_dir).GetRasterBand(1).YSize\n",
        "    template_tile_x = gdal.Open(template_tile_dir).GetRasterBand(1).XSize\n",
        "    # Load predictor stack\n",
        "    stack_filename = f\"predictor_stack_{scenario}_1.npy\"\n",
        "    predictor_stack = np.load(join(scenario_predictor_stack_dir, stack_filename))\n",
        "    if add_covariates: predictor_stack = np.hstack((predictor_stack,\n",
        "                  np.full((predictor_stack.shape[0], 1), beam_value, dtype=int),\n",
        "                  np.full((predictor_stack.shape[0], 1), sensitivity_value, dtype=float)\n",
        "                  ))\n",
        "\n",
        "  # Predict scenario for each model iteration\n",
        "  for model_iteration in range(1,scenario_iterations+1):\n",
        "    # Define the model\n",
        "    model_dir = join(model_iterations_dir,f\"model_iteration_{model_iteration}.json\")\n",
        "    # Define scenario iteration filename and check if exists\n",
        "    prediction_iteration_filename = f\"{scenario}__{selected_scenario_area}_{selected_model}_unmasked_iteration_{model_iteration}.tif\"\n",
        "    prediction_iteration_dir = join(scenario_iterations_dir, prediction_iteration_filename)\n",
        "    prediction_iteration_exists = False\n",
        "    for file in os.listdir(scenario_iterations_dir):\n",
        "      if file == prediction_iteration_filename: prediction_iteration_exists=True\n",
        "    # If scenario iteration does not exist:\n",
        "    if prediction_iteration_exists == False:\n",
        "      # Load model\n",
        "      if categorise_variate: XGBPredictor = xgb.XGBClassifier(**final_hyperparameters)\n",
        "      else: XGBPredictor = xgb.XGBRegressor(**final_hyperparameters)\n",
        "      XGBPredictor.load_model(fname=model_dir)\n",
        "      # Avoids issues using dataframe from CPU\n",
        "      xgb.set_config(verbosity=0, use_rmm=True)\n",
        "      # Get number of stacks\n",
        "      n_stacks = len(os.listdir(scenario_predictor_stack_dir))\n",
        "      if n_stacks == 1:\n",
        "        # Define prediction array and reshape\n",
        "        prediction = XGBPredictor.predict(predictor_stack)\n",
        "        prediction_array = prediction.reshape((template_tile_y, template_tile_x))\n",
        "        prediction = None # Flush prediction\n",
        "\n",
        "      # Tiling for if predictor stacks and separated into chunks\n",
        "      if n_stacks > 1:\n",
        "        # Create a tile cache directory for the prediction\n",
        "        tile_cache_iteration_dir = join(tile_prediction_cache_dir, prediction_iteration_filename[:-4])\n",
        "        makedirs(tile_cache_iteration_dir, exist_ok=True)\n",
        "        # Create a tile count to match the predictor stack chunk\n",
        "        for stack in range(1, n_stacks+1):\n",
        "          iteration_tile_filename = f\"scenario_tile_{stack}.tif\"\n",
        "          # Check if tile already exists\n",
        "          scenario_tile_exists = False\n",
        "          for scenario_tile in os.listdir(tile_cache_iteration_dir):\n",
        "            if scenario_tile == iteration_tile_filename: scenario_tile_exists=True\n",
        "          # If scenario prediction tile does not exist:\n",
        "          if scenario_tile_exists == False:\n",
        "            # Load template tile parameters\n",
        "            template_tile_dir = join(tile_templates_dir, f\"template_tile_{stack}.tif\")\n",
        "            template_tile_y = gdal.Open(template_tile_dir).GetRasterBand(1).YSize\n",
        "            template_tile_x = gdal.Open(template_tile_dir).GetRasterBand(1).XSize\n",
        "            # Load predictor tile stack\n",
        "            stack_filename = f\"predictor_stack_{scenario}_{stack}.npy\"\n",
        "            predictor_stack = np.load(join(scenario_predictor_stack_dir, stack_filename))\n",
        "            # Add covariates (sensitivity and BEAM)\n",
        "            if add_covariates: predictor_stack = np.hstack((predictor_stack,\n",
        "                              np.full((predictor_stack.shape[0], 1), beam_value, dtype=int),\n",
        "                              np.full((predictor_stack.shape[0], 1), sensitivity_value, dtype=float)\n",
        "                              ))\n",
        "            # Define prediction array and reshape\n",
        "            prediction = XGBPredictor.predict(predictor_stack)\n",
        "            predictor_stack = None # Flush predictor stack\n",
        "            prediction_tile = prediction.reshape((template_tile_y, template_tile_x))\n",
        "            prediction = None # Flush prediction\n",
        "            # Export prediction array as .tif\n",
        "            export_array_as_tif(prediction_tile, join(tile_cache_iteration_dir, iteration_tile_filename), template = template_tile_dir, compress = False)\n",
        "            prediction_tile = None # Flush prediction tile\n",
        "            # Update progress\n",
        "          tile_progress_index += 1\n",
        "          tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_stacks}\"\n",
        "        # Prepare empty array for merging tiles\n",
        "        prediction_array = np.empty((0,template_tile_x))\n",
        "        # Read each tile .tif as an array, stack, then export as a .tif\n",
        "        for subdir in os.listdir(tile_cache_iteration_dir):\n",
        "          if subdir.endswith('.tif'):\n",
        "            tile_dir = join(tile_cache_iteration_dir, subdir)\n",
        "            prediction_array = np.vstack((prediction_array, gdal.Open(tile_dir).ReadAsArray()))\n",
        "        # Delete scenario tile cache directory and reset index\n",
        "        shutil.rmtree(tile_cache_iteration_dir)\n",
        "        tile_progress_index = 0\n",
        "        tile_progress_label.value = f\"Tile progress: 0 / {n_tiles}\"\n",
        "\n",
        "      # Define scenario template\n",
        "      scenario_template = join(predictors_dir, os.listdir(predictors_dir)[0])\n",
        "      export_array_as_tif(prediction_array, prediction_iteration_dir, template = scenario_template, compress = True)\n",
        "\n",
        "    iteration_progress_index += 1\n",
        "    iteration_progress_label.value = f\"{scenario} iteration progress: {iteration_progress_index} / {scenario_iterations}\"\n",
        "print(\"\\nScenario iterations complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIWo8ATZs-Ny"
      },
      "source": [
        "# Scenario statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PG3PBLYgbUsN"
      },
      "outputs": [],
      "source": [
        "# Select a scenario iterations area\n",
        "scenario_iterations_area_exists = False\n",
        "for subdir in os.listdir(uncertainty_selected_model_dir):\n",
        "  if subdir != \"model_iterations\":\n",
        "    print(f'selected_scenario_iterations_area = \"{subdir[10:]}\"')\n",
        "    scenario_iterations_area_exists = True\n",
        "if not scenario_iterations_area_exists:\n",
        "  print(f\"Run the scenario iterations section first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9ZTJoAmSLLCM"
      },
      "outputs": [],
      "source": [
        "selected_scenario_iterations_area = \"taman\"\n",
        "\n",
        "uncertainty_scenario_area_dir = join(uncertainty_selected_model_dir,f\"scenarios_{selected_scenario_iterations_area}\")\n",
        "statistics_unmasked_dir = join(uncertainty_scenario_area_dir,\"statistics_unmasked\")\n",
        "statistics_masked_dir = join(uncertainty_scenario_area_dir,\"statistics_masked\")\n",
        "makedirs(statistics_unmasked_dir, exist_ok=True)\n",
        "makedirs(statistics_masked_dir, exist_ok=True)\n",
        "\n",
        "# Collect scenarios with iterations\n",
        "scenarios_iterations_list = []\n",
        "for subdir in os.listdir(uncertainty_scenario_area_dir):\n",
        "  if subdir.endswith(\"_iterations\"):\n",
        "    scenarios_iterations_list.append(subdir[:-11])\n",
        "# Select scenarios to calculate mean and standard deviation\n",
        "print(\"scenarios_to_calculate = [\")\n",
        "for scenario in sorted(scenarios_iterations_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6B0Por9lPQlA"
      },
      "outputs": [],
      "source": [
        "scenarios_to_calculate = [\n",
        "  \"2024\",\n",
        "  \"2024_nodef_historic\",\n",
        "  \"2024_nodeg_historic\",\n",
        "  \"9999_comrec\",\n",
        "  \"9999_comrest\",\n",
        "  \"9999_comrest_first_historic\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f_uDi5WLMmy"
      },
      "outputs": [],
      "source": [
        "# Check the number of prediction iterations\n",
        "for scenario in scenarios_to_calculate:\n",
        "  scenario_iterations_dir = join(uncertainty_scenario_area_dir,f\"{scenario}_iterations\")\n",
        "  iterations = 0\n",
        "  for subdir in os.listdir(scenario_iterations_dir):\n",
        "    if subdir.endswith(\".tif\"):\n",
        "      # Check whether the prediction iteration is valid\n",
        "      iteration = join(scenario_iterations_dir,subdir)\n",
        "      iteration_array = gdal.Open(iteration).ReadAsArray()\n",
        "      assert np.count_nonzero(iteration_array==0) == 0, f\"{subdir} contains 0 values, so the iteration may not have predicted correctly.\\n Check the file, delete and repredict if necessary.\\n If they are valid 0 values, run the cell below on:\\n {iteration}.\"\n",
        "      iterations += 1\n",
        "  print(f\"There are {iterations} prediction iterations for scenario {scenario} statistics.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LEa41iRQw2ua"
      },
      "outputs": [],
      "source": [
        "# If the predicted 0 is legitimate, convert to 0.001 and repeat the above cell.\n",
        "problem_array = ''\n",
        "\n",
        "if len(problem_array) > 0:\n",
        "  problem_array_dir = join(uncertainty_scenario_area_dir, f\"{scenario}_iterations\", problem_array)\n",
        "  problem_array = gdal.Open(problem_array_dir).ReadAsArray()\n",
        "  problem_array[problem_array == 0] = 0.001\n",
        "  export_array_as_tif(problem_array, problem_array_dir, template = problem_array_dir, compress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3ekgu_-LLOY_"
      },
      "outputs": [],
      "source": [
        "confidence_interval = 0.95\n",
        "\n",
        "for scenario in scenarios_to_calculate:\n",
        "    stat_base_filename = f\"{scenario}__{selected_scenario_iterations_area}_{selected_model}\"\n",
        "    scenario_iterations_dir = join(uncertainty_scenario_area_dir,f\"{scenario}_iterations\")\n",
        "\n",
        "    # Define statistics raster directories\n",
        "    stat_mean_filename = f\"{stat_base_filename}__mean_unmasked.tif\"\n",
        "    stat_mean_dir = join(statistics_unmasked_dir,stat_mean_filename)\n",
        "    stat_uncertainty_filename = f\"{stat_base_filename}__uncertainty_unmasked.tif\"\n",
        "    stat_uncertainty_dir = join(statistics_unmasked_dir,stat_uncertainty_filename)\n",
        "\n",
        "    # Check whether statistics rasters already exist\n",
        "    stat_mean_tif_exists, stat_uncertainty_tif_exists = False, False\n",
        "    for stat_tif in os.listdir(statistics_unmasked_dir):\n",
        "        if stat_tif == stat_mean_filename: stat_mean_tif_exists = True\n",
        "        if stat_tif == stat_uncertainty_filename: stat_uncertainty_tif_exists = True\n",
        "\n",
        "    # If either mean or uncertainty do not exist\n",
        "    if stat_mean_tif_exists == False or stat_uncertainty_tif_exists == False:\n",
        "        stat_sum = None\n",
        "        stat_sum_sq = None\n",
        "        iteration_n = 0\n",
        "        for subdir in os.listdir(scenario_iterations_dir):\n",
        "            if subdir.endswith(\".tif\"):\n",
        "                iteration = os.path.join(scenario_iterations_dir, subdir)\n",
        "                iteration_array = gdal.Open(iteration).ReadAsArray()\n",
        "                if stat_sum is None:\n",
        "                    stat_sum = np.zeros_like(iteration_array, dtype='float64')\n",
        "                    stat_sum_sq = np.zeros_like(iteration_array, dtype='float64')\n",
        "                # Sum and sum of squares\n",
        "                stat_sum += iteration_array  # Running sum for mean\n",
        "                stat_sum_sq += np.square(iteration_array, dtype=np.float64)  # Running sum of squares for variance\n",
        "                iteration_n += 1\n",
        "\n",
        "        # Calculate mean: sum / count\n",
        "        stat_mean = np.divide(stat_sum, iteration_n, dtype='float64')\n",
        "        if stat_mean_tif_exists == False:\n",
        "            export_array_as_tif(stat_mean, stat_mean_dir, template = iteration)\n",
        "            print(f\"{stat_mean_filename} has been exported.\")\n",
        "        else: print(f\"{stat_mean_filename} already exists.\")\n",
        "\n",
        "        if stat_uncertainty_tif_exists == False:\n",
        "            # Calculate variance: E[X^2] - (E[X])^2\n",
        "            stat_variance = (stat_sum_sq - (stat_sum ** 2) / iteration_n) / (iteration_n - 1)\n",
        "            # Standard error: σ / sqrt(n)\n",
        "            stat_se = np.sqrt(stat_variance) / np.sqrt(iteration_n)\n",
        "            # Calculate confidence intervals using t-distribution\n",
        "            stat_ci_lower, stat_ci_upper = st.t.interval(confidence_interval, iteration_n - 1, loc=stat_mean, scale=stat_se)\n",
        "            # CI width: (upper - lower) / 2\n",
        "            stat_ci = np.divide(np.subtract(stat_ci_upper, stat_ci_lower, dtype='float64'), 2, dtype='float64')\n",
        "            # Uncertainty: (CI / mean) * 100%\n",
        "            stat_uncertainty = np.multiply(np.divide(stat_ci, stat_mean, dtype='float64'), 100, dtype='float64')\n",
        "            # Export statistics arrays as rasters\n",
        "            export_array_as_tif(stat_se, join(statistics_unmasked_dir,f\"{stat_base_filename}__se_unmasked.tif\"), template = iteration)\n",
        "            print(f\"{stat_base_filename}__se_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_ci_lower, join(statistics_unmasked_dir,f\"{stat_base_filename}__ci_lower_unmasked.tif\"), template = iteration)\n",
        "            print(f\"{stat_base_filename}__ci_lower_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_ci_upper, join(statistics_unmasked_dir,f\"{stat_base_filename}__ci_upper_unmasked.tif\"), template = iteration)\n",
        "            print(f\"{stat_base_filename}__ci_upper_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_ci, join(statistics_unmasked_dir,f\"{stat_base_filename}__ci_unmasked.tif\"), template = iteration)\n",
        "            print(f\"{stat_base_filename}__ci_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_uncertainty, stat_uncertainty_dir, template = iteration)\n",
        "            print(f\"{stat_base_filename}__uncertainty_unmasked.tif has been exported.\")\n",
        "        else: print(f\"{stat_uncertainty_filename} already exists.\")\n",
        "\n",
        "    else: print(f\"{stat_mean_filename} and {stat_uncertainty_filename} already exist.\")\n",
        "\n",
        "print(\"Statistics calculations and .tif exports complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3dUqcHffVG4G"
      },
      "outputs": [],
      "source": [
        "last_historic_predictor_year = '2024'\n",
        "first_historic_predictor_year = '1990'\n",
        "\n",
        "# Mask statistics .tif files with the relevatant mask in 1_areas/masks.\n",
        "for statistic_unmasked in os.listdir(statistics_unmasked_dir): # Loop through each unmasked prediction\n",
        "  statistic_masked_filename = f\"{statistic_unmasked[:-13]}.tif\"\n",
        "  statistic_masked_dir = join(statistics_masked_dir, statistic_masked_filename)\n",
        "  if not exists(statistic_masked_dir):\n",
        "    # Match the year of the statistic (first four characters) to a mask which includes the year in the filename.\n",
        "    mask_exists = False\n",
        "    for mask in os.listdir(masks_dir):\n",
        "      # Match historic statistics and 9999_comrec (complete recovery) with the respective mask\n",
        "      if statistic_unmasked[:4] in mask and 'comrest' not in statistic_unmasked and 'nodef_historic' not in statistic_unmasked:\n",
        "        selected_mask_filename = mask\n",
        "        selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "        mask_exists = True\n",
        "      # Match 9999_comrest with last historic predictor year mask (e.g. forest 2024)\n",
        "      if 'comrest' in statistic_unmasked and last_historic_predictor_year in mask:\n",
        "        selected_mask_filename = mask\n",
        "        selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "        mask_exists = True\n",
        "      # Match 9999_comrest_first_historic with first historic predictor year mask (e.g. forest 1991)\n",
        "      if 'comrest' in statistic_unmasked and 'first_historic' in statistic_unmasked and str(int(first_historic_predictor_year)+1) in mask:\n",
        "        selected_mask_filename = mask\n",
        "        selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "        mask_exists = True\n",
        "      # Match nodef_historic with first historic predictor year mask (e.g. forest 1991)\n",
        "      if 'nodef_historic' in statistic_unmasked and str(int(first_historic_predictor_year)+1) in mask:\n",
        "        selected_mask_filename = mask\n",
        "        selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "        mask_exists = True\n",
        "      # Match other alternate scenarios with the last historic predictor year mask (e.g. forest 2024)\n",
        "      if int(statistic_unmasked[:4]) > int(last_historic_predictor_year) and int(statistic_unmasked[:4]) < 9999 and last_historic_predictor_year in mask:\n",
        "        selected_mask_filename = mask\n",
        "        selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "        mask_exists = True\n",
        "    if mask_exists == False: print(f\"A suitable mask for {statistic_unmasked} does not exist.\\n\")\n",
        "    else: # Mask the scenario prediction\n",
        "      print(f\"Masking {statistic_unmasked} with {selected_mask_filename}...\")\n",
        "      mask_array = gdal.Open(selected_mask_dir).ReadAsArray()\n",
        "      statistic_unmasked_dir = join(statistics_unmasked_dir, statistic_unmasked)\n",
        "      statistic_prediction_array = gdal.Open(statistic_unmasked_dir).ReadAsArray()\n",
        "      # Mask where the mask array is not 1\n",
        "      statistic_masked_array = np.where(mask_array != 1, nodatavalue, statistic_prediction_array)\n",
        "      statistic_masked_filename = f\"{statistic_unmasked[:-13]}.tif\"\n",
        "      export_array_as_tif(statistic_masked_array, statistic_masked_dir, template = selected_mask_dir, compress = True)\n",
        "      print(f\"{statistic_masked_filename} exported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui5TlyF-QZj_"
      },
      "source": [
        "# Scenario difference with uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGiuU5lULIwl"
      },
      "outputs": [],
      "source": [
        "# Select a scenario statistics area\n",
        "scenario_iterations_area_exists = False\n",
        "for subdir in os.listdir(uncertainty_selected_model_dir):\n",
        "  if subdir != \"model_iterations\":\n",
        "    print(f'selected_scenario_iterations_area = \"{subdir[10:]}\"')\n",
        "    scenario_iterations_area_exists = True\n",
        "if not scenario_iterations_area_exists:\n",
        "  print(f\"Run the scenario iterations section first.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_scenario_iterations_area = \"taman\"\n",
        "\n",
        "uncertainty_scenario_area_dir = join(uncertainty_selected_model_dir,f\"scenarios_{selected_scenario_iterations_area}\")\n",
        "statistics_masked_dir = join(uncertainty_scenario_area_dir,\"statistics_masked\")\n",
        "diff_uncertainty_dir = join(uncertainty_scenario_area_dir, \"scenario_difference\")\n",
        "makedirs(diff_uncertainty_dir, exist_ok=True)\n",
        "\n",
        "scenarios_diff_set = set()\n",
        "for masked_statistic in os.listdir(statistics_masked_dir):\n",
        "    scenarios_diff_set.add(masked_statistic.split(\"__\")[0])\n",
        "\n",
        "# Generate all possible pairs of scenarios, including all orders\n",
        "scenario_pairs = sorted(list(itertools.permutations(scenarios_diff_set, 2)))\n",
        "\n",
        "print(\"# Select scenarios to calculate mean difference with uncertainty\")\n",
        "print(\"scenario_pairs = [\")\n",
        "for s1, s2 in scenario_pairs:\n",
        "    print(f\" ('{s1}','{s2}'),\")\n",
        "print(\"]\")"
      ],
      "metadata": {
        "id": "XpmEYC36j5xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select scenarios to calculate mean difference with uncertainty\n",
        "scenario_pairs = [\n",
        "#  ('2024','2024_nodef_historic'),\n",
        "#  ('2024','2024_nodeg_historic'),\n",
        "#  ('2024','9999_comrec'),\n",
        "#  ('2024','9999_comrest'),\n",
        "#  ('2024','9999_comrest_first_historic'),\n",
        " ('2024_nodef_historic','2024'),\n",
        " ('2024_nodef_historic','2024_nodeg_historic'),\n",
        "#  ('2024_nodef_historic','9999_comrec'),\n",
        "#  ('2024_nodef_historic','9999_comrest'),\n",
        "#  ('2024_nodef_historic','9999_comrest_first_historic'),\n",
        " ('2024_nodeg_historic','2024'),\n",
        "#  ('2024_nodeg_historic','2024_nodef_historic'),\n",
        "#  ('2024_nodeg_historic','9999_comrec'),\n",
        "#  ('2024_nodeg_historic','9999_comrest'),\n",
        "#  ('2024_nodeg_historic','9999_comrest_first_historic'),\n",
        " ('9999_comrec','2024'),\n",
        " ('9999_comrec','2024_nodef_historic'),\n",
        "#  ('9999_comrec','2024_nodeg_historic'),\n",
        " ('9999_comrec','9999_comrest'),\n",
        " ('9999_comrec','9999_comrest_first_historic'),\n",
        " ('9999_comrest','2024'),\n",
        "#  ('9999_comrest','2024_nodef_historic'),\n",
        " ('9999_comrest','2024_nodeg_historic'),\n",
        "#  ('9999_comrest','9999_comrec'),\n",
        "#  ('9999_comrest','9999_comrest_first_historic'),\n",
        "#  ('9999_comrest_first_historic','2024'),\n",
        "#  ('9999_comrest_first_historic','2024_nodef_historic'),\n",
        "#  ('9999_comrest_first_historic','2024_nodeg_historic'),\n",
        "#  ('9999_comrest_first_historic','9999_comrec'),\n",
        "#  ('9999_comrest_first_historic','9999_comrest'),\n",
        "]"
      ],
      "metadata": {
        "id": "3aH-yteQ6QUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HkZIbBNFIUW"
      },
      "outputs": [],
      "source": [
        "# Functions for difference in mean and uncertainty\n",
        "def diff_mean(scenario1_mean, scenario2_mean):\n",
        "  diff_mean_array = scenario2_mean - scenario1_mean\n",
        "  return diff_mean_array\n",
        "def diff_uncertainty(scenario1_mean, scenario1_uncertainty, scenario2_mean, scenario2_uncertainty):\n",
        "  sums_of_squares = np.square( np.multiply( scenario1_mean, scenario1_uncertainty, dtype='float64'), dtype='float64') + np.square( np.multiply( scenario2_mean, scenario2_uncertainty, dtype='float64'), dtype='float64')\n",
        "  diff_uncertainty_array = np.sqrt(sums_of_squares, dtype='float64') / (scenario1_mean + scenario2_mean)\n",
        "  return diff_uncertainty_array\n",
        "\n",
        "# Loop through the scenario pairs\n",
        "for scenario1, scenario2 in scenario_pairs:\n",
        "\n",
        "  # Define filenames and directories of mean and uncertainty difference .tifs\n",
        "  diff_mean_filename = f\"diff_mean_{scenario1}_{scenario2}_{selected_scenario_iterations_area}_{selected_model}.tif\"\n",
        "  diff_mean_dir = join(diff_uncertainty_dir, diff_mean_filename)\n",
        "  diff_uncertainty_filename = f\"diff_uncertainty_{scenario1}_{scenario2}_{selected_scenario_iterations_area}_{selected_model}.tif\"\n",
        "  diff_uncertainty_raster_dir = join(diff_uncertainty_dir, diff_uncertainty_filename)\n",
        "\n",
        "  if not exists(diff_mean_dir) and not exists(diff_uncertainty_raster_dir):\n",
        "    print(f\"Calculating mean difference with uncertainty between {scenario1} and {scenario2}\")\n",
        "    scenario1_base_filename = f\"{scenario1}__{selected_scenario_iterations_area}_{selected_model}\"\n",
        "    scenario2_base_filename = f\"{scenario2}__{selected_scenario_iterations_area}_{selected_model}\"\n",
        "\n",
        "    # Define mean and uncertainty directories, assert that both exist for both scenarios\n",
        "    scenario1_mean_dir = join(statistics_masked_dir,f\"{scenario1_base_filename}__mean.tif\")\n",
        "    assert exists(scenario1_mean_dir), f\"{scenario1_base_filename}__mean.tif does not exist.\"\n",
        "    scenario1_uncertainty_dir = join(statistics_masked_dir,f\"{scenario1_base_filename}__uncertainty.tif\")\n",
        "    assert exists(scenario1_uncertainty_dir), f\"{scenario1_base_filename}__uncertainty.tif does not exist.\"\n",
        "    scenario2_mean_dir = join(statistics_masked_dir,f\"{scenario2_base_filename}__mean.tif\")\n",
        "    assert exists(scenario2_mean_dir), f\"{scenario2_base_filename}__mean.tif does not exist.\"\n",
        "    scenario2_uncertainty_dir = join(statistics_masked_dir,f\"{scenario2_base_filename}__uncertainty.tif\")\n",
        "    assert exists(scenario2_uncertainty_dir), f\"{scenario2_base_filename}__uncertainty.tif does not exist.\"\n",
        "\n",
        "    # Convert scenario mean and uncertainty .tifs to temporary arrays\n",
        "    scenario1_mean_array_temp = gdal.Open(scenario1_mean_dir).ReadAsArray()\n",
        "    scenario1_uncertainty_array_temp = gdal.Open(scenario1_uncertainty_dir).ReadAsArray()\n",
        "    scenario2_mean_array_temp = gdal.Open(scenario2_mean_dir).ReadAsArray()\n",
        "    scenario2_uncertainty_array_temp = gdal.Open(scenario2_uncertainty_dir).ReadAsArray()\n",
        "\n",
        "    # Fill scenario nodata values with 0 if they are not nodatavalues in the other scenario\n",
        "    scenario1_mean_array = np.where((scenario1_mean_array_temp == nodatavalue) & (scenario2_mean_array_temp != nodatavalue), 0, scenario1_mean_array_temp)\n",
        "    scenario1_uncertainty_array = np.where((scenario1_uncertainty_array_temp == nodatavalue) & (scenario2_uncertainty_array_temp != nodatavalue), 0, scenario1_uncertainty_array_temp)\n",
        "    scenario2_mean_array = np.where((scenario2_mean_array_temp == nodatavalue) & (scenario1_mean_array != nodatavalue), 0, scenario2_mean_array_temp)\n",
        "    scenario2_uncertainty_array = np.where((scenario2_uncertainty_array_temp == nodatavalue) & (scenario1_uncertainty_array != nodatavalue), 0, scenario2_uncertainty_array_temp)\n",
        "\n",
        "    # Create difference mean and uncertainty arrays where the value is not 'nodatavalue'\n",
        "    diff_mean_array = np.where(scenario1_mean_array==nodatavalue, nodatavalue, diff_mean(scenario1_mean_array, scenario2_mean_array))\n",
        "    diff_uncertainty_array = np.where(scenario1_uncertainty_array==nodatavalue, nodatavalue, diff_uncertainty(scenario1_mean_array, scenario1_uncertainty_array, scenario2_mean_array, scenario2_uncertainty_array))\n",
        "\n",
        "    # Export the mean and uncertainty difference .tifs if they do not exist\n",
        "    if exists(diff_mean_dir): print(f\"{diff_mean_filename} already exists.\")\n",
        "    else: export_array_as_tif(diff_mean_array, diff_mean_dir, template = scenario1_mean_dir), print(f\"{diff_mean_filename} has been exported.\")\n",
        "    if exists(diff_uncertainty_raster_dir): print(f\"{diff_uncertainty_filename} already exists.\")\n",
        "    else: export_array_as_tif(diff_uncertainty_array, diff_uncertainty_raster_dir, template = scenario1_mean_dir), print(f\"{diff_uncertainty_filename} has been exported.\")\n",
        "\n",
        "  else: print(f\"Both {diff_mean_filename} and {diff_uncertainty_filename} already exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nezNm_Gz3cnz"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kki0NPO3FKOw"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution upon completion\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "Xhx977-WIaWJ",
        "5lqjsLKZaQXo",
        "C9-hQ7G37uZi"
      ],
      "gpuType": "V28",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}