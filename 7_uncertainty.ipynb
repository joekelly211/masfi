{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/7_uncertainty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhx977-WIaWJ"
      },
      "source": [
        "# Imports and directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRnNty96IThZ"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4n2MGF0aZtJ"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install geopandas\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpmgsb8KKMGN"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "try: import cupy # Only works on GPU runtime\n",
        "except: None\n",
        "import ast\n",
        "import gc\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import ipywidgets as widgets\n",
        "import json\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy.random import normal\n",
        "from os import makedirs\n",
        "from os.path import join, exists, basename\n",
        "from osgeo import gdal\n",
        "gdal.UseExceptions()\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import scipy.stats as st\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import xgboost as xgb\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQwNO9T0_HPH"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "feature_dir = join(base_dir, \"3_features\")\n",
        "feature_final_dir = join(feature_dir, \"final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "masks_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(uncertainty_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koHn-AiGG7KM"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: read raster as array\n",
        "def read_raster_as_array(path):\n",
        "    ds = gdal.Open(path)\n",
        "    arr = ds.ReadAsArray()\n",
        "    ds = None\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lqjsLKZaQXo"
      },
      "source": [
        "# Select model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SqkKHKnaV-8"
      },
      "outputs": [],
      "source": [
        "# Select a baseline model, tested and trained in advance.\n",
        "# The target must have an uncertainty metric - otherwise\n",
        "# skip to the next notebook '8_statistics' and use the outputs\n",
        "# of the '6_scenarios' notebook.\n",
        "model_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file == 'model.json':\n",
        "      print(f'selected_model = \"{subdir.split(f\"{models_dir}/\",1)[1]}\"')\n",
        "      model_exists = True\n",
        "if not model_exists:\n",
        "  print(\"No model exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnerzp-gbapN"
      },
      "outputs": [],
      "source": [
        "selected_model = \"agbd_251203_161707\"\n",
        "\n",
        "# This must be True when using AlphaEarth features.\n",
        "alpha_earth = False\n",
        "\n",
        "# Define model directories\n",
        "selected_model_dir = join(models_dir,selected_model)\n",
        "selected_model_json = join(selected_model_dir, \"model.json\")\n",
        "selected_model_descr_dir = join(selected_model_dir, \"model_description.json\")\n",
        "selected_model_dataset_path = join(selected_model_dir, f\"{selected_model}.pkl\")\n",
        "selected_model_dataset = pd.read_pickle(selected_model_dataset_path)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(selected_model_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_target = model_dataset_description[\"selected_target\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "selected_features = model_dataset_description[\"selected_features\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_features_mappings = model_dataset_description[\"categorical_features_mappings\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "# Reload hyperparameters\n",
        "with open(selected_model_descr_dir) as model_description_json:\n",
        "  model_description = json.load(model_description_json)\n",
        "final_hyperparameters = ast.literal_eval(model_description[\"hyperparameters\"])\n",
        "\n",
        "# Remove early stopping and replace with mean n_estimators\n",
        "if \"early_stopping_rounds\" in final_hyperparameters:\n",
        "  final_hyperparameters = {k:v for k, v in final_hyperparameters.items() if k != \"early_stopping_rounds\"}\n",
        "  final_hyperparameters[\"n_estimators\"] = round(model_description[\"n_estimators mean\"])\n",
        "\n",
        "# Define directories\n",
        "scenarios_model_dir = join(scenarios_dir,selected_model)\n",
        "features_dir = join(scenarios_model_dir, \"features\")\n",
        "tile_templates_dir = join(scenarios_model_dir, 'tile_templates')\n",
        "tile_feature_stacks_dir = join(scenarios_model_dir, 'tile_feature_stacks')\n",
        "uncertainty_selected_model_dir = join(uncertainty_dir, selected_model)\n",
        "model_iterations_dir = join(uncertainty_selected_model_dir, \"model_iterations\")\n",
        "scenario_iterations_dir = join(uncertainty_selected_model_dir, \"scenario_iterations\")\n",
        "tile_prediction_cache_dir = join(uncertainty_selected_model_dir, \"tile_prediction_cache\")\n",
        "predictions_dir = join(uncertainty_selected_model_dir, \"uncertainty_predictions\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(uncertainty_selected_model_dir, exist_ok=True)\n",
        "makedirs(model_iterations_dir, exist_ok=True)\n",
        "makedirs(scenario_iterations_dir, exist_ok=True)\n",
        "makedirs(tile_prediction_cache_dir, exist_ok=True)\n",
        "makedirs(predictions_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9-hQ7G37uZi"
      },
      "source": [
        "# Model iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWymSi4ndKUA"
      },
      "outputs": [],
      "source": [
        "# Verify that the target is equal to the mean\n",
        "print(f'mean = \"{selected_target}\"')\n",
        "\n",
        "# Calculate se from columns flagged 'uncertainty'\n",
        "if len(uncertainty)==0:\n",
        "  print(\"There are no flagged uncertainty columns to calculate SE from.\")\n",
        "  print(\"Manually create the metric from the available columns.\")\n",
        "  for col in selected_model_dataset.columns:\n",
        "    print(f\"{col}\")\n",
        "else:\n",
        "  for col in selected_model_dataset.columns:\n",
        "    if col in uncertainty and col not in selected_target:\n",
        "      print(f'se = \"{col}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBalj00xwmRX"
      },
      "outputs": [],
      "source": [
        "mean = \"tar_agbd\"\n",
        "se = \"tar_agbd_se\"\n",
        "\n",
        "# GEDI L4A agbd_se represents the prediction standard error incorporating both model\n",
        "# parameter uncertainty and residual variance (GEDI L4A ATBD Eq. 9, Kellner et al. 2021).\n",
        "# For Monte Carlo uncertainty propagation, agbd_se directly parameterises the standard\n",
        "# deviation of the prediction error distribution for each footprint.\n",
        "\n",
        "# Set model iterations\n",
        "model_iterations = 10\n",
        "\n",
        "# Define model (y axis changes for each iteration based on mean and se arrays)\n",
        "model_dataset_x = selected_model_dataset[selected_features].copy()\n",
        "for col in categorical_columns:\n",
        "    if col in model_dataset_x.columns:\n",
        "        model_dataset_x[col] = model_dataset_x[col].astype('category')\n",
        "mean_array = selected_model_dataset[mean].values\n",
        "se_array = selected_model_dataset[se].values\n",
        "\n",
        "# Detect model type from existing model or determine from target\n",
        "existing_model_path = join(model_iterations_dir, \"model_iteration_1.json\")\n",
        "if exists(existing_model_path):\n",
        "    # Load existing model and detect type\n",
        "    temp_booster = xgb.Booster()\n",
        "    temp_booster.load_model(existing_model_path)\n",
        "    model_config = json.loads(temp_booster.save_config())\n",
        "\n",
        "    objective_name = model_config['learner']['objective']['name']\n",
        "    num_class = int(model_config['learner']['learner_model_param'].get('num_class', '0'))\n",
        "    classification = any(obj_type in objective_name for obj_type in ['logistic', 'softprob', 'softmax'])\n",
        "    multiclass = classification and num_class > 2\n",
        "else:\n",
        "    # Determine from target variable characteristics\n",
        "    unique_values = len(np.unique(mean_array))\n",
        "    if unique_values <= 10 and all(val == int(val) for val in np.unique(mean_array)):\n",
        "        classification = True\n",
        "        multiclass = unique_values > 2\n",
        "        num_class = unique_values if multiclass else 0\n",
        "    else:\n",
        "        classification = False\n",
        "        multiclass = False\n",
        "        num_class = 0\n",
        "\n",
        "# Set model type\n",
        "if classification:\n",
        "    XGBPredictor = xgb.XGBClassifier(**final_hyperparameters)\n",
        "    if multiclass: print(f\"Model type: Multiclass classification ({num_class} classes)\")\n",
        "    else: print(\"Model type: Binary classification\")\n",
        "else:\n",
        "    XGBPredictor = xgb.XGBRegressor(**final_hyperparameters)\n",
        "    print(\"Model type: Regression\")\n",
        "\n",
        "model_params = XGBPredictor.get_params()\n",
        "model_params['eval_metric'] = model_description['metric_used_for_training']\n",
        "# Default fix for new XGBoost version\n",
        "[model_params.pop(key, None) for key in ['n_estimators', 'enable_categorical', 'missing']]\n",
        "\n",
        "# Progress label\n",
        "model_progress_index = 0\n",
        "model_progress_label = widgets.Label(f\"Model iteration: {model_progress_index}/{model_iterations}\")\n",
        "display(model_progress_label)\n",
        "\n",
        "for model_iteration in range(1,model_iterations+1):\n",
        "  # Set model iteration filename and check if already exists\n",
        "  model_iteration_filename = f\"model_iteration_{model_iteration}.json\"\n",
        "  model_iteration_path = join(model_iterations_dir, model_iteration_filename)\n",
        "  # If model iteration does not exist...\n",
        "  if not exists(model_iteration_path):\n",
        "    # Set the random seed based on iteration for replicability\n",
        "    np.random.seed(model_iteration)\n",
        "    # Set a normal distribution sample as the y for this iteration\n",
        "    model_dataset_y = normal(mean_array, se_array)\n",
        "    # Create DMatrix objects\n",
        "    model_dtrain = xgb.DMatrix(model_dataset_x, model_dataset_y, enable_categorical=True)\n",
        "    # Train the model iteration using the tested hyperparameters\n",
        "    model = xgb.train(model_params,\n",
        "                        model_dtrain,\n",
        "                        num_boost_round=final_hyperparameters['n_estimators'],\n",
        "                        verbose_eval=True)\n",
        "    # Save the model iteration\n",
        "    model.save_model(model_iteration_path)\n",
        "  # Update progress\n",
        "  model_progress_index += 1\n",
        "  model_progress_label.value = f\"Model iteration: {model_progress_index}/{model_iterations}\"\n",
        "print(\"All model iterations have been trained and saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1UxS-kO8_1d"
      },
      "source": [
        "# Scenario iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYxqe0eeKmKi"
      },
      "outputs": [],
      "source": [
        "# Scenarios must be designed and tested using 06_scenarios first.\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = [f for f in os.listdir(tile_templates_dir) if f.endswith('.tif') and f.startswith('template_tile')]\n",
        "n_tiles = len(template_tile_list)\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "if n_tiles == 1: print(f\"# There is 1 template tile.\\n\")\n",
        "if n_tiles > 1: print(f\"# There are {n_tiles} template tiles.\\n\")\n",
        "\n",
        "# Collect available scenarios from the feature stack tiles directory\n",
        "scenario_stacks_list = []\n",
        "for scenario in os.listdir(tile_feature_stacks_dir):\n",
        "    stack_files = [f for f in os.listdir(join(tile_feature_stacks_dir, scenario)) if f.startswith('feature_stack_')]\n",
        "    if len(stack_files) == n_tiles:\n",
        "        scenario_stacks_list.append(scenario)\n",
        "\n",
        "print(\"scenarios_to_predict = [\")\n",
        "for scenario in sorted(scenario_stacks_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE-bIxq9F3Vy"
      },
      "outputs": [],
      "source": [
        "# There is 1 template tile.\n",
        "\n",
        "scenarios_to_predict = [\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_undisturbed_since_1993\",\n",
        "  \"2021_undisturbed_since_oldgrowth_1\",\n",
        "  \"2021_undisturbed_since_oldgrowth_2\",\n",
        "  \"2021_oldgrowth_recovery_1\",\n",
        "  \"2021_oldgrowth_recovery_2\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_undisturbed_since_1996\",\n",
        "  \"2024_undisturbed_since_1997\",\n",
        "  \"2024_undisturbed_since_1998\",\n",
        "  \"2024_undisturbed_since_1999\",\n",
        "  \"2024_undisturbed_since_2000\",\n",
        "  \"2024_undisturbed_since_2001\",\n",
        "  \"2024_undisturbed_since_2002\",\n",
        "  \"2024_undisturbed_since_2003\",\n",
        "  \"2024_undisturbed_since_2004\",\n",
        "  \"2024_undisturbed_since_2005\",\n",
        "  \"2024_undisturbed_since_2006\",\n",
        "  \"2024_undisturbed_since_2007\",\n",
        "  \"2024_undisturbed_since_2008\",\n",
        "  \"2024_undisturbed_since_2009\",\n",
        "  \"2024_undisturbed_since_2010\",\n",
        "  \"2024_undisturbed_since_2011\",\n",
        "  \"2024_undisturbed_since_2012\",\n",
        "  \"2024_undisturbed_since_2013\",\n",
        "  \"2024_undisturbed_since_2014\",\n",
        "  \"2024_undisturbed_since_2015\",\n",
        "  \"2024_undisturbed_since_2016\",\n",
        "  \"2024_undisturbed_since_2017\",\n",
        "  \"2024_undisturbed_since_2018\",\n",
        "  \"2024_undisturbed_since_2019\",\n",
        "  \"2024_undisturbed_since_2020\",\n",
        "  \"2024_undisturbed_since_2021\",\n",
        "  \"2024_undisturbed_since_2022\",\n",
        "  \"2024_undisturbed_since_2023\",\n",
        "  \"2024_undisturbed_since_2024\",\n",
        "  \"2024_undisturbed_since_oldgrowth_1\",\n",
        "  \"2024_undisturbed_since_oldgrowth_2\",\n",
        "  \"2024_oldgrowth_recovery_1\",\n",
        "  \"2024_oldgrowth_recovery_2\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of scenario iterations. It must be <= the number of model iterations available.\n",
        "scenario_iterations = 10\n",
        "\n",
        "# Assert the number of scenario iterations is <= the number of model iterations available.\n",
        "model_iterations_available = len([f for f in os.listdir(model_iterations_dir) if f.startswith('model_iteration_') and f.endswith('.json')])\n",
        "assert scenario_iterations <= model_iterations_available, f\"Reduce the number of scenario iterations to <= {model_iterations_available}.\"\n",
        "\n",
        "# Prediction raster precision. GEDI AGBD can be set to 0, any higher is\n",
        "# spurious due to the wide prediction intervals of the source data.\n",
        "raster_precision = 0\n",
        "\n",
        "# Probabilities instead of classes IF binary classification\n",
        "predict_probabilities = False\n",
        "\n",
        "# Classification threshold IF binary classification\n",
        "classification_threshold = 0.5\n",
        "\n",
        "# Detect GPU availability and set predictor type\n",
        "try:\n",
        "    test_array = cupy.array([1, 2, 3])\n",
        "    del test_array\n",
        "    predictor_type = 'gpu_predictor'\n",
        "    gpu_id, use_gpu = 0, True\n",
        "    print(\"GPU detected and accessible - using GPU to load the feature stack and to predict.\")\n",
        "except:\n",
        "    predictor_type = 'cpu_predictor'\n",
        "    gpu_id, use_gpu = -1, False\n",
        "    print(\"GPU not accessible - using CPU prediction\")\n",
        "xgb.set_config(verbosity=0, use_rmm=use_gpu)\n",
        "\n",
        "# Detect model type using first model iteration\n",
        "first_model_path = join(model_iterations_dir, \"model_iteration_1.json\")\n",
        "booster = xgb.Booster()\n",
        "booster.load_model(first_model_path)\n",
        "model_config = json.loads(booster.save_config())\n",
        "objective_name = model_config['learner']['objective']['name']\n",
        "num_class = int(model_config['learner']['learner_model_param'].get('num_class', '0'))\n",
        "classification = any(obj_type in objective_name for obj_type in ['logistic', 'softprob', 'softmax'])\n",
        "multiclass = classification and num_class > 2\n",
        "if classification and multiclass: print(f\"Model type: Multiclass classification ({num_class} classes)\")\n",
        "elif classification: print(\"Model type: Binary classification\")\n",
        "else: print(\"Model type: Regression\")\n",
        "booster = None\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = [f for f in os.listdir(tile_templates_dir) if f.endswith('.tif') and f.startswith('template_tile')]\n",
        "n_tiles = len(template_tile_list)\n",
        "if n_tiles < 1: print(\"There are currently no template tiles.\")\n",
        "else:\n",
        "  template_tile = gdal.Open(join(tile_templates_dir, 'template_tile_1.tif'))\n",
        "  template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "  template_tile = None\n",
        "  print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "if alpha_earth: template_base_path = next(r for r in model_scenario_features_dirs if all(c not in r for c in covariates))\n",
        "else: template_base_path = template_tif_path\n",
        "\n",
        "# Progress labels\n",
        "n_scenarios = len(scenarios_to_predict)\n",
        "scenario_progress_label = widgets.Label(value=f\"Scenario progress: 0 / {n_scenarios}\")\n",
        "tile_progress_label = widgets.Label(value=\"Tile progress: -\")\n",
        "iteration_progress_label = widgets.Label(value=\"Iteration progress: -\")\n",
        "display(scenario_progress_label, tile_progress_label, iteration_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario_index, scenario in enumerate(scenarios_to_predict):\n",
        "  scenario_feature_stack_dir = join(tile_feature_stacks_dir, scenario)\n",
        "  # Create scenario iterations directory\n",
        "  iterations_dir = join(scenario_iterations_dir, f\"{scenario}_iterations\")\n",
        "  makedirs(iterations_dir, exist_ok=True)\n",
        "\n",
        "  # Check if all scenario iterations already exist\n",
        "  scenario_iteration_list = []\n",
        "  for model_iteration in range(1, scenario_iterations + 1):\n",
        "    prediction_iteration_filename = f\"{scenario}__{selected_model}_iteration_{model_iteration}.tif\"\n",
        "    prediction_iteration_path = join(iterations_dir, prediction_iteration_filename)\n",
        "    scenario_iteration_list.append(prediction_iteration_path)\n",
        "  all_scenario_iterations_exist = all(exists(p) for p in scenario_iteration_list)\n",
        "\n",
        "  if not all_scenario_iterations_exist:\n",
        "    n_stacks = len([f for f in os.listdir(scenario_feature_stack_dir) if f.startswith('feature_stack_')])\n",
        "\n",
        "    # Single-stack prediction\n",
        "    if n_stacks == 1:\n",
        "      tile_progress_label.value = \"Tile progress: 0 / 1\"\n",
        "      iteration_progress_label.value = f\"Iteration progress: 0 / {scenario_iterations}\"\n",
        "\n",
        "      # Load template parameters\n",
        "      template_tile_path = join(tile_templates_dir, \"template_tile_1.tif\")\n",
        "      template_tile = gdal.Open(template_tile_path)\n",
        "      template_tile_y = template_tile.GetRasterBand(1).YSize\n",
        "      template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "      template_tile = None\n",
        "      n_pixels = template_tile_y * template_tile_x\n",
        "      # Load feature stack and valid indices\n",
        "      stack_path = join(scenario_feature_stack_dir, f\"feature_stack_{scenario}_1.npy\")\n",
        "      indices_path = join(scenario_feature_stack_dir, f\"valid_indices_{scenario}_1.npy\")\n",
        "      feature_stack = np.load(stack_path)\n",
        "      valid_indices = np.load(indices_path)\n",
        "      n_valid = len(valid_indices)\n",
        "      # Load to GPU if available and valid pixels exist\n",
        "      if n_valid > 0 and use_gpu:\n",
        "        try:\n",
        "          feature_stack = cupy.asarray(feature_stack)\n",
        "        except Exception as e:\n",
        "          if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "            print(\"GPU memory insufficient, switching to CPU.\")\n",
        "            cupy.get_default_memory_pool().free_all_blocks()\n",
        "            gc.collect()\n",
        "            use_gpu = False\n",
        "            predictor_type = 'cpu_predictor'\n",
        "          else: raise\n",
        "\n",
        "      # Predict all iterations using loaded stack\n",
        "      iteration_progress_index = 0\n",
        "      for model_iteration in range(1, scenario_iterations + 1):\n",
        "        model_path = join(model_iterations_dir, f\"model_iteration_{model_iteration}.json\")\n",
        "        prediction_iteration_filename = f\"{scenario}__{selected_model}_iteration_{model_iteration}.tif\"\n",
        "        prediction_iteration_path = join(iterations_dir, prediction_iteration_filename)\n",
        "\n",
        "        if not exists(prediction_iteration_path):\n",
        "          # Load model iteration\n",
        "          if classification:\n",
        "            XGBPredictor = xgb.XGBClassifier()\n",
        "            XGBPredictor.load_model(model_path)\n",
        "            XGBPredictor.set_params(predictor=predictor_type)\n",
        "            if use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "          else:\n",
        "            XGBPredictor = xgb.XGBRegressor()\n",
        "            XGBPredictor.load_model(model_path)\n",
        "            XGBPredictor.set_params(predictor=predictor_type)\n",
        "            if use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "\n",
        "          # Handle empty tiles\n",
        "          if n_valid == 0:\n",
        "            if raster_precision == 0:\n",
        "              prediction_array = np.full((template_tile_y, template_tile_x), nodatavalue, dtype=np.int16)\n",
        "            else:\n",
        "              prediction_array = np.full((template_tile_y, template_tile_x), nodatavalue, dtype=np.float32)\n",
        "          else:\n",
        "            # Predict - terminate runtime if GPU prediction fails\n",
        "            try:\n",
        "              if classification and predict_probabilities and not multiclass:\n",
        "                prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                prediction = prediction_proba[:, 1]\n",
        "              else:\n",
        "                if classification and not multiclass:\n",
        "                  prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                  prediction = (prediction_proba[:, 1] > classification_threshold).astype(int)\n",
        "                else:\n",
        "                  prediction = XGBPredictor.predict(feature_stack)\n",
        "                  if classification:\n",
        "                    if prediction.ndim > 1 and prediction.shape[1] > 1: prediction = np.argmax(prediction, axis=1)\n",
        "                    prediction = prediction.astype(int)\n",
        "            except Exception as e:\n",
        "              if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "                print(\"GPU memory insufficient for prediction. Terminating runtime to save compute units, restart with TPU.\")\n",
        "                runtime.unassign()\n",
        "              else: raise\n",
        "            # Reconstruct full array from valid indices (C-order)\n",
        "            if raster_precision == 0:\n",
        "              prediction_flat = np.full(n_pixels, nodatavalue, dtype=np.int16)\n",
        "              prediction_flat[valid_indices] = np.round(prediction).astype(np.int16)\n",
        "            else:\n",
        "              prediction_flat = np.full(n_pixels, nodatavalue, dtype=np.float32)\n",
        "              prediction_flat[valid_indices] = np.round(prediction, raster_precision)\n",
        "            prediction = None\n",
        "            prediction_array = prediction_flat.reshape((template_tile_y, template_tile_x), order='C')\n",
        "            prediction_flat = None\n",
        "          export_array_as_tif(prediction_array, prediction_iteration_path, template=template_base_path, compress=True)\n",
        "          prediction_array = None\n",
        "\n",
        "        iteration_progress_index += 1\n",
        "        iteration_progress_label.value = f\"Iteration progress: {iteration_progress_index} / {scenario_iterations}\"\n",
        "      # Clean up single-stack feature stack from memory\n",
        "      feature_stack = valid_indices = None\n",
        "      tile_progress_label.value = \"Tile progress: 1 / 1\"\n",
        "\n",
        "    # Tiled prediction - load each stack once, predict all iterations per tile\n",
        "    if n_stacks > 1:\n",
        "      # Create tile cache directories for all iterations\n",
        "      tile_cache_dirs = {}\n",
        "      for model_iteration in range(1, scenario_iterations + 1):\n",
        "        prediction_iteration_filename = f\"{scenario}__{selected_model}_iteration_{model_iteration}.tif\"\n",
        "        tile_cache_iteration_dir = join(tile_prediction_cache_dir, prediction_iteration_filename[:-4])\n",
        "        makedirs(tile_cache_iteration_dir, exist_ok=True)\n",
        "        tile_cache_dirs[model_iteration] = tile_cache_iteration_dir\n",
        "\n",
        "      # Process each tile: load stack once, predict all iterations\n",
        "      for tile_count in range(1, n_stacks + 1):\n",
        "        # Determine which iterations still need this tile\n",
        "        iterations_needing_tile = []\n",
        "        for model_iteration in range(1, scenario_iterations + 1):\n",
        "          iteration_tile_path = join(tile_cache_dirs[model_iteration], f\"scenario_tile_{tile_count}.tif\")\n",
        "          prediction_iteration_path = join(iterations_dir, f\"{scenario}__{selected_model}_iteration_{model_iteration}.tif\")\n",
        "          if not exists(prediction_iteration_path) and not exists(iteration_tile_path):\n",
        "            iterations_needing_tile.append(model_iteration)\n",
        "\n",
        "        # Skip tile if no iterations need it\n",
        "        n_iterations_for_tile = len(iterations_needing_tile)\n",
        "        if n_iterations_for_tile == 0:\n",
        "          iteration_progress_label.value = f\"Iteration progress: {scenario_iterations} / {scenario_iterations}\"\n",
        "          tile_progress_label.value = f\"Tile progress: {tile_count} / {n_stacks}\"\n",
        "          continue\n",
        "        iteration_progress_label.value = f\"Iteration progress: 0 / {n_iterations_for_tile}\"\n",
        "\n",
        "        # Load template tile parameters\n",
        "        template_tile_path = join(tile_templates_dir, f\"template_tile_{tile_count}.tif\")\n",
        "        template_tile = gdal.Open(template_tile_path)\n",
        "        template_tile_y = template_tile.GetRasterBand(1).YSize\n",
        "        template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "        template_tile = None\n",
        "        n_pixels = template_tile_y * template_tile_x\n",
        "\n",
        "        # Load feature stack and valid indices once per tile\n",
        "        stack_path = join(scenario_feature_stack_dir, f\"feature_stack_{scenario}_{tile_count}.npy\")\n",
        "        indices_path = join(scenario_feature_stack_dir, f\"valid_indices_{scenario}_{tile_count}.npy\")\n",
        "        if not exists(stack_path):\n",
        "          print(f\"Warning: {basename(stack_path)} not found. Skipping tile {tile_count}.\")\n",
        "          continue\n",
        "        feature_stack = np.load(stack_path)\n",
        "        valid_indices = np.load(indices_path)\n",
        "        n_valid = len(valid_indices)\n",
        "\n",
        "        # Handle empty tiles for all needed iterations\n",
        "        if n_valid == 0:\n",
        "          if raster_precision == 0:\n",
        "            prediction_tile = np.full((template_tile_y, template_tile_x), nodatavalue, dtype=np.int16)\n",
        "          else: prediction_tile = np.full((template_tile_y, template_tile_x), nodatavalue, dtype=np.float32)\n",
        "          tile_iteration_index = 0\n",
        "          for model_iteration in iterations_needing_tile:\n",
        "            iteration_tile_path = join(tile_cache_dirs[model_iteration], f\"scenario_tile_{tile_count}.tif\")\n",
        "            export_array_as_tif(prediction_tile, iteration_tile_path, template=template_tile_path, compress=False)\n",
        "            tile_iteration_index += 1\n",
        "            iteration_progress_label.value = f\"Iteration progress: {tile_iteration_index} / {n_iterations_for_tile}\"\n",
        "          prediction_tile = None\n",
        "          tile_progress_label.value = f\"Tile progress: {tile_count} / {n_stacks}\"\n",
        "          continue\n",
        "\n",
        "        # Load to GPU if available\n",
        "        tile_use_gpu = use_gpu\n",
        "        if use_gpu:\n",
        "          try: feature_stack = cupy.asarray(feature_stack)\n",
        "          except Exception as e:\n",
        "            if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "              print(f\"GPU memory insufficient for tile {tile_count}, using CPU.\")\n",
        "              cupy.get_default_memory_pool().free_all_blocks()\n",
        "              gc.collect()\n",
        "              tile_use_gpu = False\n",
        "            else: raise\n",
        "\n",
        "        # Predict all needed iterations for this tile\n",
        "        tile_iteration_index = 0\n",
        "        for model_iteration in iterations_needing_tile:\n",
        "          iteration_tile_path = join(tile_cache_dirs[model_iteration], f\"scenario_tile_{tile_count}.tif\")\n",
        "          model_path = join(model_iterations_dir, f\"model_iteration_{model_iteration}.json\")\n",
        "          # Load model iteration\n",
        "          if classification:\n",
        "            XGBPredictor = xgb.XGBClassifier()\n",
        "            XGBPredictor.load_model(model_path)\n",
        "            XGBPredictor.set_params(predictor='gpu_predictor' if tile_use_gpu else 'cpu_predictor')\n",
        "            if tile_use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "          else:\n",
        "            XGBPredictor = xgb.XGBRegressor()\n",
        "            XGBPredictor.load_model(model_path)\n",
        "            XGBPredictor.set_params(predictor='gpu_predictor' if tile_use_gpu else 'cpu_predictor')\n",
        "            if tile_use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "          # Predict - terminate runtime if GPU prediction fails\n",
        "          try:\n",
        "            if classification and predict_probabilities and not multiclass:\n",
        "              prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "              prediction = prediction_proba[:, 1]\n",
        "            else:\n",
        "              if classification and not multiclass:\n",
        "                prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                prediction = (prediction_proba[:, 1] > classification_threshold).astype(int)\n",
        "              else:\n",
        "                prediction = XGBPredictor.predict(feature_stack)\n",
        "                if classification:\n",
        "                  if prediction.ndim > 1 and prediction.shape[1] > 1: prediction = np.argmax(prediction, axis=1)\n",
        "                  prediction = prediction.astype(int)\n",
        "          except Exception as e:\n",
        "            if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "              print(\"GPU memory insufficient for prediction. Terminating runtime to save compute units, restart with TPU.\")\n",
        "              runtime.unassign()\n",
        "            else: raise\n",
        "          # Reconstruct full tile from valid indices (C-order)\n",
        "          if raster_precision == 0:\n",
        "            prediction_flat = np.full(n_pixels, nodatavalue, dtype=np.int16)\n",
        "            prediction_flat[valid_indices] = np.round(prediction).astype(np.int16)\n",
        "          else:\n",
        "            prediction_flat = np.full(n_pixels, nodatavalue, dtype=np.float32)\n",
        "            prediction_flat[valid_indices] = np.round(prediction, raster_precision)\n",
        "          prediction = None\n",
        "          prediction_tile = prediction_flat.reshape((template_tile_y, template_tile_x), order='C')\n",
        "          prediction_flat = None\n",
        "          export_array_as_tif(prediction_tile, iteration_tile_path, template=template_tile_path, compress=False)\n",
        "          prediction_tile = None\n",
        "\n",
        "          tile_iteration_index += 1\n",
        "          iteration_progress_label.value = f\"Iteration progress: {tile_iteration_index} / {n_iterations_for_tile}\"\n",
        "\n",
        "        # Release tile resources\n",
        "        feature_stack = valid_indices = None\n",
        "        tile_progress_label.value = f\"Tile progress: {tile_count} / {n_stacks}\"\n",
        "\n",
        "      # Merge tiles for each iteration\n",
        "      tile_progress_label.value = \"Tile progress: merging\"\n",
        "      for model_iteration in range(1, scenario_iterations + 1):\n",
        "        prediction_iteration_filename = f\"{scenario}__{selected_model}_iteration_{model_iteration}.tif\"\n",
        "        prediction_iteration_path = join(iterations_dir, prediction_iteration_filename)\n",
        "        tile_cache_iteration_dir = tile_cache_dirs[model_iteration]\n",
        "        if not exists(prediction_iteration_path):\n",
        "          prediction_array = np.empty((0, template_tile_x))\n",
        "          tile_files = sorted([f for f in os.listdir(tile_cache_iteration_dir) if f.endswith('.tif')],\n",
        "                              key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "          for tile_file in tile_files:\n",
        "            tile_path = join(tile_cache_iteration_dir, tile_file)\n",
        "            tile_array = read_raster_as_array(tile_path)\n",
        "            prediction_array = np.vstack((prediction_array, tile_array))\n",
        "          export_array_as_tif(prediction_array, prediction_iteration_path, template=template_base_path, compress=True)\n",
        "          prediction_array = None\n",
        "        # Delete tile cache for this iteration\n",
        "        if exists(tile_cache_iteration_dir):\n",
        "          shutil.rmtree(tile_cache_iteration_dir)\n",
        "\n",
        "        iteration_progress_label.value = f\"Merge progress: {model_iteration} / {scenario_iterations}\"\n",
        "\n",
        "  else:\n",
        "    tile_progress_label.value = \"Tile progress: complete\"\n",
        "    iteration_progress_label.value = \"Iteration progress: complete\"\n",
        "\n",
        "  scenario_progress_label.value = f\"Scenario progress: {scenario_index + 1} / {n_scenarios}\"\n",
        "\n",
        "print(\"\\nScenario iterations complete.\")"
      ],
      "metadata": {
        "id": "VDnbOWSegeVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge oldgrowth iteration pairs\n",
        "# Old-growth scenarios exist in two versions: _1 uses land-use proxy for pre-Landsat\n",
        "# undisturbed forest (e.g. PAs without history of exploitation) and removes all Landsat-derived\n",
        "# disturbance. _2 removes all Landsat-derived disturbance. The merge takes the maximum mean,\n",
        "# avoiding underestimation where proxy may not capture all oldgrowth characteristics of the\n",
        "# entire project area. Merging at iteration level (rather than of the calculated mean)\n",
        "# allows Monte Carlo uncertainty propagation when calculating AGBD percentage loss between\n",
        "# the old-growth alternative scenario baseline to the 'actual' state.\n",
        "\n",
        "# Identify oldgrowth _1 iteration directories\n",
        "oldgrowth_v1_dirs = [d for d in os.listdir(scenario_iterations_dir)\n",
        "                     if 'oldgrowth' in d and d.endswith('_1_iterations')]\n",
        "\n",
        "if not oldgrowth_v1_dirs: print(\"No oldgrowth version 1 iteration directories found to merge.\")\n",
        "else:\n",
        "    n_oldgrowth = len(oldgrowth_v1_dirs)\n",
        "    oldgrowth_progress_label = widgets.Label(value=f\"Oldgrowth scenario progress: 0 / {n_oldgrowth}\")\n",
        "    iteration_merge_progress_label = widgets.Label(value=\"Iteration merge progress: -\")\n",
        "    display(oldgrowth_progress_label, iteration_merge_progress_label)\n",
        "\n",
        "    for og_index, v1_dir_name in enumerate(oldgrowth_v1_dirs):\n",
        "        # Define directory names and paths\n",
        "        v2_dir_name = v1_dir_name.replace('_1_iterations', '_2_iterations')\n",
        "        merged_dir_name = v1_dir_name.replace('_1_iterations', '_iterations')\n",
        "        v1_dir_path = join(scenario_iterations_dir, v1_dir_name)\n",
        "        v2_dir_path = join(scenario_iterations_dir, v2_dir_name)\n",
        "        merged_dir_path = join(scenario_iterations_dir, merged_dir_name)\n",
        "\n",
        "        # Skip if merged directory already exists with iterations\n",
        "        if exists(merged_dir_path):\n",
        "            existing_merged = [f for f in os.listdir(merged_dir_path) if f.endswith('.tif')]\n",
        "            if existing_merged:\n",
        "                iteration_merge_progress_label.value = f\"Iteration merge progress: skipped (exists)\"\n",
        "                oldgrowth_progress_label.value = f\"Oldgrowth scenario progress: {og_index + 1} / {n_oldgrowth}\"\n",
        "                continue\n",
        "\n",
        "        # Check version 2 exists\n",
        "        if not exists(v2_dir_path):\n",
        "            print(f\"Warning: {v2_dir_name} not found, skipping merge for {v1_dir_name}\")\n",
        "            oldgrowth_progress_label.value = f\"Oldgrowth scenario progress: {og_index + 1} / {n_oldgrowth}\"\n",
        "            continue\n",
        "\n",
        "        # Collect iteration files from both versions\n",
        "        v1_files = sorted([f for f in os.listdir(v1_dir_path) if f.endswith('.tif')])\n",
        "        v2_files = sorted([f for f in os.listdir(v2_dir_path) if f.endswith('.tif')])\n",
        "        if len(v1_files) != len(v2_files):\n",
        "            print(f\"Warning: iteration count mismatch ({len(v1_files)} vs {len(v2_files)}), skipping {v1_dir_name}\")\n",
        "            oldgrowth_progress_label.value = f\"Oldgrowth scenario progress: {og_index + 1} / {n_oldgrowth}\"\n",
        "            continue\n",
        "        makedirs(merged_dir_path, exist_ok=True)\n",
        "        n_iterations = len(v1_files)\n",
        "\n",
        "        # Merge each iteration pair\n",
        "        for iter_index, v1_file in enumerate(v1_files):\n",
        "            # Construct merged filename (remove _1 suffix from scenario name in filename)\n",
        "            merged_file = v1_file.replace('_1__', '__')\n",
        "            merged_path = join(merged_dir_path, merged_file)\n",
        "            if not exists(merged_path):\n",
        "                # Construct v2 filename and paths\n",
        "                v2_file = v1_file.replace('_1__', '_2__')\n",
        "                v1_path = join(v1_dir_path, v1_file)\n",
        "                v2_path = join(v2_dir_path, v2_file)\n",
        "                if not exists(v2_path):\n",
        "                    print(f\"Warning: {v2_file} not found, skipping\")\n",
        "                    continue\n",
        "\n",
        "                # Load both iterations\n",
        "                v1_array = read_raster_as_array(v1_path)\n",
        "                v2_array = read_raster_as_array(v2_path)\n",
        "\n",
        "                # Take maximum value per pixel (nodata remains nodata as both versions share same mask)\n",
        "                merged_array = np.maximum(v1_array, v2_array)\n",
        "\n",
        "                # Export merged iteration\n",
        "                export_array_as_tif(merged_array, merged_path, template=v1_path, compress=True)\n",
        "                merged_array = v1_array = v2_array = None\n",
        "\n",
        "            iteration_merge_progress_label.value = f\"Iteration merge progress: {iter_index + 1} / {n_iterations}\"\n",
        "        oldgrowth_progress_label.value = f\"Oldgrowth scenario progress: {og_index + 1} / {n_oldgrowth}\"\n",
        "\n",
        "    print(\"Oldgrowth iteration merging complete.\")"
      ],
      "metadata": {
        "id": "iVDxVP__Jkjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIWo8ATZs-Ny"
      },
      "source": [
        "# Predictions with uncertainty\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect scenarios with iterations\n",
        "# Excludes oldgrowth _1 and _2 variants since merged versions are used for statistics\n",
        "scenarios_iterations_list = []\n",
        "for subdir in os.listdir(scenario_iterations_dir):\n",
        "    scenario_name = subdir[:-11]\n",
        "    if 'oldgrowth' in scenario_name and (scenario_name.endswith('_1') or scenario_name.endswith('_2')):\n",
        "        continue\n",
        "    scenarios_iterations_list.append(scenario_name)\n",
        "\n",
        "# Select scenarios to calculate mean, confidence intervals and uncertainty\n",
        "print(\"scenarios_to_calculate = [\")\n",
        "for scenario in sorted(scenarios_iterations_list):\n",
        "    print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ],
      "metadata": {
        "id": "9PObs4doEYve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scenarios_to_calculate = [\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_undisturbed_since_1993\",\n",
        "  \"2021_undisturbed_since_oldgrowth\",\n",
        "  \"2021_oldgrowth_recovery\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_undisturbed_since_1996\",\n",
        "  \"2024_undisturbed_since_1997\",\n",
        "  \"2024_undisturbed_since_1998\",\n",
        "  \"2024_undisturbed_since_1999\",\n",
        "  \"2024_undisturbed_since_2000\",\n",
        "  \"2024_undisturbed_since_2001\",\n",
        "  \"2024_undisturbed_since_2002\",\n",
        "  \"2024_undisturbed_since_2003\",\n",
        "  \"2024_undisturbed_since_2004\",\n",
        "  \"2024_undisturbed_since_2005\",\n",
        "  \"2024_undisturbed_since_2006\",\n",
        "  \"2024_undisturbed_since_2007\",\n",
        "  \"2024_undisturbed_since_2008\",\n",
        "  \"2024_undisturbed_since_2009\",\n",
        "  \"2024_undisturbed_since_2010\",\n",
        "  \"2024_undisturbed_since_2011\",\n",
        "  \"2024_undisturbed_since_2012\",\n",
        "  \"2024_undisturbed_since_2013\",\n",
        "  \"2024_undisturbed_since_2014\",\n",
        "  \"2024_undisturbed_since_2015\",\n",
        "  \"2024_undisturbed_since_2016\",\n",
        "  \"2024_undisturbed_since_2017\",\n",
        "  \"2024_undisturbed_since_2018\",\n",
        "  \"2024_undisturbed_since_2019\",\n",
        "  \"2024_undisturbed_since_2020\",\n",
        "  \"2024_undisturbed_since_2021\",\n",
        "  \"2024_undisturbed_since_2022\",\n",
        "  \"2024_undisturbed_since_2023\",\n",
        "  \"2024_undisturbed_since_2024\",\n",
        "  \"2024_undisturbed_since_oldgrowth\",\n",
        "  \"2024_oldgrowth_recovery\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]"
      ],
      "metadata": {
        "id": "UCErVlWNEaVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check iteration quality\n",
        "# High proportion of values <= 0 may indicate corrupt iterations.\n",
        "# Delete and repredict affected files if necessary.\n",
        "check_iterations = False\n",
        "nonpositive_threshold_percent = 1\n",
        "\n",
        "# Check the number of prediction iterations\n",
        "scenario_iterations = {}\n",
        "for scenario in scenarios_to_calculate:\n",
        "    iterations_dir = join(scenario_iterations_dir, f\"{scenario}_iterations\")\n",
        "    iterations = 0\n",
        "    for subdir in os.listdir(iterations_dir):\n",
        "        if subdir.endswith(\".tif\"):\n",
        "            if check_iterations:\n",
        "                iteration_path = join(iterations_dir, subdir)\n",
        "                iteration_array = read_raster_as_array(iteration_path)\n",
        "                valid_mask = iteration_array != nodatavalue\n",
        "                n_valid = np.count_nonzero(valid_mask)\n",
        "                if n_valid > 0:\n",
        "                    nonpositive_count = np.count_nonzero(iteration_array[valid_mask] <= 0)\n",
        "                    nonpositive_percent = (nonpositive_count / n_valid) * 100\n",
        "                    if nonpositive_percent > nonpositive_threshold_percent:\n",
        "                        print(f\"Warning: {subdir} has {nonpositive_percent:.1f}% values <= 0 in valid pixels.\")\n",
        "            iterations += 1\n",
        "    scenario_iterations[scenario] = iterations\n",
        "    print(f\"There are {iterations} prediction iterations for scenario {scenario} statistics.\")"
      ],
      "metadata": {
        "id": "VhA4eoMFEbvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Raster precision settings\n",
        "mean_precision = 2\n",
        "ci_precision = 2\n",
        "uncertainty_precision = 2\n",
        "\n",
        "# Quantify prediction uncertainty using Monte Carlo simulation with percentile-based\n",
        "# confidence intervals (IPCC Approach 2).\n",
        "\n",
        "# Each model iteration sampled target values from normal distributions using GEDI L4A\n",
        "# agbd (mean) and agbd_se (standard deviation). The resulting collection of respective\n",
        "# prediction iterations represent the distribution of possible AGBD values\n",
        "# given measurement uncertainty.\n",
        "\n",
        "# Confidence intervals derive from percentiles of the empirical (actual rather\n",
        "# than theoretical) distribution: CI half-width = (P_97.5 - P_2.5) / 2\n",
        "# Percentiles directly characterise the distribution without assuming normality,\n",
        "# are robust to outliers, and handle bounded distributions (AGBD  0).\n",
        "\n",
        "# Percentage uncertainty = CI half-width / mean * 100\n",
        "# This represents precision of estimates given input uncertainty, not total predictive\n",
        "# accuracy (Liang et al. 2023, p.6).\n",
        "\n",
        "# References:\n",
        "# IPCC (2006) Guidelines Vol.1 Ch.3: Uncertainties, Section 3.2.3.2\n",
        "# IPCC (2019) Refinement Vol.1 Ch.3: Uncertainties, Section 3.2.3.2\n",
        "# Liang et al. (2023) Remote Sensing of Environment 284:113367\n",
        "\n",
        "# Progress labels\n",
        "scenario_progress_index = 0\n",
        "scenario_progress_label = widgets.Label(f\"Scenario stats progress: {scenario_progress_index}/{len(scenarios_to_calculate)}\")\n",
        "iteration_progress_label = widgets.Label(value=\"Iteration loading progress: -\")\n",
        "display(scenario_progress_label, iteration_progress_label)\n",
        "\n",
        "# Loop through the scenarios\n",
        "for scenario, iteration_count in scenario_iterations.items():\n",
        "    base_filename = f\"{scenario}__{selected_model}\"\n",
        "    iterations_dir = join(scenario_iterations_dir, f\"{scenario}_iterations\")\n",
        "\n",
        "    # Define output paths\n",
        "    mean_filename = f\"mean__{base_filename}.tif\"\n",
        "    mean_path = join(predictions_dir, mean_filename)\n",
        "    ci_filename = f\"ci_95__{base_filename}.tif\"\n",
        "    ci_path = join(predictions_dir, ci_filename)\n",
        "    uncertainty_filename = f\"uncertainty__{base_filename}.tif\"\n",
        "    uncertainty_path = join(predictions_dir, uncertainty_filename)\n",
        "\n",
        "    # If any output does not exist, recalculate all\n",
        "    if not exists(mean_path) or not exists(ci_path) or not exists(uncertainty_path):\n",
        "\n",
        "        # Collect iteration file paths\n",
        "        iteration_paths = [join(iterations_dir, f) for f in os.listdir(iterations_dir) if f.endswith(\".tif\")]\n",
        "        n_iterations = len(iteration_paths)\n",
        "        iteration_progress_label.value = f\"Iteration loading progress: 0 / {n_iterations}\"\n",
        "\n",
        "        # Load iterations sequentially, building stack incrementally\n",
        "        for iteration_index, iteration_path in enumerate(iteration_paths):\n",
        "            iteration_array = read_raster_as_array(iteration_path)\n",
        "            if iteration_index == 0:\n",
        "                iteration_stack = np.empty((n_iterations, *iteration_array.shape), dtype=np.float64)\n",
        "                valid_mask = iteration_array != nodatavalue\n",
        "                template_path = iteration_path\n",
        "            iteration_stack[iteration_index] = np.where(iteration_array == nodatavalue, np.nan, iteration_array)\n",
        "            iteration_progress_label.value = f\"Iteration loading progress: {iteration_index + 1} / {n_iterations}\"\n",
        "\n",
        "        # Suppress warnings for all-NaN slices (expected for nodata pixels, handled by valid_mask)\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
        "\n",
        "            # Calculate mean across iterations\n",
        "            mean_array = np.nanmean(iteration_stack, axis=0)\n",
        "\n",
        "            # Sort once, index at percentile positions. Faster than np.nanpercentile.\n",
        "            # Valid because nodata mask is identical across iterations.\n",
        "            sorted_stack = np.sort(iteration_stack, axis=0)\n",
        "            iteration_stack = None\n",
        "\n",
        "            n = sorted_stack.shape[0]\n",
        "            lower_index = 0.025 * (n - 1)\n",
        "            upper_index = 0.975 * (n - 1)\n",
        "            lower_floor = int(np.floor(lower_index))\n",
        "            upper_floor = int(np.floor(upper_index))\n",
        "            lower_fraction = lower_index - lower_floor\n",
        "            upper_fraction = upper_index - upper_floor\n",
        "\n",
        "            # Interpolate between adjacent sorted values\n",
        "            ci_lower = (sorted_stack[lower_floor] * (1 - lower_fraction) +\n",
        "                        sorted_stack[min(lower_floor + 1, n - 1)] * lower_fraction)\n",
        "            ci_upper = (sorted_stack[upper_floor] * (1 - upper_fraction) +\n",
        "                        sorted_stack[min(upper_floor + 1, n - 1)] * upper_fraction)\n",
        "            sorted_stack = None\n",
        "\n",
        "            # CI half-width\n",
        "            ci_halfwidth = (ci_upper - ci_lower) / 2\n",
        "\n",
        "            # Relative uncertainty as percentage\n",
        "            percentage_uncertainty = np.zeros_like(mean_array, dtype=np.float64)\n",
        "            nonzero_mean_mask = valid_mask & (mean_array != 0)\n",
        "            percentage_uncertainty[nonzero_mean_mask] = (ci_halfwidth[nonzero_mean_mask] / np.abs(mean_array[nonzero_mean_mask])) * 100\n",
        "\n",
        "            # Round and apply nodata mask\n",
        "            if mean_precision == 0:\n",
        "                mean_rounded = np.round(mean_array, mean_precision).astype(np.int16)\n",
        "                mean_rounded = np.where(valid_mask, mean_rounded, nodatavalue).astype(np.int16)\n",
        "            else:\n",
        "                mean_rounded = np.round(mean_array, mean_precision)\n",
        "                mean_rounded = np.where(valid_mask, mean_rounded, nodatavalue)\n",
        "            if ci_precision == 0:\n",
        "                ci_rounded = np.round(ci_halfwidth, ci_precision).astype(np.int16)\n",
        "                ci_rounded = np.where(valid_mask, ci_rounded, nodatavalue).astype(np.int16)\n",
        "            else:\n",
        "                ci_rounded = np.round(ci_halfwidth, ci_precision)\n",
        "                ci_rounded = np.where(valid_mask, ci_rounded, nodatavalue)\n",
        "            if uncertainty_precision == 0:\n",
        "                uncertainty_rounded = np.round(percentage_uncertainty, uncertainty_precision).astype(np.int16)\n",
        "                uncertainty_rounded = np.where(valid_mask, uncertainty_rounded, nodatavalue).astype(np.int16)\n",
        "            else:\n",
        "                uncertainty_rounded = np.round(percentage_uncertainty, uncertainty_precision)\n",
        "                uncertainty_rounded = np.where(valid_mask, uncertainty_rounded, nodatavalue)\n",
        "\n",
        "            # Export statistics arrays as rasters\n",
        "            export_array_as_tif(mean_rounded, mean_path, template=template_path)\n",
        "            export_array_as_tif(ci_rounded, ci_path, template=template_path)\n",
        "            export_array_as_tif(uncertainty_rounded, uncertainty_path, template=template_path)\n",
        "\n",
        "        # Release memory\n",
        "        iteration_stack = sorted_stack = None\n",
        "\n",
        "    else:\n",
        "        iteration_progress_label.value = \"Iteration loading progress: skipped (exists)\"\n",
        "        print(f\"{mean_filename}, {ci_filename} and {uncertainty_filename} already exist.\")\n",
        "\n",
        "    scenario_progress_index += 1\n",
        "    scenario_progress_label.value = f\"Scenario stats progress: {scenario_progress_index}/{len(scenarios_to_calculate)}\"\n",
        "\n",
        "print(\"\\nStatistics calculations complete.\")"
      ],
      "metadata": {
        "id": "91OIyzzQpjen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba928a55-1dc0-4ec0-a336-b1e42731583c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean__2024_oldgrowth_recovery__agbd_251203_161707.tif, ci_95__2024_oldgrowth_recovery__agbd_251203_161707.tif and uncertainty__2024_oldgrowth_recovery__agbd_251203_161707.tif already exist.\n",
            "mean__2024_road_mat_daling_deforestation_2023_30m_degradation_buffer__agbd_251203_161707.tif, ci_95__2024_road_mat_daling_deforestation_2023_30m_degradation_buffer__agbd_251203_161707.tif and uncertainty__2024_road_mat_daling_deforestation_2023_30m_degradation_buffer__agbd_251203_161707.tif already exist.\n",
            "\n",
            "Statistics calculations complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nezNm_Gz3cnz"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful for stopping background execution upon completion\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "5hwTeCjzEgbB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "Xhx977-WIaWJ",
        "5lqjsLKZaQXo",
        "C9-hQ7G37uZi"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}