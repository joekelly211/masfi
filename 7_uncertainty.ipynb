{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/7_uncertainty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhx977-WIaWJ"
      },
      "source": [
        "# Imports and directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRnNty96IThZ"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4n2MGF0aZtJ"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install geopandas\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpmgsb8KKMGN"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "try: import cupy # Only works on GPU runtime\n",
        "except: None\n",
        "import ast\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import gc\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import ipywidgets as widgets\n",
        "import json\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy.random import normal\n",
        "from os import makedirs\n",
        "from os.path import join, exists, basename\n",
        "from osgeo import gdal\n",
        "gdal.UseExceptions()\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import scipy.stats as st\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQwNO9T0_HPH"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "feature_dir = join(base_dir, \"3_features\")\n",
        "feature_final_dir = join(feature_dir, \"final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "masks_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(uncertainty_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koHn-AiGG7KM"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lqjsLKZaQXo"
      },
      "source": [
        "# Select model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SqkKHKnaV-8"
      },
      "outputs": [],
      "source": [
        "# Select a baseline model, tested and trained in advance.\n",
        "# The target must have an uncertainty metric - otherwise\n",
        "# skip to the next notebook '8_statistics' and use the outputs\n",
        "# of the '6_scenarios' notebook.\n",
        "model_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file == 'model.json':\n",
        "      print(f'selected_model = \"{subdir.split(f\"{models_dir}/\",1)[1]}\"')\n",
        "      model_exists = True\n",
        "if not model_exists:\n",
        "  print(\"No model exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnerzp-gbapN"
      },
      "outputs": [],
      "source": [
        "selected_model = \"agbd_251203_161707\"\n",
        "\n",
        "# This must be True when using AlphaEarth features.\n",
        "alpha_earth = False\n",
        "\n",
        "# Define model directories\n",
        "selected_model_dir = join(models_dir,selected_model)\n",
        "selected_model_json = join(selected_model_dir, \"model.json\")\n",
        "selected_model_descr_dir = join(selected_model_dir, \"model_description.json\")\n",
        "selected_model_dataset_path = join(selected_model_dir, f\"{selected_model}.pkl\")\n",
        "selected_model_dataset = pd.read_pickle(selected_model_dataset_path)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(selected_model_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_target = model_dataset_description[\"selected_target\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "selected_features = model_dataset_description[\"selected_features\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_features_mappings = model_dataset_description[\"categorical_features_mappings\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "# Reload hyperparameters\n",
        "with open(selected_model_descr_dir) as model_description_json:\n",
        "  model_description = json.load(model_description_json)\n",
        "final_hyperparameters = ast.literal_eval(model_description[\"hyperparameters\"])\n",
        "\n",
        "# Remove early stopping and replace with mean n_estimators\n",
        "if \"early_stopping_rounds\" in final_hyperparameters:\n",
        "  final_hyperparameters = {k:v for k, v in final_hyperparameters.items() if k != \"early_stopping_rounds\"}\n",
        "  final_hyperparameters[\"n_estimators\"] = round(model_description[\"n_estimators mean\"])\n",
        "\n",
        "# Define directories\n",
        "scenarios_model_dir = join(scenarios_dir,selected_model)\n",
        "features_dir = join(scenarios_model_dir, \"features\")\n",
        "tile_templates_dir = join(scenarios_model_dir, 'tile_templates')\n",
        "tile_feature_stacks_dir = join(scenarios_model_dir, 'tile_feature_stacks')\n",
        "uncertainty_selected_model_dir = join(uncertainty_dir, selected_model)\n",
        "model_iterations_dir = join(uncertainty_selected_model_dir, \"model_iterations\")\n",
        "scenario_iterations_dir = join(uncertainty_selected_model_dir, \"scenario_iterations\")\n",
        "tile_prediction_cache_dir = join(uncertainty_selected_model_dir, \"tile_prediction_cache\")\n",
        "predictions_dir = join(uncertainty_selected_model_dir, \"uncertainty_predictions\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(uncertainty_selected_model_dir, exist_ok=True)\n",
        "makedirs(model_iterations_dir, exist_ok=True)\n",
        "makedirs(scenario_iterations_dir, exist_ok=True)\n",
        "makedirs(tile_prediction_cache_dir, exist_ok=True)\n",
        "makedirs(predictions_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9-hQ7G37uZi"
      },
      "source": [
        "# Model iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWymSi4ndKUA"
      },
      "outputs": [],
      "source": [
        "# Verify that the target is equal to the mean\n",
        "print(f'mean = \"{selected_target}\"')\n",
        "\n",
        "# Calculate se from columns flagged 'uncertainty'\n",
        "if len(uncertainty)==0:\n",
        "  print(\"There are no flagged uncertainty columns to calculate SE from.\")\n",
        "  print(\"Manually create the metric from the available columns.\")\n",
        "  for col in selected_model_dataset.columns:\n",
        "    print(f\"{col}\")\n",
        "else:\n",
        "  for col in selected_model_dataset.columns:\n",
        "    if col in uncertainty and col not in selected_target:\n",
        "      print(f'se = \"{col}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBalj00xwmRX"
      },
      "outputs": [],
      "source": [
        "mean = \"tar_agbd\"\n",
        "se = \"tar_agbd_se\"\n",
        "# GEDI L4A agbd_se represents the prediction standard error incorporating both model\n",
        "# parameter uncertainty and residual variance (GEDI L4A ATBD Eq. 9, Kellner et al. 2021).\n",
        "# For Monte Carlo uncertainty propagation, agbd_se directly parameterises the standard\n",
        "# deviation of the prediction error distribution for each footprint.\n",
        "\n",
        "# Set model iterations\n",
        "model_iterations = 10\n",
        "\n",
        "# Define model (y axis changes for each iteration based on mean and se arrays)\n",
        "model_dataset_x = selected_model_dataset[selected_features].copy()\n",
        "for col in categorical_columns:\n",
        "    if col in model_dataset_x.columns:\n",
        "        model_dataset_x[col] = model_dataset_x[col].astype('category')\n",
        "mean_array = selected_model_dataset[mean].values\n",
        "se_array = selected_model_dataset[se].values\n",
        "\n",
        "# Detect model type from existing model or determine from target\n",
        "existing_model_path = join(model_iterations_dir, \"model_iteration_1.json\")\n",
        "if exists(existing_model_path):\n",
        "    # Load existing model and detect type\n",
        "    temp_booster = xgb.Booster()\n",
        "    temp_booster.load_model(existing_model_path)\n",
        "    model_config = json.loads(temp_booster.save_config())\n",
        "\n",
        "    objective_name = model_config['learner']['objective']['name']\n",
        "    num_class = int(model_config['learner']['learner_model_param'].get('num_class', '0'))\n",
        "    classification = any(obj_type in objective_name for obj_type in ['logistic', 'softprob', 'softmax'])\n",
        "    multiclass = classification and num_class > 2\n",
        "else:\n",
        "    # Determine from target variable characteristics\n",
        "    unique_values = len(np.unique(mean_array))\n",
        "    if unique_values <= 10 and all(val == int(val) for val in np.unique(mean_array)):\n",
        "        classification = True\n",
        "        multiclass = unique_values > 2\n",
        "        num_class = unique_values if multiclass else 0\n",
        "    else:\n",
        "        classification = False\n",
        "        multiclass = False\n",
        "        num_class = 0\n",
        "\n",
        "# Set model type\n",
        "if classification:\n",
        "    XGBPredictor = xgb.XGBClassifier(**final_hyperparameters)\n",
        "    if multiclass: print(f\"Model type: Multiclass classification ({num_class} classes)\")\n",
        "    else: print(\"Model type: Binary classification\")\n",
        "else:\n",
        "    XGBPredictor = xgb.XGBRegressor(**final_hyperparameters)\n",
        "    print(\"Model type: Regression\")\n",
        "\n",
        "model_params = XGBPredictor.get_params()\n",
        "model_params['eval_metric'] = model_description['metric_used_for_training']\n",
        "# Default fix for new XGBoost version\n",
        "[model_params.pop(key, None) for key in ['n_estimators', 'enable_categorical', 'missing']]\n",
        "\n",
        "# Progress label\n",
        "model_progress_index = 0\n",
        "model_progress_label = widgets.Label(f\"Model iteration: {model_progress_index}/{model_iterations}\")\n",
        "display(model_progress_label)\n",
        "\n",
        "for model_iteration in range(1,model_iterations+1):\n",
        "  # Set model iteration filename and check if already exists\n",
        "  model_iteration_filename = f\"model_iteration_{model_iteration}.json\"\n",
        "  model_iteration_path = join(model_iterations_dir, model_iteration_filename)\n",
        "  # If model iteration does not exist...\n",
        "  if not exists(model_iteration_path):\n",
        "    # Set the random seed based on iteration for replicability\n",
        "    np.random.seed(model_iteration)\n",
        "    # Set a normal distribution sample as the y for this iteration\n",
        "    model_dataset_y = normal(mean_array, se_array)\n",
        "    # Create DMatrix objects\n",
        "    model_dtrain = xgb.DMatrix(model_dataset_x, model_dataset_y, enable_categorical=True)\n",
        "    # Train the model iteration using the tested hyperparameters\n",
        "    model = xgb.train(model_params,\n",
        "                        model_dtrain,\n",
        "                        num_boost_round=final_hyperparameters['n_estimators'],\n",
        "                        verbose_eval=True)\n",
        "    # Save the model iteration\n",
        "    model.save_model(model_iteration_path)\n",
        "  # Update progress\n",
        "  model_progress_index += 1\n",
        "  model_progress_label.value = f\"Model iteration: {model_progress_index}/{model_iterations}\"\n",
        "print(\"All model iterations have been trained and saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1UxS-kO8_1d"
      },
      "source": [
        "# Scenario iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYxqe0eeKmKi"
      },
      "outputs": [],
      "source": [
        "# Scenarios must be designed and tested using 06_scenarios first.\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = [f for f in os.listdir(tile_templates_dir) if f.endswith('.tif') and f.startswith('template_tile')]\n",
        "n_tiles = len(template_tile_list)\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "if n_tiles == 1: print(f\"# There is 1 template tile.\\n\")\n",
        "if n_tiles > 1: print(f\"# There are {n_tiles} template tiles.\\n\")\n",
        "\n",
        "# Collect available scenarios from the feature stack tiles directory\n",
        "scenario_stacks_list = []\n",
        "for scenario in os.listdir(tile_feature_stacks_dir):\n",
        "    stack_files = [f for f in os.listdir(join(tile_feature_stacks_dir, scenario)) if f.startswith('feature_stack_')]\n",
        "    if len(stack_files) == n_tiles:\n",
        "        scenario_stacks_list.append(scenario)\n",
        "\n",
        "print(\"scenarios_to_predict = [\")\n",
        "for scenario in sorted(scenario_stacks_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE-bIxq9F3Vy"
      },
      "outputs": [],
      "source": [
        "# There is 1 template tile.\n",
        "\n",
        "scenarios_to_predict = [\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_no_disturbance_since_1993\",\n",
        "  \"2021_no_disturbance_since_oldgrowth_1\",\n",
        "  \"2021_no_disturbance_since_oldgrowth_2\",\n",
        "  \"2021_oldgrowth_recovery_1\",\n",
        "  \"2021_oldgrowth_recovery_2\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_no_disturbance_since_1996\",\n",
        "  \"2024_no_disturbance_since_1997\",\n",
        "  \"2024_no_disturbance_since_1998\",\n",
        "  \"2024_no_disturbance_since_1999\",\n",
        "  \"2024_no_disturbance_since_2000\",\n",
        "  \"2024_no_disturbance_since_2001\",\n",
        "  \"2024_no_disturbance_since_2002\",\n",
        "  \"2024_no_disturbance_since_2003\",\n",
        "  \"2024_no_disturbance_since_2004\",\n",
        "  \"2024_no_disturbance_since_2005\",\n",
        "  \"2024_no_disturbance_since_2006\",\n",
        "  \"2024_no_disturbance_since_2007\",\n",
        "  \"2024_no_disturbance_since_2008\",\n",
        "  \"2024_no_disturbance_since_2009\",\n",
        "  \"2024_no_disturbance_since_2010\",\n",
        "  \"2024_no_disturbance_since_2011\",\n",
        "  \"2024_no_disturbance_since_2012\",\n",
        "  \"2024_no_disturbance_since_2013\",\n",
        "  \"2024_no_disturbance_since_2014\",\n",
        "  \"2024_no_disturbance_since_2015\",\n",
        "  \"2024_no_disturbance_since_2016\",\n",
        "  \"2024_no_disturbance_since_2017\",\n",
        "  \"2024_no_disturbance_since_2018\",\n",
        "  \"2024_no_disturbance_since_2019\",\n",
        "  \"2024_no_disturbance_since_2020\",\n",
        "  \"2024_no_disturbance_since_2021\",\n",
        "  \"2024_no_disturbance_since_2022\",\n",
        "  \"2024_no_disturbance_since_2023\",\n",
        "  \"2024_no_disturbance_since_2024\",\n",
        "  \"2024_no_disturbance_since_oldgrowth_1\",\n",
        "  \"2024_no_disturbance_since_oldgrowth_2\",\n",
        "  \"2024_oldgrowth_recovery_1\",\n",
        "  \"2024_oldgrowth_recovery_2\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of scenario iterations. It must be <= the number of model iterations available.\n",
        "scenario_iterations = 10\n",
        "\n",
        "# Assert the number of scenario iterations is <= the number of model iterations available.\n",
        "model_iterations_available = len([f for f in os.listdir(model_iterations_dir) if f.startswith('model_iteration_') and f.endswith('.json')])\n",
        "assert scenario_iterations <= model_iterations_available, f\"Reduce the number of scenario iterations to <= {model_iterations_available}.\"\n",
        "\n",
        "# Prediction raster precision. GEDI AGBD can be set to 0, any higher is\n",
        "# spurious due to the wide prediction intervals of the source data.\n",
        "raster_precision = 0\n",
        "\n",
        "# Probabilities instead of classes IF binary classification\n",
        "predict_probabilities = False\n",
        "\n",
        "# Classification threshold IF binary classification\n",
        "classification_threshold = 0.5\n",
        "\n",
        "# Detect GPU availability and set predictor type\n",
        "try:\n",
        "    test_array = cupy.array([1, 2, 3])\n",
        "    del test_array\n",
        "    predictor_type = 'gpu_predictor'\n",
        "    gpu_id, use_gpu = 0, True\n",
        "    print(\"GPU detected and accessible - using GPU to load the feature stack and to predict.\")\n",
        "except:\n",
        "    predictor_type = 'cpu_predictor'\n",
        "    gpu_id, use_gpu = -1, False\n",
        "    print(\"GPU not accessible - using CPU prediction\")\n",
        "xgb.set_config(verbosity=0, use_rmm=use_gpu)\n",
        "\n",
        "# Detect model type using first model iteration\n",
        "first_model_path = join(model_iterations_dir, \"model_iteration_1.json\")\n",
        "booster = xgb.Booster()\n",
        "booster.load_model(first_model_path)\n",
        "model_config = json.loads(booster.save_config())\n",
        "objective_name = model_config['learner']['objective']['name']\n",
        "num_class = int(model_config['learner']['learner_model_param'].get('num_class', '0'))\n",
        "classification = any(obj_type in objective_name for obj_type in ['logistic', 'softprob', 'softmax'])\n",
        "multiclass = classification and num_class > 2\n",
        "if classification and multiclass: print(f\"Model type: Multiclass classification ({num_class} classes)\")\n",
        "elif classification: print(\"Model type: Binary classification\")\n",
        "else: print(\"Model type: Regression\")\n",
        "booster = None\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = [f for f in os.listdir(tile_templates_dir) if f.endswith('.tif') and f.startswith('template_tile')]\n",
        "n_tiles = len(template_tile_list)\n",
        "if n_tiles < 1: print(\"There are currently no template tiles.\")\n",
        "else:\n",
        "  template_tile = gdal.Open(join(tile_templates_dir, 'template_tile_1.tif'))\n",
        "  template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "  template_tile = None\n",
        "  print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "if alpha_earth: template_base_path = next(r for r in model_scenario_features_dirs if all(c not in r for c in covariates))\n",
        "else: template_base_path = template_tif_path\n",
        "\n",
        "# Progress labels\n",
        "n_scenarios = len(scenarios_to_predict)\n",
        "scenario_progress_label = widgets.Label(value=f\"Scenario progress: 0 / {n_scenarios}\")\n",
        "tile_progress_label = widgets.Label(value=\"Tile progress: -\")\n",
        "iteration_progress_label = widgets.Label(value=\"Iteration progress: -\")\n",
        "display(scenario_progress_label, tile_progress_label, iteration_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario_index, scenario in enumerate(scenarios_to_predict):\n",
        "  scenario_feature_stack_dir = join(tile_feature_stacks_dir, scenario)\n",
        "  # Create scenario iterations directory\n",
        "  iterations_dir = join(scenario_iterations_dir, f\"{scenario}_iterations\")\n",
        "  makedirs(iterations_dir, exist_ok=True)\n",
        "\n",
        "  # Check if all scenario iterations already exist\n",
        "  scenario_iteration_list = []\n",
        "  for model_iteration in range(1, scenario_iterations + 1):\n",
        "    prediction_iteration_filename = f\"{scenario}__{selected_model}_iteration_{model_iteration}.tif\"\n",
        "    prediction_iteration_path = join(iterations_dir, prediction_iteration_filename)\n",
        "    scenario_iteration_list.append(prediction_iteration_path)\n",
        "  all_scenario_iterations_exist = all(exists(p) for p in scenario_iteration_list)\n",
        "\n",
        "  if not all_scenario_iterations_exist:\n",
        "    n_stacks = len([f for f in os.listdir(scenario_feature_stack_dir) if f.startswith('feature_stack_')])\n",
        "\n",
        "    # Single-stack prediction\n",
        "    if n_stacks == 1:\n",
        "      tile_progress_label.value = \"Tile progress: 0 / 1\"\n",
        "      iteration_progress_label.value = f\"Iteration progress: 0 / {scenario_iterations}\"\n",
        "\n",
        "      # Load template parameters\n",
        "      template_tile_path = join(tile_templates_dir, \"template_tile_1.tif\")\n",
        "      template_tile = gdal.Open(template_tile_path)\n",
        "      template_tile_y = template_tile.GetRasterBand(1).YSize\n",
        "      template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "      template_tile = None\n",
        "      n_pixels = template_tile_y * template_tile_x\n",
        "      # Load feature stack and valid indices\n",
        "      stack_path = join(scenario_feature_stack_dir, f\"feature_stack_{scenario}_1.npy\")\n",
        "      indices_path = join(scenario_feature_stack_dir, f\"valid_indices_{scenario}_1.npy\")\n",
        "      feature_stack = np.load(stack_path)\n",
        "      valid_indices = np.load(indices_path)\n",
        "      n_valid = len(valid_indices)\n",
        "      # Load to GPU if available and valid pixels exist\n",
        "      if n_valid > 0 and use_gpu:\n",
        "        try:\n",
        "          feature_stack = cupy.asarray(feature_stack)\n",
        "        except Exception as e:\n",
        "          if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "            print(\"GPU memory insufficient, switching to CPU.\")\n",
        "            cupy.get_default_memory_pool().free_all_blocks()\n",
        "            gc.collect()\n",
        "            use_gpu = False\n",
        "            predictor_type = 'cpu_predictor'\n",
        "          else: raise\n",
        "\n",
        "      # Predict all iterations using loaded stack\n",
        "      iteration_progress_index = 0\n",
        "      for model_iteration in range(1, scenario_iterations + 1):\n",
        "        model_path = join(model_iterations_dir, f\"model_iteration_{model_iteration}.json\")\n",
        "        prediction_iteration_filename = f\"{scenario}__{selected_model}_iteration_{model_iteration}.tif\"\n",
        "        prediction_iteration_path = join(iterations_dir, prediction_iteration_filename)\n",
        "\n",
        "        if not exists(prediction_iteration_path):\n",
        "          # Load model iteration\n",
        "          if classification:\n",
        "            XGBPredictor = xgb.XGBClassifier()\n",
        "            XGBPredictor.load_model(model_path)\n",
        "            XGBPredictor.set_params(predictor=predictor_type)\n",
        "            if use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "          else:\n",
        "            XGBPredictor = xgb.XGBRegressor()\n",
        "            XGBPredictor.load_model(model_path)\n",
        "            XGBPredictor.set_params(predictor=predictor_type)\n",
        "            if use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "\n",
        "          # Handle empty tiles\n",
        "          if n_valid == 0:\n",
        "            if raster_precision == 0:\n",
        "              prediction_array = np.full((template_tile_y, template_tile_x), nodatavalue, dtype=np.int16)\n",
        "            else:\n",
        "              prediction_array = np.full((template_tile_y, template_tile_x), nodatavalue, dtype=np.float32)\n",
        "          else:\n",
        "            # Predict - terminate runtime if GPU prediction fails\n",
        "            try:\n",
        "              if classification and predict_probabilities and not multiclass:\n",
        "                prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                prediction = prediction_proba[:, 1]\n",
        "              else:\n",
        "                if classification and not multiclass:\n",
        "                  prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                  prediction = (prediction_proba[:, 1] > classification_threshold).astype(int)\n",
        "                else:\n",
        "                  prediction = XGBPredictor.predict(feature_stack)\n",
        "                  if classification:\n",
        "                    if prediction.ndim > 1 and prediction.shape[1] > 1: prediction = np.argmax(prediction, axis=1)\n",
        "                    prediction = prediction.astype(int)\n",
        "            except Exception as e:\n",
        "              if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "                print(\"GPU memory insufficient for prediction. Terminating runtime to save compute units, restart with TPU.\")\n",
        "                runtime.unassign()\n",
        "              else: raise\n",
        "            # Reconstruct full array from valid indices (C-order)\n",
        "            if raster_precision == 0:\n",
        "              prediction_flat = np.full(n_pixels, nodatavalue, dtype=np.int16)\n",
        "              prediction_flat[valid_indices] = np.round(prediction).astype(np.int16)\n",
        "            else:\n",
        "              prediction_flat = np.full(n_pixels, nodatavalue, dtype=np.float32)\n",
        "              prediction_flat[valid_indices] = np.round(prediction, raster_precision)\n",
        "            prediction = None\n",
        "            prediction_array = prediction_flat.reshape((template_tile_y, template_tile_x), order='C')\n",
        "            prediction_flat = None\n",
        "          export_array_as_tif(prediction_array, prediction_iteration_path, template=template_base_path, compress=True)\n",
        "          prediction_array = None\n",
        "\n",
        "        iteration_progress_index += 1\n",
        "        iteration_progress_label.value = f\"Iteration progress: {iteration_progress_index} / {scenario_iterations}\"\n",
        "      # Clean up single-stack feature stack from memory\n",
        "      feature_stack = valid_indices = None\n",
        "      tile_progress_label.value = \"Tile progress: 1 / 1\"\n",
        "\n",
        "    # Tiled prediction - load each stack once, predict all iterations per tile\n",
        "    if n_stacks > 1:\n",
        "      # Create tile cache directories for all iterations\n",
        "      tile_cache_dirs = {}\n",
        "      for model_iteration in range(1, scenario_iterations + 1):\n",
        "        prediction_iteration_filename = f\"{scenario}__{selected_model}_iteration_{model_iteration}.tif\"\n",
        "        tile_cache_iteration_dir = join(tile_prediction_cache_dir, prediction_iteration_filename[:-4])\n",
        "        makedirs(tile_cache_iteration_dir, exist_ok=True)\n",
        "        tile_cache_dirs[model_iteration] = tile_cache_iteration_dir\n",
        "\n",
        "      # Process each tile: load stack once, predict all iterations\n",
        "      for tile_count in range(1, n_stacks + 1):\n",
        "        # Determine which iterations still need this tile\n",
        "        iterations_needing_tile = []\n",
        "        for model_iteration in range(1, scenario_iterations + 1):\n",
        "          iteration_tile_path = join(tile_cache_dirs[model_iteration], f\"scenario_tile_{tile_count}.tif\")\n",
        "          prediction_iteration_path = join(iterations_dir, f\"{scenario}__{selected_model}_iteration_{model_iteration}.tif\")\n",
        "          if not exists(prediction_iteration_path) and not exists(iteration_tile_path):\n",
        "            iterations_needing_tile.append(model_iteration)\n",
        "\n",
        "        # Skip tile if no iterations need it\n",
        "        n_iterations_for_tile = len(iterations_needing_tile)\n",
        "        if n_iterations_for_tile == 0:\n",
        "          iteration_progress_label.value = f\"Iteration progress: {scenario_iterations} / {scenario_iterations}\"\n",
        "          tile_progress_label.value = f\"Tile progress: {tile_count} / {n_stacks}\"\n",
        "          continue\n",
        "        iteration_progress_label.value = f\"Iteration progress: 0 / {n_iterations_for_tile}\"\n",
        "\n",
        "        # Load template tile parameters\n",
        "        template_tile_path = join(tile_templates_dir, f\"template_tile_{tile_count}.tif\")\n",
        "        template_tile = gdal.Open(template_tile_path)\n",
        "        template_tile_y = template_tile.GetRasterBand(1).YSize\n",
        "        template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "        template_tile = None\n",
        "        n_pixels = template_tile_y * template_tile_x\n",
        "\n",
        "        # Load feature stack and valid indices once per tile\n",
        "        stack_path = join(scenario_feature_stack_dir, f\"feature_stack_{scenario}_{tile_count}.npy\")\n",
        "        indices_path = join(scenario_feature_stack_dir, f\"valid_indices_{scenario}_{tile_count}.npy\")\n",
        "        if not exists(stack_path):\n",
        "          print(f\"Warning: {basename(stack_path)} not found. Skipping tile {tile_count}.\")\n",
        "          continue\n",
        "        feature_stack = np.load(stack_path)\n",
        "        valid_indices = np.load(indices_path)\n",
        "        n_valid = len(valid_indices)\n",
        "\n",
        "        # Handle empty tiles for all needed iterations\n",
        "        if n_valid == 0:\n",
        "          if raster_precision == 0:\n",
        "            prediction_tile = np.full((template_tile_y, template_tile_x), nodatavalue, dtype=np.int16)\n",
        "          else: prediction_tile = np.full((template_tile_y, template_tile_x), nodatavalue, dtype=np.float32)\n",
        "          tile_iteration_index = 0\n",
        "          for model_iteration in iterations_needing_tile:\n",
        "            iteration_tile_path = join(tile_cache_dirs[model_iteration], f\"scenario_tile_{tile_count}.tif\")\n",
        "            export_array_as_tif(prediction_tile, iteration_tile_path, template=template_tile_path, compress=False)\n",
        "            tile_iteration_index += 1\n",
        "            iteration_progress_label.value = f\"Iteration progress: {tile_iteration_index} / {n_iterations_for_tile}\"\n",
        "          prediction_tile = None\n",
        "          tile_progress_label.value = f\"Tile progress: {tile_count} / {n_stacks}\"\n",
        "          continue\n",
        "\n",
        "        # Load to GPU if available\n",
        "        tile_use_gpu = use_gpu\n",
        "        if use_gpu:\n",
        "          try: feature_stack = cupy.asarray(feature_stack)\n",
        "          except Exception as e:\n",
        "            if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "              print(f\"GPU memory insufficient for tile {tile_count}, using CPU.\")\n",
        "              cupy.get_default_memory_pool().free_all_blocks()\n",
        "              gc.collect()\n",
        "              tile_use_gpu = False\n",
        "            else: raise\n",
        "\n",
        "        # Predict all needed iterations for this tile\n",
        "        tile_iteration_index = 0\n",
        "        for model_iteration in iterations_needing_tile:\n",
        "          iteration_tile_path = join(tile_cache_dirs[model_iteration], f\"scenario_tile_{tile_count}.tif\")\n",
        "          model_path = join(model_iterations_dir, f\"model_iteration_{model_iteration}.json\")\n",
        "          # Load model iteration\n",
        "          if classification:\n",
        "            XGBPredictor = xgb.XGBClassifier()\n",
        "            XGBPredictor.load_model(model_path)\n",
        "            XGBPredictor.set_params(predictor='gpu_predictor' if tile_use_gpu else 'cpu_predictor')\n",
        "            if tile_use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "          else:\n",
        "            XGBPredictor = xgb.XGBRegressor()\n",
        "            XGBPredictor.load_model(model_path)\n",
        "            XGBPredictor.set_params(predictor='gpu_predictor' if tile_use_gpu else 'cpu_predictor')\n",
        "            if tile_use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "          # Predict - terminate runtime if GPU prediction fails\n",
        "          try:\n",
        "            if classification and predict_probabilities and not multiclass:\n",
        "              prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "              prediction = prediction_proba[:, 1]\n",
        "            else:\n",
        "              if classification and not multiclass:\n",
        "                prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                prediction = (prediction_proba[:, 1] > classification_threshold).astype(int)\n",
        "              else:\n",
        "                prediction = XGBPredictor.predict(feature_stack)\n",
        "                if classification:\n",
        "                  if prediction.ndim > 1 and prediction.shape[1] > 1: prediction = np.argmax(prediction, axis=1)\n",
        "                  prediction = prediction.astype(int)\n",
        "          except Exception as e:\n",
        "            if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "              print(\"GPU memory insufficient for prediction. Terminating runtime to save compute units, restart with TPU.\")\n",
        "              runtime.unassign()\n",
        "            else: raise\n",
        "          # Reconstruct full tile from valid indices (C-order)\n",
        "          if raster_precision == 0:\n",
        "            prediction_flat = np.full(n_pixels, nodatavalue, dtype=np.int16)\n",
        "            prediction_flat[valid_indices] = np.round(prediction).astype(np.int16)\n",
        "          else:\n",
        "            prediction_flat = np.full(n_pixels, nodatavalue, dtype=np.float32)\n",
        "            prediction_flat[valid_indices] = np.round(prediction, raster_precision)\n",
        "          prediction = None\n",
        "          prediction_tile = prediction_flat.reshape((template_tile_y, template_tile_x), order='C')\n",
        "          prediction_flat = None\n",
        "          export_array_as_tif(prediction_tile, iteration_tile_path, template=template_tile_path, compress=False)\n",
        "          prediction_tile = None\n",
        "\n",
        "          tile_iteration_index += 1\n",
        "          iteration_progress_label.value = f\"Iteration progress: {tile_iteration_index} / {n_iterations_for_tile}\"\n",
        "\n",
        "        # Release tile resources\n",
        "        feature_stack = valid_indices = None\n",
        "        tile_progress_label.value = f\"Tile progress: {tile_count} / {n_stacks}\"\n",
        "\n",
        "      # Merge tiles for each iteration\n",
        "      tile_progress_label.value = \"Tile progress: merging\"\n",
        "      for model_iteration in range(1, scenario_iterations + 1):\n",
        "        prediction_iteration_filename = f\"{scenario}__{selected_model}_iteration_{model_iteration}.tif\"\n",
        "        prediction_iteration_path = join(iterations_dir, prediction_iteration_filename)\n",
        "        tile_cache_iteration_dir = tile_cache_dirs[model_iteration]\n",
        "        if not exists(prediction_iteration_path):\n",
        "          prediction_array = np.empty((0, template_tile_x))\n",
        "          tile_files = sorted([f for f in os.listdir(tile_cache_iteration_dir) if f.endswith('.tif')],\n",
        "                              key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "          for tile_file in tile_files:\n",
        "            tile = gdal.Open(join(tile_cache_iteration_dir, tile_file))\n",
        "            tile_array = tile.ReadAsArray()\n",
        "            tile = None\n",
        "            prediction_array = np.vstack((prediction_array, tile_array))\n",
        "          export_array_as_tif(prediction_array, prediction_iteration_path, template=template_base_path, compress=True)\n",
        "          prediction_array = None\n",
        "        # Delete tile cache for this iteration\n",
        "        if exists(tile_cache_iteration_dir):\n",
        "          shutil.rmtree(tile_cache_iteration_dir)\n",
        "\n",
        "        iteration_progress_label.value = f\"Merge progress: {model_iteration} / {scenario_iterations}\"\n",
        "\n",
        "  else:\n",
        "    tile_progress_label.value = \"Tile progress: complete\"\n",
        "    iteration_progress_label.value = \"Iteration progress: complete\"\n",
        "\n",
        "  scenario_progress_label.value = f\"Scenario progress: {scenario_index + 1} / {n_scenarios}\"\n",
        "\n",
        "print(\"\\nScenario iterations complete.\")"
      ],
      "metadata": {
        "id": "VDnbOWSegeVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIWo8ATZs-Ny"
      },
      "source": [
        "# Predictions with uncertainty\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect scenarios with iterations\n",
        "scenarios_iterations_list = []\n",
        "for subdir in os.listdir(scenario_iterations_dir):\n",
        "  scenarios_iterations_list.append(subdir[:-11])\n",
        "\n",
        "# Select scenarios to calculate mean, confidence intervals and uncertainty\n",
        "print(\"scenarios_to_calculate = [\")\n",
        "for scenario in sorted(scenarios_iterations_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ],
      "metadata": {
        "id": "9PObs4doEYve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scenarios_to_calculate = [\n",
        "  # \"2018\",\n",
        "  # \"2019\",\n",
        "  # \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_no_disturbance_since_1993\",\n",
        "  \"2021_no_disturbance_since_oldgrowth_1\",\n",
        "  \"2021_no_disturbance_since_oldgrowth_2\",\n",
        "  \"2021_oldgrowth_recovery_1\",\n",
        "  \"2021_oldgrowth_recovery_2\",\n",
        "  # \"2022\",\n",
        "  # \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_no_disturbance_since_1996\",\n",
        "  \"2024_no_disturbance_since_1997\",\n",
        "  \"2024_no_disturbance_since_1998\",\n",
        "  \"2024_no_disturbance_since_1999\",\n",
        "  \"2024_no_disturbance_since_2000\",\n",
        "  \"2024_no_disturbance_since_2001\",\n",
        "  \"2024_no_disturbance_since_2002\",\n",
        "  \"2024_no_disturbance_since_2003\",\n",
        "  \"2024_no_disturbance_since_2004\",\n",
        "  \"2024_no_disturbance_since_2005\",\n",
        "  \"2024_no_disturbance_since_2006\",\n",
        "  \"2024_no_disturbance_since_2007\",\n",
        "  \"2024_no_disturbance_since_2008\",\n",
        "  \"2024_no_disturbance_since_2009\",\n",
        "  \"2024_no_disturbance_since_2010\",\n",
        "  \"2024_no_disturbance_since_2011\",\n",
        "  \"2024_no_disturbance_since_2012\",\n",
        "  \"2024_no_disturbance_since_2013\",\n",
        "  \"2024_no_disturbance_since_2014\",\n",
        "  \"2024_no_disturbance_since_2015\",\n",
        "  \"2024_no_disturbance_since_2016\",\n",
        "  \"2024_no_disturbance_since_2017\",\n",
        "  \"2024_no_disturbance_since_2018\",\n",
        "  \"2024_no_disturbance_since_2019\",\n",
        "  \"2024_no_disturbance_since_2020\",\n",
        "  \"2024_no_disturbance_since_2021\",\n",
        "  \"2024_no_disturbance_since_2022\",\n",
        "  \"2024_no_disturbance_since_2023\",\n",
        "  \"2024_no_disturbance_since_2024\",\n",
        "  \"2024_no_disturbance_since_oldgrowth_1\",\n",
        "  \"2024_no_disturbance_since_oldgrowth_2\",\n",
        "  \"2024_oldgrowth_recovery_1\",\n",
        "  \"2024_oldgrowth_recovery_2\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]"
      ],
      "metadata": {
        "id": "UCErVlWNEaVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check iteration quality\n",
        "# High proportion of values <= 0 may indicate corrupt iterations.\n",
        "# Delete and repredict affected files if necessary.\n",
        "check_iterations = False\n",
        "nonpositive_threshold_percent = 1\n",
        "\n",
        "# Check the number of prediction iterations\n",
        "scenario_iterations = {}\n",
        "for scenario in scenarios_to_calculate:\n",
        "    iterations_dir = join(scenario_iterations_dir, f\"{scenario}_iterations\")\n",
        "    iterations = 0\n",
        "    for subdir in os.listdir(iterations_dir):\n",
        "        if subdir.endswith(\".tif\"):\n",
        "            if check_iterations:\n",
        "                iteration_path = join(iterations_dir, subdir)\n",
        "                iteration = gdal.Open(iteration_path)\n",
        "                iteration_array = iteration.ReadAsArray()\n",
        "                iteration = None\n",
        "                valid_mask = iteration_array != nodatavalue\n",
        "                n_valid = np.count_nonzero(valid_mask)\n",
        "                if n_valid > 0:\n",
        "                    nonpositive_count = np.count_nonzero(iteration_array[valid_mask] <= 0)\n",
        "                    nonpositive_percent = (nonpositive_count / n_valid) * 100\n",
        "                    if nonpositive_percent > nonpositive_threshold_percent:\n",
        "                        print(f\"Warning: {subdir} has {nonpositive_percent:.1f}% values <= 0 in valid pixels.\")\n",
        "            iterations += 1\n",
        "    scenario_iterations[scenario] = iterations\n",
        "    print(f\"There are {iterations} prediction iterations for scenario {scenario} statistics.\")"
      ],
      "metadata": {
        "id": "VhA4eoMFEbvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 95% confidence interval (hardcoded)\n",
        "confidence_interval = 0.95\n",
        "\n",
        "# Raster precision settings\n",
        "mean_precision = 2\n",
        "ci_precision = 2\n",
        "uncertainty_precision = 2\n",
        "\n",
        "# Statistics progress\n",
        "stats_progress_index = 0\n",
        "stats_progress_label = widgets.Label(f\"Scenario stats progress: {stats_progress_index}/{len(scenarios_to_calculate)}\")\n",
        "display(stats_progress_label)\n",
        "\n",
        "# Read single iteration raster as array\n",
        "def read_iteration(iteration_path):\n",
        "    iteration = gdal.Open(iteration_path)\n",
        "    iteration_array = iteration.ReadAsArray()\n",
        "    iteration = None\n",
        "    return iteration_array\n",
        "\n",
        "# Loop through the scenarios\n",
        "for scenario, iteration_n in scenario_iterations.items():\n",
        "    stat_base_filename = f\"{scenario}__{selected_model}\"\n",
        "    iterations_dir = join(scenario_iterations_dir, f\"{scenario}_iterations\")\n",
        "\n",
        "    # Skip oldgrowth _1 and _2 scenarios if merged files already exist\n",
        "    if '_oldgrowth_1' in scenario or '_oldgrowth_2' in scenario or '_oldgrowth_recovery_1' in scenario or '_oldgrowth_recovery_2' in scenario or '_no_disturbance_since_oldgrowth_1' in scenario or '_no_disturbance_since_oldgrowth_2' in scenario:\n",
        "        merged_scenario = scenario.replace('_1', '').replace('_2', '')\n",
        "        merged_stat_base = f\"{merged_scenario}__{selected_model}\"\n",
        "        merged_mean_path = join(predictions_dir, f\"mean__{merged_stat_base}.tif\")\n",
        "        merged_uncertainty_path = join(predictions_dir, f\"uncertainty__{merged_stat_base}.tif\")\n",
        "        if exists(merged_mean_path) and exists(merged_uncertainty_path):\n",
        "            print(f\"Merged files exist, skipping: {scenario}\")\n",
        "            stats_progress_index += 1\n",
        "            stats_progress_label.value = f\"Scenario stats progress: {stats_progress_index}/{len(scenarios_to_calculate)}\"\n",
        "            continue\n",
        "\n",
        "    # Calculate t critical value for this scenario's iteration count\n",
        "    t_crit = st.t.ppf((1 + confidence_interval) / 2, iteration_n - 1)\n",
        "\n",
        "    # Define statistics raster paths\n",
        "    stat_mean_filename = f\"mean__{stat_base_filename}.tif\"\n",
        "    stat_mean_path = join(predictions_dir, stat_mean_filename)\n",
        "    stat_uncertainty_filename = f\"uncertainty__{stat_base_filename}.tif\"\n",
        "    stat_uncertainty_path = join(predictions_dir, stat_uncertainty_filename)\n",
        "    stat_ci_filename = f\"ci_{int(confidence_interval * 100)}__{stat_base_filename}.tif\"\n",
        "    stat_ci_path = join(predictions_dir, stat_ci_filename)\n",
        "\n",
        "    # If either mean or uncertainty do not exist\n",
        "    if not exists(stat_mean_path) or not exists(stat_uncertainty_path):\n",
        "\n",
        "        # Collect iteration file paths\n",
        "        iteration_paths = [join(iterations_dir, f) for f in os.listdir(iterations_dir) if f.endswith(\".tif\")]\n",
        "\n",
        "        # Read iterations in parallel using thread pool\n",
        "        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
        "            iteration_arrays = list(executor.map(read_iteration, iteration_paths))\n",
        "\n",
        "        # Determine valid pixels (same across all iterations)\n",
        "        valid_mask = iteration_arrays[0] != nodatavalue\n",
        "\n",
        "        # Replace nodata with 0 for summation (only affects nodata positions which we'll overwrite later)\n",
        "        iteration_arrays_clean = [np.where(arr == nodatavalue, 0, arr.astype(np.float64)) for arr in iteration_arrays]\n",
        "\n",
        "        # Sum and sum of squares in float64 for numerical stability\n",
        "        stat_sum = np.sum(iteration_arrays_clean, axis=0)\n",
        "        stat_sum_sq = np.sum([arr ** 2 for arr in iteration_arrays_clean], axis=0)\n",
        "\n",
        "        # Retain one path as export template\n",
        "        iteration_path = iteration_paths[0]\n",
        "\n",
        "        # Calculate mean: sum / count\n",
        "        stat_mean = stat_sum / iteration_n\n",
        "        if mean_precision == 0:\n",
        "            stat_mean_rounded = np.round(stat_mean, mean_precision).astype(np.int16)\n",
        "            stat_mean_rounded = np.where(valid_mask, stat_mean_rounded, nodatavalue).astype(np.int16)\n",
        "        else:\n",
        "            stat_mean_rounded = np.round(stat_mean, mean_precision)\n",
        "            stat_mean_rounded = np.where(valid_mask, stat_mean_rounded, nodatavalue)\n",
        "        if not exists(stat_mean_path):\n",
        "            export_array_as_tif(stat_mean_rounded, stat_mean_path, template=iteration_path)\n",
        "            print(f\"{stat_mean_filename} has been exported.\")\n",
        "        else: print(f\"{stat_mean_filename} already exists.\")\n",
        "\n",
        "        if not exists(stat_uncertainty_path):\n",
        "            # Calculate variance: E[X^2] - (E[X])^2\n",
        "            stat_variance = np.maximum(0, (stat_sum_sq - stat_sum ** 2 / iteration_n) / (iteration_n - 1))\n",
        "            # Standard error:  / sqrt(n)\n",
        "            stat_se = np.sqrt(stat_variance) / np.sqrt(iteration_n)\n",
        "            # Mask for non-zero SE values (within valid pixels only)\n",
        "            nonzero_se_mask = valid_mask & (stat_se != 0.0)\n",
        "            # Calculate confidence intervals using t-distribution\n",
        "            stat_ci_lower = np.zeros_like(stat_mean)\n",
        "            stat_ci_upper = np.zeros_like(stat_mean)\n",
        "            stat_ci_lower[nonzero_se_mask] = stat_mean[nonzero_se_mask] - t_crit * stat_se[nonzero_se_mask]\n",
        "            stat_ci_upper[nonzero_se_mask] = stat_mean[nonzero_se_mask] + t_crit * stat_se[nonzero_se_mask]\n",
        "            # CI width: (upper - lower) / 2\n",
        "            stat_ci = (stat_ci_upper - stat_ci_lower) / 2\n",
        "            if ci_precision == 0:\n",
        "                stat_ci = np.round(stat_ci, ci_precision).astype(np.int16)\n",
        "                stat_ci = np.where(valid_mask, stat_ci, nodatavalue).astype(np.int16)\n",
        "            else:\n",
        "                stat_ci = np.round(stat_ci, ci_precision)\n",
        "                stat_ci = np.where(valid_mask, stat_ci, nodatavalue)\n",
        "            # Uncertainty: (CI / mean) * 100%\n",
        "            stat_uncertainty = np.zeros_like(stat_mean_rounded, dtype=np.float64)\n",
        "            # Avoid division by zero in mean\n",
        "            nonzero_mean_mask = nonzero_se_mask & (stat_mean_rounded != 0)\n",
        "            stat_uncertainty[nonzero_mean_mask] = (stat_ci[nonzero_mean_mask] / np.abs(stat_mean_rounded[nonzero_mean_mask])) * 100\n",
        "            if uncertainty_precision == 0:\n",
        "                stat_uncertainty = np.round(stat_uncertainty, uncertainty_precision).astype(np.int16)\n",
        "                stat_uncertainty = np.where(valid_mask, stat_uncertainty, nodatavalue).astype(np.int16)\n",
        "            else:\n",
        "                stat_uncertainty = np.round(stat_uncertainty, uncertainty_precision)\n",
        "                stat_uncertainty = np.where(valid_mask, stat_uncertainty, nodatavalue)\n",
        "\n",
        "            # Export statistics arrays as rasters\n",
        "            export_array_as_tif(stat_ci, stat_ci_path, template=iteration_path)\n",
        "            print(f\"{stat_ci_filename} has been exported.\")\n",
        "            export_array_as_tif(stat_uncertainty, stat_uncertainty_path, template=iteration_path)\n",
        "            print(f\"{stat_uncertainty_filename} has been exported.\")\n",
        "        else: print(f\"{stat_uncertainty_filename} already exists.\")\n",
        "\n",
        "    else: print(f\"{stat_mean_filename} and {stat_uncertainty_filename} already exist.\")\n",
        "    stats_progress_index += 1\n",
        "    stats_progress_label.value = f\"Scenario stats progress: {stats_progress_index}/{len(scenarios_to_calculate)}\"\n",
        "\n",
        "print(\"\\nStatistics calculations complete.\")"
      ],
      "metadata": {
        "id": "g04KqALeEdKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge oldgrowth scenario statistics\n",
        "# Version 1 uses land-use proxy for pre-Landsat undisturbed forest.\n",
        "# Version 2 removes all disturbance without the proxy.\n",
        "# Taking maximum mean avoids underestimation where proxy may not capture all oldgrowth characteristics.\n",
        "# CI and uncertainty are selected from whichever version has greater mean at each pixel.\n",
        "\n",
        "oldgrowth_v1_files = [f for f in os.listdir(predictions_dir)\n",
        "                      if f.startswith('mean__') and ('_oldgrowth_recovery_1__' in f or '_no_disturbance_since_oldgrowth_1__' in f)\n",
        "                      and f.endswith('.tif')]\n",
        "\n",
        "if not oldgrowth_v1_files:\n",
        "    print(\"\\nNo oldgrowth version 1 statistics found to merge.\")\n",
        "else:\n",
        "    print(\"\\nMerging oldgrowth statistics...\")\n",
        "    for mean_v1_file in oldgrowth_v1_files:\n",
        "        # Construct filenames for all statistics\n",
        "        mean_v2_file = mean_v1_file.replace('_1__', '_2__')\n",
        "        mean_merged_file = mean_v1_file.replace('_1__', '__')\n",
        "        ci_v1_file = mean_v1_file.replace('mean__', f'ci_{int(confidence_interval * 100)}__')\n",
        "        ci_v2_file = ci_v1_file.replace('_1__', '_2__')\n",
        "        ci_merged_file = ci_v1_file.replace('_1__', '__')\n",
        "        uncertainty_v1_file = mean_v1_file.replace('mean__', 'uncertainty__')\n",
        "        uncertainty_v2_file = uncertainty_v1_file.replace('_1__', '_2__')\n",
        "        uncertainty_merged_file = uncertainty_v1_file.replace('_1__', '__')\n",
        "\n",
        "        # Construct paths\n",
        "        mean_v1_path = join(predictions_dir, mean_v1_file)\n",
        "        mean_v2_path = join(predictions_dir, mean_v2_file)\n",
        "        mean_merged_path = join(predictions_dir, mean_merged_file)\n",
        "        ci_v1_path = join(predictions_dir, ci_v1_file)\n",
        "        ci_v2_path = join(predictions_dir, ci_v2_file)\n",
        "        ci_merged_path = join(predictions_dir, ci_merged_file)\n",
        "        uncertainty_v1_path = join(predictions_dir, uncertainty_v1_file)\n",
        "        uncertainty_v2_path = join(predictions_dir, uncertainty_v2_file)\n",
        "        uncertainty_merged_path = join(predictions_dir, uncertainty_merged_file)\n",
        "\n",
        "        # Skip if all merged files already exist\n",
        "        if exists(mean_merged_path) and exists(ci_merged_path) and exists(uncertainty_merged_path):\n",
        "            print(f\"Merged files already exist for {mean_v1_file.split('__')[1]}\")\n",
        "            continue\n",
        "\n",
        "        # Load version 1 arrays\n",
        "        mean_v1 = gdal.Open(mean_v1_path)\n",
        "        mean_v1_array = mean_v1.ReadAsArray()\n",
        "        mean_v1 = None\n",
        "        ci_v1 = gdal.Open(ci_v1_path)\n",
        "        ci_v1_array = ci_v1.ReadAsArray()\n",
        "        ci_v1 = None\n",
        "        uncertainty_v1 = gdal.Open(uncertainty_v1_path)\n",
        "        uncertainty_v1_array = uncertainty_v1.ReadAsArray()\n",
        "        uncertainty_v1 = None\n",
        "\n",
        "        # Check if version 2 exists\n",
        "        if exists(mean_v2_path):\n",
        "            print(f\"Merging oldgrowth versions for {mean_v1_file.split('__')[1]}...\")\n",
        "            mean_v2 = gdal.Open(mean_v2_path)\n",
        "            mean_v2_array = mean_v2.ReadAsArray()\n",
        "            mean_v2 = None\n",
        "            ci_v2 = gdal.Open(ci_v2_path)\n",
        "            ci_v2_array = ci_v2.ReadAsArray()\n",
        "            ci_v2 = None\n",
        "            uncertainty_v2 = gdal.Open(uncertainty_v2_path)\n",
        "            uncertainty_v2_array = uncertainty_v2.ReadAsArray()\n",
        "            uncertainty_v2 = None\n",
        "\n",
        "            # Determine where v1 has greater mean (excluding nodata)\n",
        "            v1_greater = (mean_v1_array > mean_v2_array) & (mean_v1_array != nodatavalue)\n",
        "\n",
        "            # Merge: take maximum mean, select CI/uncertainty from version with greater mean\n",
        "            mean_merged_array = np.maximum(mean_v1_array, mean_v2_array)\n",
        "            ci_merged_array = np.where(v1_greater, ci_v1_array, ci_v2_array)\n",
        "            uncertainty_merged_array = np.where(v1_greater, uncertainty_v1_array, uncertainty_v2_array)\n",
        "\n",
        "            # Export merged arrays\n",
        "            if not exists(mean_merged_path):\n",
        "                export_array_as_tif(mean_merged_array, mean_merged_path, template=mean_v1_path)\n",
        "            if not exists(ci_merged_path):\n",
        "                export_array_as_tif(ci_merged_array, ci_merged_path, template=mean_v1_path)\n",
        "            if not exists(uncertainty_merged_path):\n",
        "                export_array_as_tif(uncertainty_merged_array, uncertainty_merged_path, template=mean_v1_path)\n",
        "            print(f\"Merged statistics exported for {mean_merged_file.split('__')[1]}\")\n",
        "\n",
        "            # Delete originals\n",
        "            os.remove(mean_v1_path)\n",
        "            os.remove(mean_v2_path)\n",
        "            os.remove(ci_v1_path)\n",
        "            os.remove(ci_v2_path)\n",
        "            os.remove(uncertainty_v1_path)\n",
        "            os.remove(uncertainty_v2_path)\n",
        "        else:\n",
        "            # Copy version 1 as merged if version 2 doesn't exist\n",
        "            print(f\"Version 2 not found, using version 1 for {mean_v1_file.split('__')[1]}\")\n",
        "            shutil.copy2(mean_v1_path, mean_merged_path)\n",
        "            shutil.copy2(ci_v1_path, ci_merged_path)\n",
        "            shutil.copy2(uncertainty_v1_path, uncertainty_merged_path)\n",
        "            os.remove(mean_v1_path)\n",
        "            os.remove(ci_v1_path)\n",
        "            os.remove(uncertainty_v1_path)\n",
        "\n",
        "    print(\"\\nOldgrowth statistics merging complete.\")"
      ],
      "metadata": {
        "id": "N1Ye87F0Eeu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nezNm_Gz3cnz"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful for stopping background execution upon completion\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "5hwTeCjzEgbB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "nezNm_Gz3cnz"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}