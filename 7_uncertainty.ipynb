{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/7_uncertainty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhx977-WIaWJ"
      },
      "source": [
        "# Imports and directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRnNty96IThZ"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "\n",
        "# Mount Google Drive and set base directory\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4n2MGF0aZtJ"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!pip install xgboost --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpmgsb8KKMGN"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "import json\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import ipywidgets as widgets\n",
        "import itertools\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy.random import normal\n",
        "from os import makedirs\n",
        "from os.path import exists, join\n",
        "from osgeo import gdal\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "import scipy.stats as st\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import xgboost as xgb\n",
        "\n",
        "# Define GPU\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found.')\n",
        "else:\n",
        "  print(f\"Found GPU at: {device_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQwNO9T0_HPH"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "predictor_dir = join(base_dir, \"3_predictors\")\n",
        "predictor_final_dir = join(predictor_dir, \"final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "masks_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(uncertainty_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koHn-AiGG7KM"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)\n",
        "\n",
        "# Burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster, polygon, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "  with rasterio.open(raster, 'r+') as src:\n",
        "      array = src.read(1)\n",
        "      transform = src.transform\n",
        "      gdf = gpd.read_file(polygon)\n",
        "      for geom in gdf.geometry:\n",
        "          if not fixed and column_name == None:\n",
        "              column_name = gdf.columns[0]\n",
        "          if not fixed: burn_value = gdf.loc[gdf.geometry == geom, column_name].values[0]\n",
        "          else: burn_value = fixed_value\n",
        "          rasterize([(geom, burn_value)], out=array, transform=transform,\n",
        "              all_touched=all_touched, dtype=src.meta['dtype'], out_shape=src.shape)\n",
        "      src.write(array, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lqjsLKZaQXo"
      },
      "source": [
        "# Select model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SqkKHKnaV-8"
      },
      "outputs": [],
      "source": [
        "# Select a baseline model, tested and trained in advance.\n",
        "model_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file == 'model.json':\n",
        "      print(f'selected_model = \"{subdir.split(f\"{models_dir}/\",1)[1]}\"')\n",
        "      model_exists = True\n",
        "if not model_exists:\n",
        "  print(\"No model exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnerzp-gbapN"
      },
      "outputs": [],
      "source": [
        "selected_model = \"agbd_240926_030225\"\n",
        "categorise_variate = False # If the variate was categorised in 5_models\n",
        "\n",
        "# Define model directories\n",
        "selected_model_dir = join(models_dir,selected_model)\n",
        "selected_model_json = join(selected_model_dir, \"model.json\")\n",
        "selected_model_descr_dir = join(selected_model_dir, \"model_description.json\")\n",
        "selected_model_dataset_path = join(selected_model_dir, f\"{selected_model}.pkl\")\n",
        "selected_model_dataset = pd.read_pickle(selected_model_dataset_path)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(selected_model_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_variate = model_dataset_description[\"selected_variate\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "covariates_categorised = model_dataset_description[\"covariates_categorised\"]\n",
        "selected_predictors = model_dataset_description[\"selected_predictors\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "filter_parameter = model_dataset_description[\"filter_parameter\"]\n",
        "filter_values_to_include = model_dataset_description[\"filter_values_to_include\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "# Reload hyperparameters\n",
        "with open(selected_model_descr_dir) as model_description_json:\n",
        "  model_description = json.load(model_description_json)\n",
        "final_hyperparameters = eval(model_description[\"hyperparameters\"])\n",
        "\n",
        "# Remove early stopping and replace with mean n_estimators\n",
        "if \"early_stopping_rounds\" in final_hyperparameters:\n",
        "  final_hyperparameters = {k:v for k, v in final_hyperparameters.items() if k != \"early_stopping_rounds\"}\n",
        "  final_hyperparameters[\"n_estimators\"] = round(model_description[\"n_estimators mean\"])\n",
        "\n",
        "# Define directories\n",
        "scenarios_model_dir = join(scenarios_dir,selected_model)\n",
        "uncertainty_selected_model_dir = join(uncertainty_dir, selected_model)\n",
        "model_iterations_dir = join(uncertainty_selected_model_dir, \"model_iterations\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(uncertainty_selected_model_dir, exist_ok=True)\n",
        "makedirs(model_iterations_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9-hQ7G37uZi"
      },
      "source": [
        "# Model iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWymSi4ndKUA"
      },
      "outputs": [],
      "source": [
        "# Verify that the variate is equal to the mean\n",
        "print(f'mean = \"{selected_variate}\"')\n",
        "\n",
        "# Calculate se from columns flagged 'uncertainty'\n",
        "if len(uncertainty)==0:\n",
        "  print(\"There are no flagged uncertainty columns to calculate SE from.\")\n",
        "  print(\"Manually create the metric from the available columns.\")\n",
        "  for col in selected_model_dataset.columns:\n",
        "    print(f\"{col}\")\n",
        "else:\n",
        "  for col in selected_model_dataset.columns:\n",
        "    if col in uncertainty and col not in selected_variate:\n",
        "      print(f'se = \"{col}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QBalj00xwmRX"
      },
      "outputs": [],
      "source": [
        "mean = \"var_agbd\"\n",
        "se = \"var_agbd_se\"\n",
        "# Liang et al. (2023) use SE as a proxy for STDEV\n",
        "\n",
        "# Set model iterations\n",
        "model_iterations = 100\n",
        "\n",
        "# Define model (y axis changes for each iteration based on mean and se arrays)\n",
        "model_dataset_x = selected_model_dataset[selected_predictors]\n",
        "mean_array = selected_model_dataset[mean].values\n",
        "se_array = selected_model_dataset[se].values\n",
        "if categorise_variate: XGBPredictor = xgb.XGBClassifier(**final_hyperparameters)\n",
        "else: XGBPredictor = xgb.XGBRegressor(**final_hyperparameters)\n",
        "model_params = XGBPredictor.get_params()\n",
        "model_params['eval_metric'] = model_description['metric_used_for_training']\n",
        "# Default fix for new XGBoost version\n",
        "[model_params.pop(key, None) for key in ['n_estimators', 'enable_categorical', 'missing']]\n",
        "\n",
        "# Progress label\n",
        "model_progress_index = 0\n",
        "model_progress_label = widgets.Label(f\"Model iteration: {model_progress_index}/{model_iterations}\")\n",
        "display(model_progress_label)\n",
        "\n",
        "for model_iteration in range(1,model_iterations+1):\n",
        "  # Set model iteration filename and check if already exists\n",
        "  model_iteration_filename = f\"model_iteration_{model_iteration}.json\"\n",
        "  model_iteration_path = join(model_iterations_dir, model_iteration_filename)\n",
        "  # If model iteration does not exist...\n",
        "  if not exists(model_iteration_path):\n",
        "    # Set the random seed based on iteration for replicability\n",
        "    np.random.seed(model_iteration)\n",
        "    # Set a normal distribution sample as the y for this iteration\n",
        "    model_dataset_y = normal(mean_array, se_array)\n",
        "    # Create DMatrix objects\n",
        "    model_dtrain = xgb.DMatrix(model_dataset_x, model_dataset_y, enable_categorical=True)\n",
        "    # Train the model iteration using the tested hyperparameters\n",
        "    model = xgb.train(model_params,\n",
        "                        model_dtrain,\n",
        "                        num_boost_round=final_hyperparameters['n_estimators'],\n",
        "                        verbose_eval=True)\n",
        "    # Save the model iteration\n",
        "    model.save_model(model_iteration_path)\n",
        "  # Update progress\n",
        "  model_progress_index += 1\n",
        "  model_progress_label.value = f\"Model iteration: {model_progress_index}/{model_iterations}\"\n",
        "print(\"All model iterations have been trained and saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1UxS-kO8_1d"
      },
      "source": [
        "# Scenario iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UL55gK2y6Ye"
      },
      "outputs": [],
      "source": [
        "# Scenarios must be designed and tested using 06_scenarios first.\n",
        "# Predictors in the 'constant' and scenario subdirs should not be moved.\n",
        "\n",
        "# Select a scenario area\n",
        "scenario_area_exists = False\n",
        "for subdir in os.listdir(scenarios_model_dir):\n",
        "  if not subdir.endswith('.json') and not subdir.endswith('.csv'):\n",
        "    print(f'selected_scenario_area = \"{subdir}\"')\n",
        "    scenario_area_exists = True\n",
        "if not scenario_area_exists:\n",
        "  print(f\"Create a scenario area directory in {scenarios_model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYxqe0eeKmKi"
      },
      "outputs": [],
      "source": [
        "selected_scenario_area = \"tekai\"\n",
        "\n",
        "# Locate scenario area directories\n",
        "selected_scenario_area_dir = join(scenarios_model_dir, selected_scenario_area)\n",
        "predictors_dir = join(selected_scenario_area_dir, \"predictors\")\n",
        "tile_templates_dir = join(selected_scenario_area_dir, 'tile_templates')\n",
        "tile_predictor_stacks_dir = join(selected_scenario_area_dir, 'tile_predictor_stacks')\n",
        "\n",
        "# Define uncertainty scenario area directory\n",
        "uncertainty_scenario_area_dir = join(uncertainty_selected_model_dir,f\"scenarios_{selected_scenario_area}\")\n",
        "tile_prediction_cache_dir = join(uncertainty_scenario_area_dir, \"tile_prediction_cache\")\n",
        "makedirs(uncertainty_scenario_area_dir, exist_ok=True)\n",
        "makedirs(tile_prediction_cache_dir, exist_ok=True)\n",
        "\n",
        "# Collect available scenarios from the predictor stack tiles directory\n",
        "scenario_stacks_list = []\n",
        "for scenario in os.listdir(tile_predictor_stacks_dir):\n",
        "    scenario_stacks_list.append(scenario)\n",
        "\n",
        "# Select scenarios to predict\n",
        "print(\"scenarios_to_predict = [\")\n",
        "for scenario in sorted(scenario_stacks_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE-bIxq9F3Vy"
      },
      "outputs": [],
      "source": [
        "scenarios_to_predict = [\n",
        "  # \"2018\",\n",
        "  # \"2019\",\n",
        "  # \"2020\",\n",
        "  # \"2021\",\n",
        "  \"2022\",\n",
        "  \"2022_no_degradation_since_1990\",\n",
        "  \"2022_oldgrowth\",\n",
        "  \"2023\",\n",
        "  \"2023_no_degradation_since_1990\",\n",
        "  \"2023_oldgrowth\",\n",
        "  \"all_oldgrowth\",\n",
        "]\n",
        "\n",
        "# Check the number of model iterations available\n",
        "model_iterations_available = len(os.listdir(model_iterations_dir))\n",
        "print(f\"\\nThere are {len(os.listdir(model_iterations_dir))} model iterations available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqP1L8LtFcph"
      },
      "outputs": [],
      "source": [
        "# Set the number of scenario iterations. It must be <= the number of model iterations available.\n",
        "scenario_iterations = 100\n",
        "\n",
        "assert scenario_iterations <= model_iterations_available, f\"Reduce the number of scenario iterations to <= {model_iterations_available}.\"\n",
        "\n",
        "# Change this and the code within the block accordingly.\n",
        "add_covariates = True # Adds a selected covariate value as the predictor\n",
        "sensitivity_value = 0.99\n",
        "beam_value = 5\n",
        "\n",
        "# Check for GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0': print('GPU device not found')\n",
        "else: print(f\"Found GPU at: {device_name}\")\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "if n_tiles < 1: print(\"There are currently no template tiles.\")\n",
        "if n_tiles >= 1:\n",
        "  template_tile_x = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).XSize\n",
        "  print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "# Tile progress\n",
        "if n_tiles > 1:\n",
        "  tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "  display(tile_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_predict:\n",
        "  scenario_predictor_stack_dir = join(tile_predictor_stacks_dir, scenario)\n",
        "  # Create scenario iterations directory\n",
        "  scenario_iterations_dir = join(uncertainty_scenario_area_dir,f\"{scenario}_iterations\")\n",
        "  makedirs(scenario_iterations_dir, exist_ok=True)\n",
        "  # Iteration progress\n",
        "  iteration_progress_index = 0\n",
        "  iteration_progress_label = widgets.Label(f\"{scenario} iteration progress: {iteration_progress_index} / {scenario_iterations}\")\n",
        "  display(iteration_progress_label)\n",
        "\n",
        "  # Check if all scenario iterations already exist, if not then load predictor stack\n",
        "  scenario_iteration_list = []\n",
        "  for model_iteration in range(1,scenario_iterations+1):\n",
        "    prediction_iteration_filename = f\"{scenario}__{selected_scenario_area}_{selected_model}_unmasked_iteration_{model_iteration}.tif\"\n",
        "    prediction_iteration_path = join(scenario_iterations_dir, prediction_iteration_filename)\n",
        "    scenario_iteration_list.append(prediction_iteration_path)\n",
        "  all_scenario_iterations_exist = True\n",
        "  for scenario_iteration in scenario_iteration_list:\n",
        "    if not exists(scenario_iteration): all_scenario_iterations_exist = False\n",
        "  if not all_scenario_iterations_exist:\n",
        "    if n_tiles == 1:\n",
        "    # Load template parameters\n",
        "      template_tile_dir = join(tile_templates_dir, f\"template_tile_1.tif\")\n",
        "      template_tile_y = gdal.Open(template_tile_dir).GetRasterBand(1).YSize\n",
        "      template_tile_x = gdal.Open(template_tile_dir).GetRasterBand(1).XSize\n",
        "      # Load predictor stack\n",
        "      stack_filename = f\"predictor_stack_{scenario}_1.npy\"\n",
        "      predictor_stack = np.load(join(scenario_predictor_stack_dir, stack_filename))\n",
        "      if add_covariates: predictor_stack = np.hstack((predictor_stack,\n",
        "                    np.full((predictor_stack.shape[0], 1), beam_value, dtype=int),\n",
        "                    np.full((predictor_stack.shape[0], 1), sensitivity_value, dtype=float)\n",
        "                    ))\n",
        "    # Predict scenario for each model iteration\n",
        "    for model_iteration in range(1,scenario_iterations+1):\n",
        "      # Define the model\n",
        "      model_dir = join(model_iterations_dir,f\"model_iteration_{model_iteration}.json\")\n",
        "      prediction_iteration_filename = f\"{scenario}__{selected_scenario_area}_{selected_model}_unmasked_iteration_{model_iteration}.tif\"\n",
        "      prediction_iteration_path = join(scenario_iterations_dir, prediction_iteration_filename)\n",
        "      # If scenario iteration does not exist:\n",
        "      if not exists(prediction_iteration_path):\n",
        "        # Load model\n",
        "        if categorise_variate: XGBPredictor = xgb.XGBClassifier(**final_hyperparameters)\n",
        "        else: XGBPredictor = xgb.XGBRegressor(**final_hyperparameters)\n",
        "        XGBPredictor.load_model(fname=model_dir)\n",
        "        # Avoids issues using dataframe from CPU\n",
        "        xgb.set_config(verbosity=0, use_rmm=True)\n",
        "        # Get number of stacks\n",
        "        n_stacks = len(os.listdir(scenario_predictor_stack_dir))\n",
        "        if n_stacks == 1:\n",
        "          # Define prediction array and reshape\n",
        "          prediction = XGBPredictor.predict(predictor_stack)\n",
        "          prediction_array = prediction.reshape((template_tile_y, template_tile_x))\n",
        "          prediction = None # Flush prediction\n",
        "\n",
        "        # Tiling for if predictor stacks and separated into chunks\n",
        "        if n_stacks > 1:\n",
        "          # Create a tile cache directory for the prediction\n",
        "          tile_cache_iteration_dir = join(tile_prediction_cache_dir, prediction_iteration_filename[:-4])\n",
        "          makedirs(tile_cache_iteration_dir, exist_ok=True)\n",
        "          # Create a tile count to match the predictor stack chunk\n",
        "          for stack in range(1, n_stacks+1):\n",
        "            iteration_tile_filename = f\"scenario_tile_{stack}.tif\"\n",
        "            # Check if tile already exists\n",
        "            scenario_tile_exists = False\n",
        "            for scenario_tile in os.listdir(tile_cache_iteration_dir):\n",
        "              if scenario_tile == iteration_tile_filename: scenario_tile_exists=True\n",
        "            # If scenario prediction tile does not exist:\n",
        "            if scenario_tile_exists == False:\n",
        "              # Load template tile parameters\n",
        "              template_tile_dir = join(tile_templates_dir, f\"template_tile_{stack}.tif\")\n",
        "              template_tile_y = gdal.Open(template_tile_dir).GetRasterBand(1).YSize\n",
        "              template_tile_x = gdal.Open(template_tile_dir).GetRasterBand(1).XSize\n",
        "              # Load predictor tile stack\n",
        "              stack_filename = f\"predictor_stack_{scenario}_{stack}.npy\"\n",
        "              predictor_stack = np.load(join(scenario_predictor_stack_dir, stack_filename))\n",
        "              # Add covariates (sensitivity and BEAM)\n",
        "              if add_covariates: predictor_stack = np.hstack((predictor_stack,\n",
        "                                np.full((predictor_stack.shape[0], 1), beam_value, dtype=int),\n",
        "                                np.full((predictor_stack.shape[0], 1), sensitivity_value, dtype=float)\n",
        "                                ))\n",
        "              # Define prediction array and reshape\n",
        "              prediction = XGBPredictor.predict(predictor_stack)\n",
        "              predictor_stack = None # Flush predictor stack\n",
        "              prediction_tile = prediction.reshape((template_tile_y, template_tile_x))\n",
        "              prediction = None # Flush prediction\n",
        "              # Export prediction array as .tif\n",
        "              export_array_as_tif(prediction_tile, join(tile_cache_iteration_dir, iteration_tile_filename), template = template_tile_dir, compress = False)\n",
        "              prediction_tile = None # Flush prediction tile\n",
        "              # Update progress\n",
        "            tile_progress_index += 1\n",
        "            tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_stacks}\"\n",
        "          # Prepare empty array for merging tiles\n",
        "          prediction_array = np.empty((0,template_tile_x))\n",
        "          # Read each tile .tif as an array, stack, then export as a .tif\n",
        "          for subdir in os.listdir(tile_cache_iteration_dir):\n",
        "            if subdir.endswith('.tif'):\n",
        "              tile_dir = join(tile_cache_iteration_dir, subdir)\n",
        "              prediction_array = np.vstack((prediction_array, gdal.Open(tile_dir).ReadAsArray()))\n",
        "          # Delete scenario tile cache directory and reset index\n",
        "          shutil.rmtree(tile_cache_iteration_dir)\n",
        "          tile_progress_index = 0\n",
        "          tile_progress_label.value = f\"Tile progress: 0 / {n_tiles}\"\n",
        "\n",
        "        # Define scenario template\n",
        "        scenario_template = join(predictors_dir, os.listdir(predictors_dir)[0])\n",
        "        export_array_as_tif(prediction_array, prediction_iteration_path, template = scenario_template, compress = True)\n",
        "\n",
        "      iteration_progress_index += 1\n",
        "      iteration_progress_label.value = f\"{scenario} iteration progress: {iteration_progress_index} / {scenario_iterations}\"\n",
        "  else:\n",
        "    iteration_progress_label.value = f\"{scenario} iteration progress: 100 / {scenario_iterations}\"\n",
        "print(\"\\nScenario iterations complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIWo8ATZs-Ny"
      },
      "source": [
        "# Scenario statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG3PBLYgbUsN"
      },
      "outputs": [],
      "source": [
        "# Select a scenario iterations area\n",
        "scenario_iterations_area_exists = False\n",
        "for subdir in os.listdir(uncertainty_selected_model_dir):\n",
        "  if subdir != \"model_iterations\":\n",
        "    print(f'selected_scenario_iterations_area = \"{subdir[10:]}\"')\n",
        "    scenario_iterations_area_exists = True\n",
        "if not scenario_iterations_area_exists:\n",
        "  print(f\"Run the scenario iterations section first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZTJoAmSLLCM"
      },
      "outputs": [],
      "source": [
        "selected_scenario_iterations_area = \"tekai\"\n",
        "\n",
        "uncertainty_scenario_area_dir = join(uncertainty_selected_model_dir,f\"scenarios_{selected_scenario_iterations_area}\")\n",
        "statistics_unmasked_dir = join(uncertainty_scenario_area_dir,\"statistics_unmasked\")\n",
        "statistics_masked_dir = join(uncertainty_scenario_area_dir,\"statistics_masked\")\n",
        "makedirs(statistics_unmasked_dir, exist_ok=True)\n",
        "makedirs(statistics_masked_dir, exist_ok=True)\n",
        "\n",
        "# Collect scenarios with iterations\n",
        "scenarios_iterations_list = []\n",
        "for subdir in os.listdir(uncertainty_scenario_area_dir):\n",
        "  if subdir.endswith(\"_iterations\"):\n",
        "    scenarios_iterations_list.append(subdir[:-11])\n",
        "# Select scenarios to calculate mean and standard deviation\n",
        "print(\"scenarios_to_calculate = [\")\n",
        "for scenario in sorted(scenarios_iterations_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B0Por9lPQlA"
      },
      "outputs": [],
      "source": [
        "scenarios_to_calculate = [\n",
        "  \"2022\",\n",
        "  \"2022_no_degradation_since_1990\",\n",
        "  \"2022_oldgrowth\",\n",
        "  \"2023\",\n",
        "  \"2023_no_degradation_since_1990\",\n",
        "  \"2023_oldgrowth\",\n",
        "  \"all_oldgrowth\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IFiOCz6mXaXm"
      },
      "outputs": [],
      "source": [
        "# Check iteration number\n",
        "\n",
        "# Exact '0' pixels without decimals can be an indicator that an iteration was incorrectly generated.\n",
        "# These must be deleted and predicted again to avoid incorrect statistics.\n",
        "check_problems = True\n",
        "\n",
        "# Exact '0' pixels might have been genuinely predicted (though unlikely in a regression),\n",
        "# In which case set fix_problems to True and add these iterations to problem_arrays list.\n",
        "# The code below will add 0.001 so they won't trigger the problem checker again.\n",
        "fix_problems = False\n",
        "problem_rasters = [\n",
        "\n",
        "]\n",
        "\n",
        "if len(problem_rasters) > 0:\n",
        "  for problem_raster in problem_rasters:\n",
        "    problem_raster_path = join(uncertainty_scenario_area_dir, f\"{scenario}_iterations\", problem_raster)\n",
        "    problem_raster_array = gdal.Open(problem_raster_path).ReadAsArray()\n",
        "    problem_raster_array[problem_raster_array == 0] = 0.001\n",
        "    export_array_as_tif(problem_raster_array, problem_raster_path, template = problem_raster_path, compress=True)\n",
        "\n",
        "# Check the number of prediction iterations\n",
        "for scenario in scenarios_to_calculate:\n",
        "  scenario_iterations_dir = join(uncertainty_scenario_area_dir,f\"{scenario}_iterations\")\n",
        "  iterations = 0\n",
        "  for subdir in os.listdir(scenario_iterations_dir):\n",
        "    if subdir.endswith(\".tif\"):\n",
        "      # Check whether the prediction iteration is valid\n",
        "      if check_problems:\n",
        "        iteration = join(scenario_iterations_dir,subdir)\n",
        "        iteration_array = gdal.Open(iteration).ReadAsArray()\n",
        "        assert np.count_nonzero(iteration_array==0) == 0, f\"{subdir} contains 0 values, so the iteration may not have predicted correctly.\\n Check the file, delete and repredict if necessary.\\n If they are valid 0 values, run the cell below on:\\n {iteration}.\"\n",
        "      iterations += 1\n",
        "  print(f\"There are {iterations} prediction iterations for scenario {scenario} statistics.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ekgu_-LLOY_"
      },
      "outputs": [],
      "source": [
        "confidence_interval = 0.95\n",
        "\n",
        "# Statistics progress\n",
        "stats_progress_index = 0\n",
        "stats_progress_label = widgets.Label(f\"Scenario stats progress: {stats_progress_index}/{len(scenarios_to_calculate)}\")\n",
        "display(stats_progress_label)\n",
        "\n",
        "for scenario in scenarios_to_calculate:\n",
        "    stat_base_filename = f\"{scenario}__{selected_scenario_iterations_area}_{selected_model}\"\n",
        "    scenario_iterations_dir = join(uncertainty_scenario_area_dir,f\"{scenario}_iterations\")\n",
        "\n",
        "    # Define statistics raster directories\n",
        "    stat_mean_filename = f\"mean__{stat_base_filename}_unmasked.tif\"\n",
        "    stat_mean_dir = join(statistics_unmasked_dir,stat_mean_filename)\n",
        "    stat_uncertainty_filename = f\"uncertainty__{stat_base_filename}_unmasked.tif\"\n",
        "    stat_uncertainty_dir = join(statistics_unmasked_dir,stat_uncertainty_filename)\n",
        "\n",
        "    # Check whether statistics rasters already exist\n",
        "    stat_mean_tif_exists, stat_uncertainty_tif_exists = False, False\n",
        "    for stat_tif in os.listdir(statistics_unmasked_dir):\n",
        "        if stat_tif == stat_mean_filename: stat_mean_tif_exists = True\n",
        "        if stat_tif == stat_uncertainty_filename: stat_uncertainty_tif_exists = True\n",
        "\n",
        "    # If either mean or uncertainty do not exist\n",
        "    if stat_mean_tif_exists == False or stat_uncertainty_tif_exists == False:\n",
        "        stat_sum = None\n",
        "        stat_sum_sq = None\n",
        "        iteration_n = 0\n",
        "        for subdir in os.listdir(scenario_iterations_dir):\n",
        "            if subdir.endswith(\".tif\"):\n",
        "                iteration = os.path.join(scenario_iterations_dir, subdir)\n",
        "                iteration_array = gdal.Open(iteration).ReadAsArray()\n",
        "                if stat_sum is None:\n",
        "                    stat_sum = np.zeros_like(iteration_array, dtype='float64')\n",
        "                    stat_sum_sq = np.zeros_like(iteration_array, dtype='float64')\n",
        "                # Sum and sum of squares\n",
        "                stat_sum += iteration_array  # Running sum for mean\n",
        "                stat_sum_sq += np.square(iteration_array, dtype=np.float64)  # Running sum of squares for variance\n",
        "                iteration_n += 1\n",
        "\n",
        "        # Calculate mean: sum / count\n",
        "        stat_mean = np.divide(stat_sum, iteration_n, dtype='float64')\n",
        "        if stat_mean_tif_exists == False:\n",
        "            export_array_as_tif(stat_mean, stat_mean_dir, template = iteration)\n",
        "            print(f\"{stat_mean_filename} has been exported.\")\n",
        "        else: print(f\"{stat_mean_filename} already exists.\")\n",
        "\n",
        "        if stat_uncertainty_tif_exists == False:\n",
        "            # Calculate variance: E[X^2] - (E[X])^2\n",
        "            stat_variance = (stat_sum_sq - (stat_sum ** 2) / iteration_n) / (iteration_n - 1)\n",
        "            # Standard error: Ïƒ / sqrt(n)\n",
        "            stat_se = np.sqrt(stat_variance) / np.sqrt(iteration_n)\n",
        "            # Calculate confidence intervals using t-distribution\n",
        "            stat_ci_lower, stat_ci_upper = st.t.interval(confidence_interval, iteration_n - 1, loc=stat_mean, scale=stat_se)\n",
        "            # CI width: (upper - lower) / 2\n",
        "            stat_ci = np.divide(np.subtract(stat_ci_upper, stat_ci_lower, dtype='float64'), 2, dtype='float64')\n",
        "            # Uncertainty: (CI / mean) * 100%\n",
        "            stat_uncertainty = np.multiply(np.divide(stat_ci, stat_mean, dtype='float64'), 100, dtype='float64')\n",
        "            # Export statistics arrays as rasters\n",
        "            export_array_as_tif(stat_se, join(statistics_unmasked_dir,f\"se__{stat_base_filename}_unmasked.tif\"), template = iteration)\n",
        "            print(f\"se__{stat_base_filename}_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_ci_lower, join(statistics_unmasked_dir,f\"ci_lower__{stat_base_filename}_unmasked.tif\"), template = iteration)\n",
        "            print(f\"ci_lower__{stat_base_filename}_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_ci_upper, join(statistics_unmasked_dir,f\"ci_upper__{stat_base_filename}_unmasked.tif\"), template = iteration)\n",
        "            print(f\"ci_upper__{stat_base_filename}_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_ci, join(statistics_unmasked_dir,f\"ci__{stat_base_filename}_unmasked.tif\"), template = iteration)\n",
        "            print(f\"ci__{stat_base_filename}_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_uncertainty, stat_uncertainty_dir, template = iteration)\n",
        "            print(f\"{stat_uncertainty_filename} has been exported.\")\n",
        "        else: print(f\"{stat_uncertainty_filename} already exists.\")\n",
        "\n",
        "    else: print(f\"{stat_mean_filename} and {stat_uncertainty_filename} already exist.\")\n",
        "    stats_progress_index += 1\n",
        "    stats_progress_label.value = (f\"Scenario stats progress: {stats_progress_index}/{len(scenarios_to_calculate)}\")\n",
        "\n",
        "print(\"Statistics calculations and .tif exports complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbqtNygUAboh"
      },
      "source": [
        "# Mask scenario statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk03YvSdG_y4"
      },
      "outputs": [],
      "source": [
        "# Select a scenario iterations area\n",
        "scenario_iterations_area_exists = False\n",
        "for subdir in os.listdir(uncertainty_selected_model_dir):\n",
        "  if subdir != \"model_iterations\":\n",
        "    print(f'selected_scenario_iterations_area = \"{subdir[10:]}\"')\n",
        "    scenario_iterations_area_exists = True\n",
        "if not scenario_iterations_area_exists:\n",
        "  print(f\"Run the scenario iterations section first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TgjqruNBu_H"
      },
      "outputs": [],
      "source": [
        "selected_scenario_iterations_area = \"tekai\"\n",
        "\n",
        "uncertainty_scenario_area_dir = join(uncertainty_selected_model_dir,f\"scenarios_{selected_scenario_iterations_area}\")\n",
        "statistics_unmasked_dir = join(uncertainty_scenario_area_dir,\"statistics_unmasked\")\n",
        "statistics_masked_dir = join(uncertainty_scenario_area_dir,\"statistics_masked\")\n",
        "\n",
        "# Use polygons for masking, only areas inside the polygons will be included\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "print(\"mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"  '{polygon[:-5]}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVMXY7mNAZFi"
      },
      "outputs": [],
      "source": [
        "mask_polygons = [\n",
        "  # 'project_area',\n",
        "  'gedi_area',\n",
        "  # 'peninsular_malaysia',\n",
        "  # 'pa_taman_krau',\n",
        "  # 'pa_ais',\n",
        "]\n",
        "\n",
        "# Create an inverse project area path for masking\n",
        "template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "for polygon in mask_polygons:\n",
        "  inverse_polygon_path = join(polygons_dir, f\"{polygon}_inverse.gpkg\")\n",
        "  if not exists(inverse_polygon_path):\n",
        "    polygon_path = join(polygons_dir, f\"{polygon}.gpkg\")\n",
        "    template_polygon = gpd.read_file(template_polygon_path)\n",
        "    polygon_read = gpd.read_file(polygon_path)\n",
        "    polygon_crs = polygon_read.crs.to_epsg()\n",
        "    inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "    inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "    inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "    print(f\"An inverse masking polygon for {polygon} has been created in {polygons_dir}.\")\n",
        "  else: print(f\"An inverse masking polygon for {polygon} already exists.\")\n",
        "\n",
        "unmasked_predictions = []\n",
        "for scenario_prediction in os.listdir(statistics_unmasked_dir):\n",
        "  # Only mask mean, uncertainty and ci for visualisation and calculating statistics\n",
        "  if ('mean__' in scenario_prediction) or ('uncertainty__' in scenario_prediction):\n",
        "    unmasked_predictions.append(scenario_prediction)\n",
        "\n",
        "# Determine last predictor year for masking future scenarios\n",
        "final_predictor_years = []\n",
        "for final_predictor in os.listdir(predictor_final_dir):\n",
        "  if final_predictor.endswith('.tif') and final_predictor[-9] == '_':\n",
        "    try: final_predictor_years.append(int(final_predictor[-8:-4]))\n",
        "    except: continue\n",
        "last_predictor_year = max(final_predictor_years)\n",
        "\n",
        "# Masking progress\n",
        "masking_progress_index = 0\n",
        "masking_progress_label = widgets.Label(f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\")\n",
        "display(masking_progress_label)\n",
        "\n",
        "# Mask scenario statistics with the relevatant mask\n",
        "for scenario_prediction in unmasked_predictions: # Loop through each unmasked scenario\n",
        "  scenario_masked_filename = f\"{scenario_prediction[:-13]}.tif\"\n",
        "  scenario_masked_dir = join(statistics_masked_dir, scenario_masked_filename)\n",
        "  if not exists(scenario_masked_dir):\n",
        "    mask_exists = False\n",
        "    for mask in os.listdir(masks_dir):\n",
        "      # Match all oldgrowth scenarios\n",
        "      if 'all_oldgrowth' in mask or 'all_oldgrowth' in scenario_prediction:\n",
        "        if 'all_oldgrowth' in mask and 'all_oldgrowth' in scenario_prediction:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "      else: # Match all other historic scenarios\n",
        "        scenario_year = int(scenario_prediction.split('__')[1][:4])\n",
        "        mask_year = int(mask[12:16])\n",
        "        if scenario_year == mask_year:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "        else: # Match future scenarios with most recent forest mask\n",
        "          if scenario_year > last_predictor_year and last_predictor_year == mask_year:\n",
        "            selected_mask_filename = mask\n",
        "            selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "            mask_exists = True\n",
        "    if mask_exists == False: print(f\"A suitable mask for {scenario_prediction} does not exist.\\n\")\n",
        "    else: # Mask the scenario prediction\n",
        "      print(f\"Masking {scenario_prediction} with {selected_mask_filename}...\")\n",
        "      mask_array = gdal.Open(selected_mask_dir).ReadAsArray()\n",
        "      scenario_prediction_unmasked_dir = join(statistics_unmasked_dir, scenario_prediction)\n",
        "      scenario_prediction_array = gdal.Open(scenario_prediction_unmasked_dir).ReadAsArray()\n",
        "      # Mask where the mask array is not 1\n",
        "      scenario_masked_array = np.where(mask_array != 1, nodatavalue, scenario_prediction_array)\n",
        "      export_array_as_tif(scenario_masked_array, scenario_masked_dir, compress = True)\n",
        "      if len(mask_polygons) > 0:\n",
        "        for polygon_mask in mask_polygons:\n",
        "          inverse_gedi_area_path = join(polygons_dir, f\"{polygon_mask}_inverse.gpkg\")\n",
        "          print(f\"Masking {scenario_prediction} with {polygon_mask}...\")\n",
        "          burn_polygon_to_raster(scenario_masked_dir, inverse_gedi_area_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        # Recompress the prediction after burning the polygon masks\n",
        "        scenario_masked_array_2 = gdal.Open(scenario_masked_dir).ReadAsArray()\n",
        "        export_array_as_tif(scenario_masked_array_2, scenario_masked_dir, compress = True)\n",
        "      print(f\"{scenario_masked_filename} exported.\")\n",
        "  # Update masking progress\n",
        "  masking_progress_index += 1\n",
        "  masking_progress_label.value = f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui5TlyF-QZj_"
      },
      "source": [
        "# Scenario difference with uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGiuU5lULIwl"
      },
      "outputs": [],
      "source": [
        "# Select a scenario statistics area\n",
        "scenario_iterations_area_exists = False\n",
        "for subdir in os.listdir(uncertainty_selected_model_dir):\n",
        "  if subdir != \"model_iterations\":\n",
        "    print(f'selected_scenario_iterations_area = \"{subdir[10:]}\"')\n",
        "    scenario_iterations_area_exists = True\n",
        "if not scenario_iterations_area_exists:\n",
        "  print(f\"Run the scenario iterations section first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpmEYC36j5xi"
      },
      "outputs": [],
      "source": [
        "selected_scenario_iterations_area = \"tekai\"\n",
        "\n",
        "uncertainty_scenario_area_dir = join(uncertainty_selected_model_dir,f\"scenarios_{selected_scenario_iterations_area}\")\n",
        "statistics_masked_dir = join(uncertainty_scenario_area_dir,\"statistics_masked\")\n",
        "diff_uncertainty_dir = join(uncertainty_scenario_area_dir, \"scenario_difference\")\n",
        "makedirs(diff_uncertainty_dir, exist_ok=True)\n",
        "\n",
        "scenarios_diff_set = set()\n",
        "for masked_statistic in os.listdir(statistics_masked_dir):\n",
        "    scenarios_diff_set.add(masked_statistic.split(\"__\")[1])\n",
        "\n",
        "# Generate all possible pairs of scenarios, including all orders\n",
        "scenario_pairs = sorted(list(itertools.permutations(scenarios_diff_set, 2)))\n",
        "\n",
        "print(\"# Select scenarios to calculate mean difference with uncertainty\")\n",
        "print(\"scenario_pairs = [\")\n",
        "for s1, s2 in scenario_pairs:\n",
        "    print(f\"# ('{s1}','{s2}'),\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aI9JUExJSXw"
      },
      "outputs": [],
      "source": [
        "# Select scenarios to calculate mean difference with uncertainty\n",
        "scenario_pairs = [\n",
        "  ('2022', 'all_oldgrowth'),\n",
        "  ('2022', '2022_no_degradation_since_1990'),\n",
        "  ('2022', '2022_oldgrowth'),\n",
        "  ('2022_no_degradation_since_1990', '2022_oldgrowth'),\n",
        "  ('2022_oldgrowth', 'all_oldgrowth'),\n",
        "  ('2023', 'all_oldgrowth'),\n",
        "  ('2023', '2023_no_degradation_since_1990'),\n",
        "  ('2023', '2023_oldgrowth'),\n",
        "  ('2023_no_degradation_since_1990', '2023_oldgrowth'),\n",
        "  ('2023_oldgrowth', 'all_oldgrowth'),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrDkC4qUbMbR"
      },
      "outputs": [],
      "source": [
        "# Rename scenario differences for semantic meaning (optional)\n",
        "difference_names = {\n",
        "  ('2022', 'all_oldgrowth'):\n",
        "    '2022_degradation_deforestation_total',\n",
        "  ('2022', '2022_no_degradation_since_1990'):\n",
        "    '2022_degradation_since_1990',\n",
        "  ('2022', '2022_oldgrowth'):\n",
        "    '2022_degradation_total',\n",
        "  ('2022_no_degradation_since_1990', '2022_oldgrowth'):\n",
        "    '2022_degradation_before_1990',\n",
        "  ('2022_oldgrowth', 'all_oldgrowth'):\n",
        "    '2022_deforestation_total',\n",
        "  ('2023', 'all_oldgrowth'):\n",
        "    '2023_degradation_deforestation_total',\n",
        "  ('2023', '2023_no_degradation_since_1990'):\n",
        "    '2023_degradation_since_1990',\n",
        "  ('2023', '2023_oldgrowth'):\n",
        "    '2023_degradation_total',\n",
        "  ('2023_no_degradation_since_1990', '2023_oldgrowth'):\n",
        "    '2023_degradation_before_1990',\n",
        "  ('2023_oldgrowth', 'all_oldgrowth'):\n",
        "    '2023_deforestation_total',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HkZIbBNFIUW"
      },
      "outputs": [],
      "source": [
        "# Functions for difference in mean and uncertainty\n",
        "def diff_mean(scenario1_mean, scenario2_mean):\n",
        "  diff_mean_array = scenario1_mean - scenario2_mean\n",
        "  return diff_mean_array\n",
        "def diff_uncertainty(scenario1_mean, scenario1_uncertainty, scenario2_mean, scenario2_uncertainty):\n",
        "  sums_of_squares = np.square( np.multiply( scenario2_mean, scenario2_uncertainty, dtype='float64'), dtype='float64') + np.square( np.multiply( scenario1_mean, scenario1_uncertainty, dtype='float64'), dtype='float64')\n",
        "  diff_uncertainty_array = np.sqrt(sums_of_squares, dtype='float64') / (scenario2_mean + scenario1_mean)\n",
        "  return diff_uncertainty_array\n",
        "\n",
        "# Loop through the scenario pairs\n",
        "for scenario1, scenario2 in scenario_pairs:\n",
        "\n",
        "  # Lookup the description from the dictionary\n",
        "  difference_name = difference_names.get((scenario1, scenario2), f\"{scenario1}_-_{scenario2}\")\n",
        "\n",
        "  # Define filenames and directories of mean and uncertainty difference .tifs\n",
        "  diff_mean_filename = f\"mean__{difference_name}__{selected_scenario_iterations_area}_{selected_model}.tif\"\n",
        "  diff_mean_dir = join(diff_uncertainty_dir, diff_mean_filename)\n",
        "  diff_uncertainty_filename = f\"uncertainty__{difference_name}__{selected_scenario_iterations_area}_{selected_model}.tif\"\n",
        "  diff_uncertainty_raster_dir = join(diff_uncertainty_dir, diff_uncertainty_filename)\n",
        "\n",
        "  if not exists(diff_mean_dir) and not exists(diff_uncertainty_raster_dir):\n",
        "    print(f\"Calculating mean difference with uncertainty between {scenario1} and {scenario2}\")\n",
        "    scenario1_base_filename = f\"{scenario1}__{selected_scenario_iterations_area}_{selected_model}\"\n",
        "    scenario2_base_filename = f\"{scenario2}__{selected_scenario_iterations_area}_{selected_model}\"\n",
        "\n",
        "    # Define mean and uncertainty directories, assert that both exist for both scenarios\n",
        "    scenario1_mean_dir = join(statistics_masked_dir,f\"mean__{scenario1_base_filename}.tif\")\n",
        "    assert exists(scenario1_mean_dir), f\"mean__{scenario1_base_filename}.tif does not exist.\"\n",
        "    scenario1_uncertainty_dir = join(statistics_masked_dir,f\"uncertainty__{scenario1_base_filename}.tif\")\n",
        "    assert exists(scenario1_uncertainty_dir), f\"uncertainty__{scenario1_base_filename}.tif does not exist.\"\n",
        "    scenario2_mean_dir = join(statistics_masked_dir,f\"mean__{scenario2_base_filename}.tif\")\n",
        "    assert exists(scenario2_mean_dir), f\"mean__{mean__scenario2_base_filename}.tif does not exist.\"\n",
        "    scenario2_uncertainty_dir = join(statistics_masked_dir,f\"uncertainty__{scenario2_base_filename}.tif\")\n",
        "    assert exists(scenario2_uncertainty_dir), f\"uncertainty__{scenario2_base_filename}.tif does not exist.\"\n",
        "\n",
        "    # Convert scenario mean and uncertainty .tifs to temporary arrays\n",
        "    scenario1_mean_array_temp = gdal.Open(scenario1_mean_dir).ReadAsArray()\n",
        "    scenario1_uncertainty_array_temp = gdal.Open(scenario1_uncertainty_dir).ReadAsArray()\n",
        "    scenario2_mean_array_temp = gdal.Open(scenario2_mean_dir).ReadAsArray()\n",
        "    scenario2_uncertainty_array_temp = gdal.Open(scenario2_uncertainty_dir).ReadAsArray()\n",
        "\n",
        "    # Fill scenario nodata values with 0 if they are not nodatavalues in the other scenario\n",
        "    scenario1_mean_array = np.where((scenario1_mean_array_temp == nodatavalue) & (scenario2_mean_array_temp != nodatavalue), 0, scenario1_mean_array_temp)\n",
        "    scenario1_uncertainty_array = np.where((scenario1_uncertainty_array_temp == nodatavalue) & (scenario2_uncertainty_array_temp != nodatavalue), 0, scenario1_uncertainty_array_temp)\n",
        "    scenario2_mean_array = np.where((scenario2_mean_array_temp == nodatavalue) & (scenario1_mean_array != nodatavalue), 0, scenario2_mean_array_temp)\n",
        "    scenario2_uncertainty_array = np.where((scenario2_uncertainty_array_temp == nodatavalue) & (scenario1_uncertainty_array != nodatavalue), 0, scenario2_uncertainty_array_temp)\n",
        "\n",
        "    # Create difference mean and uncertainty arrays where the value is not 'nodatavalue'\n",
        "    diff_mean_array = np.where(scenario1_mean_array==nodatavalue, nodatavalue, diff_mean(scenario1_mean_array, scenario2_mean_array))\n",
        "    diff_uncertainty_array = np.where(scenario1_uncertainty_array==nodatavalue, nodatavalue, diff_uncertainty(scenario1_mean_array, scenario1_uncertainty_array, scenario2_mean_array, scenario2_uncertainty_array))\n",
        "\n",
        "    # Export the mean and uncertainty difference .tifs if they do not exist\n",
        "    if exists(diff_mean_dir): print(f\"{diff_mean_filename} already exists.\")\n",
        "    else: export_array_as_tif(diff_mean_array, diff_mean_dir, template = scenario1_mean_dir), print(f\"{diff_mean_filename} has been exported.\")\n",
        "    if exists(diff_uncertainty_raster_dir): print(f\"{diff_uncertainty_filename} already exists.\")\n",
        "    else: export_array_as_tif(diff_uncertainty_array, diff_uncertainty_raster_dir, template = scenario1_mean_dir), print(f\"{diff_uncertainty_filename} has been exported.\")\n",
        "\n",
        "  else: print(f\"Both {diff_mean_filename} and {diff_uncertainty_filename} already exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intactness"
      ],
      "metadata": {
        "id": "_pFGWPj0DowZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a scenario statistics area\n",
        "scenario_iterations_area_exists = False\n",
        "for subdir in os.listdir(uncertainty_selected_model_dir):\n",
        "  if subdir != \"model_iterations\":\n",
        "    print(f'selected_scenario_iterations_area = \"{subdir[10:]}\"')\n",
        "    scenario_iterations_area_exists = True\n",
        "if not scenario_iterations_area_exists:\n",
        "  print(f\"Run the scenario iterations section first.\")"
      ],
      "metadata": {
        "id": "vgW-4RQpEamo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_scenario_iterations_area = \"tekai\"\n",
        "\n",
        "uncertainty_scenario_area_dir = join(uncertainty_selected_model_dir, f\"scenarios_{selected_scenario_iterations_area}\")\n",
        "statistics_masked_dir = join(uncertainty_scenario_area_dir, \"statistics_masked\")\n",
        "diff_uncertainty_dir = join(uncertainty_scenario_area_dir, \"scenario_difference\")\n",
        "intactness_dir = join(uncertainty_scenario_area_dir, 'intactness')\n",
        "makedirs(intactness_dir, exist_ok=True)\n",
        "\n",
        "# Select which baseline and difference raster to use for calculating intactness\n",
        "# percentage and relative intactness. Ideally this is the scenario with the least disturbance\n",
        "# and the difference between that and the current reality.\n",
        "\n",
        "for baseline in os.listdir(statistics_masked_dir):\n",
        "  if 'mean' in baseline:\n",
        "    print(f\"selected_baseline = '{baseline}'\")\n",
        "for diff in os.listdir(diff_uncertainty_dir):\n",
        "  if 'mean' in diff:\n",
        "    print(f\"selected_diff = '{diff}'\")"
      ],
      "metadata": {
        "id": "XzF5WoTiDr79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_baseline = 'mean__all_oldgrowth__tekai_agbd_240926_030225.tif'\n",
        "selected_diff = 'mean__2022_degradation_deforestation_total__tekai_agbd_240926_030225.tif'\n",
        "forest_mask_year = '2022'\n",
        "\n",
        "percentage_filename = f\"percentage_change__{selected_baseline.split('__')[0]}__{selected_diff.split('__')[0]}__{selected_diff.split('__')[1]}\"\n",
        "percentage_path = join(intactness_dir, percentage_filename)\n",
        "\n",
        "if not exists(percentage_path):\n",
        "  # Define filenames and directories\n",
        "  selected_baseline_path = join(statistics_masked_dir, selected_baseline)\n",
        "  selected_diff_path = join(diff_uncertainty_dir, selected_diff)\n",
        "  selected_mask_path = join(masks_dir, f\"mask_forest_{forest_mask_year}.tif\")\n",
        "\n",
        "  # Convert to arrays\n",
        "  selected_baseline_array = gdal.Open(selected_baseline_path).ReadAsArray()\n",
        "  selected_diff_array = gdal.Open(selected_diff_path).ReadAsArray()\n",
        "  selected_mask_array = gdal.Open(selected_mask_path).ReadAsArray()\n",
        "\n",
        "  # Create difference arrays where the value is not 'nodatavalue'\n",
        "  percentage_array = np.where(selected_mask_array==nodatavalue, nodatavalue, selected_diff_array/selected_baseline_array*100)\n",
        "  export_array_as_tif(percentage_array, percentage_path, template = selected_baseline_path)\n",
        "  print(f\"{percentage_filename} has been exported.\")\n",
        "\n",
        "else: print(f\"{percentage_filename} already exists.\")"
      ],
      "metadata": {
        "id": "AJdlTIeHD15W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use additional polygons for masking relative intactness quantiles\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"mask_polygon = '{polygon}'\")"
      ],
      "metadata": {
        "id": "w9SAFDkfD3p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_polygon = 'forest_reserves.gpkg'\n",
        "# mask_polygon = None\n",
        "\n",
        "if mask_polygon is not None:\n",
        "  # Create an inverse project area path for masking\n",
        "  template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "  inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "  if not exists(inverse_polygon_path):\n",
        "    polygon_path = join(polygons_dir, mask_polygon)\n",
        "    template_polygon = gpd.read_file(template_polygon_path)\n",
        "    polygon_read = gpd.read_file(polygon_path)\n",
        "    polygon_crs = polygon_read.crs.to_epsg()\n",
        "    inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "    inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "    inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "    print(f\"An inverse masking polygon for {polygon} has been created in {polygons_dir}.\")\n",
        "  else: print(f\"An inverse masking polygon for {polygon} already exists.\")\n",
        "\n",
        "  # Copy the percentage raster for potential masking\n",
        "  percentage_masked_filename = f\"{percentage_filename}_masked_{mask_polygon[:-5]}.tif\"\n",
        "  percentage_masked_path = join(intactness_dir, percentage_masked_filename)\n",
        "  if not exists(percentage_masked_path):\n",
        "    print(f\"Copying {percentage_filename} for masking...\")\n",
        "    copyfile(percentage_path, percentage_masked_path)\n",
        "    print(f\"Masking {percentage_filename} with {mask_polygon}...\")\n",
        "    burn_polygon_to_raster(percentage_masked_path, inverse_polygon_path, fixed_value=nodatavalue, all_touched=False)\n",
        "    # Recompress the prediction after burning the polygon masks\n",
        "    percentage_masked_array = gdal.Open(percentage_masked_path).ReadAsArray()\n",
        "    export_array_as_tif(percentage_masked_array, percentage_masked_path, compress = True)\n",
        "    print(f\"{percentage_filename} masked.\")\n",
        "  else: print(f\"{percentage_masked_path} already exists.\")\n",
        "\n",
        "else: print(\"No additional mask will be used to calculate relative intactness.\")"
      ],
      "metadata": {
        "id": "qhw5c21LD5BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of quantiles for intactness rating (e.g. 10 for 1 - 10)\n",
        "num_quantiles = 10\n",
        "\n",
        "# Define paths and arrays\n",
        "relative_intactness_name = f'intactness_{mask_polygon[:-5]}_{num_quantiles}_quantiles'\n",
        "relative_intactness_path = join(intactness_dir, f'{relative_intactness_name}.tif')\n",
        "percentage_masked_array = gdal.Open(percentage_masked_path).ReadAsArray()\n",
        "relative_intactness_array = np.empty_like(percentage_masked_array, dtype=object)\n",
        "\n",
        "# Set all values above 0 to 0, assuming negative values are not intact\n",
        "percentage_masked_array[percentage_masked_array > 0] = 0\n",
        "\n",
        "# Separate valid and invalid (nodatavalue) elements\n",
        "valid_elements = percentage_masked_array[percentage_masked_array != nodatavalue]\n",
        "invalid_elements = percentage_masked_array == nodatavalue\n",
        "\n",
        "# Calculate quantiles for valid elements\n",
        "quantiles = np.percentile(valid_elements, np.linspace(0, 100, num_quantiles + 1)[1:-1]) if len(valid_elements) > 0 else []\n",
        "for i in range(1, num_quantiles + 1):\n",
        "    lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "    upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "    relative_intactness_array[(percentage_masked_array > lower_bound) & (percentage_masked_array <= upper_bound)] = i\n",
        "# if nodatavalue is not None:\n",
        "    relative_intactness_array[invalid_elements] = nodatavalue\n",
        "export_array_as_tif(relative_intactness_array, relative_intactness_path)\n",
        "\n",
        "# Prepare data for CSV: Collect lower and upper bounds for each category\n",
        "ranges_data = {'Lower_Bound': [], 'Upper_Bound': []}\n",
        "for i in range(1, num_quantiles + 1):\n",
        "    lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "    upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "    ranges_data['Lower_Bound'].append(lower_bound)\n",
        "    ranges_data['Upper_Bound'].append(upper_bound)\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "relative_intactness_df = pd.DataFrame(ranges_data)\n",
        "relative_intactness_csv_path = os.path.join(intactness_dir, f'{relative_intactness_name}.csv')\n",
        "relative_intactness_df.to_csv(relative_intactness_csv_path, index=False)\n",
        "\n",
        "# Generate and save histogram as .png\n",
        "histogram_path = join(intactness_dir, f'{relative_intactness_name}.png')\n",
        "plt.figure()\n",
        "plt.hist(valid_elements.flatten(), bins='auto')\n",
        "plt.title(f'{relative_intactness_name} Histogram')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.savefig(histogram_path)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "395aJN_-D6d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nezNm_Gz3cnz"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kki0NPO3FKOw"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution upon completion\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "Xhx977-WIaWJ",
        "5lqjsLKZaQXo",
        "C9-hQ7G37uZi"
      ],
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}