{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/1_areas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUUhJgLFYI88"
      },
      "source": [
        "# Imports, directories and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr4kdJvHapxh"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljIpLF7_GwQb"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOQrLyVyW1Qa"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import geopandas as gpd\n",
        "import getpass\n",
        "from google.colab import runtime\n",
        "import ipywidgets as widgets\n",
        "import math\n",
        "import numpy as np\n",
        "from os import makedirs, remove\n",
        "from os.path import exists, join\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "import pandas as pd\n",
        "import requests\n",
        "from shapely.geometry import box\n",
        "from shutil import copy\n",
        "import zipfile\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8ZQtAdTl6vt"
      },
      "outputs": [],
      "source": [
        "# Define directories.\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "features_dir = join(base_dir, \"3_features\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "dem_dir = join(areas_dir, \"dem\")\n",
        "dem_tiles_dir = join(dem_dir, \"tiles\")\n",
        "\n",
        "# Create directories if they do not exist.\n",
        "makedirs(areas_dir, exist_ok=True)\n",
        "makedirs(polygons_dir, exist_ok=True)\n",
        "makedirs(dem_dir, exist_ok=True)\n",
        "makedirs(dem_tiles_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb2lqtUz4Yw2"
      },
      "outputs": [],
      "source": [
        "# Global function: read raster as array\n",
        "def read_raster_as_array(path):\n",
        "    ds = gdal.Open(path)\n",
        "    arr = ds.ReadAsArray()\n",
        "    ds = None\n",
        "    return arr\n",
        "\n",
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuNPGIEE2BI4"
      },
      "source": [
        "# Project area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xIGIHOL2DSq"
      },
      "outputs": [],
      "source": [
        "# Upload 'project_area.gpkg' polygon to the 1_areas/polygons directory.\n",
        "# This can be a polygon of any shape. A bounding box will be used to create the\n",
        "# GEDI download area in 1_variates.ipynb. # A buffered bounding box will be used\n",
        "# for the raster template, to ensure all feature edge effects are included.\n",
        "\n",
        "#Project CRS EPSG\n",
        "crs_epsg = 4326\n",
        "\n",
        "# Buffer to ~300m to account for forest / disturbance edge effects (~120m),\n",
        "# the prediction area buffer (~30m) and geolocation / clipping imprecision (x2).\n",
        "buffer_distance_metres = 300\n",
        "\n",
        "project_area_path = join(polygons_dir, 'project_area.gpkg')\n",
        "\n",
        "if exists(project_area_path):\n",
        "  print(\"Project polygon found:\\n\")\n",
        "  # Read project polygon\n",
        "  project_area_read = gpd.read_file(join(polygons_dir, 'project_area.gpkg'))\n",
        "  display(project_area_read[\"geometry\"].iloc[0])\n",
        "  if project_area_read.crs.to_epsg() == crs_epsg:\n",
        "    project_area_path = join(polygons_dir, \"project_area.gpkg\")\n",
        "    project_area_buffered_bbox_path = join(polygons_dir, 'project_area_buffered_bbox.gpkg')\n",
        "    # Calculate the bounding box of the project polygon\n",
        "    if not exists (project_area_buffered_bbox_path):\n",
        "      # Suppress warning about not being a geographic CRS, as we account for this.\n",
        "      # However larger buffers or project areas near the poles might still need to be converted.\n",
        "      warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "      # Get the centroid of the project polygon\n",
        "      project_polygon_centroid = project_area_read.centroid.values[0]\n",
        "      # Convert the buffer distance from meters to decimal degrees based on the location at the centroid\n",
        "      buffer_distance_degrees = buffer_distance_metres / (111320 * abs(math.cos(math.radians(project_polygon_centroid.y))))\n",
        "      # Buffer the polygon\n",
        "      project_area_buffered = project_area_read.buffer(buffer_distance_degrees)\n",
        "      # Create a bounding box polygon and save\n",
        "      project_area_buffered_bbox = box(*project_area_buffered.total_bounds)\n",
        "      gdf = gpd.GeoDataFrame(geometry=[project_area_buffered_bbox], crs=f\"EPSG:{crs_epsg}\")\n",
        "      gdf.to_file(project_area_buffered_bbox_path, driver='GPKG')\n",
        "      print(f\"Buffered the project area to {buffer_distance_metres} and created a bounding box: {project_area_buffered_bbox_path}\")\n",
        "    else: print(f\"Project area has already been buffered and bound to a box: {project_area_buffered_bbox_path}\")\n",
        "    # Read the buffered project area bounding box\n",
        "    project_area_buffered_bbox_read = gpd.read_file(project_area_buffered_bbox_path)\n",
        "    bbox_bounds = project_area_buffered_bbox_read.total_bounds\n",
        "    project_x_min, project_x_max = bbox_bounds[0], bbox_bounds[2]\n",
        "    project_y_min, project_y_max = bbox_bounds[1], bbox_bounds[3]\n",
        "    print(f\"\\nThe buffered polygon bounding box has the coordinates:\\n{project_x_min}, {project_y_min} to {project_x_max}, {project_y_max}.\")\n",
        "  else: print(\"Reproject 'project_area.gpkg' to EPSG:4326.\")\n",
        "else: print(\"Create 'project_area.gpkg' and upload to 1_areas/polygons\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq2a_lWnP08i"
      },
      "source": [
        "# Download DEM tiles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Copernicus 'COP-DEM_GLO-30-DGED' DEM tiles for the project area.\n",
        "# https://dataspace.copernicus.eu/explore-data/data-collections/copernicus-contributing-missions/collections-description/COP-DEM\n",
        "# First register to get credentials for 'Copernicus Contributing Missions' in the Copernicus Data Space Ecosystem:\n",
        "# https://dataspace.copernicus.eu/explore-data/data-collections/copernicus-contributing-missions/ccm-how-to-register\n",
        "# Make sure to check the box \"I am also interested in accessing Copernicus Contributing Missions data\".\n",
        "\n",
        "# Read project area bbox and create WKT 'area of interest'\n",
        "project_area_buffered_bbox_path = join(polygons_dir, 'project_area_buffered_bbox.gpkg')\n",
        "project_area_buffered_bbox_read = gpd.read_file(project_area_buffered_bbox_path)\n",
        "bbox_bounds = project_area_buffered_bbox_read.total_bounds\n",
        "project_x_min, project_y_min, project_x_max, project_y_max = bbox_bounds\n",
        "aoi_wkt = f\"POLYGON(({project_x_min} {project_y_min}, {project_x_min} {project_y_max}, {project_x_max} {project_y_max}, {project_x_max} {project_y_min}, {project_x_min} {project_y_min}))\"\n",
        "\n",
        "# Prompt for credentials and obtain OAuth2 token.\n",
        "email = getpass.getpass(\"Enter Copernicus account email: \")\n",
        "password = getpass.getpass(\"Enter Copernicus account password: \")\n",
        "token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
        "data = {\"client_id\": \"cdse-public\", \"username\": email, \"password\": password, \"grant_type\": \"password\"}\n",
        "try:\n",
        "    token_response = requests.post(token_url, data=data)\n",
        "    token_response.raise_for_status()\n",
        "    access_token = token_response.json()[\"access_token\"]\n",
        "    print(\"Authentication successful. Access token obtained.\")\n",
        "except Exception as e:\n",
        "    print(\"Authentication failed:\", e)\n",
        "    raise\n",
        "\n",
        "# Query catalogue API for DEM products intersecting AOI, use local CSV cache if available\n",
        "data_collection = \"CCM\"\n",
        "catalog_url = \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products\"\n",
        "filter_query = f\"Collection/Name eq '{data_collection}' and OData.CSC.Intersects(area=geography'SRID=4326;{aoi_wkt}')\"\n",
        "catalog_api_url = (catalog_url\n",
        "    + f\"?$filter={filter_query}\"\n",
        "    + \"&$top=1000\")  # Increase limit to avoid missing tiles\n",
        "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
        "catalog_csv = join(dem_dir, \"dem_catalogue.csv\")\n",
        "\n",
        "if exists(catalog_csv):\n",
        "    df = pd.read_csv(catalog_csv)\n",
        "    print(f\"Loaded {len(df)} DEM products from local CSV.\")\n",
        "else:\n",
        "    all_products = []\n",
        "    next_url = catalog_api_url\n",
        "    page_count = 0  # Initialize page counter\n",
        "    while next_url:\n",
        "        cat_response = requests.get(next_url, headers=headers)\n",
        "        cat_response.raise_for_status()\n",
        "        cat_json = cat_response.json()\n",
        "        all_products.extend(cat_json[\"value\"])\n",
        "        page_count += 1  # Increment page counter\n",
        "        next_url = cat_json.get(\"@odata.nextLink\")\n",
        "    df = pd.DataFrame.from_dict(all_products)\n",
        "    print(f\"Found {len(df)} DEM products across {page_count} pages intersecting the project area.\")\n",
        "    df.to_csv(catalog_csv, index=False)\n",
        "\n",
        "# Filter DEM tiles by 'Name', sort by 'ModificationDate', then drop duplicates by Footprint\n",
        "df_filtered = df[df[\"Name\"].str.startswith(\"DEM1_SAR_DGE_30\")]\n",
        "print(f\"Found {len(df_filtered)} 'COP-DEM_GLO-30-DGED' DEM tiles.\")\n",
        "df_filtered = df_filtered.sort_values(\"ModificationDate\").copy()\n",
        "df_filtered_unique = df_filtered.drop_duplicates(subset=[\"GeoFootprint\"], keep=\"last\")\n",
        "print(f\"{len(df_filtered_unique)} unique footprints will be downloaded, prioritising the most recent.\")\n",
        "\n",
        "# Build list of product IDs for download.\n",
        "dem_tiles_id_list = df_filtered_unique[\"Id\"].tolist()\n",
        "if len(dem_tiles_id_list) == 0:\n",
        "    print(\"No DEM tiles found within project area bounds.\")\n",
        "\n",
        "# Download and extract the DEM .tif from each product with up to 3 attempts.\n",
        "index = 0\n",
        "progress_label = widgets.Label(value=f\"DEM tile download progress: {index}/{len(dem_tiles_id_list)}\")\n",
        "display(progress_label)\n",
        "for product_id in dem_tiles_id_list:\n",
        "    # Retrieve product row, build download URL and file paths.\n",
        "    row = df_filtered.loc[df_filtered[\"Id\"] == product_id].iloc[0]\n",
        "    download_url_base = \"https://download.dataspace.copernicus.eu/odata/v1/Products\"\n",
        "    product_url = f\"{download_url_base}({product_id})/$value\"\n",
        "    dem_tile_zip_filename = f'{row[\"Name\"]}.zip'\n",
        "    dem_tile_zip_path = join(dem_tiles_dir, dem_tile_zip_filename)\n",
        "    # Retry loop: download .zip and extract 'DEM.tif' directly into dem_tiles_dir.\n",
        "    extracted_tif_path = None\n",
        "    attempts = 0\n",
        "    while attempts < 3:\n",
        "        try:\n",
        "            expected_size = row.get('ContentLength', None)  # If available\n",
        "            if exists(dem_tile_zip_path):\n",
        "                if expected_size and os.path.getsize(dem_tile_zip_path) != expected_size:\n",
        "                    remove(dem_tile_zip_path)\n",
        "\n",
        "            if not exists(dem_tile_zip_path):\n",
        "                response = requests.get(product_url, headers=headers, allow_redirects=True)\n",
        "                response.raise_for_status()\n",
        "                with open(dem_tile_zip_path, 'wb') as f:\n",
        "                  f.write(response.content)\n",
        "            with zipfile.ZipFile(dem_tile_zip_path, 'r') as z:\n",
        "                tif_filename = next((f for f in z.namelist() if f.endswith(\"DEM.tif\")), None)\n",
        "                if tif_filename is None:\n",
        "                    raise Exception(\"DEM.tif not found in zip\")\n",
        "                extracted_tif_name = os.path.basename(tif_filename)\n",
        "                extracted_tif_path = join(dem_tiles_dir, extracted_tif_name)\n",
        "                with open(extracted_tif_path, 'wb') as out_file:\n",
        "                    out_file.write(z.read(tif_filename))\n",
        "            break  # Exit retry loop\n",
        "        except Exception as e:\n",
        "            attempts += 1\n",
        "            if exists(dem_tile_zip_path):\n",
        "                remove(dem_tile_zip_path)\n",
        "            if extracted_tif_path and exists(extracted_tif_path):\n",
        "                remove(extracted_tif_path)\n",
        "            if attempts < 3:\n",
        "                print(f\"Attempt {attempts} failed for ID: {product_id} - {e}. Retrying...\")\n",
        "            else:\n",
        "                print(f\"Failed ID: {product_id} after 3 attempts - {e}. Moving to next product.\")\n",
        "    index += 1\n",
        "    progress_label.value = f\"DEM tile download progress: {index}/{len(dem_tiles_id_list)}\""
      ],
      "metadata": {
        "id": "hYdGul9pJt24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9TuMSdehVVK"
      },
      "source": [
        "# Merge DEM tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyTQqNr_27Fw"
      },
      "outputs": [],
      "source": [
        "# Merge the DEM tiles into a single raster\n",
        "dem_merged_path = join(dem_dir, \"dem_merged.tif\")\n",
        "\n",
        "if not exists(dem_merged_path):\n",
        "  # List tiles\n",
        "  tiles_to_merge = []\n",
        "  for file in os.listdir(dem_tiles_dir):\n",
        "    if file.endswith(\".tif\"):\n",
        "      tiles_to_merge.append(join(dem_tiles_dir, file))\n",
        "  # Create a temporary virtual file (VRT) from the tiles\n",
        "  temp_vrt = join(dem_dir, 'temp.vrt')\n",
        "  gdal.BuildVRT(temp_vrt, tiles_to_merge)\n",
        "  # Merge the input files into a single GeoTIFF file\n",
        "  merge_options = gdal.TranslateOptions(format='GTiff', outputType=gdal.GDT_Float32, noData=nodatavalue,\n",
        "                                  creationOptions=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'])\n",
        "  gdal.Translate(dem_merged_path, temp_vrt, options=merge_options)\n",
        "  # Remove the temporary VRT file\n",
        "  os.remove(temp_vrt)\n",
        "  print(f\"The merged DEM raster has been saved to: {dem_merged_path}\")\n",
        "else: print(f\"A merged DEM raster already exists at: {dem_merged_path}\")\n",
        "\n",
        "# Clip the raster to project area extent\n",
        "dem_merged_clipped_path = join(dem_dir, \"dem_merged_clipped.tif\")\n",
        "\n",
        "if not exists(dem_merged_clipped_path):\n",
        "  # Read the buffered project area bounding box\n",
        "  project_area_buffered_bbox_path = join(polygons_dir, 'project_area_buffered_bbox.gpkg')\n",
        "  project_area_buffered_bbox_read = gpd.read_file(project_area_buffered_bbox_path)\n",
        "  bbox_bounds = project_area_buffered_bbox_read.total_bounds\n",
        "  # Get coordinates\n",
        "  project_x_min, project_x_max = bbox_bounds[0], bbox_bounds[2]\n",
        "  project_y_min, project_y_max = bbox_bounds[1], bbox_bounds[3]\n",
        "  project_coords = [project_x_min, project_y_max, project_x_max, project_y_min]\n",
        "  # Define Translate options\n",
        "  clip_options = gdal.TranslateOptions(projWin=[project_x_min, project_y_max, project_x_max, project_y_min],\n",
        "                                  outputType=gdal.GDT_Float32, noData=nodatavalue, creationOptions=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'])\n",
        "  # call gdal.Translate() with the new options argument\n",
        "  gdal.Translate(dem_merged_clipped_path, dem_merged_path, options=clip_options)\n",
        "  print(f\"The clipped, merged DEM raster has been saved to: {dem_merged_clipped_path}\")\n",
        "else: print(f\"A clipped merged DEM raster already exists at: {dem_merged_clipped_path}\")\n",
        "\n",
        "# Copy the clipped, merged DEM to '3_features' directory to use as the base DEM\n",
        "base_dem_dsm_path = join(areas_dir, \"base_dem_dsm.tif\")\n",
        "\n",
        "if not exists(base_dem_dsm_path):\n",
        "  copy(dem_merged_clipped_path, base_dem_dsm_path)\n",
        "  print(f\"The clipped, merged DEM has been copied for use as a base DEM: {base_dem_dsm_path}\")\n",
        "else: print(f\"A base DEM already exists at: {base_dem_dsm_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwICylX1t4mZ"
      },
      "source": [
        "# Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys_zk5eHpPkd"
      },
      "outputs": [],
      "source": [
        "# Create template from DEM\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "if not exists(template_tif_path):\n",
        "  dem_merged_clipped_path = join(dem_dir, \"dem_merged_clipped.tif\")\n",
        "  dem_merged_clipped_array = read_raster_as_array(dem_merged_clipped_path) # Convert DEM to array\n",
        "  template_array = np.ones_like(dem_merged_clipped_array) # Change all values to 1\n",
        "  export_array_as_tif(template_array, template_tif_path, template=dem_merged_clipped_path)\n",
        "  print(f\"A template raster has been created: {template_tif_path}\")\n",
        "else: print(f\"A template raster already exists at: {template_tif_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDHujjIH7Ftq"
      },
      "outputs": [],
      "source": [
        "# Create template polygon\n",
        "template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "if not exists(template_polygon_path):\n",
        "  # Get template raster spatial data\n",
        "  template_raster = gdal.Open(template_tif_path)\n",
        "  template_raster_band = template_raster.GetRasterBand(1)\n",
        "  spatial_ref = ogr.osr.SpatialReference()\n",
        "  spatial_ref.ImportFromWkt(template_raster.GetProjection())\n",
        "  # Polygonize template raster without fields or layer name\n",
        "  template_polygon_file = ogr.GetDriverByName(\"GPKG\").CreateDataSource(template_polygon_path)\n",
        "  template_polygon_layer = template_polygon_file.CreateLayer(\"\", srs=spatial_ref, geom_type=ogr.wkbPolygon)\n",
        "  gdal.Polygonize(template_raster_band, None, template_polygon_layer, -1)\n",
        "  template_raster = None\n",
        "  print(f\"A template polygon has been created: {template_polygon_path}\")\n",
        "else: print(f\"A template polygon already exists at: {template_polygon_path}\")\n",
        "template_polygon_read = gpd.read_file(template_polygon_path)\n",
        "template_polygon_bounds = template_polygon_read.total_bounds\n",
        "print(f\"\\nThe template polygon has the coordinates:\\n{template_polygon_bounds[0]}, {template_polygon_bounds[1]} to {template_polygon_bounds[2]}, {template_polygon_bounds[3]}.\")\n",
        "\n",
        "# Create an inverse project area path for masking\n",
        "inverse_project_area_path = join(polygons_dir, \"project_area_inverse.gpkg\")\n",
        "if not exists(inverse_project_area_path):\n",
        "  template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "  template_polygon = gpd.read_file(template_polygon_path)\n",
        "  project_area_polygon = gpd.read_file(project_area_path)\n",
        "  inverse_project_area_polygon = template_polygon.dissolve().geometry.iloc[0].difference(project_area_polygon.dissolve().geometry.iloc[0])\n",
        "  inverse_project_area_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_project_area_polygon]}, crs=f\"EPSG:{crs_epsg}\")\n",
        "  inverse_project_area_polygon_gdf.to_file(inverse_project_area_path, driver=\"GPKG\")\n",
        "  print(f\"An inverse project area polygon has been created: {inverse_project_area_path}\")\n",
        "else: print(f\"An inverse project area already exists at: {inverse_project_area_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c367RQcutCrA"
      },
      "source": [
        "# Measurement rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbZ03-E1vrfA"
      },
      "outputs": [],
      "source": [
        "# Create measurement rasters for feature engineering and area-based statistics.\n",
        "# Despite some claims to the contrary, the Earth is not flat. Pixels in a geographic\n",
        "# coordinate system (degrees) represent different metric distances and areas depending\n",
        "# on latitude. These rasters store the precise dimensions of each pixel in metres.\n",
        "\n",
        "# Define template\n",
        "template_path = join(areas_dir, \"template.tif\")\n",
        "template = gdal.Open(template_path)\n",
        "template_array = template.ReadAsArray()\n",
        "rows, cols = template_array.shape\n",
        "\n",
        "# Geotransform: defines how pixel coordinates map to geographic coordinates.\n",
        "# [0] longitude of top-left corner, [1] pixel width in degrees,\n",
        "# [3] latitude of top-left corner, [5] pixel height in degrees (negative)\n",
        "geotransform = template.GetGeoTransform()\n",
        "pixel_width_degrees = geotransform[1]\n",
        "pixel_height_degrees = geotransform[5]\n",
        "template = None\n",
        "\n",
        "# Grids of column and row indices for vectorised calculations, allowing pixels to\n",
        "# be processed at once rather than looping individually.\n",
        "col_grid, row_grid = np.meshgrid(np.arange(cols, dtype=np.float64), np.arange(rows, dtype=np.float64))\n",
        "\n",
        "# Longitude at pixel centre (+0.5 shifts from corner to centre)\n",
        "longitude_path = join(areas_dir, \"longitude.tif\")\n",
        "if not exists(longitude_path):\n",
        "    longitude_array = geotransform[0] + (col_grid + 0.5) * pixel_width_degrees\n",
        "    # Antimeridian wrapping, where longitude must stay within -180 to +180.\n",
        "    # Values beyond this range wrap around (e.g. 185° becomes -175°).\n",
        "    longitude_array = np.remainder(longitude_array + 180, 360) - 180\n",
        "    export_array_as_tif(longitude_array.astype(np.float64), longitude_path, dtype=gdal.GDT_Float64)\n",
        "    print(f\"Raster with cell longitude in decimal degrees created: {longitude_path}\")\n",
        "else: print(f\"Raster with cell longitude in decimal degrees already exists: {longitude_path}\")\n",
        "\n",
        "# Latitude at pixel centre\n",
        "latitude_path = join(areas_dir, \"latitude.tif\")\n",
        "if not exists(latitude_path):\n",
        "    latitude_array = geotransform[3] + (row_grid + 0.5) * pixel_height_degrees\n",
        "    latitude_array = np.clip(latitude_array, -90, 90)\n",
        "    export_array_as_tif(latitude_array.astype(np.float64), latitude_path, dtype=gdal.GDT_Float64)\n",
        "    print(f\"Raster with cell latitude in decimal degrees created: {latitude_path}\")\n",
        "else: print(f\"Raster with cell latitude in decimal degrees already exists: {latitude_path}\")\n",
        "\n",
        "# Snyder, J. P. (1987). Map projections--A working manual (Vol. 1395). US Government Printing Office.\n",
        "\n",
        "# WGS84 ellipsoid parameters. The Earth bulges at the equator and is flattened at the poles.\n",
        "# This shape is defined by the equatorial radius and the polar radius.\n",
        "# Eccentricity squared measures how much the ellipsoid deviates from a sphere.\n",
        "# Snyder Page viii (Symbols) defines these as:\n",
        "# a = equatorial radius (semimajor axis)\n",
        "# b = polar radius (semiminor axis)\n",
        "# The eccentricity squared calcualtion is defined as e**2 = 1 - (b**2/a**2)\n",
        "equatorial_radius = 6_378_137.0\n",
        "polar_radius = 6_356_752.31425\n",
        "eccentricity_squared = 1.0 - (polar_radius**2 / equatorial_radius**2)\n",
        "degrees_to_radians = np.pi / 180\n",
        "\n",
        "# Load latitude raster and convert to radians, where one radian is the angle at which\n",
        "# arc length equals the radius, required for sin() and cos() functions\n",
        "latitude_array = read_raster_as_array(latitude_path)\n",
        "latitude_radians = np.radians(latitude_array)\n",
        "\n",
        "# Geodetic factor W = sqrt(1 - e**2 sin**2 phi), common to ellipsoidal curvature calculations.\n",
        "# Appears in Snyder eqs. 4-18 through 4-21; where e**2 = eccentricity_squared, phi = latitude.\n",
        "sin_latitude_squared = np.sin(latitude_radians)**2\n",
        "geodetic_factor = np.sqrt(1 - eccentricity_squared * sin_latitude_squared)\n",
        "\n",
        "# Cell width in metres. Unlike latitude, this varies because lines of longitude are\n",
        "# furthest apart at the equator and meet at the poles. One degree of longitude spans\n",
        "# ~111 km at the equator but nearly 0 km at the poles.\n",
        "cell_size_x_path = join(areas_dir, \"cell_size_x.tif\")\n",
        "if not exists(cell_size_x_path):\n",
        "    # Prime vertical radius of curvature: the radius of the east-west curve at a latitude.\n",
        "    # Snyder equation 4-20: N = a/W where a = equatorial_radius, W = geodetic_factor.\n",
        "    prime_vertical_radius = equatorial_radius / geodetic_factor\n",
        "    # Arc length = radius * angle in radians.\n",
        "    # Cosine accounts for lines of longitude converging towards poles.\n",
        "    # Snyder equation 4-21: L_lambda = a*cos(phi)/W for one radian, here multiplied by pixel width.\n",
        "    cell_size_x_array = degrees_to_radians * prime_vertical_radius * np.cos(latitude_radians) * pixel_width_degrees\n",
        "    export_array_as_tif(cell_size_x_array.astype(np.float64), cell_size_x_path, dtype=gdal.GDT_Float64)\n",
        "    print(f\"Raster with cell width in metres created: {cell_size_x_path}\")\n",
        "else: print(f\"Raster with cell width in metres already exists: {cell_size_x_path}\")\n",
        "\n",
        "# Cell height in metres. Varies slightly with latitude due to Earth's shape.\n",
        "cell_size_y_path = join(areas_dir, \"cell_size_y.tif\")\n",
        "if not exists(cell_size_y_path):\n",
        "    # Meridional radius of curvature: the radius of the north-south curve at a latitude.\n",
        "    # Snyder equations 4-18 and 4-19: R' = a(1-e**2)/W**3 where a = equatorial_radius, W = geodetic_factor.\n",
        "    meridional_radius = equatorial_radius * (1 - eccentricity_squared) / geodetic_factor**3\n",
        "    # Arc length = radius * angle in radians.\n",
        "    # Absolute value because pixel_height_degrees is negative.\n",
        "    cell_size_y_array = np.abs(degrees_to_radians * meridional_radius * pixel_height_degrees)\n",
        "    export_array_as_tif(cell_size_y_array.astype(np.float64), cell_size_y_path, dtype=gdal.GDT_Float64)\n",
        "    print(f\"Raster with cell height in metres created: {cell_size_y_path}\")\n",
        "else: print(f\"Raster with cell height in metres already exists: {cell_size_y_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction area"
      ],
      "metadata": {
        "id": "dhkElCCx_E3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the prediction area.\n",
        "# Smaller than the template area, as feature edge effects will have erroneous values.\n",
        "# Marginally (1 pixel) larger than the project area extent, ensuring all pixels\n",
        "# touching the project area are included.\n",
        "crs_epsg = 4326\n",
        "prediction_area_path = join(polygons_dir, \"prediction_area.gpkg\")\n",
        "if not exists(prediction_area_path):\n",
        "    # Obtain max pixel resolution to buffer polygons\n",
        "    cell_x_array = read_raster_as_array(cell_x_path)\n",
        "    cell_y_array = read_raster_as_array(cell_y_path)\n",
        "    buffer_distance_metres = np.round(max(np.max(cell_x_array), np.max(cell_y_array)))\n",
        "    cell_x = cell_x_array = cell_y = cell_y_array = None\n",
        "    # Suppress warning about not being a geographic CRS, as we account for this.\n",
        "    # However larger buffers or project areas near the poles might still need to be converted.\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "    # Get the centroid of the project polygon\n",
        "    project_polygon_centroid = project_area_read.centroid.values[0]\n",
        "    # Convert the buffer distance from meters to decimal degrees based on the location at the centroid\n",
        "    buffer_distance_degrees = buffer_distance_metres / (111320 * abs(math.cos(math.radians(project_polygon_centroid.y))))\n",
        "    # Buffer the polygon\n",
        "    project_area_buffered = project_area_read.buffer(buffer_distance_degrees)\n",
        "    # Create a bounding box polygon and save\n",
        "    prediction_area = box(*project_area_buffered.total_bounds)\n",
        "    gdf = gpd.GeoDataFrame(geometry=[prediction_area], crs=f\"EPSG:{crs_epsg}\")\n",
        "    gdf.to_file(prediction_area_path, driver='GPKG')\n",
        "    print(f\"Buffered the project area to {buffer_distance_metres}m and created a bounding box: {prediction_area_path}\")\n",
        "else: print(f\"Project area has already been buffered and bound to create a prediction area: {prediction_area_path}\")\n",
        "\n",
        "# Create an inverse project area path for masking\n",
        "inverse_prediction_area_path = join(polygons_dir, \"prediction_area_inverse.gpkg\")\n",
        "if not exists(inverse_prediction_area_path):\n",
        "  template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "  template_polygon = gpd.read_file(template_polygon_path)\n",
        "  prediction_area_polygon = gpd.read_file(prediction_area_path)\n",
        "  inverse_prediction_area_polygon = template_polygon.dissolve().geometry.iloc[0].difference(prediction_area_polygon.dissolve().geometry.iloc[0])\n",
        "  inverse_prediction_area_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_prediction_area_polygon]}, crs=f\"EPSG:{crs_epsg}\")\n",
        "  inverse_prediction_area_polygon_gdf.to_file(inverse_prediction_area_path, driver=\"GPKG\")\n",
        "  print(f\"An inverse prediction area polygon has been created: {inverse_prediction_area_path}\")\n",
        "else: print(f\"An inverse prediction area already exists at: {inverse_prediction_area_path}\")"
      ],
      "metadata": {
        "id": "8ursDzi4_EeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_RENY6twBXx"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h__20O9kwA9K"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}