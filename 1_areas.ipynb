{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/1_areas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUUhJgLFYI88"
      },
      "source": [
        "# Imports, directories and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr4kdJvHapxh"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljIpLF7_GwQb"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOQrLyVyW1Qa"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import geopandas as gpd\n",
        "import getpass\n",
        "from google.colab import runtime\n",
        "import ipywidgets as widgets\n",
        "import math\n",
        "import numpy as np\n",
        "from os import makedirs, remove\n",
        "from os.path import exists, join\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "import pandas as pd\n",
        "import requests\n",
        "from shapely.geometry import box\n",
        "from shutil import copy\n",
        "import zipfile\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8ZQtAdTl6vt"
      },
      "outputs": [],
      "source": [
        "# Define directories.\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "features_dir = join(base_dir, \"3_features\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "dem_dir = join(areas_dir, \"dem\")\n",
        "dem_tiles_dir = join(dem_dir, \"tiles\")\n",
        "\n",
        "# Create directories if they do not exist.\n",
        "makedirs(areas_dir, exist_ok=True)\n",
        "makedirs(polygons_dir, exist_ok=True)\n",
        "makedirs(dem_dir, exist_ok=True)\n",
        "makedirs(dem_tiles_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb2lqtUz4Yw2"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuNPGIEE2BI4"
      },
      "source": [
        "# Project area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xIGIHOL2DSq"
      },
      "outputs": [],
      "source": [
        "# Upload 'project_area.gpkg' polygon to the 1_areas/polygons directory.\n",
        "# This can be a polygon of any shape. A bounding box will be used to create the\n",
        "# GEDI download area in 1_variates.ipynb. # A buffered bounding box will be used\n",
        "# for the raster template, to ensure all feature edge effects are included.\n",
        "\n",
        "#Project CRS EPSG\n",
        "crs_epsg = 4326\n",
        "\n",
        "# Recommended to buffer at least 300 m to account for feature edge effects\n",
        "# and clipping imprecision\n",
        "buffer_distance_metres = 300\n",
        "\n",
        "project_area_path = join(polygons_dir, 'project_area.gpkg')\n",
        "\n",
        "if exists(project_area_path):\n",
        "  print(\"Project polygon found:\\n\")\n",
        "  # Read project polygon\n",
        "  project_area_read = gpd.read_file(join(polygons_dir, 'project_area.gpkg'))\n",
        "  display(project_area_read[\"geometry\"].iloc[0])\n",
        "  if project_area_read.crs.to_epsg() == crs_epsg:\n",
        "    project_area_path = join(polygons_dir, \"project_area.gpkg\")\n",
        "    project_area_buffered_bbox_path = join(polygons_dir, 'project_area_buffered_bbox.gpkg')\n",
        "    # Calculate the bounding box of the project polygon\n",
        "    if not exists (project_area_buffered_bbox_path):\n",
        "      # Suppress warning about not being a geographic CRS, as we account for this.\n",
        "      # However larger buffers or project areas near the poles might still need to be converted.\n",
        "      warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "      # Get the centroid of the project polygon\n",
        "      project_polygon_centroid = project_area_read.centroid.values[0]\n",
        "      # Convert the buffer distance from meters to decimal degrees based on the location at the centroid\n",
        "      buffer_distance_degrees = buffer_distance_metres / (111320 * abs(math.cos(math.radians(project_polygon_centroid.y))))\n",
        "      # Buffer the polygon\n",
        "      project_area_buffered = project_area_read.buffer(buffer_distance_degrees)\n",
        "      # Create a bounding box polygon and save\n",
        "      project_area_buffered_bbox = box(*project_area_buffered.total_bounds)\n",
        "      gdf = gpd.GeoDataFrame(geometry=[project_area_buffered_bbox], crs=f\"EPSG:{crs_epsg}\")\n",
        "      gdf.to_file(project_area_buffered_bbox_path, driver='GPKG')\n",
        "      print(f\"Buffered the project area to {buffer_distance_metres} and created a bounding box: {project_area_buffered_bbox_path}\")\n",
        "    else: print(f\"Project area has already been buffered and bound to a box: {project_area_buffered_bbox_path}\")\n",
        "    # Read the buffered project area bounding box\n",
        "    project_area_buffered_bbox_read = gpd.read_file(project_area_buffered_bbox_path)\n",
        "    bbox_bounds = project_area_buffered_bbox_read.total_bounds\n",
        "    project_x_min, project_x_max = bbox_bounds[0], bbox_bounds[2]\n",
        "    project_y_min, project_y_max = bbox_bounds[1], bbox_bounds[3]\n",
        "    print(f\"\\nThe buffered polygon bounding box has the coordinates:\\n{project_x_min}, {project_y_min} to {project_x_max}, {project_y_max}.\")\n",
        "  else: print(\"Reproject 'project_area.gpkg' to EPSG:4326.\")\n",
        "else: print(\"Create 'project_area.gpkg' and upload to 1_areas/polygons\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq2a_lWnP08i"
      },
      "source": [
        "# Download DEM tiles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Copernicus 'COP-DEM_GLO-30-DGED' DEM tiles for the project area.\n",
        "# https://dataspace.copernicus.eu/explore-data/data-collections/copernicus-contributing-missions/collections-description/COP-DEM\n",
        "# First register to get credentials for 'Copernicus Contributing Missions' in the Copernicus Data Space Ecosystem:\n",
        "# https://dataspace.copernicus.eu/explore-data/data-collections/copernicus-contributing-missions/ccm-how-to-register\n",
        "# Make sure to check the box \"I am also interested in accessing Copernicus Contributing Missions data\".\n",
        "\n",
        "# Read project area bbox and create WKT 'area of interest'\n",
        "project_area_buffered_bbox_path = join(polygons_dir, 'project_area_buffered_bbox.gpkg')\n",
        "project_area_buffered_bbox_read = gpd.read_file(project_area_buffered_bbox_path)\n",
        "bbox_bounds = project_area_buffered_bbox_read.total_bounds\n",
        "project_x_min, project_y_min, project_x_max, project_y_max = bbox_bounds\n",
        "aoi_wkt = f\"POLYGON(({project_x_min} {project_y_min}, {project_x_min} {project_y_max}, {project_x_max} {project_y_max}, {project_x_max} {project_y_min}, {project_x_min} {project_y_min}))\"\n",
        "\n",
        "# Prompt for credentials and obtain OAuth2 token.\n",
        "email = getpass.getpass(\"Enter Copernicus account email: \")\n",
        "password = getpass.getpass(\"Enter Copernicus account password: \")\n",
        "token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
        "data = {\"client_id\": \"cdse-public\", \"username\": email, \"password\": password, \"grant_type\": \"password\"}\n",
        "try:\n",
        "    token_response = requests.post(token_url, data=data)\n",
        "    token_response.raise_for_status()\n",
        "    access_token = token_response.json()[\"access_token\"]\n",
        "    print(\"Authentication successful. Access token obtained.\")\n",
        "except Exception as e:\n",
        "    print(\"Authentication failed:\", e)\n",
        "    raise\n",
        "\n",
        "# Query catalogue API for DEM products intersecting AOI, use local CSV cache if available\n",
        "data_collection = \"CCM\"\n",
        "catalog_url = \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products\"\n",
        "filter_query = f\"Collection/Name eq '{data_collection}' and OData.CSC.Intersects(area=geography'SRID=4326;{aoi_wkt}')\"\n",
        "catalog_api_url = (catalog_url\n",
        "    + f\"?$filter={filter_query}\"\n",
        "    + \"&$top=1000\")  # Increase limit to avoid missing tiles\n",
        "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
        "catalog_csv = join(dem_dir, \"dem_catalogue.csv\")\n",
        "\n",
        "if exists(catalog_csv):\n",
        "    df = pd.read_csv(catalog_csv)\n",
        "    print(f\"Loaded {len(df)} DEM products from local CSV.\")\n",
        "else:\n",
        "    all_products = []\n",
        "    next_url = catalog_api_url\n",
        "    page_count = 0  # Initialize page counter\n",
        "    while next_url:\n",
        "        cat_response = requests.get(next_url, headers=headers)\n",
        "        cat_response.raise_for_status()\n",
        "        cat_json = cat_response.json()\n",
        "        all_products.extend(cat_json[\"value\"])\n",
        "        page_count += 1  # Increment page counter\n",
        "        next_url = cat_json.get(\"@odata.nextLink\")\n",
        "    df = pd.DataFrame.from_dict(all_products)\n",
        "    print(f\"Found {len(df)} DEM products across {page_count} pages intersecting the project area.\")\n",
        "    df.to_csv(catalog_csv, index=False)\n",
        "\n",
        "# Filter DEM tiles by 'Name', sort by 'ModificationDate', then drop duplicates by Footprint\n",
        "df_filtered = df[df[\"Name\"].str.startswith(\"DEM1_SAR_DGE_30\")]\n",
        "print(f\"Found {len(df_filtered)} 'COP-DEM_GLO-30-DGED' DEM tiles.\")\n",
        "df_filtered = df_filtered.sort_values(\"ModificationDate\").copy()\n",
        "df_filtered_unique = df_filtered.drop_duplicates(subset=[\"GeoFootprint\"], keep=\"last\")\n",
        "print(f\"{len(df_filtered_unique)} unique footprints will be downloaded, prioritising the most recent.\")\n",
        "\n",
        "# Build list of product IDs for download.\n",
        "dem_tiles_id_list = df_filtered_unique[\"Id\"].tolist()\n",
        "if len(dem_tiles_id_list) == 0:\n",
        "    print(\"No DEM tiles found within project area bounds.\")\n",
        "\n",
        "# Download and extract the DEM .tif from each product with up to 3 attempts.\n",
        "index = 0\n",
        "progress_label = widgets.Label(value=f\"DEM tile download progress: {index}/{len(dem_tiles_id_list)}\")\n",
        "display(progress_label)\n",
        "for product_id in dem_tiles_id_list:\n",
        "    # Retrieve product row, build download URL and file paths.\n",
        "    row = df_filtered.loc[df_filtered[\"Id\"] == product_id].iloc[0]\n",
        "    download_url_base = \"https://download.dataspace.copernicus.eu/odata/v1/Products\"\n",
        "    product_url = f\"{download_url_base}({product_id})/$value\"\n",
        "    dem_tile_zip_filename = f'{row[\"Name\"]}.zip'\n",
        "    dem_tile_zip_path = join(dem_tiles_dir, dem_tile_zip_filename)\n",
        "    # Retry loop: download .zip and extract 'DEM.tif' directly into dem_tiles_dir.\n",
        "    extracted_tif_path = None\n",
        "    attempts = 0\n",
        "    while attempts < 3:\n",
        "        try:\n",
        "            expected_size = row.get('ContentLength', None)  # If available\n",
        "            if exists(dem_tile_zip_path):\n",
        "                if expected_size and os.path.getsize(dem_tile_zip_path) != expected_size:\n",
        "                    remove(dem_tile_zip_path)\n",
        "\n",
        "            if not exists(dem_tile_zip_path):\n",
        "                response = requests.get(product_url, headers=headers, allow_redirects=True)\n",
        "                response.raise_for_status()\n",
        "                with open(dem_tile_zip_path, 'wb') as f:\n",
        "                  f.write(response.content)\n",
        "            with zipfile.ZipFile(dem_tile_zip_path, 'r') as z:\n",
        "                tif_filename = next((f for f in z.namelist() if f.endswith(\"DEM.tif\")), None)\n",
        "                if tif_filename is None:\n",
        "                    raise Exception(\"DEM.tif not found in zip\")\n",
        "                extracted_tif_name = os.path.basename(tif_filename)\n",
        "                extracted_tif_path = join(dem_tiles_dir, extracted_tif_name)\n",
        "                with open(extracted_tif_path, 'wb') as out_file:\n",
        "                    out_file.write(z.read(tif_filename))\n",
        "            break  # Exit retry loop\n",
        "        except Exception as e:\n",
        "            attempts += 1\n",
        "            if exists(dem_tile_zip_path):\n",
        "                remove(dem_tile_zip_path)\n",
        "            if extracted_tif_path and exists(extracted_tif_path):\n",
        "                remove(extracted_tif_path)\n",
        "            if attempts < 3:\n",
        "                print(f\"Attempt {attempts} failed for ID: {product_id} - {e}. Retrying...\")\n",
        "            else:\n",
        "                print(f\"Failed ID: {product_id} after 3 attempts - {e}. Moving to next product.\")\n",
        "    index += 1\n",
        "    progress_label.value = f\"DEM tile download progress: {index}/{len(dem_tiles_id_list)}\""
      ],
      "metadata": {
        "id": "hYdGul9pJt24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9TuMSdehVVK"
      },
      "source": [
        "# Merge DEM tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyTQqNr_27Fw"
      },
      "outputs": [],
      "source": [
        "# Merge the DEM tiles into a single raster\n",
        "dem_merged_path = join(dem_dir, \"dem_merged.tif\")\n",
        "\n",
        "if not exists(dem_merged_path):\n",
        "  # List tiles\n",
        "  tiles_to_merge = []\n",
        "  for file in os.listdir(dem_tiles_dir):\n",
        "    if file.endswith(\".tif\"):\n",
        "      tiles_to_merge.append(join(dem_tiles_dir, file))\n",
        "  # Create a temporary virtual file (VRT) from the tiles\n",
        "  temp_vrt = join(dem_dir, 'temp.vrt')\n",
        "  gdal.BuildVRT(temp_vrt, tiles_to_merge)\n",
        "  # Merge the input files into a single GeoTIFF file\n",
        "  merge_options = gdal.TranslateOptions(format='GTiff', outputType=gdal.GDT_Float32, noData=nodatavalue,\n",
        "                                  creationOptions=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'])\n",
        "  gdal.Translate(dem_merged_path, temp_vrt, options=merge_options)\n",
        "  # Remove the temporary VRT file\n",
        "  os.remove(temp_vrt)\n",
        "  print(f\"The merged DEM raster has been saved to: {dem_merged_path}\")\n",
        "else: print(f\"A merged DEM raster already exists at: {dem_merged_path}\")\n",
        "\n",
        "# Clip the raster to project area extent\n",
        "dem_merged_clipped_path = join(dem_dir, \"dem_merged_clipped.tif\")\n",
        "\n",
        "if not exists(dem_merged_clipped_path):\n",
        "  # Read the buffered project area bounding box\n",
        "  project_area_buffered_bbox_path = join(polygons_dir, 'project_area_buffered_bbox.gpkg')\n",
        "  project_area_buffered_bbox_read = gpd.read_file(project_area_buffered_bbox_path)\n",
        "  bbox_bounds = project_area_buffered_bbox_read.total_bounds\n",
        "  # Get coordinates\n",
        "  project_x_min, project_x_max = bbox_bounds[0], bbox_bounds[2]\n",
        "  project_y_min, project_y_max = bbox_bounds[1], bbox_bounds[3]\n",
        "  project_coords = [project_x_min, project_y_max, project_x_max, project_y_min]\n",
        "  # Define Translate options\n",
        "  clip_options = gdal.TranslateOptions(projWin=[project_x_min, project_y_max, project_x_max, project_y_min],\n",
        "                                  outputType=gdal.GDT_Float32, noData=nodatavalue, creationOptions=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'])\n",
        "  # call gdal.Translate() with the new options argument\n",
        "  gdal.Translate(dem_merged_clipped_path, dem_merged_path, options=clip_options)\n",
        "  print(f\"The clipped, merged DEM raster has been saved to: {dem_merged_clipped_path}\")\n",
        "else: print(f\"A clipped merged DEM raster already exists at: {dem_merged_clipped_path}\")\n",
        "\n",
        "# Copy the clipped, merged DEM to '3_features' directory to use as the base DEM\n",
        "base_dem_dsm_path = join(areas_dir, \"base_dem_dsm.tif\")\n",
        "\n",
        "if not exists(base_dem_dsm_path):\n",
        "  copy(dem_merged_clipped_path, base_dem_dsm_path)\n",
        "  print(f\"The clipped, merged DEM has been copied for use as a base DEM: {base_dem_dsm_path}\")\n",
        "else: print(f\"A base DEM already exists at: {base_dem_dsm_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwICylX1t4mZ"
      },
      "source": [
        "# Create template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys_zk5eHpPkd"
      },
      "outputs": [],
      "source": [
        "# Create template from DEM\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "if not exists(template_tif_path):\n",
        "  dem_merged_clipped_path = join(dem_dir, \"dem_merged_clipped.tif\")\n",
        "  dem_merged_clipped_array = gdal.Open(dem_merged_clipped_path).ReadAsArray() # Convert DEM to array\n",
        "  template_array = np.ones_like(dem_merged_clipped_array) # Change all values to 1\n",
        "  export_array_as_tif(template_array, template_tif_path, template=dem_merged_clipped_path)\n",
        "  print(f\"A template raster has been created: {template_tif_path}\")\n",
        "else: print(f\"A template raster already exists at: {template_tif_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDHujjIH7Ftq"
      },
      "outputs": [],
      "source": [
        "# Create template polygon\n",
        "template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "if not exists(template_polygon_path):\n",
        "  # Get template raster spatial data\n",
        "  template_raster = gdal.Open(template_tif_path)\n",
        "  template_raster_band = template_raster.GetRasterBand(1)\n",
        "  spatial_ref = ogr.osr.SpatialReference()\n",
        "  spatial_ref.ImportFromWkt(template_raster.GetProjection())\n",
        "  # Polygonize template raster without fields or layer name\n",
        "  template_polygon_file = ogr.GetDriverByName(\"GPKG\").CreateDataSource(template_polygon_path)\n",
        "  template_polygon_layer = template_polygon_file.CreateLayer(\"\", srs=spatial_ref, geom_type=ogr.wkbPolygon)\n",
        "  gdal.Polygonize(template_raster_band, None, template_polygon_layer, -1)\n",
        "  print(f\"A template polygon has been created: {template_polygon_path}\")\n",
        "else: print(f\"A template polygon already exists at: {template_polygon_path}\")\n",
        "template_polygon_read = gpd.read_file(template_polygon_path)\n",
        "template_polygon_bounds = template_polygon_read.total_bounds\n",
        "print(f\"\\nThe template polygon has the coordinates:\\n{template_polygon_bounds[0]}, {template_polygon_bounds[1]} to {template_polygon_bounds[2]}, {template_polygon_bounds[3]}.\")\n",
        "\n",
        "# Create an inverse project area path for masking\n",
        "inverse_project_area_path = join(polygons_dir, \"project_area_inverse.gpkg\")\n",
        "if not exists(inverse_project_area_path):\n",
        "  template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "  template_polygon = gpd.read_file(template_polygon_path)\n",
        "  project_area_polygon = gpd.read_file(project_area_path)\n",
        "  inverse_project_area_polygon = template_polygon.unary_union.difference(project_area_polygon.unary_union)\n",
        "  inverse_project_area_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_project_area_polygon]}, crs=f\"EPSG:{crs_epsg}\")\n",
        "  inverse_project_area_polygon_gdf.to_file(inverse_project_area_path, driver=\"GPKG\")\n",
        "  print(f\"An inverse project area polygon has been created: {inverse_project_area_path}\")\n",
        "else: print(f\"An inverse project area already exists at: {inverse_project_area_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c367RQcutCrA"
      },
      "source": [
        "# Create measurement rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbZ03-E1vrfA"
      },
      "outputs": [],
      "source": [
        "# Create measurement rasters for precise area-based statistics\n",
        "\n",
        "# Define template\n",
        "template_path = join(areas_dir, \"template.tif\")\n",
        "template = gdal.Open(template_path)\n",
        "template_array = template.ReadAsArray()\n",
        "rows, cols = template_array.shape\n",
        "\n",
        "# WGS84 ellipsoid parameters\n",
        "equatorial_radius = 6_378_137.0\n",
        "inverse_flattening = 298.257223563\n",
        "polar_radius = equatorial_radius * (1 - 1 / inverse_flattening)\n",
        "\n",
        "# Vectorised function for latitude distance in metres from decimal degrees, at a specific latitude\n",
        "def distance_of_decimal_degrees_latitude(latitude_array: np.ndarray, decimal_degrees: float) -> np.ndarray:\n",
        "    # Calculate the eccentricity squared (e2)\n",
        "    e2 = (equatorial_radius**2 - polar_radius**2) / equatorial_radius**2\n",
        "    # Convert latitude to radians\n",
        "    latitude_rad = np.radians(latitude_array)\n",
        "    # Calculate the meridional radius of curvature (M)\n",
        "    M = equatorial_radius * (1 - e2) / (1 - e2 * np.sin(latitude_rad)**2)**(3/2)\n",
        "    # Calculate the distance of one degree of latitude\n",
        "    distance_per_degree = np.pi * M / 180\n",
        "    # Calculate the distance of the specified decimal degrees\n",
        "    return distance_per_degree * decimal_degrees\n",
        "\n",
        "# Vectorised function for longitude distance in metres from decimal degrees, at a specific latitude\n",
        "def distance_of_decimal_degrees_longitude(latitude_array: np.ndarray, decimal_degrees: float) -> np.ndarray:\n",
        "    # Calculate the eccentricity squared (e2)\n",
        "    e2 = (equatorial_radius**2 - polar_radius**2) / equatorial_radius**2\n",
        "    # Convert latitude to radians\n",
        "    latitude_rad = np.radians(latitude_array)\n",
        "    # Calculate the radius of curvature in the prime vertical (N)\n",
        "    N = equatorial_radius / np.sqrt(1 - e2 * np.sin(latitude_rad)**2)\n",
        "    # Calculate the distance of one degree of longitude at the given latitude\n",
        "    distance_per_degree = np.pi * N * np.cos(latitude_rad) / 180\n",
        "    # Calculate the distance of the specified decimal degrees\n",
        "    return distance_per_degree * decimal_degrees\n",
        "\n",
        "geotransform = template.GetGeoTransform()\n",
        "\n",
        "# Create pixel coordinate grids\n",
        "col_grid, row_grid = np.meshgrid(np.arange(cols, dtype=np.float64), np.arange(rows, dtype=np.float64))\n",
        "\n",
        "# Create a raster for the longitude in decimal degrees at the centre of each pixel\n",
        "longitude_path = join(areas_dir, \"longitude.tif\")\n",
        "if not exists(longitude_path):\n",
        "    # Affine transform: X = gt[0] + col*gt[1] + row*gt[2], with 0.5 pixel offset for centre\n",
        "    longitude_array = geotransform[0] + (col_grid + 0.5) * geotransform[1] + (row_grid + 0.5) * geotransform[2]\n",
        "    # Handle antimeridian wrapping\n",
        "    longitude_array = (longitude_array + 180) % 360 - 180\n",
        "    export_array_as_tif(longitude_array.astype(np.float64), longitude_path, dtype=gdal.GDT_Float64)\n",
        "    print(f\"Raster with cell longitude in decimal degrees created: {longitude_path}\")\n",
        "else: print(f\"Raster with cell longitude in decimal degrees already exists: {longitude_path}\")\n",
        "\n",
        "# Create a raster for the latitude in decimal degrees at the centre of each pixel\n",
        "latitude_path = join(areas_dir, \"latitude.tif\")\n",
        "if not exists(latitude_path):\n",
        "    # Affine transform: Y = gt[3] + col*gt[4] + row*gt[5], with 0.5 pixel offset for centre\n",
        "    latitude_array = geotransform[3] + (col_grid + 0.5) * geotransform[4] + (row_grid + 0.5) * geotransform[5]\n",
        "    # Clamp latitude to valid range\n",
        "    latitude_array = np.clip(latitude_array, -90, 90)\n",
        "    export_array_as_tif(latitude_array.astype(np.float64), latitude_path, dtype=gdal.GDT_Float64)\n",
        "    print(f\"Raster with cell latitude in decimal degrees created: {latitude_path}\")\n",
        "else: print(f\"Raster with cell latitude in decimal degrees already exists: {latitude_path}\")\n",
        "latitude_ds = gdal.Open(latitude_path)\n",
        "latitude_array = latitude_ds.ReadAsArray().astype(np.float64)\n",
        "latitude_ds = None\n",
        "\n",
        "# Compute pixel direction vectors in metres (accounts for rotation via gt[2] and gt[4])\n",
        "# Column direction vector: displacement when moving one pixel in the column direction\n",
        "col_dx_m = distance_of_decimal_degrees_longitude(latitude_array, geotransform[1])\n",
        "col_dy_m = distance_of_decimal_degrees_latitude(latitude_array, geotransform[4])\n",
        "# Row direction vector: displacement when moving one pixel in the row direction\n",
        "row_dx_m = distance_of_decimal_degrees_longitude(latitude_array, geotransform[2])\n",
        "row_dy_m = distance_of_decimal_degrees_latitude(latitude_array, geotransform[5])\n",
        "\n",
        "# Create a raster for the cell width in metres\n",
        "cell_size_x_path = join(areas_dir, \"cell_size_x.tif\")\n",
        "if not exists(cell_size_x_path):\n",
        "    # Magnitude of column direction vector\n",
        "    cell_size_x_array = np.sqrt(col_dx_m**2 + col_dy_m**2)\n",
        "    export_array_as_tif(cell_size_x_array.astype(np.float64), cell_size_x_path, dtype=gdal.GDT_Float64)\n",
        "    print(f\"Raster with cell width in metres created: {cell_size_x_path}\")\n",
        "else: print(f\"Raster with cell width in metres already exists: {cell_size_x_path}\")\n",
        "\n",
        "# Create a raster for the cell height in metres\n",
        "cell_size_y_path = join(areas_dir, \"cell_size_y.tif\")\n",
        "if not exists(cell_size_y_path):\n",
        "    # Magnitude of row direction vector\n",
        "    cell_size_y_array = np.sqrt(row_dx_m**2 + row_dy_m**2)\n",
        "    export_array_as_tif(cell_size_y_array.astype(np.float64), cell_size_y_path, dtype=gdal.GDT_Float64)\n",
        "    print(f\"Raster with cell height in metres created: {cell_size_y_path}\")\n",
        "else: print(f\"Raster with cell height in metres already exists: {cell_size_y_path}\")\n",
        "\n",
        "# Create a raster for the cell area in square metres\n",
        "cell_area_path = join(areas_dir, \"cell_area.tif\")\n",
        "if not exists(cell_area_path):\n",
        "    # Cross product magnitude for parallelogram area\n",
        "    cell_area_array = np.abs(col_dx_m * row_dy_m - col_dy_m * row_dx_m)\n",
        "    export_array_as_tif(cell_area_array.astype(np.float64), cell_area_path, dtype=gdal.GDT_Float64)\n",
        "    print(f\"Raster with cell area in square metres created: {cell_area_path}\")\n",
        "else: print(f\"Raster with cell area in square metres already exists: {cell_area_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_RENY6twBXx"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h__20O9kwA9K"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V6E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}