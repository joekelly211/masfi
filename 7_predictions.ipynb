{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/dev/7_predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhx977-WIaWJ"
      },
      "source": [
        "# Imports and directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRnNty96IThZ"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi_asartr\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi_asartr'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4n2MGF0aZtJ"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!pip install tensorflow\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpmgsb8KKMGN"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "import json\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import ipywidgets as widgets\n",
        "import itertools\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy.random import normal\n",
        "from os import makedirs\n",
        "from os.path import exists, join\n",
        "from osgeo import gdal\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "import scipy.stats as st\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import xgboost as xgb\n",
        "\n",
        "# # Define GPU\n",
        "# import tensorflow as tf\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   print('GPU device not found.')\n",
        "# else:\n",
        "#   print(f\"Found GPU at: {device_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQwNO9T0_HPH"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "feature_dir = join(base_dir, \"3_features\")\n",
        "feature_final_dir = join(feature_dir, \"final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "masks_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "predictions_dir = join(base_dir, \"7_predictions\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(predictions_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koHn-AiGG7KM"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)\n",
        "\n",
        "# Burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster, polygon, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "  with rasterio.open(raster, 'r+') as src:\n",
        "      array = src.read(1)\n",
        "      transform = src.transform\n",
        "      gdf = gpd.read_file(polygon)\n",
        "      for geom in gdf.geometry:\n",
        "          if not fixed and column_name == None:\n",
        "              column_name = gdf.columns[0]\n",
        "          if not fixed: burn_value = gdf.loc[gdf.geometry == geom, column_name].values[0]\n",
        "          else: burn_value = fixed_value\n",
        "          rasterize([(geom, burn_value)], out=array, transform=transform,\n",
        "              all_touched=all_touched, dtype=src.meta['dtype'], out_shape=src.shape)\n",
        "      src.write(array, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lqjsLKZaQXo"
      },
      "source": [
        "# Select model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SqkKHKnaV-8"
      },
      "outputs": [],
      "source": [
        "# Select a baseline model, tested and trained in advance.\n",
        "# The target must have an uncertainty metric - otherwise\n",
        "# skip to the next notebook '8_statistics' and use the outputs\n",
        "# of the '6_scenarios' notebook.\n",
        "model_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file == 'model.json':\n",
        "      print(f'selected_model = \"{subdir.split(f\"{models_dir}/\",1)[1]}\"')\n",
        "      model_exists = True\n",
        "if not model_exists:\n",
        "  print(\"No model exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnerzp-gbapN"
      },
      "outputs": [],
      "source": [
        "selected_model = \"agbd_historic_250429_223033\"\n",
        "categorise_target = False # If the target was categorised in 5_models\n",
        "\n",
        "# Define model directories\n",
        "selected_model_dir = join(models_dir,selected_model)\n",
        "selected_model_json = join(selected_model_dir, \"model.json\")\n",
        "selected_model_descr_dir = join(selected_model_dir, \"model_description.json\")\n",
        "selected_model_dataset_path = join(selected_model_dir, f\"{selected_model}.pkl\")\n",
        "selected_model_dataset = pd.read_pickle(selected_model_dataset_path)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(selected_model_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_target = model_dataset_description[\"selected_target\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "covariates_categorised = model_dataset_description[\"covariates_categorised\"]\n",
        "selected_features = model_dataset_description[\"selected_features\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "filter_parameter = model_dataset_description[\"filter_parameter\"]\n",
        "filter_values_to_include = model_dataset_description[\"filter_values_to_include\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "# Reload hyperparameters\n",
        "with open(selected_model_descr_dir) as model_description_json:\n",
        "  model_description = json.load(model_description_json)\n",
        "final_hyperparameters = eval(model_description[\"hyperparameters\"])\n",
        "\n",
        "# Remove early stopping and replace with mean n_estimators\n",
        "if \"early_stopping_rounds\" in final_hyperparameters:\n",
        "  final_hyperparameters = {k:v for k, v in final_hyperparameters.items() if k != \"early_stopping_rounds\"}\n",
        "  final_hyperparameters[\"n_estimators\"] = round(model_description[\"n_estimators mean\"])\n",
        "\n",
        "# Define directories\n",
        "scenarios_model_dir = join(scenarios_dir,selected_model)\n",
        "uncertainty_selected_model_dir = join(predictions_dir, selected_model)\n",
        "model_iterations_dir = join(uncertainty_selected_model_dir, \"model_iterations\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(uncertainty_selected_model_dir, exist_ok=True)\n",
        "makedirs(model_iterations_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9-hQ7G37uZi"
      },
      "source": [
        "# Model iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWymSi4ndKUA"
      },
      "outputs": [],
      "source": [
        "# Verify that the target is equal to the mean\n",
        "print(f'mean = \"{selected_target}\"')\n",
        "\n",
        "# Calculate se from columns flagged 'uncertainty'\n",
        "if len(uncertainty)==0:\n",
        "  print(\"There are no flagged uncertainty columns to calculate SE from.\")\n",
        "  print(\"Manually create the metric from the available columns.\")\n",
        "  for col in selected_model_dataset.columns:\n",
        "    print(f\"{col}\")\n",
        "else:\n",
        "  for col in selected_model_dataset.columns:\n",
        "    if col in uncertainty and col not in selected_target:\n",
        "      print(f'se = \"{col}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBalj00xwmRX"
      },
      "outputs": [],
      "source": [
        "mean = \"tar_agbd\"\n",
        "se = \"tar_agbd_se\"\n",
        "# Liang et al. (2023) use SE as a proxy for STDEV\n",
        "\n",
        "# Set model iterations\n",
        "model_iterations = 100\n",
        "\n",
        "# Define model (y axis changes for each iteration based on mean and se arrays)\n",
        "model_dataset_x = selected_model_dataset[selected_features]\n",
        "mean_array = selected_model_dataset[mean].values\n",
        "se_array = selected_model_dataset[se].values\n",
        "if categorise_target: XGBPredictor = xgb.XGBClassifier(**final_hyperparameters)\n",
        "else: XGBPredictor = xgb.XGBRegressor(**final_hyperparameters)\n",
        "model_params = XGBPredictor.get_params()\n",
        "model_params['eval_metric'] = model_description['metric_used_for_training']\n",
        "# Default fix for new XGBoost version\n",
        "[model_params.pop(key, None) for key in ['n_estimators', 'enable_categorical', 'missing']]\n",
        "\n",
        "# Progress label\n",
        "model_progress_index = 0\n",
        "model_progress_label = widgets.Label(f\"Model iteration: {model_progress_index}/{model_iterations}\")\n",
        "display(model_progress_label)\n",
        "\n",
        "for model_iteration in range(1,model_iterations+1):\n",
        "  # Set model iteration filename and check if already exists\n",
        "  model_iteration_filename = f\"model_iteration_{model_iteration}.json\"\n",
        "  model_iteration_path = join(model_iterations_dir, model_iteration_filename)\n",
        "  # If model iteration does not exist...\n",
        "  if not exists(model_iteration_path):\n",
        "    # Set the random seed based on iteration for replicability\n",
        "    np.random.seed(model_iteration)\n",
        "    # Set a normal distribution sample as the y for this iteration\n",
        "    model_dataset_y = normal(mean_array, se_array)\n",
        "    # Create DMatrix objects\n",
        "    model_dtrain = xgb.DMatrix(model_dataset_x, model_dataset_y, enable_categorical=True)\n",
        "    # Train the model iteration using the tested hyperparameters\n",
        "    model = xgb.train(model_params,\n",
        "                        model_dtrain,\n",
        "                        num_boost_round=final_hyperparameters['n_estimators'],\n",
        "                        verbose_eval=True)\n",
        "    # Save the model iteration\n",
        "    model.save_model(model_iteration_path)\n",
        "  # Update progress\n",
        "  model_progress_index += 1\n",
        "  model_progress_label.value = f\"Model iteration: {model_progress_index}/{model_iterations}\"\n",
        "print(\"All model iterations have been trained and saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1UxS-kO8_1d"
      },
      "source": [
        "# Scenario iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UL55gK2y6Ye"
      },
      "outputs": [],
      "source": [
        "# Scenarios must be designed and tested using 06_scenarios first.\n",
        "# Features in the 'constant' and scenario subdirs should not be moved.\n",
        "\n",
        "# Select a scenario area\n",
        "scenario_area_exists = False\n",
        "for subdir in os.listdir(scenarios_model_dir):\n",
        "  if not subdir.endswith('.json') and not subdir.endswith('.csv'):\n",
        "    print(f'selected_scenario_area = \"{subdir}\"')\n",
        "    scenario_area_exists = True\n",
        "if not scenario_area_exists:\n",
        "  print(f\"Create a scenario area directory in {scenarios_model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYxqe0eeKmKi"
      },
      "outputs": [],
      "source": [
        "selected_scenario_area = \"asartr\"\n",
        "\n",
        "# Locate scenario area directories\n",
        "selected_scenario_area_dir = join(scenarios_model_dir, selected_scenario_area)\n",
        "features_dir = join(selected_scenario_area_dir, \"features\")\n",
        "tile_templates_dir = join(selected_scenario_area_dir, 'tile_templates')\n",
        "tile_feature_stacks_dir = join(selected_scenario_area_dir, 'tile_feature_stacks')\n",
        "\n",
        "# Define uncertainty scenario area directory\n",
        "uncertainty_scenario_area_dir = join(uncertainty_selected_model_dir,f\"scenarios_{selected_scenario_area}\")\n",
        "tile_prediction_cache_dir = join(uncertainty_scenario_area_dir, \"tile_prediction_cache\")\n",
        "makedirs(uncertainty_scenario_area_dir, exist_ok=True)\n",
        "makedirs(tile_prediction_cache_dir, exist_ok=True)\n",
        "\n",
        "# Collect available scenarios from the feature stack tiles directory\n",
        "scenario_stacks_list = []\n",
        "for scenario in os.listdir(tile_feature_stacks_dir):\n",
        "    scenario_stacks_list.append(scenario)\n",
        "\n",
        "# Select scenarios to predict\n",
        "print(\"scenarios_to_predict = [\")\n",
        "for scenario in sorted(scenario_stacks_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE-bIxq9F3Vy"
      },
      "outputs": [],
      "source": [
        "scenarios_to_predict = [\n",
        "  \"1990_oldgrowth_1\",\n",
        "  \"1990_oldgrowth_2\",\n",
        "  \"2014\",\n",
        "  \"2014_no_degradation_since_1991\",\n",
        "  \"2014_oldgrowth_1\",\n",
        "  \"2014_oldgrowth_2\",\n",
        "  \"2015\",\n",
        "  \"2016\",\n",
        "  \"2017\",\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_oldgrowth_1\",\n",
        "  \"2021_oldgrowth_2\",\n",
        "  \"2022\",\n",
        "  \"2022_alternate_degradation_2021\",\n",
        "  \"2022_no_degradation_since_2022\",\n",
        "  \"2022_oldgrowth_1\",\n",
        "  \"2022_oldgrowth_2\",\n",
        "  \"2023\",\n",
        "  \"2023_alternate_degradation_2022\",\n",
        "  \"2023_no_degradation_since_2023\",\n",
        "  \"2023_oldgrowth_1\",\n",
        "  \"2023_oldgrowth_2\",\n",
        "  \"2024\",\n",
        "  \"2024_alternate_degradation_2014\",\n",
        "  \"2024_alternate_degradation_2021\",\n",
        "  \"2024_alternate_degradation_2022\",\n",
        "  \"2024_alternate_degradation_2023\",\n",
        "  \"2024_no_degradation_since_2000\",\n",
        "  \"2024_no_degradation_since_2015\",\n",
        "  \"2024_no_degradation_since_2022\",\n",
        "  \"2024_no_degradation_since_2023\",\n",
        "  \"2024_no_degradation_since_2024\",\n",
        "  \"2024_oldgrowth_1\",\n",
        "  \"2024_oldgrowth_2\",\n",
        "  \"all_oldgrowth_1\",\n",
        "  \"all_oldgrowth_2\",\n",
        "]\n",
        "\n",
        "# Check the number of model iterations available\n",
        "model_iterations_available = len(os.listdir(model_iterations_dir))\n",
        "print(f\"\\nThere are {len(os.listdir(model_iterations_dir))} model iterations available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqP1L8LtFcph"
      },
      "outputs": [],
      "source": [
        "# Set the number of scenario iterations. It must be <= the number of model iterations available.\n",
        "scenario_iterations = 100\n",
        "\n",
        "assert scenario_iterations <= model_iterations_available, f\"Reduce the number of scenario iterations to <= {model_iterations_available}.\"\n",
        "\n",
        "# Change this and the code within the block accordingly.\n",
        "add_covariates = True # Adds a selected covariate value as the feature\n",
        "sensitivity_value = 0.99\n",
        "beam_value = 5\n",
        "\n",
        "# # Check for GPU\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0': print('GPU device not found')\n",
        "# else: print(f\"Found GPU at: {device_name}\")\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "if n_tiles < 1: print(\"There are currently no template tiles.\")\n",
        "if n_tiles >= 1:\n",
        "  template_tile_x = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).XSize\n",
        "  print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "# Tile progress\n",
        "if n_tiles > 1:\n",
        "  tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "  display(tile_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_predict:\n",
        "  scenario_feature_stack_dir = join(tile_feature_stacks_dir, scenario)\n",
        "  # Create scenario iterations directory\n",
        "  scenario_iterations_dir = join(uncertainty_scenario_area_dir,f\"{scenario}_iterations\")\n",
        "  makedirs(scenario_iterations_dir, exist_ok=True)\n",
        "  # Iteration progress\n",
        "  iteration_progress_index = 0\n",
        "  iteration_progress_label = widgets.Label(f\"{scenario} iteration progress: {iteration_progress_index} / {scenario_iterations}\")\n",
        "  display(iteration_progress_label)\n",
        "\n",
        "  # Check if all scenario iterations already exist, if not then load feature stack\n",
        "  scenario_iteration_list = []\n",
        "  for model_iteration in range(1,scenario_iterations+1):\n",
        "    prediction_iteration_filename = f\"{scenario}__{selected_scenario_area}_{selected_model}_unmasked_iteration_{model_iteration}.tif\"\n",
        "    prediction_iteration_path = join(scenario_iterations_dir, prediction_iteration_filename)\n",
        "    scenario_iteration_list.append(prediction_iteration_path)\n",
        "  all_scenario_iterations_exist = True\n",
        "  for scenario_iteration in scenario_iteration_list:\n",
        "    if not exists(scenario_iteration): all_scenario_iterations_exist = False\n",
        "  if not all_scenario_iterations_exist:\n",
        "    if n_tiles == 1:\n",
        "    # Load template parameters\n",
        "      template_tile_dir = join(tile_templates_dir, f\"template_tile_1.tif\")\n",
        "      template_tile_y = gdal.Open(template_tile_dir).GetRasterBand(1).YSize\n",
        "      template_tile_x = gdal.Open(template_tile_dir).GetRasterBand(1).XSize\n",
        "      # Load feature stack\n",
        "      stack_filename = f\"feature_stack_{scenario}_1.npy\"\n",
        "      feature_stack = np.load(join(scenario_feature_stack_dir, stack_filename))\n",
        "      if add_covariates: feature_stack = np.hstack((feature_stack,\n",
        "                    np.full((feature_stack.shape[0], 1), beam_value, dtype=int),\n",
        "                    np.full((feature_stack.shape[0], 1), sensitivity_value, dtype=float)\n",
        "                    ))\n",
        "    # Predict scenario for each model iteration\n",
        "    for model_iteration in range(1,scenario_iterations+1):\n",
        "      # Define the model\n",
        "      model_dir = join(model_iterations_dir,f\"model_iteration_{model_iteration}.json\")\n",
        "      prediction_iteration_filename = f\"{scenario}__{selected_scenario_area}_{selected_model}_unmasked_iteration_{model_iteration}.tif\"\n",
        "      prediction_iteration_path = join(scenario_iterations_dir, prediction_iteration_filename)\n",
        "      # If scenario iteration does not exist:\n",
        "      if not exists(prediction_iteration_path):\n",
        "        # Load model\n",
        "        booster = xgb.Booster()\n",
        "        booster.load_model(model_dir)\n",
        "        if categorise_target: XGBPredictor = xgb.XGBClassifier()\n",
        "        else: XGBPredictor = xgb.XGBRegressor()\n",
        "        XGBPredictor._Booster = booster\n",
        "        # Avoids issues using dataframe from CPU\n",
        "        xgb.set_config(verbosity=0, use_rmm=True)\n",
        "        # Get number of stacks\n",
        "        n_stacks = len(os.listdir(scenario_feature_stack_dir))\n",
        "        if n_stacks == 1:\n",
        "          # Define prediction array and reshape\n",
        "          prediction = XGBPredictor.predict(feature_stack)\n",
        "          prediction_array = prediction.reshape((template_tile_y, template_tile_x))\n",
        "          prediction = None # Flush prediction\n",
        "\n",
        "        # Tiling for if feature stacks and separated into chunks\n",
        "        if n_stacks > 1:\n",
        "          # Create a tile cache directory for the prediction\n",
        "          tile_cache_iteration_dir = join(tile_prediction_cache_dir, prediction_iteration_filename[:-4])\n",
        "          makedirs(tile_cache_iteration_dir, exist_ok=True)\n",
        "          # Create a tile count to match the feature stack chunk\n",
        "          for stack in range(1, n_stacks+1):\n",
        "            iteration_tile_filename = f\"scenario_tile_{stack}.tif\"\n",
        "            # Check if tile already exists\n",
        "            scenario_tile_exists = False\n",
        "            for scenario_tile in os.listdir(tile_cache_iteration_dir):\n",
        "              if scenario_tile == iteration_tile_filename: scenario_tile_exists=True\n",
        "            # If scenario prediction tile does not exist:\n",
        "            if scenario_tile_exists == False:\n",
        "              # Load template tile parameters\n",
        "              template_tile_dir = join(tile_templates_dir, f\"template_tile_{stack}.tif\")\n",
        "              template_tile_y = gdal.Open(template_tile_dir).GetRasterBand(1).YSize\n",
        "              template_tile_x = gdal.Open(template_tile_dir).GetRasterBand(1).XSize\n",
        "              # Load feature tile stack\n",
        "              stack_filename = f\"feature_stack_{scenario}_{stack}.npy\"\n",
        "              feature_stack = np.load(join(scenario_feature_stack_dir, stack_filename))\n",
        "              # Add covariates (sensitivity and BEAM)\n",
        "              if add_covariates: feature_stack = np.hstack((feature_stack,\n",
        "                                np.full((feature_stack.shape[0], 1), beam_value, dtype=int),\n",
        "                                np.full((feature_stack.shape[0], 1), sensitivity_value, dtype=float)\n",
        "                                ))\n",
        "              # Define prediction array and reshape\n",
        "              prediction = XGBPredictor.predict(feature_stack)\n",
        "              feature_stack = None # Flush feature stack\n",
        "              prediction_tile = prediction.reshape((template_tile_y, template_tile_x))\n",
        "              prediction = None # Flush prediction\n",
        "              # Export prediction array as .tif\n",
        "              export_array_as_tif(prediction_tile, join(tile_cache_iteration_dir, iteration_tile_filename), template = template_tile_dir, compress = False)\n",
        "              prediction_tile = None # Flush prediction tile\n",
        "              # Update progress\n",
        "            tile_progress_index += 1\n",
        "            tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_stacks}\"\n",
        "          # Prepare empty array for merging tiles\n",
        "          prediction_array = np.empty((0,template_tile_x))\n",
        "          # Read each tile .tif as an array, stack, then export as a .tif\n",
        "          for subdir in os.listdir(tile_cache_iteration_dir):\n",
        "            if subdir.endswith('.tif'):\n",
        "              tile_dir = join(tile_cache_iteration_dir, subdir)\n",
        "              prediction_array = np.vstack((prediction_array, gdal.Open(tile_dir).ReadAsArray()))\n",
        "          # Delete scenario tile cache directory and reset index\n",
        "          shutil.rmtree(tile_cache_iteration_dir)\n",
        "          tile_progress_index = 0\n",
        "          tile_progress_label.value = f\"Tile progress: 0 / {n_tiles}\"\n",
        "\n",
        "        # Define scenario template\n",
        "        scenario_template = join(features_dir, os.listdir(features_dir)[0])\n",
        "        export_array_as_tif(prediction_array, prediction_iteration_path, template = scenario_template, compress = True)\n",
        "\n",
        "      iteration_progress_index += 1\n",
        "      iteration_progress_label.value = f\"{scenario} iteration progress: {iteration_progress_index} / {scenario_iterations}\"\n",
        "  else:\n",
        "    iteration_progress_label.value = f\"{scenario} iteration progress: 100 / {scenario_iterations}\"\n",
        "print(\"\\nScenario iterations complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIWo8ATZs-Ny"
      },
      "source": [
        "# Scenario statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG3PBLYgbUsN"
      },
      "outputs": [],
      "source": [
        "# Select a scenario iterations area\n",
        "scenario_iterations_area_exists = False\n",
        "for subdir in os.listdir(uncertainty_selected_model_dir):\n",
        "  if subdir != \"model_iterations\":\n",
        "    print(f'selected_scenario_iterations_area = \"{subdir[10:]}\"')\n",
        "    scenario_iterations_area_exists = True\n",
        "if not scenario_iterations_area_exists:\n",
        "  print(f\"Run the scenario iterations section first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZTJoAmSLLCM"
      },
      "outputs": [],
      "source": [
        "selected_scenario_iterations_area = \"asartr\"\n",
        "\n",
        "uncertainty_scenario_area_dir = join(uncertainty_selected_model_dir,f\"scenarios_{selected_scenario_iterations_area}\")\n",
        "statistics_unmasked_dir = join(uncertainty_scenario_area_dir,\"statistics_unmasked\")\n",
        "statistics_masked_dir = join(uncertainty_scenario_area_dir,\"statistics_masked\")\n",
        "makedirs(statistics_unmasked_dir, exist_ok=True)\n",
        "makedirs(statistics_masked_dir, exist_ok=True)\n",
        "\n",
        "# Collect scenarios with iterations\n",
        "scenarios_iterations_list = []\n",
        "for subdir in os.listdir(uncertainty_scenario_area_dir):\n",
        "  if subdir.endswith(\"_iterations\"):\n",
        "    scenarios_iterations_list.append(subdir[:-11])\n",
        "# Select scenarios to calculate mean and standard deviation\n",
        "print(\"scenarios_to_calculate = [\")\n",
        "for scenario in sorted(scenarios_iterations_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B0Por9lPQlA"
      },
      "outputs": [],
      "source": [
        "scenarios_to_calculate = [\n",
        "  \"1990_oldgrowth_1\",\n",
        "  \"1990_oldgrowth_2\",\n",
        "  \"2014\",\n",
        "  \"2014_no_degradation_since_1991\",\n",
        "  \"2014_oldgrowth_1\",\n",
        "  \"2014_oldgrowth_2\",\n",
        "  \"2015\",\n",
        "  \"2016\",\n",
        "  \"2017\",\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_oldgrowth_1\",\n",
        "  \"2021_oldgrowth_2\",\n",
        "  \"2022\",\n",
        "  \"2022_alternate_degradation_2021\",\n",
        "  \"2022_no_degradation_since_2022\",\n",
        "  \"2022_oldgrowth_1\",\n",
        "  \"2022_oldgrowth_2\",\n",
        "  \"2023\",\n",
        "  \"2023_alternate_degradation_2022\",\n",
        "  \"2023_no_degradation_since_2023\",\n",
        "  \"2023_oldgrowth_1\",\n",
        "  \"2023_oldgrowth_2\",\n",
        "  \"2024\",\n",
        "  \"2024_no_degradation_since_2000\",\n",
        "  \"2024_alternate_degradation_2014\",\n",
        "  \"2024_alternate_degradation_2021\",\n",
        "  \"2024_alternate_degradation_2022\",\n",
        "  \"2024_alternate_degradation_2023\",\n",
        "  \"2024_no_degradation_since_2015\",\n",
        "  \"2024_no_degradation_since_2022\",\n",
        "  \"2024_no_degradation_since_2023\",\n",
        "  \"2024_no_degradation_since_2024\",\n",
        "  \"2024_oldgrowth_1\",\n",
        "  \"2024_oldgrowth_2\",\n",
        "  \"all_oldgrowth_1\",\n",
        "  \"all_oldgrowth_2\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IFiOCz6mXaXm"
      },
      "outputs": [],
      "source": [
        "# Check iteration number\n",
        "\n",
        "# Exact '0' pixels without decimals can be an indicator that an iteration was incorrectly generated.\n",
        "# These must be deleted and predicted again to avoid incorrect statistics.\n",
        "check_problems = True\n",
        "\n",
        "# Exact '0' pixels might have been genuinely predicted (though unlikely in a regression),\n",
        "# In which case set fix_problems to True and add these iterations to problem_arrays list.\n",
        "# The code below will add 0.001 so they won't trigger the problem checker again.\n",
        "fix_problems = False\n",
        "problem_rasters = [\n",
        "\n",
        "]\n",
        "\n",
        "if len(problem_rasters) > 0:\n",
        "  for problem_raster in problem_rasters:\n",
        "    problem_raster_path = join(uncertainty_scenario_area_dir, f\"{scenario}_iterations\", problem_raster)\n",
        "    problem_raster_array = gdal.Open(problem_raster_path).ReadAsArray()\n",
        "    problem_raster_array[problem_raster_array == 0] = 0.001\n",
        "    export_array_as_tif(problem_raster_array, problem_raster_path, template = problem_raster_path, compress=True)\n",
        "\n",
        "# Check the number of prediction iterations\n",
        "for scenario in scenarios_to_calculate:\n",
        "  scenario_iterations_dir = join(uncertainty_scenario_area_dir,f\"{scenario}_iterations\")\n",
        "  iterations = 0\n",
        "  for subdir in os.listdir(scenario_iterations_dir):\n",
        "    if subdir.endswith(\".tif\"):\n",
        "      # Check whether the prediction iteration is valid\n",
        "      if check_problems:\n",
        "        iteration = join(scenario_iterations_dir,subdir)\n",
        "        iteration_array = gdal.Open(iteration).ReadAsArray()\n",
        "        assert np.count_nonzero(iteration_array==0) == 0, f\"{subdir} contains 0 values, so the iteration may not have predicted correctly.\\n Check the file, delete and repredict if necessary.\\n If they are valid 0 values, run the cell below on:\\n {iteration}.\"\n",
        "      iterations += 1\n",
        "  print(f\"There are {iterations} prediction iterations for scenario {scenario} statistics.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ekgu_-LLOY_"
      },
      "outputs": [],
      "source": [
        "confidence_interval = 0.95\n",
        "\n",
        "# Statistics progress\n",
        "stats_progress_index = 0\n",
        "stats_progress_label = widgets.Label(f\"Scenario stats progress: {stats_progress_index}/{len(scenarios_to_calculate)}\")\n",
        "display(stats_progress_label)\n",
        "\n",
        "for scenario in scenarios_to_calculate:\n",
        "    stat_base_filename = f\"{scenario}__{selected_scenario_iterations_area}_{selected_model}\"\n",
        "    scenario_iterations_dir = join(uncertainty_scenario_area_dir,f\"{scenario}_iterations\")\n",
        "\n",
        "    # Define statistics raster directories\n",
        "    stat_mean_filename = f\"mean__{stat_base_filename}_unmasked.tif\"\n",
        "    stat_mean_dir = join(statistics_unmasked_dir,stat_mean_filename)\n",
        "    stat_uncertainty_filename = f\"uncertainty__{stat_base_filename}_unmasked.tif\"\n",
        "    stat_predictions_dir = join(statistics_unmasked_dir,stat_uncertainty_filename)\n",
        "\n",
        "    # Check whether statistics rasters already exist\n",
        "    stat_mean_tif_exists, stat_uncertainty_tif_exists = False, False\n",
        "    for stat_tif in os.listdir(statistics_unmasked_dir):\n",
        "        if stat_tif == stat_mean_filename: stat_mean_tif_exists = True\n",
        "        if stat_tif == stat_uncertainty_filename: stat_uncertainty_tif_exists = True\n",
        "\n",
        "    # If either mean or uncertainty do not exist\n",
        "    if stat_mean_tif_exists == False or stat_uncertainty_tif_exists == False:\n",
        "        stat_sum = None\n",
        "        stat_sum_sq = None\n",
        "        iteration_n = 0\n",
        "        for subdir in os.listdir(scenario_iterations_dir):\n",
        "            if subdir.endswith(\".tif\"):\n",
        "                iteration = os.path.join(scenario_iterations_dir, subdir)\n",
        "                iteration_array = gdal.Open(iteration).ReadAsArray()\n",
        "                if stat_sum is None:\n",
        "                    stat_sum = np.zeros_like(iteration_array, dtype='float64')\n",
        "                    stat_sum_sq = np.zeros_like(iteration_array, dtype='float64')\n",
        "                # Sum and sum of squares\n",
        "                stat_sum += iteration_array  # Running sum for mean\n",
        "                stat_sum_sq += np.square(iteration_array, dtype=np.float64)  # Running sum of squares for variance\n",
        "                iteration_n += 1\n",
        "\n",
        "        # Calculate mean: sum / count\n",
        "        stat_mean = np.divide(stat_sum, iteration_n, dtype='float64')\n",
        "        if stat_mean_tif_exists == False:\n",
        "            export_array_as_tif(stat_mean, stat_mean_dir, template = iteration)\n",
        "            print(f\"{stat_mean_filename} has been exported.\")\n",
        "        else: print(f\"{stat_mean_filename} already exists.\")\n",
        "\n",
        "        if stat_uncertainty_tif_exists == False:\n",
        "            # Calculate variance: E[X^2] - (E[X])^2\n",
        "            stat_variance = (stat_sum_sq - (stat_sum ** 2) / iteration_n) / (iteration_n - 1)\n",
        "            # Standard error: Ïƒ / sqrt(n)\n",
        "            stat_se = np.sqrt(stat_variance) / np.sqrt(iteration_n)\n",
        "            # Calculate confidence intervals using t-distribution\n",
        "            stat_ci_lower, stat_ci_upper = st.t.interval(confidence_interval, iteration_n - 1, loc=stat_mean, scale=stat_se)\n",
        "            # CI width: (upper - lower) / 2\n",
        "            stat_ci = np.divide(np.subtract(stat_ci_upper, stat_ci_lower, dtype='float64'), 2, dtype='float64')\n",
        "            # Uncertainty: (CI / mean) * 100%\n",
        "            stat_uncertainty = np.multiply(np.divide(stat_ci, stat_mean, dtype='float64'), 100, dtype='float64')\n",
        "            # Export statistics arrays as rasters\n",
        "            export_array_as_tif(stat_se, join(statistics_unmasked_dir,f\"se__{stat_base_filename}_unmasked.tif\"), template = iteration)\n",
        "            print(f\"se__{stat_base_filename}_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_ci_lower, join(statistics_unmasked_dir,f\"ci_lower__{stat_base_filename}_unmasked.tif\"), template = iteration)\n",
        "            print(f\"ci_lower__{stat_base_filename}_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_ci_upper, join(statistics_unmasked_dir,f\"ci_upper__{stat_base_filename}_unmasked.tif\"), template = iteration)\n",
        "            print(f\"ci_upper__{stat_base_filename}_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_ci, join(statistics_unmasked_dir,f\"ci__{stat_base_filename}_unmasked.tif\"), template = iteration)\n",
        "            print(f\"ci__{stat_base_filename}_unmasked.tif has been exported.\")\n",
        "            export_array_as_tif(stat_uncertainty, stat_predictions_dir, template = iteration)\n",
        "            print(f\"{stat_uncertainty_filename} has been exported.\")\n",
        "        else: print(f\"{stat_uncertainty_filename} already exists.\")\n",
        "\n",
        "    else: print(f\"{stat_mean_filename} and {stat_uncertainty_filename} already exist.\")\n",
        "    stats_progress_index += 1\n",
        "    stats_progress_label.value = (f\"Scenario stats progress: {stats_progress_index}/{len(scenarios_to_calculate)}\")\n",
        "\n",
        "print(\"Statistics calculations and .tif exports complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbqtNygUAboh"
      },
      "source": [
        "# Mask scenario statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk03YvSdG_y4"
      },
      "outputs": [],
      "source": [
        "# Select a scenario iterations area\n",
        "scenario_iterations_area_exists = False\n",
        "for subdir in os.listdir(uncertainty_selected_model_dir):\n",
        "  if subdir != \"model_iterations\":\n",
        "    print(f'selected_scenario_iterations_area = \"{subdir[10:]}\"')\n",
        "    scenario_iterations_area_exists = True\n",
        "if not scenario_iterations_area_exists:\n",
        "  print(f\"Run the scenario iterations section first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TgjqruNBu_H"
      },
      "outputs": [],
      "source": [
        "selected_scenario_iterations_area = \"asartr\"\n",
        "\n",
        "uncertainty_scenario_area_dir = join(uncertainty_selected_model_dir,f\"scenarios_{selected_scenario_iterations_area}\")\n",
        "statistics_unmasked_dir = join(uncertainty_scenario_area_dir,\"statistics_unmasked\")\n",
        "statistics_masked_dir = join(uncertainty_scenario_area_dir,\"statistics_masked\")\n",
        "\n",
        "# Use polygons for masking, only areas inside the polygons will be included\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "print(\"mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"  '{polygon[:-5]}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVMXY7mNAZFi"
      },
      "outputs": [],
      "source": [
        "mask_polygons = [\n",
        "  # 'project_area',\n",
        "  'gedi_area',\n",
        "  # 'peninsular_malaysia',\n",
        "  # 'pa_taman_krau',\n",
        "  # 'pa_ais',\n",
        "]\n",
        "\n",
        "\n",
        "# Create an inverse project area path for masking\n",
        "template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "for polygon in mask_polygons:\n",
        "  inverse_polygon_path = join(polygons_dir, f\"{polygon}_inverse.gpkg\")\n",
        "  if not exists(inverse_polygon_path):\n",
        "    polygon_path = join(polygons_dir, f\"{polygon}.gpkg\")\n",
        "    template_polygon = gpd.read_file(template_polygon_path)\n",
        "    polygon_read = gpd.read_file(polygon_path)\n",
        "    polygon_crs = polygon_read.crs.to_epsg()\n",
        "    inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "    inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "    inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "    print(f\"An inverse masking polygon for {polygon} has been created in {polygons_dir}.\")\n",
        "  else: print(f\"An inverse masking polygon for {polygon} already exists.\")\n",
        "\n",
        "\n",
        "# If only [scenario]_oldgrowth_1 exists, simply all disturbance from all disturbance features\n",
        "# This will be masked to [scenario]_oldgrowth.\n",
        "\n",
        "# If both oldgrowth_1 and oldgrowth_2 exist,\n",
        "# oldgrowth_1 uses an area-based proxy for pre-Landsat undisturbed forest\n",
        "# oldgrowth_2 simply removes all disturbance from all disturbance features\n",
        "# The final masked [scenario]_oldgrowth chooses the maximum pixel values from comparing each.\n",
        "\n",
        "# Identify all oldgrowth 1 files (both mean and uncertainty)\n",
        "oldgrowth_oldgrowth_1_files = [f for f in os.listdir(statistics_unmasked_dir)\n",
        "                     if ('oldgrowth_1__' in f) and\n",
        "                        (f.startswith('mean__') or f.startswith('uncertainty__')) and\n",
        "                        f.endswith('_unmasked.tif')]\n",
        "\n",
        "# First find all mean oldgrowth_1 files\n",
        "mean_oldgrowth_1_files = [f for f in os.listdir(statistics_unmasked_dir)\n",
        "                if f.startswith('mean__') and 'oldgrowth_1__' in f and f.endswith('_unmasked.tif')]\n",
        "\n",
        "for mean_oldgrowth_1_file in mean_oldgrowth_1_files:\n",
        "    # Get corresponding uncertainty oldgrowth_1 file\n",
        "    uncertainty_oldgrowth_1_file = mean_oldgrowth_1_file.replace('mean__', 'uncertainty__')\n",
        "    # Get oldgrowth_2 files\n",
        "    mean_oldgrowth_2_file = mean_oldgrowth_1_file.replace('oldgrowth_1__', 'oldgrowth_2__')\n",
        "    uncertainty_oldgrowth_2_file = uncertainty_oldgrowth_1_file.replace('oldgrowth_1__', 'oldgrowth_2__')\n",
        "    # Create merged filenames\n",
        "    mean_merged_file = mean_oldgrowth_1_file.replace('oldgrowth_1__', 'oldgrowth__')\n",
        "    uncertainty_merged_file = uncertainty_oldgrowth_1_file.replace('oldgrowth_1__', 'oldgrowth__')\n",
        "    # Paths\n",
        "    mean_merged_path = join(statistics_unmasked_dir, mean_merged_file)\n",
        "    uncertainty_merged_path = join(statistics_unmasked_dir, uncertainty_merged_file)\n",
        "    # Skip if merged files already exist\n",
        "    if exists(mean_merged_path) and exists(uncertainty_merged_path):\n",
        "        print(f\"Merged files already exist for {mean_oldgrowth_1_file}\")\n",
        "        continue\n",
        "    print(f\"Processing {mean_oldgrowth_1_file}\")\n",
        "\n",
        "    # Check if oldgrowth_2 exists\n",
        "    if exists(join(statistics_unmasked_dir, mean_oldgrowth_2_file)):\n",
        "        # Process with oldgrowth_2\n",
        "        mean_oldgrowth_1_array = gdal.Open(join(statistics_unmasked_dir, mean_oldgrowth_1_file)).ReadAsArray()\n",
        "        mean_oldgrowth_2_array = gdal.Open(join(statistics_unmasked_dir, mean_oldgrowth_2_file)).ReadAsArray()\n",
        "        # Which one is greater?\n",
        "        oldgrowth_1_is_greater = mean_oldgrowth_1_array > mean_oldgrowth_2_array\n",
        "        # Take maximum value\n",
        "        merged_mean = np.maximum(mean_oldgrowth_1_array, mean_oldgrowth_2_array)\n",
        "        # Save merged mean\n",
        "        export_array_as_tif(merged_mean, mean_merged_path, compress=True)\n",
        "        print(f\"Saved merged mean: {mean_merged_file}\")\n",
        "        # Process uncertainty if oldgrowth_1 exists\n",
        "        if exists(join(statistics_unmasked_dir, uncertainty_oldgrowth_1_file)):\n",
        "            uncertainty_oldgrowth_1_array = gdal.Open(join(statistics_unmasked_dir, uncertainty_oldgrowth_1_file)).ReadAsArray()\n",
        "            # If oldgrowth_2 uncertainty exists, use it where appropriate\n",
        "            if exists(join(statistics_unmasked_dir, uncertainty_oldgrowth_2_file)):\n",
        "                uncertainty_oldgrowth_2_array = gdal.Open(join(statistics_unmasked_dir, uncertainty_oldgrowth_2_file)).ReadAsArray()\n",
        "                merged_uncertainty = np.where(oldgrowth_1_is_greater, uncertainty_oldgrowth_1_array, uncertainty_oldgrowth_2_array)\n",
        "            else: merged_uncertainty = uncertainty_oldgrowth_1_array\n",
        "            # Save merged uncertainty\n",
        "            export_array_as_tif(merged_uncertainty, uncertainty_merged_path, compress=True)\n",
        "            print(f\"Saved merged uncertainty: {uncertainty_merged_file}\")\n",
        "    else: # Just use '_1'\n",
        "        if not exists(mean_merged_path):\n",
        "            shutil.copy2(join(statistics_unmasked_dir, mean_oldgrowth_1_file), mean_merged_path)\n",
        "            print(f\"Copied oldgrowth_1 mean to {mean_merged_file}\")\n",
        "        if exists(join(statistics_unmasked_dir, uncertainty_oldgrowth_1_file)) and not exists(uncertainty_merged_path):\n",
        "            shutil.copy2(join(statistics_unmasked_dir, uncertainty_oldgrowth_1_file), uncertainty_merged_path)\n",
        "            print(f\"Copied oldgrowth_1 uncertainty to {uncertainty_merged_file}\")\n",
        "\n",
        "\n",
        "unmasked_predictions = []\n",
        "for scenario_prediction in os.listdir(statistics_unmasked_dir):\n",
        "  # Only mask mean, uncertainty and ci for visualisation and calculating statistics\n",
        "  if ('mean__' in scenario_prediction) or ('uncertainty__' in scenario_prediction):\n",
        "    if ('oldgrowth_1' not in scenario_prediction) and ('oldgrowth_2' not in scenario_prediction):\n",
        "      unmasked_predictions.append(scenario_prediction)\n",
        "\n",
        "# Determine last feature year for masking future scenarios\n",
        "final_feature_years = []\n",
        "for final_feature in os.listdir(feature_final_dir):\n",
        "  if final_feature.endswith('.tif') and final_feature[-9] == '_':\n",
        "    try: final_feature_years.append(int(final_feature[-8:-4]))\n",
        "    except: continue\n",
        "last_feature_year = max(final_feature_years)\n",
        "\n",
        "# Masking progress\n",
        "masking_progress_index = 0\n",
        "masking_progress_label = widgets.Label(f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\")\n",
        "display(masking_progress_label)\n",
        "\n",
        "# Mask scenario statistics with the relevatant mask\n",
        "for scenario_prediction in unmasked_predictions: # Loop through each unmasked scenario\n",
        "  scenario_masked_filename = f\"{scenario_prediction[:-13]}.tif\"\n",
        "  scenario_masked_dir = join(statistics_masked_dir, scenario_masked_filename)\n",
        "  if not exists(scenario_masked_dir):\n",
        "    mask_exists = False\n",
        "    for mask in os.listdir(masks_dir):\n",
        "      # Match all oldgrowth scenarios\n",
        "      if 'all_oldgrowth' in mask or 'all_oldgrowth' in scenario_prediction:\n",
        "        if 'all_oldgrowth' in mask and 'all_oldgrowth' in scenario_prediction:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "      else: # Match all other historic scenarios\n",
        "        scenario_year = int(scenario_prediction.split('__')[1][:4])\n",
        "        mask_year = int(mask[12:16])\n",
        "        if scenario_year == mask_year:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "        else: # Match future scenarios with most recent forest mask\n",
        "          if scenario_year > last_feature_year and last_feature_year == mask_year:\n",
        "            selected_mask_filename = mask\n",
        "            selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "            mask_exists = True\n",
        "    if mask_exists == False: print(f\"A suitable mask for {scenario_prediction} does not exist.\\n\")\n",
        "    else: # Mask the scenario prediction\n",
        "      print(f\"Masking {scenario_prediction} with {selected_mask_filename}...\")\n",
        "      mask_array = gdal.Open(selected_mask_dir).ReadAsArray()\n",
        "      scenario_prediction_unmasked_dir = join(statistics_unmasked_dir, scenario_prediction)\n",
        "      scenario_prediction_array = gdal.Open(scenario_prediction_unmasked_dir).ReadAsArray()\n",
        "      # Mask where the mask array is not 1\n",
        "      scenario_masked_array = np.where(mask_array != 1, nodatavalue, scenario_prediction_array)\n",
        "      export_array_as_tif(scenario_masked_array, scenario_masked_dir, compress = True)\n",
        "      if len(mask_polygons) > 0:\n",
        "        for polygon_mask in mask_polygons:\n",
        "          inverse_gedi_area_path = join(polygons_dir, f\"{polygon_mask}_inverse.gpkg\")\n",
        "          print(f\"Masking {scenario_prediction} with {polygon_mask}...\")\n",
        "          burn_polygon_to_raster(scenario_masked_dir, inverse_gedi_area_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        # Recompress the prediction after burning the polygon masks\n",
        "        scenario_masked_array_2 = gdal.Open(scenario_masked_dir).ReadAsArray()\n",
        "        export_array_as_tif(scenario_masked_array_2, scenario_masked_dir, compress = True)\n",
        "      print(f\"{scenario_masked_filename} exported.\")\n",
        "  # Update masking progress\n",
        "  masking_progress_index += 1\n",
        "  masking_progress_label.value = f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui5TlyF-QZj_"
      },
      "source": [
        "# Scenario disturbance / change with uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGiuU5lULIwl"
      },
      "outputs": [],
      "source": [
        "# Select a scenario statistics area\n",
        "scenario_iterations_area_exists = False\n",
        "for subdir in os.listdir(uncertainty_selected_model_dir):\n",
        "  if subdir != \"model_iterations\":\n",
        "    print(f'selected_scenario_iterations_area = \"{subdir[10:]}\"')\n",
        "    scenario_iterations_area_exists = True\n",
        "if not scenario_iterations_area_exists:\n",
        "  print(f\"Run the scenario iterations section first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpmEYC36j5xi"
      },
      "outputs": [],
      "source": [
        "selected_scenario_iterations_area = \"asartr\"\n",
        "\n",
        "uncertainty_scenario_area_dir = join(uncertainty_selected_model_dir,f\"scenarios_{selected_scenario_iterations_area}\")\n",
        "statistics_masked_dir = join(uncertainty_scenario_area_dir,\"statistics_masked\")\n",
        "dist_predictions_dir = join(uncertainty_scenario_area_dir, \"scenario_disturbance\")\n",
        "makedirs(dist_predictions_dir, exist_ok=True)\n",
        "\n",
        "# Build dictionaries of disturbance / change options based on available files\n",
        "\n",
        "# Extract all available scenarios from directory\n",
        "scenario_stats = {}\n",
        "for file in os.listdir(statistics_masked_dir):\n",
        "    parts = file.split(\"__\")\n",
        "    if len(parts) >= 2:\n",
        "        stat, scenario = parts[0], parts[1]\n",
        "        if scenario not in scenario_stats:\n",
        "            scenario_stats[scenario] = set()\n",
        "        scenario_stats[scenario].add(stat)\n",
        "\n",
        "# Only keep scenarios that have both 'uncertainty' and 'mean' statistics\n",
        "scenarios = {scenario for scenario, stats in scenario_stats.items()\n",
        "             if 'uncertainty' in stats and 'mean' in stats}\n",
        "\n",
        "# Extract and categorize years from scenarios\n",
        "years = set()\n",
        "plain_years = set()  # Years as standalone scenarios (e.g. \"2014\")\n",
        "oldgrowth_years = set()  # Years with oldgrowth variants\n",
        "\n",
        "for s in scenarios:\n",
        "    if s.isdigit():\n",
        "        years.add(int(s))\n",
        "        plain_years.add(int(s))\n",
        "    elif \"_oldgrowth\" in s:\n",
        "        year = s.split(\"_oldgrowth\")[0]\n",
        "        if year.isdigit():\n",
        "            years.add(int(year))\n",
        "            oldgrowth_years.add(int(year))\n",
        "    elif any(pattern in s for pattern in [\"_no_degradation_since_\", \"_alternate_degradation_\"]):\n",
        "        year = s.split(\"_\")[0]\n",
        "        if year.isdigit():\n",
        "            years.add(int(year))\n",
        "        if \"_since_\" in s:\n",
        "            since_year = s.split(\"_since_\")[1]\n",
        "            if since_year.isdigit():\n",
        "                years.add(int(since_year) - 1)  # Add baseline year (year before \"since\")\n",
        "\n",
        "years_sorted = sorted(list(years))\n",
        "\n",
        "# Track scenario availability and dependencies\n",
        "deforest_since_scenarios = set()\n",
        "direct_degradation_pairs = set()\n",
        "direct_deforestation_pairs = set()\n",
        "\n",
        "# Output dictionaries\n",
        "scenario_difference_dictionary = {}\n",
        "\n",
        "print(\"# Differences in scenario_difference_dictionary and in before_baseline_dictionary are \")\n",
        "print(\"# calculated by subtracting the second scenario / difference from the first. The \")\n",
        "print(\"# differences in degradation_deforestation_dictionary are summed.\")\n",
        "print(\"\")\n",
        "print(\"scenario_difference_dictionary = {\")\n",
        "print(\"\")\n",
        "\n",
        "# Track deforestation scenarios for dependencies\n",
        "deforest_since_scenarios = set()\n",
        "for year_a in years_sorted:\n",
        "    for year_b in years_sorted:\n",
        "        if year_a <= year_b:\n",
        "            continue\n",
        "\n",
        "        a_str, b_str = str(year_a), str(year_b)\n",
        "        b_plus1 = str(year_b + 1)\n",
        "\n",
        "        # Check for deforestation_since scenarios\n",
        "        if (year_b in plain_years and f\"{a_str}_alternate_degradation_{b_str}\" in scenarios):\n",
        "            deforest_since_scenarios.add(f\"{a_str}_deforestation_since_{b_plus1}\")\n",
        "\n",
        "# 1. Process year-to-year baselines for direct differences\n",
        "target_entries = {}\n",
        "\n",
        "for year_a in sorted(years_sorted):  # Primary sort by target year\n",
        "    a_str = str(year_a)\n",
        "    target_entries[a_str] = []\n",
        "\n",
        "    for year_b in sorted(years_sorted):  # Secondary sort by baseline year\n",
        "        if year_a <= year_b:\n",
        "            continue\n",
        "\n",
        "        b_str = str(year_b)\n",
        "        b_plus1 = str(year_b + 1)\n",
        "\n",
        "        deforest_since_key = f\"{a_str}_deforestation_since_{b_plus1}\"\n",
        "        has_deforestation_since = deforest_since_key in deforest_since_scenarios\n",
        "\n",
        "        # Collect all comparisons for alphabetical sorting\n",
        "        comparisons = []\n",
        "\n",
        "        # Deforestation since [year_b+1]\n",
        "        if has_deforestation_since:\n",
        "            comparisons.append((\n",
        "                f\"  ('{a_str}_alternate_degradation_{b_str}', '{b_str}'):\",\n",
        "                f\"    '{a_str}_deforestation_since_{b_plus1}',\"\n",
        "            ))\n",
        "            scenario_difference_dictionary[(f\"{a_str}_alternate_degradation_{b_str}\", b_str)] = f\"{a_str}_deforestation_since_{b_plus1}\"\n",
        "            direct_deforestation_pairs.add((a_str, b_plus1))\n",
        "\n",
        "        # Degradation since [year_b+1]\n",
        "        if (year_a in plain_years and f\"{a_str}_no_degradation_since_{b_plus1}\" in scenarios):\n",
        "            comparisons.append((\n",
        "                f\"  ('{a_str}', '{a_str}_no_degradation_since_{b_plus1}'):\",\n",
        "                f\"    '{a_str}_degradation_since_{b_plus1}',\"\n",
        "            ))\n",
        "            scenario_difference_dictionary[(a_str, f\"{a_str}_no_degradation_since_{b_plus1}\")] = f\"{a_str}_degradation_since_{b_plus1}\"\n",
        "            direct_degradation_pairs.add((a_str, b_plus1))\n",
        "\n",
        "        if comparisons:\n",
        "            section_text = [f\"# Disturbance by {a_str}, using {b_str} as a baseline\"]\n",
        "            # Sort comparisons alphabetically\n",
        "            for line1, line2 in sorted(comparisons, key=lambda x: x[0]):\n",
        "                section_text.append(line1)\n",
        "                section_text.append(line2)\n",
        "            section_text.append(\"\")\n",
        "\n",
        "            target_entries[a_str].append((b_str, section_text))\n",
        "\n",
        "# Output entries sorted by target year first\n",
        "for a_str in sorted(target_entries.keys()):\n",
        "    # Sort by baseline year within each target year\n",
        "    for b_str, section in sorted(target_entries[a_str], key=lambda x: x[0]):\n",
        "        if section:  # Only print non-empty sections\n",
        "            print(\"\\n\".join(section))\n",
        "\n",
        "# 2. Process oldgrowth baseline sections\n",
        "oldgrowth_entries = []\n",
        "for year in years_sorted:\n",
        "    y_str = str(year)\n",
        "\n",
        "    if (year in plain_years and\n",
        "        year in oldgrowth_years and\n",
        "        \"all_oldgrowth\" in scenarios):\n",
        "\n",
        "        if not oldgrowth_entries:\n",
        "            oldgrowth_entries.append(\"# Disturbance using oldgrowth as a baseline\")\n",
        "\n",
        "        oldgrowth_entries.append(f\"  ('{y_str}', '{y_str}_oldgrowth'):\")\n",
        "        oldgrowth_entries.append(f\"    '{y_str}_degradation_total',\")\n",
        "        scenario_difference_dictionary[(y_str, f\"{y_str}_oldgrowth\")] = f\"{y_str}_degradation_total\"\n",
        "\n",
        "        oldgrowth_entries.append(f\"  ('{y_str}_oldgrowth', 'all_oldgrowth'):\")\n",
        "        oldgrowth_entries.append(f\"    '{y_str}_deforestation_total',\")\n",
        "        scenario_difference_dictionary[(f\"{y_str}_oldgrowth\", \"all_oldgrowth\")] = f\"{y_str}_deforestation_total\"\n",
        "        oldgrowth_entries.append(\"\")\n",
        "\n",
        "# Output oldgrowth entries\n",
        "if oldgrowth_entries:\n",
        "    print(\"\\n\".join(oldgrowth_entries))\n",
        "\n",
        "# 3. Process year-to-year change sections\n",
        "years_available = sorted([y for y in years_sorted if y in plain_years])\n",
        "\n",
        "if len(years_available) >= 2:\n",
        "    # Single-year consecutive changes\n",
        "    print(\"# Change between single years\")\n",
        "    for i in range(1, len(years_available)):\n",
        "        current = str(years_available[i])\n",
        "        previous = str(years_available[i-1])\n",
        "\n",
        "        print(f\"  ('{current}', '{previous}'):\")\n",
        "        print(f\"    '{current}_change_{previous}',\")\n",
        "        scenario_difference_dictionary[(current, previous)] = f\"{current}_change_{previous}\"\n",
        "    print(\"\")\n",
        "\n",
        "    # Multi-year comparison (earliest to latest only)\n",
        "    if len(years_available) > 2:\n",
        "        earliest = str(years_available[0])\n",
        "        latest = str(years_available[-1])\n",
        "\n",
        "        print(\"# Change between multiple years\")\n",
        "        print(\"# Add any other desired year combinations manually using the pattern below\")\n",
        "        print(f\"  ('{latest}', '{earliest}'):\")\n",
        "        print(f\"    '{latest}_change_{earliest}',\")\n",
        "        scenario_difference_dictionary[(latest, earliest)] = f\"{latest}_change_{earliest}\"\n",
        "        print(\"\")\n",
        "\n",
        "print(\"}\")\n",
        "\n",
        "# Dictionary for calculated 'before' differences\n",
        "print(\"\")\n",
        "print(\"before_baseline_dictionary = {\")\n",
        "\n",
        "# Collection for degradation before metrics\n",
        "degradation_before_entries = []\n",
        "for year_a, year_b_plus1 in sorted(direct_degradation_pairs):\n",
        "    # Check if required components exist\n",
        "    if (year_a, f\"{year_a}_oldgrowth\") in scenario_difference_dictionary:\n",
        "        deg_since = f\"{year_a}_degradation_since_{year_b_plus1}\"\n",
        "        deg_total = f\"{year_a}_degradation_total\"\n",
        "        deg_before = f\"{year_a}_degradation_before_{year_b_plus1}\"\n",
        "\n",
        "        degradation_before_entries.append(f\"  '{deg_before}': ('{deg_total}', '{deg_since}'),\")\n",
        "\n",
        "# Print degradation before metrics if any exist\n",
        "if degradation_before_entries:\n",
        "    print(\"\\n# Degradation before metrics (Total - Since)\")\n",
        "    for entry in degradation_before_entries:\n",
        "        print(entry)\n",
        "\n",
        "# Collection for deforestation before metrics\n",
        "deforestation_before_entries = []\n",
        "for year_a, year_b_plus1 in sorted(direct_deforestation_pairs):\n",
        "    # Check if required components exist\n",
        "    if (f\"{year_a}_oldgrowth\", \"all_oldgrowth\") in scenario_difference_dictionary:\n",
        "        def_since = f\"{year_a}_deforestation_since_{year_b_plus1}\"\n",
        "        def_total = f\"{year_a}_deforestation_total\"\n",
        "        def_before = f\"{year_a}_deforestation_before_{year_b_plus1}\"\n",
        "\n",
        "        deforestation_before_entries.append(f\"  '{def_before}': ('{def_total}', '{def_since}'),\")\n",
        "\n",
        "# Print deforestation before metrics if any exist\n",
        "if deforestation_before_entries:\n",
        "    if degradation_before_entries:\n",
        "        print(\"\")\n",
        "    print(\"# Deforestation before metrics (Total - Since)\")\n",
        "    for entry in deforestation_before_entries:\n",
        "        print(entry)\n",
        "\n",
        "print(\"}\")\n",
        "\n",
        "# Dictionary for combined degradation and deforestation calculations\n",
        "print(\"\")\n",
        "print(\"degradation_deforestation_dictionary = {\")\n",
        "\n",
        "# Collection for combined disturbance totals\n",
        "combined_total_entries = []\n",
        "for year in years_sorted:\n",
        "    y_str = str(year)\n",
        "\n",
        "    # Check if required components exist\n",
        "    if ((y_str, f\"{y_str}_oldgrowth\") in scenario_difference_dictionary and\n",
        "        (f\"{y_str}_oldgrowth\", \"all_oldgrowth\") in scenario_difference_dictionary):\n",
        "        deg_total = f\"{y_str}_degradation_total\"\n",
        "        def_total = f\"{y_str}_deforestation_total\"\n",
        "        combined = f\"{y_str}_degradation_deforestation_total\"\n",
        "\n",
        "        combined_total_entries.append(f\"  '{combined}': ('{deg_total}', '{def_total}'),\")\n",
        "\n",
        "# Print combined total entries if any exist\n",
        "if combined_total_entries:\n",
        "    print(\"\\n# Combined degradation and deforestation totals\")\n",
        "    for entry in combined_total_entries:\n",
        "        print(entry)\n",
        "\n",
        "# Collection for combined since metrics\n",
        "combined_pairs = direct_degradation_pairs.intersection(direct_deforestation_pairs)\n",
        "combined_since_entries = []\n",
        "\n",
        "for year_a, year_b_plus1 in sorted(combined_pairs):\n",
        "    # Verify components exist\n",
        "    deg_since = f\"{year_a}_degradation_since_{year_b_plus1}\"\n",
        "    def_since = f\"{year_a}_deforestation_since_{year_b_plus1}\"\n",
        "    combined = f\"{year_a}_degradation_deforestation_since_{year_b_plus1}\"\n",
        "\n",
        "    combined_since_entries.append(f\"  '{combined}': ('{deg_since}', '{def_since}'),\")\n",
        "\n",
        "# Print combined since entries if any exist\n",
        "if combined_since_entries:\n",
        "    if combined_total_entries:\n",
        "        print(\"\")\n",
        "    print(\"# Combined degradation and deforestation since\")\n",
        "    for entry in combined_since_entries:\n",
        "        print(entry)\n",
        "\n",
        "# Collection for combined before metrics\n",
        "combined_before_entries = []\n",
        "for year_a, year_b_plus1 in sorted(combined_pairs):\n",
        "    # Check if individual before metrics defined\n",
        "    deg_before = f\"{year_a}_degradation_before_{year_b_plus1}\"\n",
        "    def_before = f\"{year_a}_deforestation_before_{year_b_plus1}\"\n",
        "\n",
        "    # Only include if components would exist\n",
        "    if ((year_a, f\"{year_a}_oldgrowth\") in scenario_difference_dictionary and\n",
        "        (f\"{year_a}_oldgrowth\", \"all_oldgrowth\") in scenario_difference_dictionary):\n",
        "        combined = f\"{year_a}_degradation_deforestation_before_{year_b_plus1}\"\n",
        "\n",
        "        combined_before_entries.append(f\"  '{combined}': ('{deg_before}', '{def_before}'),\")\n",
        "\n",
        "# Print combined before entries if any exist\n",
        "if combined_before_entries:\n",
        "    if combined_total_entries or combined_since_entries:\n",
        "        print(\"\")\n",
        "    print(\"# Combined degradation and deforestation before\")\n",
        "    for entry in combined_before_entries:\n",
        "        print(entry)\n",
        "\n",
        "print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aI9JUExJSXw"
      },
      "outputs": [],
      "source": [
        "# Differences in scenario_difference_dictionary and in before_baseline_dictionary are\n",
        "# calculated by subtracting the second scenario / difference from the first. The\n",
        "# differences in degradation_deforestation_dictionary are summed.\n",
        "\n",
        "scenario_difference_dictionary = {\n",
        "\n",
        "# Disturbance by 2014, using 1990 as a baseline\n",
        "  ('2014', '2014_no_degradation_since_1991'):\n",
        "    '2014_degradation_since_1991',\n",
        "\n",
        "# Disturbance by 2022, using 2021 as a baseline\n",
        "  ('2022', '2022_no_degradation_since_2022'):\n",
        "    '2022_degradation_since_2022',\n",
        "  ('2022_alternate_degradation_2021', '2021'):\n",
        "    '2022_deforestation_since_2022',\n",
        "\n",
        "# Disturbance by 2023, using 2022 as a baseline\n",
        "  ('2023', '2023_no_degradation_since_2023'):\n",
        "    '2023_degradation_since_2023',\n",
        "  ('2023_alternate_degradation_2022', '2022'):\n",
        "    '2023_deforestation_since_2023',\n",
        "\n",
        "# Disturbance by 2024, using 1999 as a baseline\n",
        "  ('2024', '2024_no_degradation_since_2000'):\n",
        "    '2024_degradation_since_2000',\n",
        "\n",
        "# Disturbance by 2024, using 2014 as a baseline\n",
        "  ('2024', '2024_no_degradation_since_2015'):\n",
        "    '2024_degradation_since_2015',\n",
        "  ('2024_alternate_degradation_2014', '2014'):\n",
        "    '2024_deforestation_since_2015',\n",
        "\n",
        "# Disturbance by 2024, using 2021 as a baseline\n",
        "  ('2024', '2024_no_degradation_since_2022'):\n",
        "    '2024_degradation_since_2022',\n",
        "  ('2024_alternate_degradation_2021', '2021'):\n",
        "    '2024_deforestation_since_2022',\n",
        "\n",
        "# Disturbance by 2024, using 2022 as a baseline\n",
        "  ('2024', '2024_no_degradation_since_2023'):\n",
        "    '2024_degradation_since_2023',\n",
        "  ('2024_alternate_degradation_2022', '2022'):\n",
        "    '2024_deforestation_since_2023',\n",
        "\n",
        "# Disturbance by 2024, using 2023 as a baseline\n",
        "  ('2024', '2024_no_degradation_since_2024'):\n",
        "    '2024_degradation_since_2024',\n",
        "  ('2024_alternate_degradation_2023', '2023'):\n",
        "    '2024_deforestation_since_2024',\n",
        "\n",
        "# Disturbance using oldgrowth as a baseline\n",
        "  ('2014', '2014_oldgrowth'):\n",
        "    '2014_degradation_total',\n",
        "  ('2014_oldgrowth', 'all_oldgrowth'):\n",
        "    '2014_deforestation_total',\n",
        "\n",
        "  ('2021', '2021_oldgrowth'):\n",
        "    '2021_degradation_total',\n",
        "  ('2021_oldgrowth', 'all_oldgrowth'):\n",
        "    '2021_deforestation_total',\n",
        "\n",
        "  ('2022', '2022_oldgrowth'):\n",
        "    '2022_degradation_total',\n",
        "  ('2022_oldgrowth', 'all_oldgrowth'):\n",
        "    '2022_deforestation_total',\n",
        "\n",
        "  ('2023', '2023_oldgrowth'):\n",
        "    '2023_degradation_total',\n",
        "  ('2023_oldgrowth', 'all_oldgrowth'):\n",
        "    '2023_deforestation_total',\n",
        "\n",
        "  ('2024', '2024_oldgrowth'):\n",
        "    '2024_degradation_total',\n",
        "  ('2024_oldgrowth', 'all_oldgrowth'):\n",
        "    '2024_deforestation_total',\n",
        "\n",
        "# Change between single years\n",
        "  ('2015', '2014'):\n",
        "    '2015_change_2014',\n",
        "  ('2016', '2015'):\n",
        "    '2016_change_2015',\n",
        "  ('2017', '2016'):\n",
        "    '2017_change_2016',\n",
        "  ('2018', '2017'):\n",
        "    '2018_change_2017',\n",
        "  ('2019', '2018'):\n",
        "    '2019_change_2018',\n",
        "  ('2020', '2019'):\n",
        "    '2020_change_2019',\n",
        "  ('2021', '2020'):\n",
        "    '2021_change_2020',\n",
        "  ('2022', '2021'):\n",
        "    '2022_change_2021',\n",
        "  ('2023', '2022'):\n",
        "    '2023_change_2022',\n",
        "  ('2024', '2023'):\n",
        "    '2024_change_2023',\n",
        "\n",
        "# Change between multiple years\n",
        "# Add any other desired year combinations manually using the pattern below\n",
        "  ('2024', '2014'):\n",
        "    '2024_change_2014',\n",
        "\n",
        "}\n",
        "\n",
        "before_baseline_dictionary = {\n",
        "\n",
        "# Degradation before metrics (Total - Since)\n",
        "  '2014_degradation_before_1991': ('2014_degradation_total', '2014_degradation_since_1991'),\n",
        "  '2022_degradation_before_2022': ('2022_degradation_total', '2022_degradation_since_2022'),\n",
        "  '2023_degradation_before_2023': ('2023_degradation_total', '2023_degradation_since_2023'),\n",
        "  '2024_degradation_before_2000': ('2024_degradation_total', '2024_degradation_since_2000'),\n",
        "  '2024_degradation_before_2015': ('2024_degradation_total', '2024_degradation_since_2015'),\n",
        "  '2024_degradation_before_2022': ('2024_degradation_total', '2024_degradation_since_2022'),\n",
        "  '2024_degradation_before_2023': ('2024_degradation_total', '2024_degradation_since_2023'),\n",
        "  '2024_degradation_before_2024': ('2024_degradation_total', '2024_degradation_since_2024'),\n",
        "\n",
        "# Deforestation before metrics (Total - Since)\n",
        "  '2022_deforestation_before_2022': ('2022_deforestation_total', '2022_deforestation_since_2022'),\n",
        "  '2023_deforestation_before_2023': ('2023_deforestation_total', '2023_deforestation_since_2023'),\n",
        "  '2024_deforestation_before_2015': ('2024_deforestation_total', '2024_deforestation_since_2015'),\n",
        "  '2024_deforestation_before_2022': ('2024_deforestation_total', '2024_deforestation_since_2022'),\n",
        "  '2024_deforestation_before_2023': ('2024_deforestation_total', '2024_deforestation_since_2023'),\n",
        "  '2024_deforestation_before_2024': ('2024_deforestation_total', '2024_deforestation_since_2024'),\n",
        "}\n",
        "\n",
        "degradation_deforestation_dictionary = {\n",
        "\n",
        "# Combined degradation and deforestation totals\n",
        "  '2014_degradation_deforestation_total': ('2014_degradation_total', '2014_deforestation_total'),\n",
        "  '2021_degradation_deforestation_total': ('2021_degradation_total', '2021_deforestation_total'),\n",
        "  '2022_degradation_deforestation_total': ('2022_degradation_total', '2022_deforestation_total'),\n",
        "  '2023_degradation_deforestation_total': ('2023_degradation_total', '2023_deforestation_total'),\n",
        "  '2024_degradation_deforestation_total': ('2024_degradation_total', '2024_deforestation_total'),\n",
        "\n",
        "# Combined degradation and deforestation since\n",
        "  '2022_degradation_deforestation_since_2022': ('2022_degradation_since_2022', '2022_deforestation_since_2022'),\n",
        "  '2023_degradation_deforestation_since_2023': ('2023_degradation_since_2023', '2023_deforestation_since_2023'),\n",
        "  '2024_degradation_deforestation_since_2015': ('2024_degradation_since_2015', '2024_deforestation_since_2015'),\n",
        "  '2024_degradation_deforestation_since_2022': ('2024_degradation_since_2022', '2024_deforestation_since_2022'),\n",
        "  '2024_degradation_deforestation_since_2023': ('2024_degradation_since_2023', '2024_deforestation_since_2023'),\n",
        "  '2024_degradation_deforestation_since_2024': ('2024_degradation_since_2024', '2024_deforestation_since_2024'),\n",
        "\n",
        "# Combined degradation and deforestation before\n",
        "  '2022_degradation_deforestation_before_2022': ('2022_degradation_before_2022', '2022_deforestation_before_2022'),\n",
        "  '2023_degradation_deforestation_before_2023': ('2023_degradation_before_2023', '2023_deforestation_before_2023'),\n",
        "  '2024_degradation_deforestation_before_2015': ('2024_degradation_before_2015', '2024_deforestation_before_2015'),\n",
        "  '2024_degradation_deforestation_before_2022': ('2024_degradation_before_2022', '2024_deforestation_before_2022'),\n",
        "  '2024_degradation_deforestation_before_2023': ('2024_degradation_before_2023', '2024_deforestation_before_2023'),\n",
        "  '2024_degradation_deforestation_before_2024': ('2024_degradation_before_2024', '2024_deforestation_before_2024'),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HkZIbBNFIUW"
      },
      "outputs": [],
      "source": [
        "# Functions for difference and sum calculations with uncertainty\n",
        "def subtract_mean_arrays(array1, array2):\n",
        "    return array1 - array2\n",
        "\n",
        "def sum_mean_arrays(array1, array2):\n",
        "    return array1 + array2\n",
        "\n",
        "# Uncertainty propagation following Liang et al. (2023)\n",
        "# Same function used for both subtraction and addition operations\n",
        "# Based on IPCC and CEOS Land Product Validation Protocol methods\n",
        "# Represents relative uncertainty propagation for biomass change estimates\n",
        "def propagate_uncertainty(mean1, uncertainty1, mean2, uncertainty2):\n",
        "    absolute_uncertainty1 = np.multiply(mean1, uncertainty1, dtype='float64')\n",
        "    absolute_uncertainty2 = np.multiply(mean2, uncertainty2, dtype='float64')\n",
        "    sums_of_squares = np.square(absolute_uncertainty1, dtype='float64') + np.square(absolute_uncertainty2, dtype='float64')\n",
        "    # Avoid division by zero\n",
        "    denominator = np.abs(mean1 + mean2)\n",
        "    epsilon = np.finfo(np.float64).eps\n",
        "    denominator = np.where(denominator == 0, epsilon, denominator)\n",
        "    return np.sqrt(sums_of_squares, dtype='float64') / denominator\n",
        "\n",
        "# Set up progress tracking\n",
        "total_operations = len(scenario_difference_dictionary) + len(before_baseline_dictionary) + len(degradation_deforestation_dictionary)\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Calculation progress: {progress_index}/{total_operations}\")\n",
        "display(progress_label)\n",
        "\n",
        "# 1. Process direct scenario differences\n",
        "for (scenario1, scenario2), difference_name in scenario_difference_dictionary.items():\n",
        "    # Define filenames and directories for mean and uncertainty\n",
        "    mean_filename = f\"mean__{difference_name}__{selected_scenario_iterations_area}_{selected_model}.tif\"\n",
        "    mean_dir = join(dist_predictions_dir, mean_filename)\n",
        "    uncertainty_filename = f\"uncertainty__{difference_name}__{selected_scenario_iterations_area}_{selected_model}.tif\"\n",
        "    uncertainty_dir = join(dist_predictions_dir, uncertainty_filename)\n",
        "    # Skip if both files already exist\n",
        "    if exists(mean_dir) and exists(uncertainty_dir):\n",
        "        # Update progress and continue\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"Calculation progress: {progress_index}/{total_operations}\"\n",
        "        continue\n",
        "\n",
        "    scenario1_base = f\"{scenario1}__{selected_scenario_iterations_area}_{selected_model}\"\n",
        "    scenario2_base = f\"{scenario2}__{selected_scenario_iterations_area}_{selected_model}\"\n",
        "    # Assert files exist\n",
        "    scenario1_mean_dir = join(statistics_masked_dir, f\"mean__{scenario1_base}.tif\")\n",
        "    scenario1_uncertainty_dir = join(statistics_masked_dir, f\"uncertainty__{scenario1_base}.tif\")\n",
        "    scenario2_mean_dir = join(statistics_masked_dir, f\"mean__{scenario2_base}.tif\")\n",
        "    scenario2_uncertainty_dir = join(statistics_masked_dir, f\"uncertainty__{scenario2_base}.tif\")\n",
        "    assert exists(scenario1_mean_dir), f\"mean__{scenario1_base}.tif does not exist.\"\n",
        "    assert exists(scenario1_uncertainty_dir), f\"uncertainty__{scenario1_base}.tif does not exist.\"\n",
        "    assert exists(scenario2_mean_dir), f\"mean__{scenario2_base}.tif does not exist.\"\n",
        "    assert exists(scenario2_uncertainty_dir), f\"uncertainty__{scenario2_base}.tif does not exist.\"\n",
        "    # Read arrays\n",
        "    scenario1_mean = gdal.Open(scenario1_mean_dir).ReadAsArray()\n",
        "    scenario1_uncertainty = gdal.Open(scenario1_uncertainty_dir).ReadAsArray()\n",
        "    scenario2_mean = gdal.Open(scenario2_mean_dir).ReadAsArray()\n",
        "    scenario2_uncertainty = gdal.Open(scenario2_uncertainty_dir).ReadAsArray()\n",
        "    # Handle nodata values\n",
        "    scenario1_mean = np.where((scenario1_mean == nodatavalue) & (scenario2_mean != nodatavalue), 0, scenario1_mean)\n",
        "    scenario1_uncertainty = np.where((scenario1_uncertainty == nodatavalue) & (scenario2_uncertainty != nodatavalue), 0, scenario1_uncertainty)\n",
        "    scenario2_mean = np.where((scenario2_mean == nodatavalue) & (scenario1_mean != nodatavalue), 0, scenario2_mean)\n",
        "    scenario2_uncertainty = np.where((scenario2_uncertainty == nodatavalue) & (scenario1_uncertainty != nodatavalue), 0, scenario2_uncertainty)\n",
        "    # Calculate differences\n",
        "    diff_mean_array = np.where(scenario1_mean == nodatavalue, nodatavalue, subtract_mean_arrays(scenario1_mean, scenario2_mean))\n",
        "    diff_uncertainty_array = np.where(scenario1_mean == nodatavalue, nodatavalue,\n",
        "                                     propagate_uncertainty(scenario1_mean, scenario1_uncertainty, scenario2_mean, scenario2_uncertainty))\n",
        "    # Set uncertainty to 0 where mean is 0\n",
        "    diff_uncertainty_array = np.where(diff_mean_array == 0, 0, diff_uncertainty_array)\n",
        "    # Export results\n",
        "    if not exists(mean_dir):\n",
        "        export_array_as_tif(diff_mean_array, mean_dir, template=scenario1_mean_dir)\n",
        "    if not exists(uncertainty_dir):\n",
        "        export_array_as_tif(diff_uncertainty_array, uncertainty_dir, template=scenario1_mean_dir)\n",
        "    # Update progress\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "# 2. Process before baseline differences\n",
        "for difference_name, (diff1_name, diff2_name) in before_baseline_dictionary.items():\n",
        "    # Define filenames\n",
        "    mean_filename = f\"mean__{difference_name}__{selected_scenario_iterations_area}_{selected_model}.tif\"\n",
        "    mean_dir = join(dist_predictions_dir, mean_filename)\n",
        "    uncertainty_filename = f\"uncertainty__{difference_name}__{selected_scenario_iterations_area}_{selected_model}.tif\"\n",
        "    uncertainty_dir = join(dist_predictions_dir, uncertainty_filename)\n",
        "    # Skip if both files already exist\n",
        "    if exists(mean_dir) and exists(uncertainty_dir):\n",
        "        # Update progress and continue\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"Calculation progress: {progress_index}/{total_operations}\"\n",
        "        continue\n",
        "    diff1_mean_dir = join(dist_predictions_dir, f\"mean__{diff1_name}__{selected_scenario_iterations_area}_{selected_model}.tif\")\n",
        "    diff1_uncertainty_dir = join(dist_predictions_dir, f\"uncertainty__{diff1_name}__{selected_scenario_iterations_area}_{selected_model}.tif\")\n",
        "    diff2_mean_dir = join(dist_predictions_dir, f\"mean__{diff2_name}__{selected_scenario_iterations_area}_{selected_model}.tif\")\n",
        "    diff2_uncertainty_dir = join(dist_predictions_dir, f\"uncertainty__{diff2_name}__{selected_scenario_iterations_area}_{selected_model}.tif\")\n",
        "    assert exists(diff1_mean_dir), f\"{diff1_mean_dir} does not exist.\"\n",
        "    assert exists(diff1_uncertainty_dir), f\"{diff1_uncertainty_dir} does not exist.\"\n",
        "    assert exists(diff2_mean_dir), f\"{diff2_mean_dir} does not exist.\"\n",
        "    assert exists(diff2_uncertainty_dir), f\"{diff2_uncertainty_dir} does not exist.\"\n",
        "    # Read arrays\n",
        "    diff1_mean = gdal.Open(diff1_mean_dir).ReadAsArray()\n",
        "    diff1_uncertainty = gdal.Open(diff1_uncertainty_dir).ReadAsArray()\n",
        "    diff2_mean = gdal.Open(diff2_mean_dir).ReadAsArray()\n",
        "    diff2_uncertainty = gdal.Open(diff2_uncertainty_dir).ReadAsArray()\n",
        "    # Handle nodata values\n",
        "    diff1_mean = np.where((diff1_mean == nodatavalue) & (diff2_mean != nodatavalue), 0, diff1_mean)\n",
        "    diff1_uncertainty = np.where((diff1_uncertainty == nodatavalue) & (diff2_uncertainty != nodatavalue), 0, diff1_uncertainty)\n",
        "    diff2_mean = np.where((diff2_mean == nodatavalue) & (diff1_mean != nodatavalue), 0, diff2_mean)\n",
        "    diff2_uncertainty = np.where((diff2_uncertainty == nodatavalue) & (diff1_uncertainty != nodatavalue), 0, diff2_uncertainty)\n",
        "    # Calculate difference\n",
        "    result_mean = np.where(diff1_mean == nodatavalue, nodatavalue, subtract_mean_arrays(diff1_mean, diff2_mean))\n",
        "    result_uncertainty = np.where(diff1_mean == nodatavalue, nodatavalue,\n",
        "                                 propagate_uncertainty(diff1_mean, diff1_uncertainty, diff2_mean, diff2_uncertainty))\n",
        "    # Set uncertainty to 0 where mean is 0\n",
        "    result_uncertainty = np.where(result_mean == 0, 0, result_uncertainty)\n",
        "    # Export results\n",
        "    if not exists(mean_dir):\n",
        "        export_array_as_tif(result_mean, mean_dir, template=diff1_mean_dir)\n",
        "    if not exists(uncertainty_dir):\n",
        "        export_array_as_tif(result_uncertainty, uncertainty_dir, template=diff1_mean_dir)\n",
        "    # Update progress\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "# 3. Process combined degradation and deforestation\n",
        "for difference_name, (diff1_name, diff2_name) in degradation_deforestation_dictionary.items():\n",
        "    # Define filenames\n",
        "    mean_filename = f\"mean__{difference_name}__{selected_scenario_iterations_area}_{selected_model}.tif\"\n",
        "    mean_dir = join(dist_predictions_dir, mean_filename)\n",
        "    uncertainty_filename = f\"uncertainty__{difference_name}__{selected_scenario_iterations_area}_{selected_model}.tif\"\n",
        "    uncertainty_dir = join(dist_predictions_dir, uncertainty_filename)\n",
        "    # Skip if both files already exist\n",
        "    if exists(mean_dir) and exists(uncertainty_dir):\n",
        "        # Update progress and continue\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"Calculation progress: {progress_index}/{total_operations}\"\n",
        "        continue\n",
        "    diff1_mean_dir = join(dist_predictions_dir, f\"mean__{diff1_name}__{selected_scenario_iterations_area}_{selected_model}.tif\")\n",
        "    diff1_uncertainty_dir = join(dist_predictions_dir, f\"uncertainty__{diff1_name}__{selected_scenario_iterations_area}_{selected_model}.tif\")\n",
        "    diff2_mean_dir = join(dist_predictions_dir, f\"mean__{diff2_name}__{selected_scenario_iterations_area}_{selected_model}.tif\")\n",
        "    diff2_uncertainty_dir = join(dist_predictions_dir, f\"uncertainty__{diff2_name}__{selected_scenario_iterations_area}_{selected_model}.tif\")\n",
        "    assert exists(diff1_mean_dir), f\"{diff1_mean_dir} does not exist.\"\n",
        "    assert exists(diff1_uncertainty_dir), f\"{diff1_uncertainty_dir} does not exist.\"\n",
        "    assert exists(diff2_mean_dir), f\"{diff2_mean_dir} does not exist.\"\n",
        "    assert exists(diff2_uncertainty_dir), f\"{diff2_uncertainty_dir} does not exist.\"\n",
        "    # Read arrays\n",
        "    diff1_mean = gdal.Open(diff1_mean_dir).ReadAsArray()\n",
        "    diff1_uncertainty = gdal.Open(diff1_uncertainty_dir).ReadAsArray()\n",
        "    diff2_mean = gdal.Open(diff2_mean_dir).ReadAsArray()\n",
        "    diff2_uncertainty = gdal.Open(diff2_uncertainty_dir).ReadAsArray()\n",
        "    # Handle nodata values\n",
        "    diff1_mean = np.where((diff1_mean == nodatavalue) & (diff2_mean != nodatavalue), 0, diff1_mean)\n",
        "    diff1_uncertainty = np.where((diff1_uncertainty == nodatavalue) & (diff2_uncertainty != nodatavalue), 0, diff1_uncertainty)\n",
        "    diff2_mean = np.where((diff2_mean == nodatavalue) & (diff1_mean != nodatavalue), 0, diff2_mean)\n",
        "    diff2_uncertainty = np.where((diff2_uncertainty == nodatavalue) & (diff1_uncertainty != nodatavalue), 0, diff2_uncertainty)\n",
        "    # Calculate sum\n",
        "    result_mean = np.where(diff1_mean == nodatavalue, nodatavalue, sum_mean_arrays(diff1_mean, diff2_mean))\n",
        "    result_uncertainty = np.where(diff1_mean == nodatavalue, nodatavalue,\n",
        "                                 propagate_uncertainty(diff1_mean, diff1_uncertainty, diff2_mean, diff2_uncertainty))\n",
        "    # Set uncertainty to 0 where mean is 0\n",
        "    result_uncertainty = np.where(result_mean == 0, 0, result_uncertainty)\n",
        "    # Export results\n",
        "    if not exists(mean_dir):\n",
        "        export_array_as_tif(result_mean, mean_dir, template=diff1_mean_dir)\n",
        "    if not exists(uncertainty_dir):\n",
        "        export_array_as_tif(result_uncertainty, uncertainty_dir, template=diff1_mean_dir)\n",
        "    # Update progress\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "print(\"All calculations completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pFGWPj0DowZ"
      },
      "source": [
        "# Intactness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgW-4RQpEamo"
      },
      "outputs": [],
      "source": [
        "# Select a scenario statistics area\n",
        "scenario_iterations_area_exists = False\n",
        "for subdir in os.listdir(uncertainty_selected_model_dir):\n",
        "  if subdir != \"model_iterations\":\n",
        "    print(f'selected_scenario_iterations_area = \"{subdir[10:]}\"')\n",
        "    scenario_iterations_area_exists = True\n",
        "if not scenario_iterations_area_exists:\n",
        "  print(f\"Run the scenario iterations section first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzF5WoTiDr79"
      },
      "outputs": [],
      "source": [
        "selected_scenario_iterations_area = \"asartr\"\n",
        "\n",
        "uncertainty_scenario_area_dir = join(uncertainty_selected_model_dir, f\"scenarios_{selected_scenario_iterations_area}\")\n",
        "statistics_masked_dir = join(uncertainty_scenario_area_dir, \"statistics_masked\")\n",
        "dist_predictions_dir = join(uncertainty_scenario_area_dir, \"scenario_disturbance\")\n",
        "intactness_dir = join(uncertainty_scenario_area_dir, 'intactness')\n",
        "makedirs(intactness_dir, exist_ok=True)\n",
        "\n",
        "# Select which baseline and disturbance raster to use for calculating intactness\n",
        "# percentage and relative intactness. Ideally this is the scenario with the least disturbance\n",
        "# and the difference between that and the current reality.\n",
        "\n",
        "for baseline in os.listdir(statistics_masked_dir):\n",
        "  if 'mean' in baseline:\n",
        "    print(f\"selected_baseline = '{baseline}'\")\n",
        "for dist in os.listdir(dist_predictions_dir):\n",
        "  if 'mean' in dist:\n",
        "    print(f\"selected_dist = '{dist}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJdlTIeHD15W"
      },
      "outputs": [],
      "source": [
        "# selected_baseline = 'mean__all_oldgrowth__asartr_agbd_historic_250429_223033.tif'\n",
        "selected_baseline = 'mean__2024_no_degradation_since_2000__asartr_agbd_historic_250429_223033.tif'\n",
        "# selected_dist = 'mean__2024_degradation_deforestation_total__asartr_agbd_historic_250429_223033.tif'\n",
        "selected_dist = 'mean__2024_degradation_since_2000__asartr_agbd_historic_250429_223033.tif'\n",
        "\n",
        "forest_mask_year = '2024'\n",
        "\n",
        "base_dist_name = f\"{selected_baseline.split('__')[1]}__{selected_dist.split('__')[1]}\"\n",
        "intactness_baseline_dist_dir = join(intactness_dir, base_dist_name)\n",
        "makedirs(intactness_baseline_dist_dir, exist_ok=True)\n",
        "\n",
        "percentage_filename = f\"percentage_change__{base_dist_name}__{selected_model}.tif\"\n",
        "percentage_path = join(intactness_baseline_dist_dir, percentage_filename)\n",
        "\n",
        "if not exists(percentage_path):\n",
        "  # Define filenames and directories\n",
        "  selected_baseline_path = join(statistics_masked_dir, selected_baseline)\n",
        "  selected_dist_path = join(dist_predictions_dir, selected_dist)\n",
        "  selected_mask_path = join(masks_dir, f\"mask_forest_{forest_mask_year}.tif\")\n",
        "\n",
        "  # Convert to arrays\n",
        "  selected_baseline_array = gdal.Open(selected_baseline_path).ReadAsArray()\n",
        "  selected_dist_array = gdal.Open(selected_dist_path).ReadAsArray()\n",
        "  selected_mask_array = gdal.Open(selected_mask_path).ReadAsArray()\n",
        "\n",
        "  # Create percentage array where the value is not 'nodatavalue' in any of the inputs\n",
        "  percentage_array = np.where((selected_mask_array==nodatavalue) | (selected_baseline_array==nodatavalue) | (selected_dist_array==nodatavalue), nodatavalue,\n",
        "                              selected_dist_array/selected_baseline_array*100)\n",
        "  export_array_as_tif(percentage_array, percentage_path, template = selected_baseline_path)\n",
        "  print(f\"{percentage_filename} has been exported.\")\n",
        "\n",
        "else: print(f\"{percentage_filename} already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9SAFDkfD3p_"
      },
      "outputs": [],
      "source": [
        "# Use additional polygons for masking relative intactness quantiles\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "\n",
        "# Select baseline / disturbance pairs to measure relative intactness\n",
        "print(\"baseline_disturbance_pairs = [\")\n",
        "for dir in os.listdir(intactness_dir):\n",
        "  print(f\"'{dir}',\")\n",
        "print(\"]\\n\")\n",
        "\n",
        "# Select polygons to mask and calculate quantiles\n",
        "print(\"mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"'{polygon}',\")\n",
        "print(None)\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhw5c21LD5BE"
      },
      "outputs": [],
      "source": [
        "baseline_disturbance_pairs = [\n",
        "'all_oldgrowth__2024_degradation_deforestation_total',\n",
        "'2024_no_degradation_since_2000__2024_degradation_since_2000',\n",
        "]\n",
        "\n",
        "mask_polygons = [\n",
        "'project_area.gpkg',\n",
        "# 'gedi_area.gpkg',\n",
        "# 'peninsular_malaysia.gpkg',\n",
        "# 'lu_oldgrowth.gpkg',\n",
        "'asartr_phase_2.gpkg',\n",
        "'intactness_wo_tn.gpkg',\n",
        "None\n",
        "]\n",
        "\n",
        "# Define number of quantiles for intactness rating (e.g. 10 for 1 - 10)\n",
        "num_quantiles = 10\n",
        "\n",
        "\n",
        "for mask_polygon in mask_polygons:\n",
        "  if mask_polygon is not None:\n",
        "    # Create an inverse project area path for masking\n",
        "    template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "    inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "    if not exists(inverse_polygon_path):\n",
        "      polygon_path = join(polygons_dir, mask_polygon)\n",
        "      template_polygon = gpd.read_file(template_polygon_path)\n",
        "      polygon_read = gpd.read_file(polygon_path)\n",
        "      polygon_crs = polygon_read.crs.to_epsg()\n",
        "      inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "      inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "      inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "      print(f\"An inverse masking polygon for {mask_polygon} has been created in {polygons_dir}.\")\n",
        "    else: print(f\"An inverse masking polygon for {mask_polygon} already exists.\")\n",
        "\n",
        "for base_dist_name in baseline_disturbance_pairs:\n",
        "  intactness_baseline_dist_dir = join(intactness_dir, base_dist_name)\n",
        "  percentage_filename = f\"percentage_change__{base_dist_name}__{selected_model}\"\n",
        "  percentage_path = join(intactness_baseline_dist_dir, f\"{percentage_filename}.tif\")\n",
        "\n",
        "  for mask_polygon in mask_polygons:\n",
        "\n",
        "    if mask_polygon is not None:\n",
        "      # Copy the percentage raster for potential masking\n",
        "      percentage_masked_filename = f\"{percentage_filename}__masked_{mask_polygon[:-5]}.tif\"\n",
        "      percentage_masked_path = join(intactness_baseline_dist_dir, percentage_masked_filename)\n",
        "      if not exists(percentage_masked_path):\n",
        "        print(f\"Copying {percentage_filename} for masking...\")\n",
        "        copyfile(percentage_path, percentage_masked_path)\n",
        "        print(f\"Masking {percentage_filename} with {mask_polygon}...\")\n",
        "        inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "        burn_polygon_to_raster(percentage_masked_path, inverse_polygon_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        # Recompress the prediction after burning the polygon masks\n",
        "        percentage_masked_array = gdal.Open(percentage_masked_path).ReadAsArray()\n",
        "        export_array_as_tif(percentage_masked_array, percentage_masked_path, compress = True)\n",
        "        print(f\"{percentage_filename} masked.\")\n",
        "      else: print(f\"{percentage_masked_filename} already exists.\")\n",
        "\n",
        "    # Define paths and arrays\n",
        "    if mask_polygon is None: relative_intactness_name = f'intactness__{num_quantiles}_quantiles__{base_dist_name}__{selected_model}'\n",
        "    else: relative_intactness_name = f'intactness__{mask_polygon[:-5]}_{num_quantiles}_quantiles__{base_dist_name}__{selected_model}'\n",
        "    relative_intactness_path = join(intactness_baseline_dist_dir, f'{relative_intactness_name}.tif')\n",
        "    if not exists(relative_intactness_path):\n",
        "      if mask_polygon is None: percentage_array = gdal.Open(percentage_path).ReadAsArray()\n",
        "      else: percentage_array = gdal.Open(percentage_masked_path).ReadAsArray()\n",
        "      relative_intactness_array = np.empty_like(percentage_array, dtype=object)\n",
        "\n",
        "      # Set all values above 0 to 0, assuming negative values are not intact\n",
        "      percentage_array[percentage_array > 0] = 0\n",
        "\n",
        "      # Separate valid and invalid (nodatavalue) elements\n",
        "      valid_elements = percentage_array[percentage_array != nodatavalue]\n",
        "      invalid_elements = percentage_array == nodatavalue\n",
        "\n",
        "      # Calculate quantiles for valid elements\n",
        "      quantiles = np.percentile(valid_elements, np.linspace(0, 100, num_quantiles + 1)[1:-1]) if len(valid_elements) > 0 else []\n",
        "      for i in range(1, num_quantiles + 1):\n",
        "          lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "          upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "          relative_intactness_array[(percentage_array > lower_bound) & (percentage_array <= upper_bound)] = i\n",
        "          relative_intactness_array[invalid_elements] = nodatavalue\n",
        "          # Set all perfectly intact pixels (0 % change) to max score\n",
        "          relative_intactness_array[percentage_array == 0] = num_quantiles\n",
        "      export_array_as_tif(relative_intactness_array, relative_intactness_path)\n",
        "\n",
        "      # Prepare data for CSV: Collect lower and upper bounds for each category\n",
        "      ranges_data = {'Lower_Bound': [], 'Upper_Bound': []}\n",
        "      for i in range(1, num_quantiles + 1):\n",
        "          lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "          upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "          ranges_data['Lower_Bound'].append(lower_bound)\n",
        "          ranges_data['Upper_Bound'].append(upper_bound)\n",
        "\n",
        "      # Create DataFrame and save to CSV\n",
        "      relative_intactness_df = pd.DataFrame(ranges_data)\n",
        "      relative_intactness_csv_path = os.path.join(intactness_baseline_dist_dir, f'{relative_intactness_name}.csv')\n",
        "      relative_intactness_df.to_csv(relative_intactness_csv_path, index=False)\n",
        "\n",
        "      # Generate and save histogram as .png\n",
        "      histogram_path = join(intactness_baseline_dist_dir, f'{relative_intactness_name}.png')\n",
        "      plt.figure()\n",
        "      plt.hist(valid_elements.flatten(), bins='auto')\n",
        "      plt.title(f'{relative_intactness_name} Histogram')\n",
        "      plt.xlabel('Value')\n",
        "      plt.ylabel('Frequency')\n",
        "      plt.savefig(histogram_path)\n",
        "      plt.close()\n",
        "\n",
        "    else: print(f\"{relative_intactness_name} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nezNm_Gz3cnz"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kki0NPO3FKOw"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution upon completion\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "Xhx977-WIaWJ",
        "5lqjsLKZaQXo",
        "C9-hQ7G37uZi",
        "cIWo8ATZs-Ny"
      ],
      "gpuType": "V28",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}