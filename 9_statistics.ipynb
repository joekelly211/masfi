{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/9_statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j0th2jhtnAR"
      },
      "source": [
        "# Imports & Subdirectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9drq4euvG0MS"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGWgfUqto0F"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Imports and upgrades\n",
        "!pip install geopandas\n",
        "!pip install kaleido==0.2.1\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MTERlyxuPd4"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from datetime import datetime\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "import kaleido\n",
        "import math\n",
        "import numpy as np\n",
        "from os.path import exists, join\n",
        "from os import makedirs\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import rasterio\n",
        "from rasterio import mask as msk\n",
        "from sklearn.metrics import (\n",
        "    root_mean_squared_error,\n",
        "    r2_score,\n",
        "    accuracy_score,\n",
        "    log_loss,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    matthews_corrcoef,\n",
        "    balanced_accuracy_score,\n",
        "    average_precision_score,\n",
        "    cohen_kappa_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "from shutil import copyfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwar9sxbts-3"
      },
      "outputs": [],
      "source": [
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "datasets_dir = join(base_dir, \"4_datasets/final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "mask_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "differences_dir = join(base_dir, \"8_differences\")\n",
        "statistics_dir = join(base_dir, \"9_statistics\")\n",
        "sample_polygons_dir = join(statistics_dir, \"sample_polygons\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(statistics_dir, exist_ok=True)\n",
        "makedirs(sample_polygons_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atvGRSPYgwk9"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = [\"COMPRESS=DEFLATE\", \"PREDICTOR=3\", \"ZLEVEL=9\"]\n",
        "    else: options = []\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjy-T1TqScbE"
      },
      "source": [
        "# Select model, area and sample polygons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj5RbVSuEp-B"
      },
      "outputs": [],
      "source": [
        "# Select if to source predictions from scenarios_dir or uncertainty_dir\n",
        "source_dir = uncertainty_dir\n",
        "# source_dir = scenarios_dir\n",
        "source_dir_name = f\"{source_dir.split('_')[-1]}_dir\"\n",
        "\n",
        "# Select the model\n",
        "for subdir in os.listdir(source_dir):\n",
        "  if 'scenario_masks' not in subdir:\n",
        "    print(f\"selected_model = '{subdir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txTXPYFLpH6v"
      },
      "outputs": [],
      "source": [
        "selected_model = 'agbd_tekai_250625_003858'\n",
        "\n",
        "# Define prediction, disturbance and intactness directories\n",
        "selected_model_dir = join(models_dir, selected_model)\n",
        "selected_model_prediction_dir = join(source_dir, selected_model)\n",
        "if source_dir == scenarios_dir: prediction_raster_dir = join(selected_model_prediction_dir, 'scenario_predictions')\n",
        "if source_dir == uncertainty_dir: prediction_raster_dir = join(selected_model_prediction_dir, 'uncertainty_predictions')\n",
        "model_differences_dir = join(differences_dir, f\"{selected_model}_{source_dir_name}\")\n",
        "disturbance_dir = join(model_differences_dir, 'disturbance')\n",
        "intactness_dir = join(model_differences_dir, 'intactness')\n",
        "\n",
        "# Check prediction directory\n",
        "if not exists(prediction_raster_dir):\n",
        "  print(f\"Prediction directory doesn't exist yet: {prediction_raster_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(prediction_raster_dir))} rasters in {prediction_raster_dir}\")\n",
        "# Check disturbance directory\n",
        "if not exists(disturbance_dir):\n",
        "  print(f\"Disturbance directory doesn't exist yet: {disturbance_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(disturbance_dir))} rasters in {disturbance_dir}\")\n",
        "# Check intactness directory\n",
        "if not exists(intactness_dir):\n",
        "  print(f\"Intactness directory doesn't exist yet: {intactness_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(intactness_dir))} rasters in {intactness_dir}\")\n",
        "\n",
        "# Define model stats directory\n",
        "model_statistics_dir = join(statistics_dir, f\"{selected_model}_{source_dir_name}\")\n",
        "makedirs(model_statistics_dir, exist_ok=True)\n",
        "\n",
        "# Select sample area polygons. This should be a single .gpkg with the field 'name' differentiating polygons.\n",
        "sample_polygons = []\n",
        "for geopackage in os.listdir(sample_polygons_dir):\n",
        "  sample_polygons.append(geopackage)\n",
        "if len(sample_polygons) == 0:\n",
        "  print(f\"No sample areas found. Upload .gpkg polygons to {sample_polygons_dir}\")\n",
        "else:\n",
        "  for sample_polygon in sample_polygons: print(f\"selected_sample_polygons = '{sample_polygon}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwWvaX5qDANc"
      },
      "outputs": [],
      "source": [
        "selected_sample_polygons = 'tekai_sample_polygons.gpkg'\n",
        "\n",
        "# Load cell area raster for accurate pixel-by-pixel area calculations\n",
        "cell_area_path = join(areas_dir, \"cell_area.tif\")\n",
        "\n",
        "selected_sample_polygons_dir = join(sample_polygons_dir, selected_sample_polygons)\n",
        "selected_sample_polygons_gpkg = gpd.read_file(selected_sample_polygons_dir)\n",
        "sample_polygons_statistics_dir = join(model_statistics_dir, selected_sample_polygons[:-5])\n",
        "makedirs(sample_polygons_statistics_dir, exist_ok=True)\n",
        "agb_total_raster_dir = join(model_statistics_dir, 'agb_total_rasters')\n",
        "makedirs(agb_total_raster_dir, exist_ok=True)\n",
        "agb_total_scenario_dir = join(agb_total_raster_dir, 'scenarios')\n",
        "makedirs(agb_total_scenario_dir, exist_ok=True)\n",
        "agb_total_dist_dir = join(agb_total_raster_dir, 'disturbance')\n",
        "makedirs(agb_total_dist_dir, exist_ok=True)\n",
        "detailed_stats_by_area_dir = join(sample_polygons_statistics_dir, 'detailed_stats_by_area')\n",
        "makedirs(detailed_stats_by_area_dir, exist_ok=True)\n",
        "detailed_stats_by_scenario_dir = join(sample_polygons_statistics_dir, 'detailed_stats_by_scenario')\n",
        "makedirs(detailed_stats_by_scenario_dir, exist_ok=True)\n",
        "detailed_dist_stats_by_area_dir = join(sample_polygons_statistics_dir, 'detailed_dist_stats_by_area')\n",
        "makedirs(detailed_dist_stats_by_area_dir, exist_ok=True)\n",
        "detailed_dist_stats_by_scenario_dir = join(sample_polygons_statistics_dir, 'detailed_dist_stats_by_scenario')\n",
        "makedirs(detailed_dist_stats_by_scenario_dir, exist_ok=True)\n",
        "intactness_stats_dir = join(sample_polygons_statistics_dir, 'intactness')\n",
        "makedirs(intactness_stats_dir, exist_ok=True)\n",
        "\n",
        "# More intuitive data structure\n",
        "report_statistics_dir = join(sample_polygons_statistics_dir, 'report_statistics')\n",
        "makedirs(report_statistics_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPrMgTxDr3qE"
      },
      "source": [
        "# Convert AGBD to AGB total rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yYVaDPyr3T9"
      },
      "outputs": [],
      "source": [
        "## Takes account of variation in pixel size across latitude and longitude\n",
        "\n",
        "# List all raster files in source directories\n",
        "scenario_mean_rasters = []\n",
        "scenario_uncertainty_rasters = []\n",
        "dist_mean_rasters = []\n",
        "dist_uncertainty_rasters = []\n",
        "\n",
        "# Collect scenario rasters\n",
        "if os.path.exists(prediction_raster_dir):\n",
        "    for f in os.listdir(prediction_raster_dir):\n",
        "        if f.endswith('.tif'):\n",
        "          full_path = join(prediction_raster_dir, f)\n",
        "          if source_dir == uncertainty_dir:\n",
        "              if 'mean__' in f: scenario_mean_rasters.append(full_path)\n",
        "              elif 'uncertainty__' in f: scenario_uncertainty_rasters.append(full_path)\n",
        "          # scenarios_dir doesn't contain uncertainty rasters\n",
        "          else: scenario_mean_rasters.append(full_path)\n",
        "\n",
        "# Collect disturbance rasters\n",
        "if os.path.exists(disturbance_dir):\n",
        "    for f in os.listdir(disturbance_dir):\n",
        "        if f.endswith('.tif'):\n",
        "          full_path = join(disturbance_dir, f)\n",
        "          if source_dir == uncertainty_dir:\n",
        "              if 'mean__' in f: dist_mean_rasters.append(full_path)\n",
        "              elif 'uncertainty__' in f: dist_uncertainty_rasters.append(full_path)\n",
        "          # scenarios_dir doesn't contain uncertainty rasters\n",
        "          else: dist_mean_rasters.append(full_path)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "scenario_mean_rasters = sorted(scenario_mean_rasters)\n",
        "scenario_uncertainty_rasters = sorted(scenario_uncertainty_rasters)\n",
        "dist_mean_rasters = sorted(dist_mean_rasters)\n",
        "dist_uncertainty_rasters = sorted(dist_uncertainty_rasters)\n",
        "\n",
        "# Create lookup dictionaries for faster uncertainty matching\n",
        "scenario_uncertainty_lookup = {}\n",
        "for uncertainty_raster in scenario_uncertainty_rasters:\n",
        "    base_name = os.path.basename(uncertainty_raster).replace('uncertainty__', 'mean__')\n",
        "    scenario_uncertainty_lookup[base_name] = uncertainty_raster\n",
        "\n",
        "dist_uncertainty_lookup = {}\n",
        "for uncertainty_raster in dist_uncertainty_rasters:\n",
        "    base_name = os.path.basename(uncertainty_raster).replace('uncertainty__', 'mean__')\n",
        "    dist_uncertainty_lookup[base_name] = uncertainty_raster\n",
        "\n",
        "# Load cell area raster once for efficiency\n",
        "cell_area_ds = gdal.Open(cell_area_path)\n",
        "cell_area_array = cell_area_ds.ReadAsArray().astype(np.float64)\n",
        "# Convert cell area from m2 to ha\n",
        "cell_area_ha = cell_area_array / 10000\n",
        "\n",
        "# Function to process a batch of rasters\n",
        "def process_rasters(raster_paths, uncertainty_lookup, output_dir, is_disturbance=False):\n",
        "\n",
        "    progress_index = 0\n",
        "    progress_total = len(raster_paths)\n",
        "    raster_type = \"Disturbance\" if is_disturbance else \"Scenario\"\n",
        "    progress_label = widgets.Label(f\"{raster_type} rasters progress: {progress_index}/{progress_total}\")\n",
        "    display(progress_label)\n",
        "    print(f\"Processing {progress_total} {raster_type.lower()} rasters...\")\n",
        "\n",
        "    for raster_path in raster_paths:\n",
        "        base_filename = os.path.basename(raster_path)\n",
        "        # Extract scenario name based on source directory\n",
        "        if source_dir == uncertainty_dir: raster_name = base_filename.split('__')[1].split('.')[0]\n",
        "        else: raster_name = base_filename.split('__')[0].split('.')[0]\n",
        "        output_agb_mg = join(output_dir, f\"agb_total_mg__{raster_name}.tif\")\n",
        "        output_agb_ci95_mg = join(output_dir, f\"agb_total_ci95_mg__{raster_name}.tif\")\n",
        "\n",
        "        # Check if output files already exist\n",
        "        agb_exists = os.path.exists(output_agb_mg)\n",
        "        # Variables to store the raster data and mask\n",
        "        agbd_array = nodata = valid_mask = None\n",
        "\n",
        "        # Create total AGB raster if it doesn't exist\n",
        "        if not agb_exists:\n",
        "            # Read the mean raster\n",
        "            raster = gdal.Open(raster_path)\n",
        "            agbd_array = raster.ReadAsArray()\n",
        "            nodata = int(raster.GetRasterBand(1).GetNoDataValue())\n",
        "            # Create mask for valid data\n",
        "            valid_mask = (agbd_array != nodata)\n",
        "            # Calculate total AGB (Mg) = AGBD (Mg/ha) × pixel area (ha)\n",
        "            total_agb_mg = np.zeros_like(agbd_array, dtype='float64')\n",
        "            total_agb_mg[valid_mask] = agbd_array[valid_mask] * cell_area_ha[valid_mask]\n",
        "            total_agb_mg[~valid_mask] = nodata\n",
        "            # Export total AGB raster\n",
        "            export_array_as_tif(total_agb_mg, output_agb_mg, template=raster_path)\n",
        "\n",
        "        # Process uncertainty if available\n",
        "        if source_dir == uncertainty_dir:\n",
        "            base_name = os.path.basename(raster_path)\n",
        "            if base_name in uncertainty_lookup:\n",
        "                # Check if CI95 raster already exists\n",
        "                agb_ci95_exists = os.path.exists(output_agb_ci95_mg)\n",
        "                if not agb_ci95_exists:\n",
        "                    uncertainty_path = uncertainty_lookup[base_name]\n",
        "                    # Read uncertainty raster\n",
        "                    uncertainty_raster = gdal.Open(uncertainty_path)\n",
        "                    uncertainty_array = uncertainty_raster.ReadAsArray().astype(np.float64)\n",
        "                    # Read original raster if needed\n",
        "                    if agbd_array is None or valid_mask is None:\n",
        "                        raster = gdal.Open(raster_path)\n",
        "                        agbd_array = raster.ReadAsArray()\n",
        "                        nodata = int(raster.GetRasterBand(1).GetNoDataValue())\n",
        "                        valid_mask = (agbd_array != nodata)\n",
        "                    # Calculate uncertainty as proportion for CI95 calculation\n",
        "                    # Uncertainty is stored as percentage (0-100), divide by 100 to get proportion (0-1)\n",
        "                    uncertainty_proportion = uncertainty_array / 100\n",
        "                    # Calculate total AGB CI95 (Mg) = AGBD (Mg/ha) × uncertainty proportion × area (ha)\n",
        "                    total_agb_ci95_mg = np.zeros_like(agbd_array, dtype='float64')\n",
        "                    total_agb_ci95_mg[valid_mask] = np.abs(agbd_array[valid_mask]) * uncertainty_proportion[valid_mask] * cell_area_ha[valid_mask]\n",
        "                    total_agb_ci95_mg[~valid_mask] = nodata\n",
        "                    # Export total AGB CI95 raster\n",
        "                    export_array_as_tif(total_agb_ci95_mg, output_agb_ci95_mg, template=raster_path)\n",
        "\n",
        "                # Create CI95 per hectare subdirectory and export\n",
        "                agbd_ci95_mgha_dir = join(output_dir, 'agbd_ci95_mgha')\n",
        "                os.makedirs(agbd_ci95_mgha_dir, exist_ok=True)\n",
        "                output_agbd_ci95_mgha = join(agbd_ci95_mgha_dir, f\"agbd_ci95_mgha__{raster_name}.tif\")\n",
        "                agbd_ci95_mgha_exists = os.path.exists(output_agbd_ci95_mgha)\n",
        "\n",
        "                if not agbd_ci95_mgha_exists:\n",
        "                    # Use existing uncertainty data if already loaded\n",
        "                    if 'uncertainty_proportion' not in locals():\n",
        "                        uncertainty_path = uncertainty_lookup[base_name]\n",
        "                        uncertainty_raster = gdal.Open(uncertainty_path)\n",
        "                        uncertainty_array = uncertainty_raster.ReadAsArray().astype(np.float64)\n",
        "                        uncertainty_proportion = uncertainty_array / 100\n",
        "                    # Read original raster if needed\n",
        "                    if agbd_array is None or valid_mask is None:\n",
        "                        raster = gdal.Open(raster_path)\n",
        "                        agbd_array = raster.ReadAsArray()\n",
        "                        nodata = int(raster.GetRasterBand(1).GetNoDataValue())\n",
        "                        valid_mask = (agbd_array != nodata)\n",
        "                    # Calculate CI95 per hectare without area conversion\n",
        "                    agbd_ci95_mgha = np.zeros_like(agbd_array, dtype='float64')\n",
        "                    agbd_ci95_mgha[valid_mask] = np.abs(agbd_array[valid_mask]) * uncertainty_proportion[valid_mask]\n",
        "                    agbd_ci95_mgha[~valid_mask] = nodata\n",
        "                    # Export CI95 per hectare raster\n",
        "                    export_array_as_tif(agbd_ci95_mgha, output_agbd_ci95_mgha, template=raster_path)\n",
        "\n",
        "        # Update progress\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"{raster_type} rasters progress: {progress_index}/{progress_total}\"\n",
        "    return progress_total\n",
        "\n",
        "# Process scenario and disturbance rasters\n",
        "scenario_count = process_rasters(scenario_mean_rasters, scenario_uncertainty_lookup, agb_total_scenario_dir)\n",
        "dist_count = process_rasters(dist_mean_rasters, dist_uncertainty_lookup, agb_total_dist_dir, is_disturbance=True)\n",
        "\n",
        "print(f\"Processed {scenario_count} scenario rasters and {dist_count} disturbance rasters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlQ2iLAiwQoB"
      },
      "source": [
        "# Scenario statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgC9laZXPB9Y"
      },
      "outputs": [],
      "source": [
        "# Create list of available AGB total rasters and extract scenarios\n",
        "scenarios = set()\n",
        "for agb_total_raster in os.listdir(agb_total_scenario_dir):\n",
        "    if agb_total_raster.endswith('.tif') and 'agb_total_mg__' in agb_total_raster:\n",
        "        scenario_name = agb_total_raster.split(\"agb_total_mg__\")[1].split('.')[0]\n",
        "        scenarios.add(scenario_name)\n",
        "\n",
        "scenarios = sorted(list(scenarios))\n",
        "\n",
        "# Select scenario predictions to calculate statistics\n",
        "print('selected_scenarios = [')\n",
        "for scenario in scenarios:\n",
        "    print(f'  \"{scenario}\",')\n",
        "print(']\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utWU_0AEG0UX"
      },
      "outputs": [],
      "source": [
        "selected_scenarios = [\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_no_degradation_since_1993\",\n",
        "  \"2021_no_disturbance_since_1993\",\n",
        "  \"2021_oldgrowth\",\n",
        "  \"2021_oldgrowth_1\",\n",
        "  \"2021_oldgrowth_2\",\n",
        "  \"2021_oldgrowth_all_land\",\n",
        "  \"2021_oldgrowth_all_land_1\",\n",
        "  \"2021_oldgrowth_all_land_2\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_no_degradation_since_1996\",\n",
        "  \"2024_no_disturbance_since_1996\",\n",
        "  \"2024_no_disturbance_since_1997\",\n",
        "  \"2024_no_disturbance_since_1998\",\n",
        "  \"2024_no_disturbance_since_1999\",\n",
        "  \"2024_no_disturbance_since_2000\",\n",
        "  \"2024_no_disturbance_since_2001\",\n",
        "  \"2024_no_disturbance_since_2002\",\n",
        "  \"2024_no_disturbance_since_2003\",\n",
        "  \"2024_no_disturbance_since_2004\",\n",
        "  \"2024_no_disturbance_since_2005\",\n",
        "  \"2024_no_disturbance_since_2006\",\n",
        "  \"2024_no_disturbance_since_2007\",\n",
        "  \"2024_no_disturbance_since_2008\",\n",
        "  \"2024_no_disturbance_since_2009\",\n",
        "  \"2024_no_disturbance_since_2010\",\n",
        "  \"2024_no_disturbance_since_2011\",\n",
        "  \"2024_no_disturbance_since_2012\",\n",
        "  \"2024_no_disturbance_since_2013\",\n",
        "  \"2024_no_disturbance_since_2014\",\n",
        "  \"2024_no_disturbance_since_2015\",\n",
        "  \"2024_no_disturbance_since_2016\",\n",
        "  \"2024_no_disturbance_since_2017\",\n",
        "  \"2024_no_disturbance_since_2018\",\n",
        "  \"2024_no_disturbance_since_2019\",\n",
        "  \"2024_no_disturbance_since_2020\",\n",
        "  \"2024_no_disturbance_since_2021\",\n",
        "  \"2024_no_disturbance_since_2022\",\n",
        "  \"2024_no_disturbance_since_2023\",\n",
        "  \"2024_no_disturbance_since_2024\",\n",
        "  \"2024_oldgrowth\",\n",
        "  \"2024_oldgrowth_1\",\n",
        "  \"2024_oldgrowth_2\",\n",
        "  \"2024_oldgrowth_all_land\",\n",
        "  \"2024_oldgrowth_all_land_1\",\n",
        "  \"2024_oldgrowth_all_land_2\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]\n",
        "\n",
        "# Build lists of AGB total rasters for selected scenarios\n",
        "agb_total_rasters = []\n",
        "\n",
        "for scenario in selected_scenarios:\n",
        "    agb_total_path = join(agb_total_scenario_dir, f\"agb_total_mg__{scenario}.tif\")\n",
        "    if os.path.exists(agb_total_path):\n",
        "        agb_total_rasters.append(agb_total_path)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "agb_total_rasters = sorted(agb_total_rasters)\n",
        "\n",
        "# Toggle whether to generate uncertainty stats (only possible with uncertainty_dir)\n",
        "generate_uncertainty_stats = (source_dir == uncertainty_dir)\n",
        "\n",
        "# Pre-allocate arrays for statistics\n",
        "polygon_names = [row[\"name\"] for _, row in selected_sample_polygons_gpkg.iterrows()]\n",
        "n_polygons = len(polygon_names)\n",
        "n_scenarios = len(agb_total_rasters)\n",
        "\n",
        "forest_cover_data = np.zeros((n_scenarios, n_polygons))\n",
        "agbd_mean_data = np.zeros((n_scenarios, n_polygons))\n",
        "agbd_stdev_data = np.zeros((n_scenarios, n_polygons))\n",
        "agb_total_data = np.zeros((n_scenarios, n_polygons))\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    agbd_mean_ci95_data = np.zeros((n_scenarios, n_polygons))\n",
        "    agbd_mean_uncertainty_data = np.zeros((n_scenarios, n_polygons))\n",
        "    agb_total_ci95_data = np.zeros((n_scenarios, n_polygons))\n",
        "\n",
        "# Open AGB total raster datasets\n",
        "agb_total_datasets = {path: rasterio.open(path) for path in agb_total_rasters}\n",
        "\n",
        "# Open AGB total CI95 datasets only if uncertainty stats are generated\n",
        "agb_total_ci95_datasets = {}\n",
        "if generate_uncertainty_stats:\n",
        "    for agb_total_raster in agb_total_rasters:\n",
        "        scenario_name = os.path.basename(agb_total_raster).split('agb_total_mg__')[1].split('.')[0]\n",
        "        agb_total_ci95_path = join(agb_total_scenario_dir, f\"agb_total_ci95_mg__{scenario_name}.tif\")\n",
        "        if os.path.exists(agb_total_ci95_path):\n",
        "            agb_total_ci95_datasets[agb_total_raster] = rasterio.open(agb_total_ci95_path)\n",
        "\n",
        "# Load cell area raster once for all calculations\n",
        "cell_area_dataset = rasterio.open(cell_area_path)\n",
        "\n",
        "try:\n",
        "    # Initialise polygon area dataframe\n",
        "    df_polygon_area_km2 = pd.DataFrame(columns=[\"Name\", \"Area (km^2)\"])\n",
        "\n",
        "    # Loop through each polygon to generate statistics\n",
        "    for poly_idx, (index, row) in enumerate(selected_sample_polygons_gpkg.iterrows()):\n",
        "\n",
        "        # Define the polygon\n",
        "        sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"]\n",
        "        polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "        # Mask the cell area raster to the polygon once\n",
        "        cell_area_masked, transform_1 = msk.mask(cell_area_dataset, polygons, crop=True, filled=False)\n",
        "\n",
        "        # Calculate total area of all pixels within polygon in hectares\n",
        "        pixel_area_sum_m2 = np.ma.sum(cell_area_masked, dtype='float64')\n",
        "        pixel_area_sum_ha = pixel_area_sum_m2 / 10000\n",
        "\n",
        "        # Convert cell areas from m2 to ha\n",
        "        cell_area_masked_ha = cell_area_masked / 10000\n",
        "\n",
        "        # Add polygon area to dataframe\n",
        "        new_row = pd.DataFrame([{'Name': sample_polygon_name, 'Area (km^2)': pixel_area_sum_ha / 100}], dtype=object)\n",
        "        df_polygon_area_km2 = pd.concat([df_polygon_area_km2, new_row], ignore_index=True, sort=False)\n",
        "\n",
        "        # Loop through AGB total rasters\n",
        "        for raster_idx, agb_total_raster in enumerate(agb_total_rasters):\n",
        "\n",
        "            # Mask AGB total raster to polygon\n",
        "            agb_total = agb_total_datasets[agb_total_raster]\n",
        "            agb_total_array_masked, transform_2 = msk.mask(agb_total, polygons, crop=True, filled=False)\n",
        "\n",
        "            # Extract forest pixels from valid AGB total pixels\n",
        "            forest_pixels_mask = ~np.ma.getmaskarray(agb_total_array_masked)\n",
        "\n",
        "            # Calculate forest area by summing cell areas of forest pixels\n",
        "            forest_cell_areas_ha = np.ma.array(cell_area_masked_ha.data, mask=~forest_pixels_mask)\n",
        "            forest_cover_ha = np.ma.sum(forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "            # Sum total AGB in Mg\n",
        "            agb_total_mg = np.ma.sum(agb_total_array_masked, dtype='float64')\n",
        "\n",
        "            # Calculate statistics with masked value handling\n",
        "            if np.ma.is_masked(agb_total_mg) or forest_cover_ha <= 0:\n",
        "                agbd_mean_mg_ha = 0.0\n",
        "                agbd_mean_stdev_ha = 0.0\n",
        "                agb_total_tg = 0.0\n",
        "            else:\n",
        "                # Calculate area-weighted mean AGBD\n",
        "                agbd_mean_mg_ha = agb_total_mg / forest_cover_ha\n",
        "\n",
        "                # Back-calculate individual AGBD values for standard deviation\n",
        "                agbd_values = agb_total_array_masked / cell_area_masked_ha\n",
        "                valid_agbd = agbd_values[forest_pixels_mask]\n",
        "                valid_areas = cell_area_masked_ha[forest_pixels_mask]\n",
        "\n",
        "                # Calculate area-weighted standard deviation\n",
        "                variance_weighted = np.sum(valid_areas * (valid_agbd - agbd_mean_mg_ha)**2) / forest_cover_ha\n",
        "                agbd_mean_stdev_ha = np.sqrt(variance_weighted)\n",
        "\n",
        "                # Convert total AGB from Mg to Tg\n",
        "                agb_total_tg = agb_total_mg / 1000000\n",
        "\n",
        "            # Store results in pre-allocated arrays\n",
        "            forest_cover_data[raster_idx, poly_idx] = forest_cover_ha\n",
        "            agbd_mean_data[raster_idx, poly_idx] = agbd_mean_mg_ha\n",
        "            agbd_stdev_data[raster_idx, poly_idx] = agbd_mean_stdev_ha\n",
        "            agb_total_data[raster_idx, poly_idx] = agb_total_tg\n",
        "\n",
        "            if generate_uncertainty_stats and agb_total_raster in agb_total_ci95_datasets:\n",
        "                # Get total AGB CI95 from pre-calculated raster\n",
        "                agb_total_ci95_raster = agb_total_ci95_datasets[agb_total_raster]\n",
        "                agb_total_ci95_array_masked, _ = msk.mask(agb_total_ci95_raster, polygons, crop=True, filled=False)\n",
        "\n",
        "                # Sum total AGB CI95 in Mg\n",
        "                agb_total_ci95_mg = abs(np.ma.sum(agb_total_ci95_array_masked, dtype='float64'))\n",
        "\n",
        "                # Calculate uncertainty statistics\n",
        "                if abs(agb_total_mg) > 0:\n",
        "                    agbd_mean_mg_ha_ci95 = agb_total_ci95_mg / forest_cover_ha\n",
        "                    agbd_mean_mg_ha_uncertainty = agb_total_ci95_mg / abs(agb_total_mg)) * 100\n",
        "                else:\n",
        "                    agbd_mean_mg_ha_ci95 = 0\n",
        "                    agbd_mean_mg_ha_uncertainty = 0\n",
        "\n",
        "                # Convert total AGB CI95 from Mg to Tg\n",
        "                agb_total_tg_ci95 = agb_total_ci95_mg / 1000000\n",
        "\n",
        "                # Store uncertainty results\n",
        "                agbd_mean_ci95_data[raster_idx, poly_idx] = agbd_mean_mg_ha_ci95\n",
        "                agbd_mean_uncertainty_data[raster_idx, poly_idx] = agbd_mean_mg_ha_uncertainty\n",
        "                agb_total_ci95_data[raster_idx, poly_idx] = agb_total_tg_ci95\n",
        "\n",
        "finally:\n",
        "    # Close all opened datasets\n",
        "    cell_area_dataset.close()\n",
        "    for dataset in agb_total_datasets.values():\n",
        "        dataset.close()\n",
        "    for dataset in agb_total_ci95_datasets.values():\n",
        "        dataset.close()\n",
        "\n",
        "# Create DataFrames from pre-allocated arrays\n",
        "df_forest_cover_ha = pd.DataFrame(forest_cover_data, index=selected_scenarios, columns=polygon_names)\n",
        "df_forest_cover_ha.rename_axis('scenario', inplace=True)\n",
        "\n",
        "df_agbd_mean_mg_ha = pd.DataFrame(agbd_mean_data, index=selected_scenarios, columns=polygon_names)\n",
        "df_agbd_mean_mg_ha.rename_axis('scenario', inplace=True)\n",
        "\n",
        "df_agbd_stdev_mg_ha = pd.DataFrame(agbd_stdev_data, index=selected_scenarios, columns=polygon_names)\n",
        "df_agbd_stdev_mg_ha.rename_axis('scenario', inplace=True)\n",
        "\n",
        "df_agb_total_tg = pd.DataFrame(agb_total_data, index=selected_scenarios, columns=polygon_names)\n",
        "df_agb_total_tg.rename_axis('scenario', inplace=True)\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    df_agbd_mean_mg_ha_ci95 = pd.DataFrame(agbd_mean_ci95_data, index=selected_scenarios, columns=polygon_names)\n",
        "    df_agbd_mean_mg_ha_ci95.rename_axis('scenario', inplace=True)\n",
        "\n",
        "    df_agbd_mean_mg_ha_uncertainty = pd.DataFrame(agbd_mean_uncertainty_data, index=selected_scenarios, columns=polygon_names)\n",
        "    df_agbd_mean_mg_ha_uncertainty.rename_axis('scenario', inplace=True)\n",
        "\n",
        "    df_agb_total_tg_ci95 = pd.DataFrame(agb_total_ci95_data, index=selected_scenarios, columns=polygon_names)\n",
        "    df_agb_total_tg_ci95.rename_axis('scenario', inplace=True)\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "    df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_mean_mg_ha_ci95,\n",
        "                     df_agbd_mean_mg_ha_uncertainty, df_agbd_stdev_mg_ha, df_agb_total_tg, df_agb_total_tg_ci95]\n",
        "else:\n",
        "    df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Set index of the polygon area km2 dataframe to 'Name' of the polygon\n",
        "df_polygon_area_km2 = df_polygon_area_km2.set_index('Name')\n",
        "\n",
        "# Generate summary stats\n",
        "df_forest_cover_ha_t = df_forest_cover_ha.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest cover (ha)\")\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB (Tg)\")\n",
        "\n",
        "# Use list for efficient concatenation\n",
        "summary_components = [df_polygon_area_km2, df_forest_cover_ha_t, df_agb_total_tg_t]\n",
        "if generate_uncertainty_stats:\n",
        "    df_agb_total_tg_ci95_t = df_agb_total_tg_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB CI95 (Tg)\")\n",
        "    summary_components.append(df_agb_total_tg_ci95_t)\n",
        "\n",
        "summary_stats = pd.concat(summary_components, axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_stats.to_csv(join(sample_polygons_statistics_dir, 'summary_stats.csv'))\n",
        "\n",
        "# Generate empty dataframes for statistics\n",
        "df_base = pd.DataFrame(index=selected_scenarios)\n",
        "df_base.rename_axis('scenario', inplace=True)\n",
        "\n",
        "# Generate detailed stats by area\n",
        "for polygon_area in polygon_names:\n",
        "    polygon_area_km2 = df_polygon_area_km2.loc[polygon_area][\"Area (km^2)\"]\n",
        "    df_detailed_stats = df_base.copy()\n",
        "    df_detailed_stats[\"Area (km^2)\"] = polygon_area_km2\n",
        "    for df_stats in df_stats_list:\n",
        "        if df_stats.equals(df_forest_cover_ha): stat_col = \"Forest cover (ha)\"\n",
        "        if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"Forest AGBD mean (Mg / ha)\"\n",
        "        if df_stats.equals(df_agbd_stdev_mg_ha): stat_col = \"Forest AGBD stdev (Mg / ha)\"\n",
        "        if df_stats.equals(df_agb_total_tg): stat_col = \"Forest AGB total (Tg)\"\n",
        "        if generate_uncertainty_stats:\n",
        "            if df_stats.equals(df_agbd_mean_mg_ha_ci95): stat_col = \"Forest AGBD CI95 (Mg / ha)\"\n",
        "            if df_stats.equals(df_agbd_mean_mg_ha_uncertainty): stat_col = \"Forest AGBD uncertainty (%)\"\n",
        "            if df_stats.equals(df_agb_total_tg_ci95): stat_col = \"Forest AGB total CI95 (Tg)\"\n",
        "        for stats_polygon_area in df_stats:\n",
        "            if stats_polygon_area == polygon_area:\n",
        "                df_stats_renamed = df_stats.rename(columns={stats_polygon_area:stat_col})\n",
        "                df_detailed_stats = pd.concat([df_detailed_stats, df_stats_renamed[stat_col]], axis=1)\n",
        "    df_detailed_stats.to_csv(join(detailed_stats_by_area_dir, f'{polygon_area}.csv'))\n",
        "\n",
        "# Generate detailed stats by scenario - build dictionary once then process\n",
        "scenarios = {}\n",
        "for stats_csv in os.listdir(detailed_stats_by_area_dir):\n",
        "    polygon_name = f\"{stats_csv[:-4]}\"\n",
        "    stats_csv_path = join(detailed_stats_by_area_dir, stats_csv)\n",
        "    stats_csv_df = pd.read_csv(stats_csv_path)\n",
        "    # Process all scenarios for this polygon in one pass\n",
        "    for scenario in stats_csv_df['scenario'].unique():\n",
        "        scenario_df = stats_csv_df[stats_csv_df['scenario'] == scenario].copy()\n",
        "        scenario_df.drop('scenario', axis=1, inplace=True)\n",
        "        scenario_df.insert(0, 'Name', polygon_name)\n",
        "        if scenario in scenarios:\n",
        "            scenarios[scenario] = pd.concat([scenarios[scenario], scenario_df], ignore_index=True)\n",
        "        else:\n",
        "            scenarios[scenario] = scenario_df\n",
        "\n",
        "# Write all scenario CSVs\n",
        "for scenario, scenario_df in scenarios.items():\n",
        "    output_file_path = join(detailed_stats_by_scenario_dir,f'{scenario}.csv')\n",
        "    scenario_df.to_csv(output_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHEWz3QcGh1b"
      },
      "source": [
        "# Disturbance statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPm9wZwJBJnq"
      },
      "outputs": [],
      "source": [
        "# Create list of available AGB total disturbance rasters and extract disturbances\n",
        "dists = set()\n",
        "for agb_total_dist_raster in os.listdir(agb_total_dist_dir):\n",
        "    if agb_total_dist_raster.endswith('.tif') and 'agb_total_mg__' in agb_total_dist_raster:\n",
        "        dist_name = agb_total_dist_raster.split(\"agb_total_mg__\")[1].split('.')[0]\n",
        "        dists.add(dist_name)\n",
        "\n",
        "dists = sorted(list(dists))\n",
        "\n",
        "# Select disturbance rasters to calculate statistics\n",
        "print('selected_dists = [')\n",
        "for dist in dists:\n",
        "    print(f'  \"{dist}\",')\n",
        "print(']')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm_v5xr2G4sV"
      },
      "outputs": [],
      "source": [
        "selected_dists = [\n",
        "  \"2021_deforestation_since_1993\",\n",
        "  \"2021_deforestation_since_oldgrowth\",\n",
        "  \"2021_degradation_since_1993\",\n",
        "  \"2021_degradation_since_oldgrowth\",\n",
        "  \"2021_disturbance_since_1993\",\n",
        "  \"2021_disturbance_since_oldgrowth\",\n",
        "  \"2024_deforestation_of_road_mat_daling_2023\",\n",
        "  \"2024_deforestation_since_1996\",\n",
        "  \"2024_deforestation_since_oldgrowth\",\n",
        "  \"2024_degradation_since_1996\",\n",
        "  \"2024_degradation_since_oldgrowth\",\n",
        "  \"2024_disturbance_since_1996\",\n",
        "  \"2024_disturbance_since_1997\",\n",
        "  \"2024_disturbance_since_1998\",\n",
        "  \"2024_disturbance_since_1999\",\n",
        "  \"2024_disturbance_since_2000\",\n",
        "  \"2024_disturbance_since_2001\",\n",
        "  \"2024_disturbance_since_2002\",\n",
        "  \"2024_disturbance_since_2003\",\n",
        "  \"2024_disturbance_since_2004\",\n",
        "  \"2024_disturbance_since_2005\",\n",
        "  \"2024_disturbance_since_2006\",\n",
        "  \"2024_disturbance_since_2007\",\n",
        "  \"2024_disturbance_since_2008\",\n",
        "  \"2024_disturbance_since_2009\",\n",
        "  \"2024_disturbance_since_2010\",\n",
        "  \"2024_disturbance_since_2011\",\n",
        "  \"2024_disturbance_since_2012\",\n",
        "  \"2024_disturbance_since_2013\",\n",
        "  \"2024_disturbance_since_2014\",\n",
        "  \"2024_disturbance_since_2015\",\n",
        "  \"2024_disturbance_since_2016\",\n",
        "  \"2024_disturbance_since_2017\",\n",
        "  \"2024_disturbance_since_2018\",\n",
        "  \"2024_disturbance_since_2019\",\n",
        "  \"2024_disturbance_since_2020\",\n",
        "  \"2024_disturbance_since_2021\",\n",
        "  \"2024_disturbance_since_2022\",\n",
        "  \"2024_disturbance_since_2023\",\n",
        "  \"2024_disturbance_since_2024\",\n",
        "  \"2024_disturbance_since_oldgrowth\",\n",
        "  \"2024_effect_of_disturbance_in_1996\",\n",
        "  \"2024_effect_of_disturbance_in_1997\",\n",
        "  \"2024_effect_of_disturbance_in_1998\",\n",
        "  \"2024_effect_of_disturbance_in_1999\",\n",
        "  \"2024_effect_of_disturbance_in_2000\",\n",
        "  \"2024_effect_of_disturbance_in_2001\",\n",
        "  \"2024_effect_of_disturbance_in_2002\",\n",
        "  \"2024_effect_of_disturbance_in_2003\",\n",
        "  \"2024_effect_of_disturbance_in_2004\",\n",
        "  \"2024_effect_of_disturbance_in_2005\",\n",
        "  \"2024_effect_of_disturbance_in_2006\",\n",
        "  \"2024_effect_of_disturbance_in_2007\",\n",
        "  \"2024_effect_of_disturbance_in_2008\",\n",
        "  \"2024_effect_of_disturbance_in_2009\",\n",
        "  \"2024_effect_of_disturbance_in_2010\",\n",
        "  \"2024_effect_of_disturbance_in_2011\",\n",
        "  \"2024_effect_of_disturbance_in_2012\",\n",
        "  \"2024_effect_of_disturbance_in_2013\",\n",
        "  \"2024_effect_of_disturbance_in_2014\",\n",
        "  \"2024_effect_of_disturbance_in_2015\",\n",
        "  \"2024_effect_of_disturbance_in_2016\",\n",
        "  \"2024_effect_of_disturbance_in_2017\",\n",
        "  \"2024_effect_of_disturbance_in_2018\",\n",
        "  \"2024_effect_of_disturbance_in_2019\",\n",
        "  \"2024_effect_of_disturbance_in_2020\",\n",
        "  \"2024_effect_of_disturbance_in_2021\",\n",
        "  \"2024_effect_of_disturbance_in_2022\",\n",
        "  \"2024_effect_of_disturbance_in_2023\",\n",
        "  \"2024_effect_of_disturbance_in_2024\",\n",
        "]\n",
        "\n",
        "# Build lists of AGB total disturbance rasters for selected disturbances\n",
        "agb_total_dist_rasters = []\n",
        "\n",
        "for dist in selected_dists:\n",
        "    agb_total_path = join(agb_total_dist_dir, f\"agb_total_mg__{dist}.tif\")\n",
        "    if os.path.exists(agb_total_path):\n",
        "        agb_total_dist_rasters.append(agb_total_path)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "agb_total_dist_rasters = sorted(agb_total_dist_rasters)\n",
        "\n",
        "# Toggle whether to generate uncertainty stats (only possible with uncertainty_dir)\n",
        "generate_uncertainty_stats = (source_dir == uncertainty_dir)\n",
        "\n",
        "# Pre-allocate arrays for statistics\n",
        "polygon_names = [row[\"name\"] for _, row in selected_sample_polygons_gpkg.iterrows()]\n",
        "n_polygons = len(polygon_names)\n",
        "n_dists = len(agb_total_dist_rasters)\n",
        "\n",
        "agbd_mean_data = np.zeros((n_dists, n_polygons))\n",
        "agbd_stdev_data = np.zeros((n_dists, n_polygons))\n",
        "agb_total_data = np.zeros((n_dists, n_polygons))\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    agbd_mean_ci95_data = np.zeros((n_dists, n_polygons))\n",
        "    agbd_mean_uncertainty_data = np.zeros((n_dists, n_polygons))\n",
        "    agb_total_ci95_data = np.zeros((n_dists, n_polygons))\n",
        "\n",
        "# Open AGB total disturbance raster datasets\n",
        "agb_total_dist_datasets = {path: rasterio.open(path) for path in agb_total_dist_rasters}\n",
        "\n",
        "# Open AGB total CI95 disturbance datasets only if uncertainty stats are generated\n",
        "agb_total_ci95_dist_datasets = {}\n",
        "if generate_uncertainty_stats:\n",
        "    for agb_total_dist_raster in agb_total_dist_rasters:\n",
        "        dist_name = os.path.basename(agb_total_dist_raster).split('agb_total_mg__')[1].split('.')[0]\n",
        "        agb_total_ci95_path = join(agb_total_dist_dir, f\"agb_total_ci95_mg__{dist_name}.tif\")\n",
        "        if os.path.exists(agb_total_ci95_path):\n",
        "            agb_total_ci95_dist_datasets[agb_total_dist_raster] = rasterio.open(agb_total_ci95_path)\n",
        "\n",
        "# Load cell area raster once for all calculations\n",
        "cell_area_dataset = rasterio.open(cell_area_path)\n",
        "\n",
        "try:\n",
        "    # Initialise polygon area dataframe\n",
        "    df_polygon_area_km2 = pd.DataFrame(columns=[\"Name\", \"Area (km^2)\"])\n",
        "\n",
        "    # Loop through each polygon to generate statistics\n",
        "    for poly_idx, (index, row) in enumerate(selected_sample_polygons_gpkg.iterrows()):\n",
        "\n",
        "        # Define the polygon\n",
        "        sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"]\n",
        "        polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "        # Mask the cell area raster to the polygon once\n",
        "        cell_area_masked, transform_1 = msk.mask(cell_area_dataset, polygons, crop=True, filled=False)\n",
        "\n",
        "        # Calculate total area of all pixels within polygon in hectares\n",
        "        pixel_area_sum_m2 = np.ma.sum(cell_area_masked, dtype='float64')\n",
        "        pixel_area_sum_ha = pixel_area_sum_m2 / 10000\n",
        "\n",
        "        # Convert cell areas from m2 to ha\n",
        "        cell_area_masked_ha = cell_area_masked / 10000\n",
        "\n",
        "        # Add polygon area to dataframe\n",
        "        new_row = pd.DataFrame([{'Name': sample_polygon_name, 'Area (km^2)': pixel_area_sum_ha / 100}], dtype=object)\n",
        "        df_polygon_area_km2 = pd.concat([df_polygon_area_km2, new_row], ignore_index=True, sort=False)\n",
        "\n",
        "        # Loop through AGB total disturbance rasters\n",
        "        for raster_idx, agb_total_dist_raster in enumerate(agb_total_dist_rasters):\n",
        "\n",
        "            # Mask AGB total disturbance raster to polygon\n",
        "            agb_total_dist = agb_total_dist_datasets[agb_total_dist_raster]\n",
        "            agb_total_array_masked, transform_2 = msk.mask(agb_total_dist, polygons, crop=True, filled=False)\n",
        "\n",
        "            # Extract forest pixels from valid AGB total pixels\n",
        "            forest_pixels_mask = ~np.ma.getmaskarray(agb_total_array_masked)\n",
        "\n",
        "            # Calculate forest area by summing cell areas of forest pixels\n",
        "            forest_cell_areas_ha = np.ma.array(cell_area_masked_ha.data, mask=~forest_pixels_mask)\n",
        "            forest_cover_ha = np.ma.sum(forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "            # Sum total AGB in Mg\n",
        "            agb_total_mg = np.ma.sum(agb_total_array_masked, dtype='float64')\n",
        "\n",
        "            # Calculate statistics with masked value handling\n",
        "            if np.ma.is_masked(agb_total_mg) or forest_cover_ha <= 0:\n",
        "                agbd_mean_mg_ha = 0.0\n",
        "                agbd_mean_stdev_ha = 0.0\n",
        "                agb_total_tg = 0.0\n",
        "            else:\n",
        "                # Calculate area-weighted mean AGBD\n",
        "                agbd_mean_mg_ha = agb_total_mg / forest_cover_ha\n",
        "\n",
        "                # Back-calculate individual AGBD values for standard deviation\n",
        "                agbd_values = agb_total_array_masked / cell_area_masked_ha\n",
        "                valid_agbd = agbd_values[forest_pixels_mask]\n",
        "                valid_areas = cell_area_masked_ha[forest_pixels_mask]\n",
        "\n",
        "                # Calculate area-weighted standard deviation\n",
        "                variance_weighted = np.sum(valid_areas * (valid_agbd - agbd_mean_mg_ha)**2) / forest_cover_ha\n",
        "                agbd_mean_stdev_ha = np.sqrt(variance_weighted)\n",
        "\n",
        "                # Convert total AGB from Mg to Tg\n",
        "                agb_total_tg = agb_total_mg / 1000000\n",
        "\n",
        "            # Store results in pre-allocated arrays\n",
        "            agbd_mean_data[raster_idx, poly_idx] = agbd_mean_mg_ha\n",
        "            agbd_stdev_data[raster_idx, poly_idx] = agbd_mean_stdev_ha\n",
        "            agb_total_data[raster_idx, poly_idx] = agb_total_tg\n",
        "\n",
        "            if generate_uncertainty_stats and agb_total_dist_raster in agb_total_ci95_dist_datasets:\n",
        "                # Get total AGB CI95 from pre-calculated raster\n",
        "                agb_total_ci95_raster = agb_total_ci95_dist_datasets[agb_total_dist_raster]\n",
        "                agb_total_ci95_array_masked, _ = msk.mask(agb_total_ci95_raster, polygons, crop=True, filled=False)\n",
        "\n",
        "                # Sum total AGB CI95 in Mg\n",
        "                agb_total_ci95_mg = abs(np.ma.sum(agb_total_ci95_array_masked, dtype='float64'))\n",
        "\n",
        "                # Calculate uncertainty statistics\n",
        "                if abs(agb_total_mg) > 0:\n",
        "                    agbd_mean_mg_ha_ci95 = agb_total_ci95_mg / forest_cover_ha\n",
        "                    agbd_mean_mg_ha_uncertainty = agb_total_ci95_mg / abs(agb_total_mg) * 100\n",
        "                else:\n",
        "                    agbd_mean_mg_ha_ci95 = 0\n",
        "                    agbd_mean_mg_ha_uncertainty = 0\n",
        "\n",
        "                # Convert total AGB CI95 from Mg to Tg\n",
        "                agb_total_tg_ci95 = agb_total_ci95_mg / 1000000\n",
        "\n",
        "                # Store uncertainty results\n",
        "                agbd_mean_ci95_data[raster_idx, poly_idx] = agbd_mean_mg_ha_ci95\n",
        "                agbd_mean_uncertainty_data[raster_idx, poly_idx] = agbd_mean_mg_ha_uncertainty\n",
        "                agb_total_ci95_data[raster_idx, poly_idx] = agb_total_tg_ci95\n",
        "\n",
        "finally:\n",
        "    # Close all opened datasets\n",
        "    cell_area_dataset.close()\n",
        "    for dataset in agb_total_dist_datasets.values():\n",
        "        dataset.close()\n",
        "    for dataset in agb_total_ci95_dist_datasets.values():\n",
        "        dataset.close()\n",
        "\n",
        "# Create DataFrames from pre-allocated arrays\n",
        "df_agbd_mean_mg_ha = pd.DataFrame(agbd_mean_data, index=selected_dists, columns=polygon_names)\n",
        "df_agbd_mean_mg_ha.rename_axis('dist', inplace=True)\n",
        "\n",
        "df_agbd_stdev_mg_ha = pd.DataFrame(agbd_stdev_data, index=selected_dists, columns=polygon_names)\n",
        "df_agbd_stdev_mg_ha.rename_axis('dist', inplace=True)\n",
        "\n",
        "df_agb_total_tg = pd.DataFrame(agb_total_data, index=selected_dists, columns=polygon_names)\n",
        "df_agb_total_tg.rename_axis('dist', inplace=True)\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    df_agbd_mean_mg_ha_ci95 = pd.DataFrame(agbd_mean_ci95_data, index=selected_dists, columns=polygon_names)\n",
        "    df_agbd_mean_mg_ha_ci95.rename_axis('dist', inplace=True)\n",
        "\n",
        "    df_agbd_mean_mg_ha_uncertainty = pd.DataFrame(agbd_mean_uncertainty_data, index=selected_dists, columns=polygon_names)\n",
        "    df_agbd_mean_mg_ha_uncertainty.rename_axis('dist', inplace=True)\n",
        "\n",
        "    df_agb_total_tg_ci95 = pd.DataFrame(agb_total_ci95_data, index=selected_dists, columns=polygon_names)\n",
        "    df_agb_total_tg_ci95.rename_axis('dist', inplace=True)\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "    df_stats_list = [df_agbd_mean_mg_ha, df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agbd_stdev_mg_ha, df_agb_total_tg, df_agb_total_tg_ci95]\n",
        "else:\n",
        "    df_stats_list = [df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Set index of the polygon area km2 dataframe to 'Name' of the polygon\n",
        "df_polygon_area_km2 = df_polygon_area_km2.set_index('Name')\n",
        "\n",
        "# Generate summary stats\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB (Tg)\")\n",
        "\n",
        "# Use list for efficient concatenation\n",
        "summary_components = [df_polygon_area_km2, df_agb_total_tg_t]\n",
        "if generate_uncertainty_stats:\n",
        "    df_agb_total_tg_ci95_t = df_agb_total_tg_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB CI95 (Tg)\")\n",
        "    summary_components.append(df_agb_total_tg_ci95_t)\n",
        "\n",
        "summary_stats = pd.concat(summary_components, axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_stats.to_csv(join(sample_polygons_statistics_dir, 'summary_dist_stats.csv'))\n",
        "\n",
        "# Generate empty dataframes for statistics\n",
        "df_base = pd.DataFrame(index=selected_dists)\n",
        "df_base.rename_axis('dist', inplace=True)\n",
        "\n",
        "# Generate detailed stats by polygon\n",
        "for polygon_area in polygon_names:\n",
        "    polygon_area_km2 = df_polygon_area_km2.loc[polygon_area][\"Area (km^2)\"]\n",
        "    df_detailed_dist_stats = df_base.copy()\n",
        "    df_detailed_dist_stats[\"Area (km^2)\"] = polygon_area_km2\n",
        "    for df_stats in df_stats_list:\n",
        "        if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"Forest AGBD mean (Mg / ha)\"\n",
        "        if df_stats.equals(df_agbd_stdev_mg_ha): stat_col = \"Forest AGBD stdev (Mg / ha)\"\n",
        "        if df_stats.equals(df_agb_total_tg): stat_col = \"Forest AGB total (Tg)\"\n",
        "        if generate_uncertainty_stats:\n",
        "            if df_stats.equals(df_agbd_mean_mg_ha_ci95): stat_col = \"Forest AGBD CI95 (Mg / ha)\"\n",
        "            if df_stats.equals(df_agbd_mean_mg_ha_uncertainty): stat_col = \"Forest AGBD uncertainty (%)\"\n",
        "            if df_stats.equals(df_agb_total_tg_ci95): stat_col = \"Forest AGB total CI95 (Tg)\"\n",
        "        for stats_polygon_area in df_stats:\n",
        "            if stats_polygon_area == polygon_area:\n",
        "                df_stats_renamed = df_stats.rename(columns={stats_polygon_area:stat_col})\n",
        "                df_detailed_dist_stats = pd.concat([df_detailed_dist_stats, df_stats_renamed[stat_col]], axis=1)\n",
        "    df_detailed_dist_stats.to_csv(join(detailed_dist_stats_by_area_dir, f'{polygon_area}.csv'))\n",
        "\n",
        "# Generate detailed stats by disturbance type - build dictionary once then process\n",
        "dists = {}\n",
        "for stats_csv in os.listdir(detailed_dist_stats_by_area_dir):\n",
        "    polygon_name = f\"{stats_csv[:-4]}\"\n",
        "    stats_csv_path = join(detailed_dist_stats_by_area_dir, stats_csv)\n",
        "    stats_csv_df = pd.read_csv(stats_csv_path)\n",
        "    # Process all disturbance types for this polygon in one pass\n",
        "    for dist in stats_csv_df['dist'].unique():\n",
        "        dist_df = stats_csv_df[stats_csv_df['dist'] == dist].copy()\n",
        "        dist_df.drop('dist', axis=1, inplace=True)\n",
        "        dist_df.insert(0, 'Name', polygon_name)\n",
        "        if dist in dists:\n",
        "            dists[dist] = pd.concat([dists[dist], dist_df], ignore_index=True)\n",
        "        else:\n",
        "            dists[dist] = dist_df\n",
        "\n",
        "# Write all disturbance CSVs\n",
        "for dist, dist_df in dists.items():\n",
        "    output_file_path = join(detailed_dist_stats_by_scenario_dir,f'{dist}.csv')\n",
        "    dist_df.to_csv(output_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t45TVipaVma"
      },
      "source": [
        "# Intactness statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6p_6d-0adF4"
      },
      "outputs": [],
      "source": [
        "# Create list of available percentage change and intactness rasters\n",
        "percentage_change_rasters = []\n",
        "intactness_rasters = []\n",
        "for root, dirs, files in os.walk(intactness_dir):\n",
        "    for file in files:\n",
        "        if \"intactness__\" in file and file.endswith('tif'):\n",
        "            relative_path = os.path.relpath(join(root, file), intactness_dir)\n",
        "            intactness_rasters.append(relative_path)\n",
        "\n",
        "# Select intactness rasters to calculate statistics\n",
        "print(\"# Select intactness raster to calculate statistics\")\n",
        "print(\"intactness_rasters = [\")\n",
        "for raster in intactness_rasters:\n",
        "    print(f\"'{raster}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8mA9ax8IjCj"
      },
      "outputs": [],
      "source": [
        "# Select intactness raster to calculate statistics\n",
        "intactness_rasters = [\n",
        "'2024_oldgrowth_all_land__2024_disturbance_since_oldgrowth/intactness__forest_reserves_10_quantiles__2024_oldgrowth_all_land__2024_disturbance_since_oldgrowth__agbd_tekai_250625_003858.tif',\n",
        "'2024_no_disturbance_since_1996__2024_disturbance_since_1996/intactness__forest_reserves_10_quantiles__2024_no_disturbance_since_1996__2024_disturbance_since_1996__agbd_tekai_250625_003858.tif',\n",
        "'2021_oldgrowth_all_land__2021_disturbance_since_oldgrowth/intactness__forest_reserves_10_quantiles__2021_oldgrowth_all_land__2021_disturbance_since_oldgrowth__agbd_tekai_250625_003858.tif',\n",
        "'2021_no_disturbance_since_1993__2021_disturbance_since_1993/intactness__forest_reserves_10_quantiles__2021_no_disturbance_since_1993__2021_disturbance_since_1993__agbd_tekai_250625_003858.tif',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6Q51AedNXu6"
      },
      "outputs": [],
      "source": [
        "# Load cell area raster for accurate pixel-by-pixel area calculations\n",
        "cell_area_path = join(areas_dir, \"cell_area.tif\")\n",
        "\n",
        "# Toggle for non-forest definition\n",
        "# True: non-forest = land pixels with intactness == 0 (excludes water)\n",
        "# False: non-forest = any pixels with intactness == 0 (includes water)\n",
        "# Note: True requires appropriate land mask created in 'Oldgrowth scenarios' section of 6_scenarios.ipynb\n",
        "restrict_non_forest_to_land_only = True\n",
        "\n",
        "# Match percentage change rasters\n",
        "intactness_percentage_raster_paths = {}\n",
        "for intactness_raster in intactness_rasters:\n",
        "    intactness_raster_path = join(intactness_dir, intactness_raster)\n",
        "    intactness_baseline_dist_dir = intactness_raster.split('/')[0]\n",
        "    percentage_change_filename = f\"percentage_change__{intactness_baseline_dist_dir}__{selected_model}.tif\"\n",
        "    percentage_change_path = join(intactness_dir, intactness_baseline_dist_dir, percentage_change_filename)\n",
        "    intactness_percentage_raster_paths[intactness_raster_path] = percentage_change_path\n",
        "\n",
        "# Function to calculate area-weighted statistics\n",
        "def weighted_stats(values, weights):\n",
        "    # Handle empty arrays\n",
        "    if len(values) == 0:\n",
        "        return None, None\n",
        "    # Calculate weighted mean\n",
        "    weighted_sum = np.sum(values * weights)\n",
        "    sum_of_weights = np.sum(weights)\n",
        "    weighted_mean = weighted_sum / sum_of_weights if sum_of_weights > 0 else 0\n",
        "    # Calculate weighted standard deviation\n",
        "    if sum_of_weights > 0:\n",
        "        variance = np.sum(weights * (values - weighted_mean) ** 2) / sum_of_weights\n",
        "        weighted_std = np.sqrt(variance)\n",
        "    else: weighted_std = 0\n",
        "    return weighted_mean, weighted_std\n",
        "\n",
        "# Function to calculate area for each intactness score\n",
        "def calculate_score_areas(intactness_masked, cell_area_masked_ha):\n",
        "    score_areas = {}\n",
        "    if np.ma.count(intactness_masked) > 0:\n",
        "        # Get unique values from valid (unmasked) data\n",
        "        unique_scores = np.unique(intactness_masked.compressed())\n",
        "        for score in unique_scores:\n",
        "            # Find pixels with this score (not masked)\n",
        "            score_mask = (intactness_masked == score) & (~intactness_masked.mask)\n",
        "            if np.any(score_mask): score_area_ha = np.sum(cell_area_masked_ha[score_mask], dtype='float64')\n",
        "            else: score_area_ha = 0.0\n",
        "            score_areas[int(score)] = score_area_ha\n",
        "    return score_areas\n",
        "\n",
        "# Pre-open cell area dataset\n",
        "cell_area_dataset = rasterio.open(cell_area_path)\n",
        "\n",
        "try:\n",
        "    # Loop through each polygon stored in GPKG to generate statistics\n",
        "    for intactness_raster, percentage_raster in intactness_percentage_raster_paths.items():\n",
        "        polygon_quantiles = intactness_raster.split('/')[-1].split('__')[1]\n",
        "        baseline = intactness_raster.split('/')[-1].split('__')[2]\n",
        "        disturbance = intactness_raster.split('/')[-1].split('__')[3]\n",
        "        current_year = disturbance.split('_')[0]\n",
        "\n",
        "        # Dynamic land mask path based on intactness raster year\n",
        "        if restrict_non_forest_to_land_only:\n",
        "            all_land_mask_path = join(mask_dir, f\"mask_forest_{current_year}_oldgrowth_all_land.tif\")\n",
        "            if not exists(all_land_mask_path):\n",
        "                raise FileNotFoundError(f\"Land mask not found: {all_land_mask_path}. \"\n",
        "                                      f\"Please create this mask in the 'Oldgrowth scenarios' section of 6_scenarios.ipynb\")\n",
        "            all_land_mask_dataset = rasterio.open(all_land_mask_path)\n",
        "\n",
        "        intactness_csv_name = f\"{polygon_quantiles}__{baseline}__{disturbance}.csv\"\n",
        "        intactness_csv_path = join(intactness_stats_dir, intactness_csv_name)\n",
        "        total_score = int(intactness_raster.split('/')[-1].split('__')[1].split('_')[-2])\n",
        "        total_stdev = int(total_score / 2)\n",
        "        df_intactness_stats = pd.DataFrame(columns=[\n",
        "            \"Name\",\n",
        "            \"Percentage change (remaining forest) mean\",\n",
        "            \"Percentage change (remaining forest) stdev\",\n",
        "            \"Percentage change (non-forest = -100) mean\",\n",
        "            \"Percentage change (non-forest = -100) stdev\",\n",
        "            f\"Intactness (remaining forest) mean / {total_score}\",\n",
        "            f\"Intactness (remaining forest) stdev / {total_stdev}\",\n",
        "            f\"Intactness (non-forest = 0) mean / {total_score}\",\n",
        "            f\"Intactness (non-forest = 0) stdev / {total_stdev}\"\n",
        "        ])\n",
        "\n",
        "        for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "\n",
        "            # Define the polygon\n",
        "            sample_polygon_geometry = row[\"geometry\"]\n",
        "            sample_polygon_name = row[\"name\"]\n",
        "            polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "            # Calculate sample_polygon_geometry area (ellipsoidal as opposed to planimetric)\n",
        "            sample_polygons_crs = selected_sample_polygons_gpkg.crs\n",
        "            temp_gdf = gpd.GeoDataFrame({'name': [sample_polygon_name], 'geometry': sample_polygon_geometry}, crs=sample_polygons_crs)\n",
        "            temp_gdf_utm = temp_gdf.estimate_utm_crs()\n",
        "            polygon_area_ha = np.divide(temp_gdf.to_crs(temp_gdf_utm).area[0], 10000, dtype='float64')\n",
        "\n",
        "            # Read & mask intactness to polygon\n",
        "            with rasterio.open(intactness_raster) as src:\n",
        "                nodata_value = src.nodata\n",
        "                intactness_masked, transform_2 = msk.mask(src, polygons, crop=True, filled=False)\n",
        "\n",
        "            # Check if all values are masked (outside polygon)\n",
        "            if np.ma.count(intactness_masked) == 0:\n",
        "                # No valid intactness data - set all stats to None and continue to next polygon\n",
        "                new_row = pd.DataFrame([{\n",
        "                    'Name': sample_polygon_name,\n",
        "                    'Percentage change (remaining forest) mean': None,\n",
        "                    'Percentage change (remaining forest) stdev': None,\n",
        "                    'Percentage change (non-forest = -100) mean': None,\n",
        "                    'Percentage change (non-forest = -100) stdev': None,\n",
        "                    f'Intactness (remaining forest) mean / {total_score}': None,\n",
        "                    f'Intactness (remaining forest) stdev / {total_stdev}': None,\n",
        "                    f'Intactness (non-forest = 0) mean / {total_score}': None,\n",
        "                    f'Intactness (non-forest = 0) stdev / {total_stdev}': None,\n",
        "                }], dtype=object)\n",
        "\n",
        "                df_intactness_stats = pd.concat([df_intactness_stats, new_row], ignore_index=True)\n",
        "                continue\n",
        "\n",
        "            # Mask the cell area raster to the polygon using pre-opened dataset\n",
        "            cell_area_masked, transform_ca = msk.mask(cell_area_dataset, polygons, crop=True, filled=False)\n",
        "\n",
        "            # Convert adjusted cell areas from m2 to ha for easier calculations\n",
        "            cell_area_masked_ha = cell_area_masked / 10000\n",
        "\n",
        "            # Create forest mask from intactness raster (forest = intactness > 0)\n",
        "            forest_mask = (~intactness_masked.mask) & (intactness_masked > 0)\n",
        "\n",
        "            # Create non-forest mask based on toggle setting\n",
        "            potential_non_forest_mask = (~intactness_masked.mask) & (intactness_masked == 0)\n",
        "\n",
        "            if restrict_non_forest_to_land_only:\n",
        "                # Get land mask and restrict non-forest to land pixels only (excludes water)\n",
        "                all_land_mask_data, transform_alm = msk.mask(all_land_mask_dataset, polygons, crop=True, filled=False)\n",
        "                all_land_mask = ~np.ma.getmaskarray(all_land_mask_data) & (all_land_mask_data == 1)\n",
        "                non_forest_mask = potential_non_forest_mask & all_land_mask\n",
        "            else:\n",
        "                # Include all pixels with intactness == 0 (including water)\n",
        "                non_forest_mask = potential_non_forest_mask\n",
        "\n",
        "            # Extract forest and non-forest land areas\n",
        "            forest_cell_areas_ha = np.ma.array(cell_area_masked_ha.data, mask=~forest_mask)\n",
        "            forest_area_ha_sum = np.ma.sum(forest_cell_areas_ha)\n",
        "\n",
        "            non_forest_cell_areas_ha = np.ma.array(cell_area_masked_ha.data, mask=~non_forest_mask)\n",
        "            non_forest_area_ha_sum = np.ma.sum(non_forest_cell_areas_ha)\n",
        "\n",
        "            # Mask percentage change raster to the polygon\n",
        "            with rasterio.open(percentage_raster) as percent_change:\n",
        "                percent_change_masked, transform_pc = msk.mask(percent_change, polygons, crop=True, filled=False)\n",
        "\n",
        "            # Apply forest mask to percentage change values (for forest-only statistics)\n",
        "            percent_change_forest_only = np.ma.array(percent_change_masked.data, mask=~forest_mask)\n",
        "\n",
        "            if forest_area_ha_sum > 0:\n",
        "                # Extract percentage change values for forest pixels\n",
        "                forest_percent_values = np.ma.compressed(percent_change_forest_only)\n",
        "                forest_percent_weights = np.ma.compressed(forest_cell_areas_ha)\n",
        "\n",
        "                # Calculate area-weighted percentage change statistics for remaining forest\n",
        "                percent_change_forest_mean, percent_change_forest_std = weighted_stats(\n",
        "                    forest_percent_values, forest_percent_weights\n",
        "                )\n",
        "\n",
        "                # For all land including non-forest land (treated as -100% change)\n",
        "                if non_forest_area_ha_sum > 0:\n",
        "                    # Calculate the weighted mean directly\n",
        "                    all_mean_numerator = np.sum(forest_percent_values * forest_percent_weights)\n",
        "                    all_mean_denominator = forest_area_ha_sum + non_forest_area_ha_sum\n",
        "                    all_mean_numerator += non_forest_area_ha_sum * (-100.0)\n",
        "                    percent_change_all_mean = all_mean_numerator / all_mean_denominator\n",
        "\n",
        "                    # Calculate the weighted variance directly\n",
        "                    forest_variance_contribution = np.sum(\n",
        "                        forest_percent_weights * np.square(forest_percent_values - percent_change_all_mean)\n",
        "                    )\n",
        "                    non_forest_variance_contribution = non_forest_area_ha_sum * np.square((-100.0) - percent_change_all_mean)\n",
        "                    all_variance = (forest_variance_contribution + non_forest_variance_contribution) / all_mean_denominator\n",
        "                    percent_change_all_std = np.sqrt(all_variance)\n",
        "                else:\n",
        "                    # If no non-forest land area, all-land stats are the same as forest stats\n",
        "                    percent_change_all_mean = percent_change_forest_mean\n",
        "                    percent_change_all_std = percent_change_forest_std\n",
        "            else:\n",
        "                # If no forest, set forest stats to None and all-land stats to -100% change\n",
        "                percent_change_forest_mean = percent_change_forest_std = None\n",
        "                percent_change_all_mean = -100.0\n",
        "                percent_change_all_std = 0.0\n",
        "\n",
        "            # Compute intactness stats\n",
        "            if forest_area_ha_sum > 0:\n",
        "                # Remaining-forest intactness (only forest pixels)\n",
        "                forest_intact_vals = intactness_masked.data[forest_mask]\n",
        "                forest_intact_weights = cell_area_masked_ha.data[forest_mask]\n",
        "                intactness_remaining_mean, intactness_remaining_std = weighted_stats(\n",
        "                    forest_intact_vals, forest_intact_weights\n",
        "                )\n",
        "\n",
        "                # All-land intactness (non-forest land = 0)\n",
        "                total_land = forest_area_ha_sum + non_forest_area_ha_sum\n",
        "                num = np.sum(forest_intact_vals * forest_intact_weights)\n",
        "                den = total_land\n",
        "                intactness_all_mean = num / den\n",
        "\n",
        "                # Variance: forest + non-forest land contributions\n",
        "                var_forest = np.sum(forest_intact_weights * np.square(forest_intact_vals - intactness_all_mean))\n",
        "                var_nonforest = non_forest_area_ha_sum * np.square(0 - intactness_all_mean)\n",
        "                intactness_all_std = np.sqrt((var_forest + var_nonforest) / den)\n",
        "            else:\n",
        "                # No forest present\n",
        "                intactness_remaining_mean = intactness_remaining_std = None\n",
        "                intactness_all_mean = 0.0\n",
        "                intactness_all_std = 0.0\n",
        "\n",
        "            # Calculate area for each intactness score\n",
        "            score_areas = calculate_score_areas(intactness_masked, cell_area_masked_ha)\n",
        "\n",
        "            # Create new row with all statistics including score areas\n",
        "            new_row_dict = {\n",
        "                'Name': sample_polygon_name,\n",
        "                'Percentage change (remaining forest) mean': percent_change_forest_mean,\n",
        "                'Percentage change (remaining forest) stdev': percent_change_forest_std,\n",
        "                'Percentage change (non-forest = -100) mean': percent_change_all_mean,\n",
        "                'Percentage change (non-forest = -100) stdev': percent_change_all_std,\n",
        "                f'Intactness (remaining forest) mean / {total_score}': intactness_remaining_mean,\n",
        "                f'Intactness (remaining forest) stdev / {total_stdev}': intactness_remaining_std,\n",
        "                f'Intactness (non-forest = 0) mean / {total_score}': intactness_all_mean,\n",
        "                f'Intactness (non-forest = 0) stdev / {total_stdev}': intactness_all_std,\n",
        "            }\n",
        "\n",
        "            # Add score area columns\n",
        "            for score, area in score_areas.items():\n",
        "                new_row_dict[f'Intactness score {score} area (ha)'] = area\n",
        "\n",
        "            new_row = pd.DataFrame([new_row_dict], dtype=object)\n",
        "\n",
        "            # Append to main dataframe\n",
        "            df_intactness_stats = pd.concat([df_intactness_stats, new_row], ignore_index=True)\n",
        "\n",
        "        # Set index to Name and save to CSV\n",
        "        df_intactness_stats = df_intactness_stats.set_index('Name')\n",
        "        df_intactness_stats.to_csv(intactness_csv_path)\n",
        "        print(f\"Saved statistics to {intactness_csv_path}\")\n",
        "\n",
        "        # Close land mask dataset if opened\n",
        "        if restrict_non_forest_to_land_only:\n",
        "            all_land_mask_dataset.close()\n",
        "\n",
        "finally: cell_area_dataset.close() # Close static datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC-rviHPi4q5"
      },
      "source": [
        "# Report statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHpAf6qg1dEf"
      },
      "outputs": [],
      "source": [
        "# Reduces statistics to a more specific and intuitive format.\n",
        "\n",
        "# Define scenarios for report\n",
        "print(\"# Remember that order matters\\n\")\n",
        "print(\"scenario_list = [\")\n",
        "for csv in os.listdir(detailed_stats_by_scenario_dir):\n",
        "  print(f\"'{csv[:-4]}',\")\n",
        "print(\"]\")\n",
        "print(\"\")\n",
        "\n",
        "disturbance_csv_files = [f[:-4] for f in os.listdir(detailed_dist_stats_by_scenario_dir) if f.endswith('.csv')]\n",
        "\n",
        "def get_disturbance_type(filename):\n",
        "    if 'degradation_deforestation' in filename: return 3  # comes third\n",
        "    elif 'deforestation' in filename: return 2  # comes second\n",
        "    else: return 1  # comes first (degradation)\n",
        "\n",
        "# Sort by year, then disturbance type\n",
        "files_by_category = {}\n",
        "for file in disturbance_csv_files:\n",
        "    year = file.split('_')[0]\n",
        "    dist_type = get_disturbance_type(file)\n",
        "    key = (year, dist_type)\n",
        "\n",
        "    if key not in files_by_category:\n",
        "        files_by_category[key] = []\n",
        "    files_by_category[key].append(file)\n",
        "print(\"disturbance_list = [\")\n",
        "current_year = None\n",
        "\n",
        "# Process each category in order\n",
        "for key in sorted(files_by_category.keys(), key=lambda k: (int(k[0]), k[1])):\n",
        "    files = files_by_category[key]\n",
        "    # First add 'total' files\n",
        "    total_files = [f for f in files if '_total' in f]\n",
        "    for file in total_files:\n",
        "        print(f\"    '{file}',\")\n",
        "    # Group remaining files by reference year\n",
        "    ref_year_files = {}\n",
        "    for file in files:\n",
        "        if '_total' in file:\n",
        "            continue\n",
        "        ref_year = file.split('_')[-1]\n",
        "        if ref_year not in ref_year_files:\n",
        "            ref_year_files[ref_year] = []\n",
        "        ref_year_files[ref_year].append(file)\n",
        "    # Process each reference year, placing 'since' before 'before'\n",
        "    for ref_year in sorted(ref_year_files.keys(), key=int, reverse=True):\n",
        "        year_files = ref_year_files[ref_year]\n",
        "        since_files = [f for f in year_files if '_since_' in f]\n",
        "        before_files = [f for f in year_files if '_before_' in f]\n",
        "        # Add 'since' files normally\n",
        "        for file in since_files:\n",
        "            print(f\"    '{file}',\")\n",
        "        # Add 'before' files commented out\n",
        "        for file in before_files:\n",
        "            print(f\"    # '{file}',\")\n",
        "\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCMvXcIa2Q0D"
      },
      "outputs": [],
      "source": [
        "# Remember that order matters\n",
        "\n",
        "scenario_list = [\n",
        "'2018',\n",
        "'2024',\n",
        "# '2024_oldgrowth',\n",
        "# '2024_oldgrowth_all_land',\n",
        "# '2024_alternate_degradation_2018',\n",
        "# '2024_no_degradation_since_1996',\n",
        "# '2024_no_degradation_since_2019',\n",
        "]\n",
        "\n",
        "disturbance_list = [\n",
        "    # '2024_degradation_total',\n",
        "    '2024_degradation_since_2019',\n",
        "    # '2024_degradation_before_2019',\n",
        "    # '2024_degradation_since_1996',\n",
        "    # '2024_degradation_before_1996',\n",
        "    # '2024_deforestation_total',\n",
        "    '2024_deforestation_since_2019',\n",
        "    # '2024_deforestation_before_2019',\n",
        "    # '2024_degradation_deforestation_total',\n",
        "    '2024_degradation_deforestation_since_2019',\n",
        "    # '2024_degradation_deforestation_before_2019',\n",
        "]\n",
        "\n",
        "report_year = '2024'\n",
        "\n",
        "all_land_scenario = None\n",
        "for scenario in scenario_list:\n",
        "  if 'all_land' in scenario:\n",
        "    all_land_scenario = scenario\n",
        "if all_land_scenario == None: print(\"No all land scenario exists in the detailed stats.\")\n",
        "\n",
        "# Read summary stats\n",
        "summary_stats_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_stats.csv'))\n",
        "summary_dist_stats_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_dist_stats.csv'))\n",
        "\n",
        "# Create attributes CSV\n",
        "attributes = pd.DataFrame()\n",
        "attributes['Name'] = summary_stats_df['Unnamed: 0']\n",
        "attributes['Area (km^2)'] = summary_stats_df['Area (km^2)']\n",
        "attributes[f'{report_year} forest cover (ha)'] = summary_stats_df[f'{report_year} forest cover (ha)']\n",
        "if all_land_scenario:\n",
        "  attributes[f'{all_land_scenario} forest cover (ha)'] = summary_stats_df[f'{all_land_scenario} forest cover (ha)']\n",
        "attributes.to_csv(join(report_statistics_dir, f'{report_year}_attributes.csv'), index=False)\n",
        "\n",
        "# Create scenarios total AGB CSV\n",
        "scenarios_total_agb = pd.DataFrame()\n",
        "scenarios_total_agb['Name'] = summary_stats_df['Unnamed: 0']\n",
        "for scenario in scenario_list:\n",
        "  scenarios_total_agb[f'{scenario} forest AGB (Tg)'] = summary_stats_df[f'{scenario} forest AGB (Tg)']\n",
        "if source_dir == uncertainty_dir:\n",
        "  for scenario in scenario_list:\n",
        "    scenarios_total_agb[f'{scenario} forest AGB CI95 (Tg)'] = summary_stats_df[f'{scenario} forest AGB CI95 (Tg)']\n",
        "scenarios_total_agb.to_csv(join(report_statistics_dir, f'{report_year}_scenarios_total_agb.csv'), index=False)\n",
        "\n",
        "# Create scenarios AGBD CSV\n",
        "scenarios_agbd = pd.DataFrame()\n",
        "scenarios_agbd['Name'] = summary_stats_df['Unnamed: 0']\n",
        "for scenario in scenario_list:\n",
        "  scenario_detailed_stats_df = pd.read_csv(join(detailed_stats_by_scenario_dir, f'{scenario}.csv'))\n",
        "  scenarios_agbd[f'{scenario} forest AGBD (Mg / ha)'] = scenario_detailed_stats_df['Forest AGBD mean (Mg / ha)']\n",
        "if source_dir == uncertainty_dir:\n",
        "  for scenario in scenario_list:\n",
        "    scenario_detailed_stats_df = pd.read_csv(join(detailed_stats_by_scenario_dir, f'{scenario}.csv'))\n",
        "    scenarios_agbd[f'{scenario} forest AGBD CI95 (Mg / ha)'] = scenario_detailed_stats_df['Forest AGBD CI95 (Mg / ha)']\n",
        "scenarios_agbd.to_csv(join(report_statistics_dir, f'{report_year}_scenarios_agbd.csv'), index=False)\n",
        "\n",
        "# Create disturbance total AGB CSV\n",
        "disturbance_total_agb = pd.DataFrame()\n",
        "disturbance_total_agb['Name'] = summary_dist_stats_df['Unnamed: 0']\n",
        "for disturbance in disturbance_list:\n",
        "  disturbance_total_agb[f'{disturbance} forest AGB (Tg)'] = summary_dist_stats_df[f'{disturbance} forest AGB (Tg)']\n",
        "if source_dir == uncertainty_dir:\n",
        "  for disturbance in disturbance_list:\n",
        "    disturbance_total_agb[f'{disturbance} forest AGB CI95 (Tg)'] = summary_dist_stats_df[f'{disturbance} forest AGB CI95 (Tg)']\n",
        "disturbance_total_agb.to_csv(join(report_statistics_dir, f'{report_year}_disturbance_total_agb.csv'), index=False)\n",
        "\n",
        "# Create disturbance AGBD CSV\n",
        "disturbance_agbd = pd.DataFrame()\n",
        "disturbance_agbd['Name'] = summary_stats_df['Unnamed: 0']\n",
        "for disturbance in disturbance_list:\n",
        "  disturbance_detailed_stats_df = pd.read_csv(join(detailed_dist_stats_by_scenario_dir, f'{disturbance}.csv'))\n",
        "  disturbance_agbd[f'{disturbance} forest AGBD (Mg / ha)'] = disturbance_detailed_stats_df['Forest AGBD mean (Mg / ha)']\n",
        "if source_dir == uncertainty_dir:\n",
        "  for disturbance in disturbance_list:\n",
        "    disturbance_detailed_stats_df = pd.read_csv(join(detailed_dist_stats_by_scenario_dir, f'{disturbance}.csv'))\n",
        "    disturbance_agbd[f'{disturbance} forest AGBD CI95 (Mg / ha)'] = disturbance_detailed_stats_df['Forest AGBD CI95 (Mg / ha)']\n",
        "disturbance_agbd.to_csv(join(report_statistics_dir, f'{report_year}_disturbance_agbd.csv'), index=False)\n",
        "\n",
        "print(\"Report statistics completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b70oSO_VNtkY"
      },
      "source": [
        "# Sankey plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHEy0mBX2yNj"
      },
      "outputs": [],
      "source": [
        "# Define and create directories\n",
        "sankey_labelled = join(sample_polygons_statistics_dir, 'sankey_labelled')\n",
        "sankey_unlabelled = join(sample_polygons_statistics_dir, 'sankey_unlabelled')\n",
        "sankey_labelled_svg = join(sample_polygons_statistics_dir, 'sankey_labelled_svg')\n",
        "sankey_unlabelled_svg = join(sample_polygons_statistics_dir, 'sankey_unlabelled_svg')\n",
        "\n",
        "for dir in [sankey_labelled, sankey_unlabelled, sankey_labelled_svg, sankey_unlabelled_svg]:\n",
        "    makedirs(dir, exist_ok=True)\n",
        "\n",
        "# Load the CSV files\n",
        "summary_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_stats.csv'))\n",
        "summary_dist_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_dist_stats.csv'))\n",
        "\n",
        "# Check that all rows in both .csv files have the same strings (polygon areas) in column A\n",
        "polygon_areas_stats = summary_stats.iloc[:, 0]\n",
        "polygon_areas_dist_stats = summary_dist_stats.iloc[:, 0]\n",
        "\n",
        "assert all(polygon_areas_stats == polygon_areas_dist_stats), \"Polygon areas do not match between the two CSV files.\"\n",
        "\n",
        "# Print columns relevant for sankey diagram configuration\n",
        "\n",
        "# Filter for AGB columns only (exclude forest cover and CI95 for initial selection)\n",
        "summary_agb_cols = [col for col in summary_stats.columns[1:] if 'forest AGB (Tg)' in col and 'CI95' not in col]\n",
        "dist_agb_cols = [col for col in summary_dist_stats.columns[1:] if 'forest AGB (Tg)' in col and 'CI95' not in col]\n",
        "\n",
        "print(\"=== summary_stats.csv AGB columns ===\")\n",
        "print(\"(for old_growth_agb_column and current_agb_column)\\n\")\n",
        "\n",
        "# Group by category\n",
        "current_year_cols = [col for col in summary_agb_cols if col.startswith('2024 ') or col.startswith('2023 ') or col.startswith('2022 ')]\n",
        "oldgrowth_cols = [col for col in summary_agb_cols if 'oldgrowth' in col and not col.endswith('_1 forest AGB (Tg)') and not col.endswith('_2 forest AGB (Tg)')]\n",
        "no_disturbance_cols = [col for col in summary_agb_cols if 'no_disturbance' in col]\n",
        "no_degradation_cols = [col for col in summary_agb_cols if 'no_degradation' in col]\n",
        "\n",
        "print(\"Current year scenarios:\")\n",
        "for i, col in enumerate(current_year_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\nOld-growth scenarios:\")\n",
        "for i, col in enumerate(oldgrowth_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\nNo disturbance scenarios:\")\n",
        "for i, col in enumerate(no_disturbance_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\nNo degradation scenarios:\")\n",
        "for i, col in enumerate(no_degradation_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"=== summary_dist_stats.csv AGB columns ===\")\n",
        "print(\"(for degradation/deforestation since/total columns)\\n\")\n",
        "\n",
        "# Group disturbance columns\n",
        "degradation_cols = [col for col in dist_agb_cols if 'degradation' in col]\n",
        "deforestation_cols = [col for col in dist_agb_cols if 'deforestation' in col]\n",
        "disturbance_cols = [col for col in dist_agb_cols if 'disturbance' in col and 'effect' not in col]\n",
        "\n",
        "print(\"Degradation columns:\")\n",
        "for i, col in enumerate(degradation_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\nDeforestation columns:\")\n",
        "for i, col in enumerate(deforestation_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\nDisturbance (combined) columns:\")\n",
        "for i, col in enumerate(disturbance_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ykformx2-nl"
      },
      "outputs": [],
      "source": [
        "# Plot degradation and deforestation separately\n",
        "separate_disturbance = True\n",
        "# Plot degradation before and since a date separately\n",
        "separate_degradation = True\n",
        "# Plot deforestation before and since a date separately\n",
        "separate_deforestation = True\n",
        "# Plot total disturbance before and since a date separately (when separate_disturbance is False)\n",
        "separate_disturbance_temporal = False\n",
        "\n",
        "# DPI (default is 96, output image will scale accordingly)\n",
        "dpi = 300\n",
        "# Relative width modifier (ratio, e.g. 0.5 or 2)\n",
        "width_modifier = 0.85\n",
        "\n",
        "# Title (polygon area), density and label variables (weight of 800 ~ bold, 400 ~ normal)\n",
        "show_title = True\n",
        "show_density = True\n",
        "show_labels = True\n",
        "left_axis_label = True\n",
        "svg_transparent_background = True\n",
        "title_font_size = 20\n",
        "title_font_weight = 600\n",
        "density_font_size = 17\n",
        "density_font_weight = 600\n",
        "label_font_size = 17\n",
        "label_font_weight = 600\n",
        "\n",
        "# Base columns and year (summary_stats)\n",
        "old_growth_agb_column = '2021_oldgrowth_all_land forest AGB (Tg)'\n",
        "current_agb_column = '2021 forest AGB (Tg)'\n",
        "current_year = current_agb_column.split(' ')[0]\n",
        "\n",
        "# Disturbance columns (summary_dist_stats)\n",
        "degradation_since_column = '2021_degradation_since_1993 forest AGB (Tg)'\n",
        "degradation_total_column = '2021_degradation_since_oldgrowth forest AGB (Tg)'\n",
        "deforestation_since_column = '2021_deforestation_since_1993 forest AGB (Tg)'\n",
        "deforestation_total_column = '2021_deforestation_since_oldgrowth forest AGB (Tg)'\n",
        "disturbance_since_column = '2021_disturbance_since_1993 forest AGB (Tg)'\n",
        "disturbance_total_column = '2021_disturbance_since_oldgrowth forest AGB (Tg)'\n",
        "\n",
        "# Node labels and colours\n",
        "remaining_name = f'Remaining in {current_year}:'\n",
        "remaining_colour = '#007fff'\n",
        "degradation_before_name = 'Degradation loss before 1993'\n",
        "degradation_before_colour = '#1a801a'\n",
        "degradation_since_name = 'Degradation loss since 1993'\n",
        "degradation_since_colour = '#8dc00d'\n",
        "degradation_total_name = 'Degradation loss'\n",
        "degradation_total_colour = '#8dc00d'\n",
        "deforestation_before_name = 'Deforestation loss before 1993'\n",
        "deforestation_before_colour = '#ffffff'\n",
        "deforestation_since_name = 'Deforestation loss since 1993'\n",
        "deforestation_since_colour = '#ffff00'\n",
        "deforestation_total_name = 'Deforestation loss'\n",
        "deforestation_total_colour = '#ffffff'\n",
        "disturbance_before_name = 'Disturbance loss before 1993'\n",
        "disturbance_before_colour = '#cccccc'\n",
        "disturbance_since_name = 'Disturbance loss since 1993'\n",
        "disturbance_since_colour = '#999999'\n",
        "disturbance_total_name = 'Disturbance loss'\n",
        "disturbance_total_colour = '#ffffff'\n",
        "\n",
        "# Validate separation settings\n",
        "assert not separate_degradation or separate_disturbance, \"separate_disturbance must be True if separate_degradation is True.\"\n",
        "assert not separate_deforestation or separate_disturbance, \"separate_disturbance must be True if separate_deforestation is True.\"\n",
        "assert not separate_disturbance_temporal or not separate_disturbance, \"separate_disturbance must be False if separate_disturbance_temporal is True.\"\n",
        "\n",
        "# Function to get values from statistics\n",
        "def get_value(df, idx, column_name):\n",
        "    try:\n",
        "        value = df.loc[idx, column_name]\n",
        "        return 0.0 if pd.isnull(value) else float(value)\n",
        "    except KeyError:\n",
        "        print(f\"Column '{column_name}' not found in the dataframe.\")\n",
        "        return 0.0\n",
        "\n",
        "# Loop through each row (polygon area)\n",
        "for idx in summary_stats.index:\n",
        "    polygon_name = summary_stats.iloc[idx, 0]\n",
        "\n",
        "    # Get old-growth and current AGB values\n",
        "    old_growth_agb = get_value(summary_stats, idx, old_growth_agb_column)\n",
        "    current_agb = get_value(summary_stats, idx, current_agb_column)\n",
        "\n",
        "    # Get disturbance values and calculate before values\n",
        "    degradation_since = get_value(summary_dist_stats, idx, degradation_since_column)\n",
        "    degradation_total = get_value(summary_dist_stats, idx, degradation_total_column)\n",
        "    degradation_before = degradation_total - degradation_since\n",
        "\n",
        "    deforestation_since = get_value(summary_dist_stats, idx, deforestation_since_column)\n",
        "    deforestation_total = get_value(summary_dist_stats, idx, deforestation_total_column)\n",
        "    deforestation_before = deforestation_total - deforestation_since\n",
        "\n",
        "    disturbance_since = get_value(summary_dist_stats, idx, disturbance_since_column)\n",
        "    disturbance_total = get_value(summary_dist_stats, idx, disturbance_total_column)\n",
        "    disturbance_before = disturbance_total - disturbance_since\n",
        "\n",
        "    # Statistical assertions\n",
        "    if separate_degradation:\n",
        "        discrepancy = abs(degradation_before + degradation_since - degradation_total)\n",
        "        if discrepancy >= 1e-6:\n",
        "            print(f\"{polygon_name}: degradation_before + degradation_since != degradation_total (discrepancy: {discrepancy:.6e})\")\n",
        "    if separate_deforestation:\n",
        "        discrepancy = abs(deforestation_before + deforestation_since - deforestation_total)\n",
        "        if discrepancy >= 1e-6:\n",
        "            print(f\"{polygon_name}: deforestation_before + deforestation_since != deforestation_total (discrepancy: {discrepancy:.6e})\")\n",
        "    if separate_disturbance:\n",
        "        discrepancy = abs(degradation_total + deforestation_total - disturbance_total)\n",
        "        if discrepancy >= 1e-6:\n",
        "            print(f\"{polygon_name}: degradation_total + deforestation_total != disturbance_total (discrepancy: {discrepancy:.6e})\")\n",
        "    discrepancy = abs(current_agb - disturbance_total - old_growth_agb)\n",
        "    if discrepancy >= 1e-6:\n",
        "        print(f\"{polygon_name}: current_agb - disturbance_total != old_growth_agb (discrepancy: {discrepancy:.6e})\")\n",
        "        print(\"Note: Constraining degradation floor to disturbance or capping disturbances to 0 can break equality when amalgamating across areas\")\n",
        "\n",
        "    # Load detailed stats for AGBD and CI95 values\n",
        "    detailed_stats_df = pd.read_csv(join(detailed_stats_by_area_dir, f\"{polygon_name}.csv\"))\n",
        "    old_growth_index = detailed_stats_df.index[detailed_stats_df['scenario'] == f\"{old_growth_agb_column.split(' ')[0]}\"].item()\n",
        "    current_index = detailed_stats_df.index[detailed_stats_df['scenario'] == f\"{current_agb_column.split(' ')[0]}\"].item()\n",
        "\n",
        "    old_growth_mean_agbd = get_value(detailed_stats_df, old_growth_index, \"Forest AGBD mean (Mg / ha)\")\n",
        "    current_mean_agbd = get_value(detailed_stats_df, current_index, \"Forest AGBD mean (Mg / ha)\")\n",
        "\n",
        "    uncertainty = 'Forest AGB total CI95 (Tg)' in detailed_stats_df.columns\n",
        "    if uncertainty:\n",
        "        old_growth_agb_ci95 = get_value(detailed_stats_df, old_growth_index, \"Forest AGB total CI95 (Tg)\")\n",
        "        old_growth_mean_agbd_ci95 = get_value(detailed_stats_df, old_growth_index, \"Forest AGBD CI95 (Mg / ha)\")\n",
        "        current_agb_ci95 = get_value(detailed_stats_df, current_index, \"Forest AGB total CI95 (Tg)\")\n",
        "        current_mean_agbd_ci95 = get_value(detailed_stats_df, current_index, \"Forest AGBD CI95 (Mg / ha)\")\n",
        "\n",
        "    # Build title and subtitle text\n",
        "    title_name = f\"{polygon_name}\"\n",
        "\n",
        "    if uncertainty:\n",
        "        subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} ± {old_growth_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "        subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} ± {current_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "        left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} ± {old_growth_agb_ci95:.2f} Tg\" if left_axis_label else ''\n",
        "        remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} ± {current_agb_ci95:.2f} Tg\"\n",
        "    else:\n",
        "        subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} Mg / ha\"\n",
        "        subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} Mg / ha\"\n",
        "        left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} Tg\" if left_axis_label else ''\n",
        "        remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} Tg\"\n",
        "\n",
        "    # Configure nodes and links based on separation settings\n",
        "    if separate_disturbance and separate_degradation and separate_deforestation:\n",
        "        nodes = [left_axis, degradation_before_name, degradation_since_name, deforestation_before_name, deforestation_since_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0, 0, 0], [1, 2, 3, 4, 5]\n",
        "        values = [-degradation_before, -degradation_since, -deforestation_before, -deforestation_since, current_agb]\n",
        "        colors = [degradation_before_colour, degradation_since_colour, deforestation_before_colour, deforestation_since_colour, remaining_colour]\n",
        "\n",
        "    elif separate_disturbance and separate_degradation and not separate_deforestation:\n",
        "        nodes = [left_axis, degradation_before_name, degradation_since_name, deforestation_total_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0, 0], [1, 2, 3, 4]\n",
        "        values = [-degradation_before, -degradation_since, -deforestation_total, current_agb]\n",
        "        colors = [degradation_before_colour, degradation_since_colour, deforestation_total_colour, remaining_colour]\n",
        "\n",
        "    elif separate_disturbance and not separate_degradation and separate_deforestation:\n",
        "        nodes = [left_axis, degradation_total_name, deforestation_before_name, deforestation_since_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0, 0], [1, 2, 3, 4]\n",
        "        values = [-degradation_total, -deforestation_before, -deforestation_since, current_agb]\n",
        "        colors = [degradation_total_colour, deforestation_before_colour, deforestation_since_colour, remaining_colour]\n",
        "\n",
        "    elif separate_disturbance and not separate_degradation and not separate_deforestation:\n",
        "        nodes = [left_axis, degradation_total_name, deforestation_total_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0], [1, 2, 3]\n",
        "        values = [-degradation_total, -deforestation_total, current_agb]\n",
        "        colors = [degradation_total_colour, deforestation_total_colour, remaining_colour]\n",
        "\n",
        "    elif not separate_disturbance and separate_disturbance_temporal:\n",
        "        nodes = [left_axis, disturbance_before_name, disturbance_since_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0], [1, 2, 3]\n",
        "        values = [-disturbance_before, -disturbance_since, current_agb]\n",
        "        colors = [disturbance_before_colour, disturbance_since_colour, remaining_colour]\n",
        "\n",
        "    else:\n",
        "        nodes = [left_axis, disturbance_total_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0], [1, 2]\n",
        "        values = [-disturbance_total, current_agb]\n",
        "        colors = [disturbance_total_colour, remaining_colour]\n",
        "\n",
        "    node_colors = [remaining_colour] + colors\n",
        "\n",
        "    # Add percentages to node labels\n",
        "    percentages = [(abs(val) / old_growth_agb * 100) for val in values]\n",
        "    for i in range(1, len(nodes)):\n",
        "        if i - 1 < len(percentages):\n",
        "            nodes[i] += f\" ({percentages[i-1]:.0f}%)\"\n",
        "\n",
        "    # Configure title and density annotations\n",
        "    title_and_density = [\n",
        "        dict(x=0, y=1.28, xref='paper', yref='paper', text=title_name, showarrow=False, xanchor='left', align='left',\n",
        "             font=dict(family=\"arial, sans serif\", size=title_font_size, color=\"black\", weight=title_font_weight)),\n",
        "        dict(x=0, y=1.19, xref='paper', yref='paper', text=subtitle_1_name, showarrow=False, xanchor='left', align='left',\n",
        "             font=dict(family=\"arial, sans serif\", size=density_font_size, color=\"black\", weight=density_font_weight)),\n",
        "        dict(x=0, y=1.11, xref='paper', yref='paper', text=subtitle_2_name, showarrow=False, xanchor='left', align='left',\n",
        "             font=dict(family=\"arial, sans serif\", size=density_font_size, color=\"black\", weight=density_font_weight))\n",
        "    ]\n",
        "\n",
        "    if show_title and not show_density:\n",
        "        title_and_density = title_and_density[0:1]\n",
        "    elif not show_title and show_density:\n",
        "        title_and_density = title_and_density[1:3]\n",
        "    elif not show_title and not show_density:\n",
        "        title_and_density = []\n",
        "\n",
        "    # Remove labels if toggled off\n",
        "    if not show_labels:\n",
        "        nodes = [''] * len(nodes)\n",
        "\n",
        "    # Create sankey diagram\n",
        "    fig = go.Figure(data=[go.Sankey(\n",
        "        arrangement=\"freeform\",\n",
        "        node=dict(label=nodes, color=node_colors, pad=15, thickness=20, line=dict(color=\"black\", width=1)),\n",
        "        link=dict(source=sources, target=targets, value=values, color=colors, line=dict(color=\"black\", width=1))\n",
        "    )])\n",
        "\n",
        "    fig.update_layout(\n",
        "        width=700 * width_modifier, height=500,\n",
        "        font=dict(family=\"arial, sans serif\", size=label_font_size, color=\"black\", weight=label_font_weight),\n",
        "        margin=dict(l=25, r=25, t=115, b=25),\n",
        "        annotations=title_and_density\n",
        "    )\n",
        "\n",
        "    # Save labelled versions\n",
        "    fig.write_image(join(sankey_labelled, f'sankey_diagram_{polygon_name}.png'), scale=dpi / 96)\n",
        "    if svg_transparent_background:\n",
        "        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "    fig.write_image(join(sankey_labelled_svg, f'sankey_diagram_vector_{polygon_name}.svg'), scale=dpi / 96)\n",
        "\n",
        "    # Create and save unlabelled versions\n",
        "    fig_unlabelled = go.Figure(data=[go.Sankey(\n",
        "        arrangement=\"freeform\",\n",
        "        node=dict(label=[''] * len(nodes), color=node_colors, pad=15, thickness=20, line=dict(color=\"black\", width=1)),\n",
        "        link=dict(source=sources, target=targets, value=values, color=colors, line=dict(color=\"black\", width=1))\n",
        "    )])\n",
        "\n",
        "    fig_unlabelled.update_layout(\n",
        "        width=700 * width_modifier, height=500,\n",
        "        font=dict(family=\"arial, sans serif\", size=label_font_size, color=\"black\", weight=label_font_weight),\n",
        "        margin=dict(l=25, r=25, t=115, b=25)\n",
        "    )\n",
        "\n",
        "    fig_unlabelled.write_image(join(sankey_unlabelled, f'sankey_diagram_{polygon_name}.png'), scale=dpi / 96)\n",
        "    if svg_transparent_background:\n",
        "        fig_unlabelled.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "    fig_unlabelled.write_image(join(sankey_unlabelled_svg, f'sankey_diagram_vector_{polygon_name}.svg'), scale=dpi / 96)\n",
        "\n",
        "    print(f\"Statistical assertions and sankey diagram complete for {polygon_name}.\")\n",
        "\n",
        "    # Display figure with white background\n",
        "    fig.update_layout(plot_bgcolor='white', paper_bgcolor='white')\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXAGIvpk_KWS"
      },
      "source": [
        "# Disconnected runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgzXDe-Fnm3T"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "zjy-T1TqScbE"
      ],
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}