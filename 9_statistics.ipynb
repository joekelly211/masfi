{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/9_statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j0th2jhtnAR"
      },
      "source": [
        "# Imports & Subdirectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9drq4euvG0MS"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGWgfUqto0F"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Imports and upgrades\n",
        "!pip install geopandas\n",
        "!pip install kaleido==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MTERlyxuPd4"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from datetime import datetime\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "import kaleido\n",
        "import numpy as np\n",
        "from os.path import exists, join\n",
        "from os import makedirs\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import re\n",
        "from shutil import copyfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwar9sxbts-3"
      },
      "outputs": [],
      "source": [
        "# Define existing directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "masks_dir = join(areas_dir, \"masks\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "predictions_dir = join(base_dir, \"7_predictions\")\n",
        "differences_dir = join(base_dir, \"8_differences\")\n",
        "\n",
        "# Define and create statistics directories\n",
        "statistics_dir = join(base_dir, \"9_statistics\")\n",
        "sample_polygons_dir = join(statistics_dir, \"sample_polygons\")\n",
        "makedirs(statistics_dir, exist_ok=True)\n",
        "makedirs(sample_polygons_dir, exist_ok=True)\n",
        "\n",
        "# Cell area raster for accurate pixel-by-pixel area calculations\n",
        "cell_area_path = join(areas_dir, \"cell_area.tif\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atvGRSPYgwk9"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjy-T1TqScbE"
      },
      "source": [
        "# Select model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj5RbVSuEp-B"
      },
      "outputs": [],
      "source": [
        "# Select if to source predictions from scenarios_dir or predictions_dir\n",
        "source_dir = predictions_dir\n",
        "# source_dir = scenarios_dir\n",
        "source_dir_name = f\"{source_dir.split('_')[-1]}_dir\"\n",
        "\n",
        "# Select the model\n",
        "for subdir in os.listdir(source_dir):\n",
        "  if 'scenario_masks' not in subdir:\n",
        "    print(f\"selected_model = '{subdir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txTXPYFLpH6v"
      },
      "outputs": [],
      "source": [
        "selected_model = 'agbd_v2_1_260206_133525'\n",
        "\n",
        "# Define prediction, disturbance and intactness directories\n",
        "selected_model_prediction_dir = join(source_dir, selected_model)\n",
        "prediction_raster_dir = join(selected_model_prediction_dir, 'scenario_predictions')\n",
        "model_differences_dir = join(differences_dir, f\"{selected_model}_{source_dir_name}\")\n",
        "disturbance_dir = join(model_differences_dir, 'disturbance')\n",
        "intactness_dir = join(model_differences_dir, 'intactness')\n",
        "restoration_dir = join(model_differences_dir, 'restoration')\n",
        "\n",
        "# Check prediction directory\n",
        "if not exists(prediction_raster_dir):\n",
        "  print(f\"Prediction directory doesn't exist yet: {prediction_raster_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(prediction_raster_dir))} rasters in {prediction_raster_dir}\")\n",
        "# Check disturbance directory\n",
        "if not exists(disturbance_dir):\n",
        "  print(f\"Disturbance directory doesn't exist yet: {disturbance_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(disturbance_dir))} rasters in {disturbance_dir}\")\n",
        "# Check intactness directory\n",
        "if not exists(intactness_dir):\n",
        "  print(f\"Intactness directory doesn't exist yet: {intactness_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(intactness_dir))} rasters in {intactness_dir}\")\n",
        "# Check restoration directory\n",
        "if not exists(restoration_dir):\n",
        "  print(f\"Intactness directory doesn't exist yet: {restoration_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(restoration_dir))} rasters in {restoration_dir}\")\n",
        "\n",
        "# Define model stats directory\n",
        "model_statistics_dir = join(statistics_dir, f\"{selected_model}_{source_dir_name}\")\n",
        "makedirs(model_statistics_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select sample polygons"
      ],
      "metadata": {
        "id": "TaKqkzjhRtDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select sample area polygons. This should be a single .gpkg with the field 'name' differentiating polygons.\n",
        "sample_polygons = []\n",
        "for geopackage in os.listdir(sample_polygons_dir):\n",
        "  sample_polygons.append(geopackage)\n",
        "if len(sample_polygons) == 0:\n",
        "  print(f\"No sample areas found. Upload .gpkg polygons to {sample_polygons_dir}\")\n",
        "else:\n",
        "  for sample_polygon in sample_polygons: print(f\"selected_sample_polygons = '{sample_polygon}'\")"
      ],
      "metadata": {
        "id": "y3qKKPXeSaqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_sample_polygons = 'tekai_sample_polygons.gpkg'\n",
        "\n",
        "# Define and create sample polygons directory\n",
        "selected_sample_polygons_dir = join(sample_polygons_dir, selected_sample_polygons)\n",
        "selected_sample_polygons_gpkg = gpd.read_file(selected_sample_polygons_dir)\n",
        "sample_polygons_statistics_dir = join(model_statistics_dir, selected_sample_polygons[:-5])\n",
        "makedirs(sample_polygons_statistics_dir, exist_ok=True)\n",
        "\n",
        "# Define and create statistic .csv directories\n",
        "polygon_indices_dir = join(sample_polygons_statistics_dir, 'polygon_indices')\n",
        "land_and_forest_cover_by_area_dir = join(sample_polygons_statistics_dir, 'land_and_forest_cover_by_area')\n",
        "land_and_forest_cover_by_scenario_dir = join(sample_polygons_statistics_dir, 'land_and_forest_cover_by_scenario')\n",
        "scenario_stats_by_area_dir = join(sample_polygons_statistics_dir, 'scenario_stats_by_area')\n",
        "scenario_stats_by_scenario_dir = join(sample_polygons_statistics_dir, 'scenario_stats_by_scenario')\n",
        "disturbance_stats_by_area_dir = join(sample_polygons_statistics_dir, 'disturbance_stats_by_area')\n",
        "disturbance_stats_by_disturbance_dir = join(sample_polygons_statistics_dir, 'disturbance_stats_by_disturbance')\n",
        "restoration_stats_by_area_dir = join(sample_polygons_statistics_dir, 'restoration_stats_by_area')\n",
        "restoration_stats_by_restoration_dir = join(sample_polygons_statistics_dir, 'restoration_stats_by_restoration')\n",
        "intactness_stats_dir = join(sample_polygons_statistics_dir, 'intactness')\n",
        "report_statistics_dir = join(sample_polygons_statistics_dir, 'report_statistics')\n",
        "disturbance_trends_dir = join(sample_polygons_statistics_dir, 'plots_disturbance_trends')\n",
        "sankey_dir = join(sample_polygons_statistics_dir, 'plots_sankey')\n",
        "\n",
        "makedirs(polygon_indices_dir, exist_ok=True)\n",
        "makedirs(land_and_forest_cover_by_area_dir, exist_ok=True)\n",
        "makedirs(land_and_forest_cover_by_scenario_dir, exist_ok=True)\n",
        "makedirs(scenario_stats_by_area_dir, exist_ok=True)\n",
        "makedirs(scenario_stats_by_scenario_dir, exist_ok=True)\n",
        "makedirs(disturbance_stats_by_area_dir, exist_ok=True)\n",
        "makedirs(disturbance_stats_by_disturbance_dir, exist_ok=True)\n",
        "makedirs(restoration_stats_by_area_dir, exist_ok=True)\n",
        "makedirs(restoration_stats_by_restoration_dir, exist_ok=True)\n",
        "makedirs(intactness_stats_dir, exist_ok=True)\n",
        "makedirs(report_statistics_dir, exist_ok=True)\n",
        "makedirs(disturbance_trends_dir, exist_ok=True)\n",
        "makedirs(sankey_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "Yokn9D4pSeaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rasterise each sample polygon, save row/col indices and cell areas.\n",
        "# Pixel centre intersection (all_touched=False) avoids double-counting.\n",
        "\n",
        "# Structure of .npz files:\n",
        "#   rows, cols: 1D int arrays of pixel indices within polygon\n",
        "#   cell_area_ha: 1D float array of pixel areas in hectares\n",
        "# Later statistics sections load these arrays, extract raster values at [rows, cols],\n",
        "# filter nodata, then compute area-weighted statistics using cell_area_ha.\n",
        "\n",
        "# Sort polygons alphabetically instead of geopackage order\n",
        "sort_polygons_alphabetically = False\n",
        "\n",
        "polygon_indices_dir = join(sample_polygons_statistics_dir, 'polygon_indices')\n",
        "makedirs(polygon_indices_dir, exist_ok=True)\n",
        "\n",
        "polygon_names = [row['name'] for _, row in selected_sample_polygons_gpkg.iterrows()]\n",
        "if sort_polygons_alphabetically:\n",
        "    polygon_names = sorted(polygon_names)\n",
        "\n",
        "# Zero-padded prefix width based on polygon count\n",
        "n_polygons = len(polygon_names)\n",
        "prefix_width = len(str(n_polygons))\n",
        "\n",
        "# Load cell area array\n",
        "cell_area_ds = gdal.Open(cell_area_path)\n",
        "cell_area_array = cell_area_ds.ReadAsArray().astype('float64') / 10000\n",
        "zeros_array = np.zeros(cell_area_array.shape, dtype='int16')\n",
        "cell_area_ds = None\n",
        "\n",
        "temp_raster_path = join(polygon_indices_dir, 'temp_burn.tif')\n",
        "export_array_as_tif(zeros_array, temp_raster_path)\n",
        "\n",
        "temp_ds = gdal.Open(temp_raster_path, gdal.GA_Update)\n",
        "band = temp_ds.GetRasterBand(1)\n",
        "\n",
        "progress_total = n_polygons\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Polygon progress: {progress_index}/{progress_total}\")\n",
        "display(progress_label)\n",
        "\n",
        "for i, polygon_name in enumerate(polygon_names):\n",
        "    prefix = str(i).zfill(prefix_width)\n",
        "    npz_path = join(polygon_indices_dir, f'{prefix}_{polygon_name}.npz')\n",
        "    if exists(npz_path):\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"Polygon progress: {progress_index}/{progress_total}\"\n",
        "        continue\n",
        "\n",
        "    # Burn polygon with unique value\n",
        "    vector = ogr.Open(selected_sample_polygons_dir)\n",
        "    layer = vector.GetLayer()\n",
        "    layer.SetAttributeFilter(f\"name = '{polygon_name}'\")\n",
        "    gdal.RasterizeLayer(temp_ds, [1], layer, burn_values=[i + 1], options=[])\n",
        "    temp_ds.FlushCache()\n",
        "    vector = None\n",
        "\n",
        "    # Extract indices and cell areas\n",
        "    rows, cols = np.where(band.ReadAsArray() == i + 1)\n",
        "    cell_area_ha = cell_area_array[rows, cols]\n",
        "    np.savez(npz_path, rows=rows, cols=cols, cell_area_ha=cell_area_ha)\n",
        "\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Polygon progress: {progress_index}/{progress_total}\"\n",
        "\n",
        "temp_ds = None\n",
        "os.remove(temp_raster_path)\n",
        "\n",
        "print(f\"Polygon index arrays saved to {polygon_indices_dir}\")"
      ],
      "metadata": {
        "id": "aZJCa3SG6dkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Land area and forest cover"
      ],
      "metadata": {
        "id": "M8BULeeszQan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List available forest masks, which will be matched with land masks\n",
        "forest_masks = []\n",
        "for mask_file in os.listdir(masks_dir):\n",
        "    if mask_file.startswith('mask_forest_') and mask_file.endswith('.tif'):\n",
        "        scenario_name = mask_file.replace('mask_forest_', '').replace('.tif', '')\n",
        "        forest_masks.append(scenario_name)\n",
        "forest_masks = sorted(forest_masks)\n",
        "\n",
        "print('selected_forest_scenarios = [')\n",
        "for scenario in forest_masks:\n",
        "    print(f'  \"{scenario}\",')\n",
        "print(']')"
      ],
      "metadata": {
        "id": "pjaSLi2HzSCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_forest_scenarios = [\n",
        "  \"1990\",\n",
        "  \"1991\",\n",
        "  \"1992\",\n",
        "  \"1993\",\n",
        "  \"1994\",\n",
        "  \"1995\",\n",
        "  \"1996\",\n",
        "  \"1997\",\n",
        "  \"1998\",\n",
        "  \"1999\",\n",
        "  \"2000\",\n",
        "  \"2001\",\n",
        "  \"2002\",\n",
        "  \"2003\",\n",
        "  \"2004\",\n",
        "  \"2005\",\n",
        "  \"2006\",\n",
        "  \"2007\",\n",
        "  \"2008\",\n",
        "  \"2009\",\n",
        "  \"2010\",\n",
        "  \"2011\",\n",
        "  \"2012\",\n",
        "  \"2013\",\n",
        "  \"2014\",\n",
        "  \"2015\",\n",
        "  \"2016\",\n",
        "  \"2017\",\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_oldgrowth_recovery\",\n",
        "  \"2024_road_mat_daling_deforestation\",\n",
        "  \"2024_undisturbed_since_oldgrowth\",\n",
        "]"
      ],
      "metadata": {
        "id": "1jV9ara96EVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forest cover statistics\n",
        "\n",
        "# Match forest masks to land masks by year\n",
        "forest_to_land_mask = {}\n",
        "missing_land_masks = []\n",
        "\n",
        "for scenario in selected_forest_scenarios:\n",
        "    year = scenario[:4]\n",
        "    land_mask_path = join(masks_dir, f\"mask_land_{year}.tif\")\n",
        "    if exists(land_mask_path):\n",
        "        forest_to_land_mask[scenario] = land_mask_path\n",
        "    else:\n",
        "        missing_land_masks.append((scenario, f\"mask_land_{year}.tif\"))\n",
        "\n",
        "# Validate all land masks exist\n",
        "if missing_land_masks:\n",
        "    print(\"Missing land masks. Return to 3a_features_scenario.ipynb to create:\")\n",
        "    for scenario, mask_name in missing_land_masks:\n",
        "        print(f\"  {mask_name} (required for {scenario})\")\n",
        "    raise FileNotFoundError(\"Land masks missing\")\n",
        "\n",
        "# Load polygon data from .npz files\n",
        "polygon_data = {}\n",
        "for npz_file in sorted(os.listdir(polygon_indices_dir)):\n",
        "    if npz_file.endswith('.npz'):\n",
        "        name = npz_file.split('_', 1)[1][:-4]\n",
        "        data = np.load(join(polygon_indices_dir, npz_file))\n",
        "        polygon_data[name] = {\n",
        "            'rows': data['rows'],\n",
        "            'cols': data['cols'],\n",
        "            'cell_area_ha': data['cell_area_ha']\n",
        "        }\n",
        "\n",
        "polygon_names = list(polygon_data.keys())\n",
        "n_polygons = len(polygon_names)\n",
        "n_scenarios = len(selected_forest_scenarios)\n",
        "\n",
        "# Pre-allocate arrays\n",
        "polygon_area_data = np.zeros(n_polygons)\n",
        "land_area_data = np.zeros((n_scenarios, n_polygons))\n",
        "forest_cover_data = np.zeros((n_scenarios, n_polygons))\n",
        "\n",
        "# Calculate polygon area in km²\n",
        "for poly_idx, polygon_name in enumerate(polygon_names):\n",
        "    cell_area_ha = polygon_data[polygon_name]['cell_area_ha']\n",
        "    polygon_area_data[poly_idx] = cell_area_ha.sum(dtype='float64') / 100\n",
        "\n",
        "# Progress tracking\n",
        "progress_total = n_scenarios\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Scenario progress: {progress_index}/{progress_total}\")\n",
        "display(progress_label)\n",
        "\n",
        "for scenario_idx, scenario in enumerate(selected_forest_scenarios):\n",
        "    # Land mask\n",
        "    land_mask_path = forest_to_land_mask[scenario]\n",
        "    land_ds = gdal.Open(land_mask_path)\n",
        "    land_array = land_ds.ReadAsArray()\n",
        "    land_ds = None\n",
        "\n",
        "    # Forest mask\n",
        "    forest_mask_path = join(masks_dir, f\"mask_forest_{scenario}.tif\")\n",
        "    forest_ds = gdal.Open(forest_mask_path)\n",
        "    forest_array = forest_ds.ReadAsArray()\n",
        "    forest_ds = None\n",
        "\n",
        "    for poly_idx, polygon_name in enumerate(polygon_names):\n",
        "        rows = polygon_data[polygon_name]['rows']\n",
        "        cols = polygon_data[polygon_name]['cols']\n",
        "        cell_area_ha = polygon_data[polygon_name]['cell_area_ha']\n",
        "\n",
        "        # Land area\n",
        "        land_values = land_array[rows, cols]\n",
        "        land_mask = (land_values == 1)\n",
        "        land_area_data[scenario_idx, poly_idx] = cell_area_ha[land_mask].sum(dtype='float64')\n",
        "\n",
        "        # Forest cover\n",
        "        forest_values = forest_array[rows, cols]\n",
        "        forest_mask = (forest_values == 1)\n",
        "        forest_cover_data[scenario_idx, poly_idx] = cell_area_ha[forest_mask].sum(dtype='float64')\n",
        "\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Scenario progress: {progress_index}/{progress_total}\"\n",
        "\n",
        "# Calculate percentages\n",
        "pct_area_land = np.zeros((n_scenarios, n_polygons))\n",
        "pct_area_forest = np.zeros((n_scenarios, n_polygons))\n",
        "pct_land_forest = np.zeros((n_scenarios, n_polygons))\n",
        "for poly_idx in range(n_polygons):\n",
        "    area_ha = polygon_area_data[poly_idx] * 100\n",
        "    for scenario_idx in range(n_scenarios):\n",
        "        land_ha = land_area_data[scenario_idx, poly_idx]\n",
        "        forest_ha = forest_cover_data[scenario_idx, poly_idx]\n",
        "        pct_area_land[scenario_idx, poly_idx] = (land_ha / area_ha * 100) if area_ha > 0 else 0\n",
        "        pct_area_forest[scenario_idx, poly_idx] = (forest_ha / area_ha * 100) if area_ha > 0 else 0\n",
        "        pct_land_forest[scenario_idx, poly_idx] = (forest_ha / land_ha * 100) if land_ha > 0 else 0\n",
        "\n",
        "# Generate detailed stats by area\n",
        "for poly_idx, polygon_name in enumerate(polygon_names):\n",
        "    df = pd.DataFrame(index=selected_forest_scenarios)\n",
        "    df.index.name = 'scenario'\n",
        "    df['Area (km^2)'] = polygon_area_data[poly_idx]\n",
        "    df['Land area (ha)'] = land_area_data[:, poly_idx]\n",
        "    df['Forest cover (ha)'] = forest_cover_data[:, poly_idx]\n",
        "    df['% of area that is land'] = pct_area_land[:, poly_idx]\n",
        "    df['% of area that is forest'] = pct_area_forest[:, poly_idx]\n",
        "    df['% of land that is forest'] = pct_land_forest[:, poly_idx]\n",
        "    df.to_csv(join(land_and_forest_cover_by_area_dir, f'{polygon_name}.csv'))\n",
        "\n",
        "# Generate detailed stats by scenario\n",
        "for scenario_idx, scenario in enumerate(selected_forest_scenarios):\n",
        "    df = pd.DataFrame(index=polygon_names)\n",
        "    df.index.name = 'Name'\n",
        "    df['Area (km^2)'] = polygon_area_data\n",
        "    df['Land area (ha)'] = land_area_data[scenario_idx, :]\n",
        "    df['Forest cover (ha)'] = forest_cover_data[scenario_idx, :]\n",
        "    df['% of area that is land'] = pct_area_land[scenario_idx, :]\n",
        "    df['% of area that is forest'] = pct_area_forest[scenario_idx, :]\n",
        "    df['% of land that is forest'] = pct_land_forest[scenario_idx, :]\n",
        "    df.to_csv(join(land_and_forest_cover_by_scenario_dir, f'{scenario}.csv'))\n",
        "\n",
        "# Generate summary stats\n",
        "summary_land_and_forest_cover = pd.DataFrame(index=polygon_names)\n",
        "summary_land_and_forest_cover.index.name = 'Name'\n",
        "summary_land_and_forest_cover['Area (km^2)'] = polygon_area_data\n",
        "\n",
        "for scenario_idx, scenario in enumerate(selected_forest_scenarios):\n",
        "    summary_land_and_forest_cover[f'{scenario} land area (ha)'] = land_area_data[scenario_idx, :]\n",
        "    summary_land_and_forest_cover[f'{scenario} forest cover (ha)'] = forest_cover_data[scenario_idx, :]\n",
        "\n",
        "summary_land_and_forest_cover.to_csv(join(sample_polygons_statistics_dir, 'summary_land_and_forest_cover_stats.csv'))\n",
        "\n",
        "print(\"Forest cover statistics completed.\")"
      ],
      "metadata": {
        "id": "mONrp5px8Ncu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AGB statistics"
      ],
      "metadata": {
        "id": "7gj6COMW_E43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List available scenario rasters\n",
        "scenarios = set()\n",
        "if exists(prediction_raster_dir):\n",
        "    for f in os.listdir(prediction_raster_dir):\n",
        "        if f.endswith('.tif'):\n",
        "            if source_dir == predictions_dir:\n",
        "                if 'mean__' in f:\n",
        "                    scenario_name = f.split('__')[1].split('.')[0]\n",
        "                    scenarios.add(scenario_name)\n",
        "            else:\n",
        "                scenario_name = f.split('__')[0].split('.')[0]\n",
        "                scenarios.add(scenario_name)\n",
        "scenarios = sorted(list(scenarios))\n",
        "print('# Scenarios')\n",
        "print('selected_scenarios = [')\n",
        "for scenario in scenarios:\n",
        "    print(f'  \"{scenario}\",')\n",
        "print(']\\n')\n",
        "\n",
        "\n",
        "# List available disturbance rasters\n",
        "disturbances = set()\n",
        "if exists(disturbance_dir):\n",
        "    for f in os.listdir(disturbance_dir):\n",
        "        if f.endswith('.tif'):\n",
        "            if source_dir == predictions_dir:\n",
        "                if 'mean__' in f:\n",
        "                    disturbance_name = f.split('__')[1].split('.')[0]\n",
        "                    disturbances.add(disturbance_name)\n",
        "            else:\n",
        "                disturbance_name = f.split('__')[0].split('.')[0]\n",
        "                disturbances.add(disturbance_name)\n",
        "disturbances = sorted(list(disturbances))\n",
        "print('# Disturbance')\n",
        "print('selected_disturbance = [')\n",
        "for disturbance in disturbances:\n",
        "    print(f'  \"{disturbance}\",')\n",
        "print(']\\n')\n",
        "\n",
        "\n",
        "# List available restoration rasters\n",
        "restorations = set()\n",
        "if exists(restoration_dir):\n",
        "    for f in os.listdir(restoration_dir):\n",
        "        if f.endswith('.tif'):\n",
        "            if source_dir == predictions_dir:\n",
        "                if 'mean__' in f:\n",
        "                    restoration_name = f.split('__')[1].split('.')[0]\n",
        "                    restorations.add(restoration_name)\n",
        "            else:\n",
        "                restoration_name = f.split('__')[0].split('.')[0]\n",
        "                restorations.add(restoration_name)\n",
        "restorations = sorted(list(restorations))\n",
        "print('# Restoration')\n",
        "print('selected_restoration = [')\n",
        "for restoration in restorations:\n",
        "    print(f'  \"{restoration}\",')\n",
        "print(']')"
      ],
      "metadata": {
        "id": "M3b9UBsx_Ejy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenarios\n",
        "selected_scenarios = [\n",
        "  # \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_oldgrowth_recovery\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "  \"2024_undisturbed_since_1996\",\n",
        "  \"2024_undisturbed_since_1997\",\n",
        "  \"2024_undisturbed_since_1998\",\n",
        "  \"2024_undisturbed_since_1999\",\n",
        "  \"2024_undisturbed_since_2000\",\n",
        "  \"2024_undisturbed_since_2001\",\n",
        "  \"2024_undisturbed_since_2002\",\n",
        "  \"2024_undisturbed_since_2003\",\n",
        "  \"2024_undisturbed_since_2004\",\n",
        "  \"2024_undisturbed_since_2005\",\n",
        "  \"2024_undisturbed_since_2006\",\n",
        "  \"2024_undisturbed_since_2007\",\n",
        "  \"2024_undisturbed_since_2008\",\n",
        "  \"2024_undisturbed_since_2009\",\n",
        "  \"2024_undisturbed_since_2010\",\n",
        "  \"2024_undisturbed_since_2011\",\n",
        "  \"2024_undisturbed_since_2012\",\n",
        "  \"2024_undisturbed_since_2013\",\n",
        "  \"2024_undisturbed_since_2014\",\n",
        "  \"2024_undisturbed_since_2015\",\n",
        "  \"2024_undisturbed_since_2016\",\n",
        "  \"2024_undisturbed_since_2017\",\n",
        "  \"2024_undisturbed_since_2018\",\n",
        "  \"2024_undisturbed_since_2019\",\n",
        "  \"2024_undisturbed_since_2020\",\n",
        "  \"2024_undisturbed_since_2021\",\n",
        "  \"2024_undisturbed_since_2022\",\n",
        "  \"2024_undisturbed_since_2023\",\n",
        "  \"2024_undisturbed_since_2024\",\n",
        "  \"2024_undisturbed_since_oldgrowth\",\n",
        "]\n",
        "\n",
        "# Disturbance\n",
        "selected_disturbance = [\n",
        "  \"2024_deforestation_of_road_mat_daling_2023_deforestation\",\n",
        "  \"2024_deforestation_of_road_mat_daling_2023_degradation\",\n",
        "  \"2024_deforestation_since_oldgrowth\",\n",
        "  \"2024_degradation_since_1996\",\n",
        "  \"2024_degradation_since_oldgrowth\",\n",
        "  \"2024_effect_of_degradation_in_1996\",\n",
        "  \"2024_effect_of_degradation_in_1997\",\n",
        "  \"2024_effect_of_degradation_in_1998\",\n",
        "  \"2024_effect_of_degradation_in_1999\",\n",
        "  \"2024_effect_of_degradation_in_2000\",\n",
        "  \"2024_effect_of_degradation_in_2001\",\n",
        "  \"2024_effect_of_degradation_in_2002\",\n",
        "  \"2024_effect_of_degradation_in_2003\",\n",
        "  \"2024_effect_of_degradation_in_2004\",\n",
        "  \"2024_effect_of_degradation_in_2005\",\n",
        "  \"2024_effect_of_degradation_in_2006\",\n",
        "  \"2024_effect_of_degradation_in_2007\",\n",
        "  \"2024_effect_of_degradation_in_2008\",\n",
        "  \"2024_effect_of_degradation_in_2009\",\n",
        "  \"2024_effect_of_degradation_in_2010\",\n",
        "  \"2024_effect_of_degradation_in_2011\",\n",
        "  \"2024_effect_of_degradation_in_2012\",\n",
        "  \"2024_effect_of_degradation_in_2013\",\n",
        "  \"2024_effect_of_degradation_in_2014\",\n",
        "  \"2024_effect_of_degradation_in_2015\",\n",
        "  \"2024_effect_of_degradation_in_2016\",\n",
        "  \"2024_effect_of_degradation_in_2017\",\n",
        "  \"2024_effect_of_degradation_in_2018\",\n",
        "  \"2024_effect_of_degradation_in_2019\",\n",
        "  \"2024_effect_of_degradation_in_2020\",\n",
        "  \"2024_effect_of_degradation_in_2021\",\n",
        "  \"2024_effect_of_degradation_in_2022\",\n",
        "  \"2024_effect_of_degradation_in_2023\",\n",
        "  \"2024_effect_of_degradation_in_2024\",\n",
        "]\n",
        "\n",
        "# Restoration\n",
        "selected_restoration = [\n",
        "  \"2024_recovery_potential_with_edge_effects\",\n",
        "  \"2024_reforestation_potential\",\n",
        "  \"2024_restoration_potential\",\n",
        "]"
      ],
      "metadata": {
        "id": "lrhIZTj0_VKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AGB statistics\n",
        "\n",
        "# CI95 aggregation assumes perfect positive spatial correlation between prediction\n",
        "# errors: CI_polygon = sum(CI_pixel × area_pixel). Yields maximum possible aggregate uncertainty,\n",
        "# almost certainly an overestimate. Alternative assumption of complete independence\n",
        "# uses root-sum-of-squares: CI_polygon = sqrt(sum(CI_pixel^2)), yielding narrower\n",
        "# intervals. True CI lies between these bounds. Spatial clustering visible in percentage\n",
        "# uncertainty rasters supports correlated errors, making sum-of-CI a defensible\n",
        "# conservative upper bound.\n",
        "\n",
        "# Uncertainty stats only available from predictions_dir\n",
        "generate_uncertainty_stats = (source_dir == predictions_dir)\n",
        "\n",
        "# Load polygon data from .npz files\n",
        "polygon_data = {}\n",
        "for npz_file in sorted(os.listdir(polygon_indices_dir)):\n",
        "    if npz_file.endswith('.npz'):\n",
        "        name = npz_file.split('_', 1)[1][:-4]\n",
        "        data = np.load(join(polygon_indices_dir, npz_file))\n",
        "        polygon_data[name] = {\n",
        "            'rows': data['rows'],\n",
        "            'cols': data['cols'],\n",
        "            'cell_area_ha': data['cell_area_ha']\n",
        "        }\n",
        "\n",
        "polygon_names = list(polygon_data.keys())\n",
        "n_polygons = len(polygon_names)\n",
        "\n",
        "# Category configurations\n",
        "categories = [\n",
        "    {\n",
        "        'name': 'scenario',\n",
        "        'selected': selected_scenarios,\n",
        "        'raster_dir': prediction_raster_dir,\n",
        "        'by_area_dir': scenario_stats_by_area_dir,\n",
        "        'by_item_dir': scenario_stats_by_scenario_dir,\n",
        "        'summary_filename': 'summary_scenario_stats.csv',\n",
        "        'index_name': 'scenario'\n",
        "    },\n",
        "    {\n",
        "        'name': 'disturbance',\n",
        "        'selected': selected_disturbance,\n",
        "        'raster_dir': disturbance_dir,\n",
        "        'by_area_dir': disturbance_stats_by_area_dir,\n",
        "        'by_item_dir': disturbance_stats_by_disturbance_dir,\n",
        "        'summary_filename': 'summary_disturbance_stats.csv',\n",
        "        'index_name': 'disturbance'\n",
        "    },\n",
        "    {\n",
        "        'name': 'restoration',\n",
        "        'selected': selected_restoration,\n",
        "        'raster_dir': restoration_dir,\n",
        "        'by_area_dir': restoration_stats_by_area_dir,\n",
        "        'by_item_dir': restoration_stats_by_restoration_dir,\n",
        "        'summary_filename': 'summary_restoration_stats.csv',\n",
        "        'index_name': 'restoration'\n",
        "    }\n",
        "]\n",
        "\n",
        "for category in categories:\n",
        "    selected_items = category['selected']\n",
        "    if not selected_items: continue\n",
        "\n",
        "    n_items = len(selected_items)\n",
        "    category_name = category['name']\n",
        "    raster_dir = category['raster_dir']\n",
        "    by_area_dir = category['by_area_dir']\n",
        "    by_item_dir = category['by_item_dir']\n",
        "    summary_filename = category['summary_filename']\n",
        "    index_name = category['index_name']\n",
        "\n",
        "    # Build raster paths\n",
        "    agbd_paths = {}\n",
        "    ci95_paths = {}\n",
        "\n",
        "    if source_dir == predictions_dir:\n",
        "        for item_name in selected_items:\n",
        "            agbd_paths[item_name] = join(raster_dir, f'mean__{item_name}__{selected_model}.tif')\n",
        "            ci95_path = join(raster_dir, f'ci95__{item_name}__{selected_model}.tif')\n",
        "            if exists(ci95_path): ci95_paths[item_name] = ci95_path\n",
        "    else:\n",
        "        for item_name in selected_items:\n",
        "            agbd_paths[item_name] = join(raster_dir, f'{item_name}__{selected_model}.tif')\n",
        "\n",
        "    # Pre-allocate arrays\n",
        "    agbd_mean_data = np.zeros((n_items, n_polygons), dtype='float64')\n",
        "    agbd_stdev_data = np.zeros((n_items, n_polygons), dtype='float64')\n",
        "    agb_total_data = np.zeros((n_items, n_polygons), dtype='float64')\n",
        "\n",
        "    if generate_uncertainty_stats:\n",
        "        agbd_mean_ci95_data = np.zeros((n_items, n_polygons), dtype='float64')\n",
        "        agbd_uncertainty_data = np.zeros((n_items, n_polygons), dtype='float64')\n",
        "        agb_total_ci95_data = np.zeros((n_items, n_polygons), dtype='float64')\n",
        "\n",
        "    # Progress tracking\n",
        "    progress_total = n_items\n",
        "    progress_index = 0\n",
        "    progress_label = widgets.Label(f\"{category_name.capitalize()} raster progress: {progress_index}/{progress_total}\")\n",
        "    display(progress_label)\n",
        "\n",
        "    for item_idx, item_name in enumerate(selected_items):\n",
        "        # Load AGBD raster\n",
        "        agbd_ds = gdal.Open(agbd_paths[item_name])\n",
        "        agbd_array = agbd_ds.ReadAsArray().astype('float64')\n",
        "        nodata = agbd_ds.GetRasterBand(1).GetNoDataValue()\n",
        "        agbd_ds = None\n",
        "\n",
        "        # Load CI95 raster if available\n",
        "        ci95_array = None\n",
        "        if generate_uncertainty_stats and item_name in ci95_paths:\n",
        "            ci95_ds = gdal.Open(ci95_paths[item_name])\n",
        "            ci95_array = ci95_ds.ReadAsArray().astype('float64')\n",
        "            ci95_ds = None\n",
        "\n",
        "        for poly_idx, polygon_name in enumerate(polygon_names):\n",
        "            rows = polygon_data[polygon_name]['rows']\n",
        "            cols = polygon_data[polygon_name]['cols']\n",
        "            cell_area_ha = polygon_data[polygon_name]['cell_area_ha']\n",
        "\n",
        "            # Extract values and filter nodata\n",
        "            agbd_values = agbd_array[rows, cols]\n",
        "            valid_mask = (agbd_values != nodata)\n",
        "            agbd_valid = agbd_values[valid_mask]\n",
        "            areas_valid = cell_area_ha[valid_mask]\n",
        "\n",
        "            # Total area and AGB\n",
        "            total_area_ha = areas_valid.sum(dtype='float64')\n",
        "            agb_total_mg = (agbd_valid * areas_valid).sum(dtype='float64')\n",
        "\n",
        "            if total_area_ha > 0:\n",
        "                # Area-weighted mean: sum(AGBD * area) / sum(area)\n",
        "                mean_agbd = agb_total_mg / total_area_ha\n",
        "\n",
        "                # Area-weighted stdev: sqrt(sum(area * (AGBD - mean)^2) / sum(area))\n",
        "                variance = (areas_valid * (agbd_valid - mean_agbd)**2).sum(dtype='float64') / total_area_ha\n",
        "                stdev_agbd = np.sqrt(variance)\n",
        "\n",
        "                agb_total_tg = agb_total_mg / 1e6\n",
        "            else:\n",
        "                mean_agbd = 0.0\n",
        "                stdev_agbd = 0.0\n",
        "                agb_total_tg = 0.0\n",
        "\n",
        "            agbd_mean_data[item_idx, poly_idx] = mean_agbd\n",
        "            agbd_stdev_data[item_idx, poly_idx] = stdev_agbd\n",
        "            agb_total_data[item_idx, poly_idx] = agb_total_tg\n",
        "\n",
        "            if ci95_array is not None:\n",
        "                ci95_values = ci95_array[rows, cols]\n",
        "                ci95_valid = ci95_values[valid_mask]\n",
        "                ci95_total_mg = (ci95_valid * areas_valid).sum(dtype='float64')\n",
        "\n",
        "                if total_area_ha > 0 and abs(agb_total_mg) > 0:\n",
        "                    ci95_mean_agbd = ci95_total_mg / total_area_ha\n",
        "                    uncertainty_pct = ci95_total_mg / abs(agb_total_mg) * 100\n",
        "                    ci95_total_tg = ci95_total_mg / 1e6\n",
        "                else:\n",
        "                    ci95_mean_agbd = 0.0\n",
        "                    uncertainty_pct = 0.0\n",
        "                    ci95_total_tg = 0.0\n",
        "\n",
        "                agbd_mean_ci95_data[item_idx, poly_idx] = ci95_mean_agbd\n",
        "                agbd_uncertainty_data[item_idx, poly_idx] = uncertainty_pct\n",
        "                agb_total_ci95_data[item_idx, poly_idx] = ci95_total_tg\n",
        "\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"{category_name.capitalize()} raster progress: {progress_index}/{progress_total}\"\n",
        "\n",
        "    # Create DataFrames\n",
        "    df_agbd_mean = pd.DataFrame(agbd_mean_data, index=selected_items, columns=polygon_names)\n",
        "    df_agbd_mean.rename_axis(index_name, inplace=True)\n",
        "\n",
        "    df_agbd_stdev = pd.DataFrame(agbd_stdev_data, index=selected_items, columns=polygon_names)\n",
        "    df_agbd_stdev.rename_axis(index_name, inplace=True)\n",
        "\n",
        "    df_agb_total = pd.DataFrame(agb_total_data, index=selected_items, columns=polygon_names)\n",
        "    df_agb_total.rename_axis(index_name, inplace=True)\n",
        "\n",
        "    if generate_uncertainty_stats:\n",
        "        df_agbd_ci95 = pd.DataFrame(agbd_mean_ci95_data, index=selected_items, columns=polygon_names)\n",
        "        df_agbd_ci95.rename_axis(index_name, inplace=True)\n",
        "\n",
        "        df_agbd_uncertainty = pd.DataFrame(agbd_uncertainty_data, index=selected_items, columns=polygon_names)\n",
        "        df_agbd_uncertainty.rename_axis(index_name, inplace=True)\n",
        "\n",
        "        df_agb_ci95 = pd.DataFrame(agb_total_ci95_data, index=selected_items, columns=polygon_names)\n",
        "        df_agb_ci95.rename_axis(index_name, inplace=True)\n",
        "\n",
        "    # Stats list for CSV output\n",
        "    if generate_uncertainty_stats:\n",
        "        df_stats_list = [\n",
        "            (df_agbd_mean, \"AGBD mean (Mg / ha)\"),\n",
        "            (df_agbd_ci95, \"AGBD CI95 (Mg / ha)\"),\n",
        "            (df_agbd_uncertainty, \"AGBD uncertainty (%)\"),\n",
        "            (df_agbd_stdev, \"AGBD stdev (Mg / ha)\"),\n",
        "            (df_agb_total, \"AGB total (Tg)\"),\n",
        "            (df_agb_ci95, \"AGB total CI95 (Tg)\")\n",
        "        ]\n",
        "    else:\n",
        "        df_stats_list = [\n",
        "            (df_agbd_mean, \"AGBD mean (Mg / ha)\"),\n",
        "            (df_agbd_stdev, \"AGBD stdev (Mg / ha)\"),\n",
        "            (df_agb_total, \"AGB total (Tg)\")\n",
        "        ]\n",
        "\n",
        "    # Summary stats\n",
        "    df_agb_total_t = df_agb_total.T.rename_axis(\"Name\", axis=1).add_suffix(\" AGB (Tg)\")\n",
        "    summary_components = [df_agb_total_t]\n",
        "\n",
        "    if generate_uncertainty_stats:\n",
        "        df_agb_ci95_t = df_agb_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" AGB CI95 (Tg)\")\n",
        "        summary_components.append(df_agb_ci95_t)\n",
        "\n",
        "    summary_stats = pd.concat(summary_components, axis=1).rename_axis(\"Name\", axis=1)\n",
        "    summary_stats.to_csv(join(sample_polygons_statistics_dir, summary_filename))\n",
        "\n",
        "    # Detailed stats by area\n",
        "    for polygon_name in polygon_names:\n",
        "        df_by_area = pd.DataFrame(index=selected_items)\n",
        "        df_by_area.rename_axis(index_name, inplace=True)\n",
        "        for df_stats, col_name in df_stats_list:\n",
        "            df_by_area[col_name] = df_stats[polygon_name]\n",
        "        df_by_area.to_csv(join(by_area_dir, f'{polygon_name}.csv'))\n",
        "\n",
        "    # Detailed stats by item\n",
        "    for item_name in selected_items:\n",
        "        df_by_item = pd.DataFrame(index=polygon_names)\n",
        "        df_by_item.index.name = 'Name'\n",
        "        for df_stats, col_name in df_stats_list:\n",
        "            df_by_item[col_name] = df_stats.loc[item_name]\n",
        "        df_by_item.to_csv(join(by_item_dir, f'{item_name}.csv'))\n",
        "\n",
        "    print(f\"{category_name.capitalize()} AGB statistics completed.\")"
      ],
      "metadata": {
        "id": "6SsERSNGArzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t45TVipaVma"
      },
      "source": [
        "# Intactness statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6p_6d-0adF4"
      },
      "outputs": [],
      "source": [
        "# Create list of available intactness rasters\n",
        "intactness_rasters = []\n",
        "for root, dirs, files in os.walk(intactness_dir):\n",
        "    for file in files:\n",
        "        if \"intactness__\" in file and file.endswith('tif'):\n",
        "            relative_path = os.path.relpath(join(root, file), intactness_dir)\n",
        "            intactness_rasters.append(relative_path)\n",
        "\n",
        "# Select intactness rasters to calculate statistics\n",
        "print(\"# Select intactness raster to calculate statistics\")\n",
        "print(\"intactness_rasters = [\")\n",
        "for raster in intactness_rasters:\n",
        "    print(f\"'{raster}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8mA9ax8IjCj"
      },
      "outputs": [],
      "source": [
        "# Select intactness raster to calculate statistics\n",
        "intactness_rasters = [\n",
        "'2024_undisturbed_since_1996/prediction_area/intactness__prediction_area_10_quantiles__2024_undisturbed_since_1996__agbd_v2_1_260206_133525.tif',\n",
        "'2024_undisturbed_since_1996/forest_reserves/intactness__forest_reserves_10_quantiles__2024_undisturbed_since_1996__agbd_v2_1_260206_133525.tif',\n",
        "'2024_undisturbed_since_oldgrowth/prediction_area/intactness__prediction_area_10_quantiles__2024_undisturbed_since_oldgrowth__agbd_v2_1_260206_133525.tif',\n",
        "'2024_undisturbed_since_oldgrowth/forest_reserves/intactness__forest_reserves_10_quantiles__2024_undisturbed_since_oldgrowth__agbd_v2_1_260206_133525.tif',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Intactness statistics\n",
        "# Calculates area-weighted forest intactness and percentage change statistics per polygon.\n",
        "# Forest intactness scores are aggregated using area-weighted means and standard deviations,\n",
        "# but not CI95, as they represent ordinal categories derived from percentage loss quantiles.\n",
        "# Two versions are calculated per polygon:\n",
        "#   - 'extant': only forest pixel scores (1-10) contribute\n",
        "#   - 'original': deforested pixels also contribute, assigned score 0 for intactness\n",
        "#     or -100% for percentage change\n",
        "# This differentiates degraded forest without deforestation (lower extant, higher original)\n",
        "# from undegraded forest alongside deforestation (higher extant, lower original).\n",
        "# Standard deviation of forest intactness indicates heterogeneity in degradation for\n",
        "# extant scores and in disturbance for original scores.\n",
        "\n",
        "# Regarding percentage loss CI95, used for polygon-level uncertainty aggregation:\n",
        "\n",
        "# As mentioned in the AGB total raster section, the optimal approach would be to\n",
        "# aggregate all Monte Carlo iterations per polygon.For each iteration, calculate the\n",
        "# mean area-weighted mean percentage loss within the polygon, then derive CI95 from\n",
        "# the distribution of polygon means. This preserves the spatial correlation structure\n",
        "# of prediction errors. However, it requires loading all iterations for every intactness\n",
        "# baseline for every sample polygon, which is extremely computationally expensive for\n",
        "# large extents and many polygons. In other words, it scales poorly.\n",
        "\n",
        "# Instead, the approach taken calculates the area-weighted mean of pixel-level CI95\n",
        "# values. So ci95_polygon = sum(weight * ci95_pixel) / sum(weight). This assumes perfect\n",
        "# spatial correlation between pixel errors, i.e. if one pixel overestimates, all\n",
        "# pixels overestimate by the same relative magnitude. It yields the maximum possible\n",
        "# aggregate CI, providing a conservative upper bound on uncertainty. In reality, true\n",
        "# CI is between this value and the root-sum-of-squares approach:\n",
        "# ci95_polygon = sqrt(sum(weight^2 * ci95_pixel^2)) / sum(weight), which assumes complete\n",
        "# independence. Examination of the percentage uncertainty rasters does show spatial\n",
        "# clustering, suggesting the conservative approach is better justified.\n",
        "\n",
        "# Deforested pixels (percentage loss = -100%) have CI = 0 because complete forest\n",
        "# removal is an observed state from Landsat-derived classification, not a model\n",
        "# prediction. These pixels contribute to the mean percentage loss but not to aggregate\n",
        "# uncertainty. Original CI is therefore smaller than extant CI when\n",
        "# deforestation is present, as the same uncertainty is spread over a larger area.\n",
        "\n",
        "# Match intactness rasters to percentage change and CI rasters\n",
        "intactness_percentage_paths = {}\n",
        "intactness_percentage_ci95_paths = {}\n",
        "for intactness_raster in intactness_rasters:\n",
        "    intactness_raster_path = join(intactness_dir, intactness_raster)\n",
        "\n",
        "    # Percentage change raster located in same subdirectory as intactness raster\n",
        "    subdir = intactness_raster.split('/')[0]\n",
        "    percentage_change_filename = f\"percentage_loss__{subdir}__{selected_model}.tif\"\n",
        "    percentage_change_path = join(intactness_dir, subdir, percentage_change_filename)\n",
        "    intactness_percentage_paths[intactness_raster_path] = percentage_change_path\n",
        "\n",
        "    # CI halfwidth raster located in same subdirectory\n",
        "    ci95_halfwidth_filename = f\"ci95_halfwidth__percentage_loss__{subdir}__{selected_model}.tif\"\n",
        "    ci95_halfwidth_path = join(intactness_dir, subdir, ci95_halfwidth_filename)\n",
        "    intactness_percentage_ci95_paths[intactness_raster_path] = ci95_halfwidth_path\n",
        "\n",
        "# Load polygon data from .npz files\n",
        "polygon_data = {}\n",
        "for npz_file in sorted(os.listdir(polygon_indices_dir)):\n",
        "    if npz_file.endswith('.npz'):\n",
        "        name = npz_file.split('_', 1)[1][:-4]\n",
        "        data = np.load(join(polygon_indices_dir, npz_file))\n",
        "        polygon_data[name] = {\n",
        "            'rows': data['rows'],\n",
        "            'cols': data['cols'],\n",
        "            'cell_area_ha': data['cell_area_ha']\n",
        "        }\n",
        "\n",
        "polygon_names = list(polygon_data.keys())\n",
        "n_polygons = len(polygon_names)\n",
        "\n",
        "# Progress tracking\n",
        "n_intactness_rasters = len(intactness_rasters)\n",
        "progress_total = n_intactness_rasters\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Intactness raster progress: {progress_index}/{progress_total}\")\n",
        "display(progress_label)\n",
        "\n",
        "# Outer loop: one CSV output per intactness raster\n",
        "for intactness_raster in intactness_rasters:\n",
        "    intactness_raster_path = join(intactness_dir, intactness_raster)\n",
        "    percentage_raster_path = intactness_percentage_paths[intactness_raster_path]\n",
        "    ci95_raster_path = intactness_percentage_ci95_paths[intactness_raster_path]\n",
        "    ci95_available = exists(ci95_raster_path)\n",
        "\n",
        "    # Extract metadata from filename\n",
        "    # Format: intactness__{quantiles}__{baseline}__{disturbance}__{model}.tif\n",
        "    filename = intactness_raster.split('/')[-1]\n",
        "    parts = filename.split('__')\n",
        "    polygon_quantiles = parts[1]\n",
        "    baseline_name = parts[2]\n",
        "    disturbance = parts[3]\n",
        "\n",
        "    # Total score derived from quantiles (e.g. 'forest_reserves_10_quantiles' -> 10)\n",
        "    total_score = int(polygon_quantiles.split('_')[-2])\n",
        "    total_stdev = int(total_score / 2)\n",
        "\n",
        "    # Output CSV path\n",
        "    intactness_csv_name = f\"{polygon_quantiles}__{baseline_name}__{disturbance}.csv\"\n",
        "    intactness_csv_path = join(intactness_stats_dir, intactness_csv_name)\n",
        "\n",
        "    # Load rasters\n",
        "    intactness_ds = gdal.Open(intactness_raster_path)\n",
        "    intactness_array = intactness_ds.ReadAsArray().astype('float64')\n",
        "    intactness_nodata = intactness_ds.GetRasterBand(1).GetNoDataValue()\n",
        "    intactness_ds = None\n",
        "\n",
        "    percent_ds = gdal.Open(percentage_raster_path)\n",
        "    percent_array = percent_ds.ReadAsArray().astype('float64')\n",
        "    percent_ds = None\n",
        "\n",
        "    # Percentage loss CI95 has additional NaN values where intactness score = 0\n",
        "    # (deforested pixels), as percentage loss CI95 was not calculated for these pixels.\n",
        "    ci95_array = None\n",
        "    if ci95_available:\n",
        "        ci95_ds = gdal.Open(ci95_raster_path)\n",
        "        ci95_array = ci95_ds.ReadAsArray().astype('float64')\n",
        "        ci95_ds = None\n",
        "\n",
        "    # Initialise output dataframe\n",
        "    columns = [\n",
        "        \"Name\",\n",
        "        \"Percentage change (extant) mean\",\n",
        "        \"Percentage change (extant) stdev\",\n",
        "        \"Percentage change (extant) CI95\",\n",
        "        \"Percentage change (original) mean\",\n",
        "        \"Percentage change (original) stdev\",\n",
        "        \"Percentage change (original) CI95\",\n",
        "        f\"Intactness (extant) mean / {total_score}\",\n",
        "        f\"Intactness (extant) stdev / {total_stdev}\",\n",
        "        f\"Intactness (original) mean / {total_score}\",\n",
        "        f\"Intactness (original) stdev / {total_stdev}\"\n",
        "    ]\n",
        "    df_intactness_stats = pd.DataFrame(columns=columns)\n",
        "\n",
        "    # Inner loop: one row per polygon\n",
        "    for polygon_name in polygon_names:\n",
        "        rows = polygon_data[polygon_name]['rows']\n",
        "        cols = polygon_data[polygon_name]['cols']\n",
        "        cell_area_ha = polygon_data[polygon_name]['cell_area_ha']\n",
        "\n",
        "        # Extract intactness values\n",
        "        intactness_values = intactness_array[rows, cols]\n",
        "        valid_mask = (intactness_values != intactness_nodata)\n",
        "\n",
        "        # Skip polygon if no valid intactness data\n",
        "        if not np.any(valid_mask):\n",
        "            new_row = pd.DataFrame([{\n",
        "                'Name': polygon_name,\n",
        "                'Percentage change (extant) mean': None,\n",
        "                'Percentage change (extant) stdev': None,\n",
        "                'Percentage change (extant) CI95': None,\n",
        "                'Percentage change (original) mean': None,\n",
        "                'Percentage change (original) stdev': None,\n",
        "                'Percentage change (original) CI95': None,\n",
        "                f'Intactness (extant) mean / {total_score}': None,\n",
        "                f'Intactness (extant) stdev / {total_stdev}': None,\n",
        "                f'Intactness (original) mean / {total_score}': None,\n",
        "                f'Intactness (original) stdev / {total_stdev}': None,\n",
        "            }], dtype=object)\n",
        "            df_intactness_stats = pd.concat([df_intactness_stats, new_row], ignore_index=True)\n",
        "            continue\n",
        "\n",
        "        # Extant mask: valid pixels with intactness > 0 (extant forest)\n",
        "        extant_mask = valid_mask & (intactness_values > 0)\n",
        "\n",
        "        # Deforested mask: valid pixels with intactness == 0 (lost since baseline)\n",
        "        deforested_mask = valid_mask & (intactness_values == 0)\n",
        "\n",
        "        # Calculate areas in hectares\n",
        "        extant_area_ha = cell_area_ha[extant_mask].sum(dtype='float64')\n",
        "        deforested_area_ha = cell_area_ha[deforested_mask].sum(dtype='float64')\n",
        "        original_area_ha = extant_area_ha + deforested_area_ha\n",
        "\n",
        "        # Extract percentage change values\n",
        "        percent_values = percent_array[rows, cols]\n",
        "\n",
        "        # Extract CI95 values if available, filter NaN\n",
        "        ci95_values = None\n",
        "        ci95_valid_mask = None\n",
        "        if ci95_available:\n",
        "            ci95_values = ci95_array[rows, cols]\n",
        "            ci95_valid_mask = extant_mask & ~np.isnan(ci95_values)\n",
        "\n",
        "        # Calculate percentage change statistics\n",
        "        if extant_area_ha > 0:\n",
        "            # Extant: only pixels with intactness > 0\n",
        "            extant_percent_values = percent_values[extant_mask]\n",
        "            extant_percent_weights = cell_area_ha[extant_mask]\n",
        "\n",
        "            # Area-weighted mean\n",
        "            percent_extant_mean = np.sum(extant_percent_values * extant_percent_weights, dtype='float64') / extant_area_ha\n",
        "\n",
        "            # Area-weighted stdev\n",
        "            variance = np.sum(extant_percent_weights * (extant_percent_values - percent_extant_mean)**2, dtype='float64') / extant_area_ha\n",
        "            percent_extant_std = np.sqrt(variance)\n",
        "\n",
        "            # Extant CI (using only valid CI95 pixels)\n",
        "            if ci95_available and np.any(ci95_valid_mask):\n",
        "                ci95_extant_values = ci95_values[ci95_valid_mask]\n",
        "                ci95_extant_weights = cell_area_ha[ci95_valid_mask]\n",
        "                ci95_extant_area = ci95_extant_weights.sum(dtype='float64')\n",
        "                percent_extant_ci = np.sum(ci95_extant_weights * ci95_extant_values, dtype='float64') / ci95_extant_area\n",
        "            else: percent_extant_ci = None\n",
        "\n",
        "            # Original: include deforested pixels as -100% change\n",
        "            if deforested_area_ha > 0:\n",
        "                original_mean_num = np.sum(extant_percent_values * extant_percent_weights, dtype='float64') + deforested_area_ha * (-100.0)\n",
        "                percent_original_mean = original_mean_num / original_area_ha\n",
        "\n",
        "                extant_var_contrib = np.sum(extant_percent_weights * np.square(extant_percent_values - percent_original_mean), dtype='float64')\n",
        "                deforested_var_contrib = deforested_area_ha * np.square((-100.0) - percent_original_mean)\n",
        "                percent_original_std = np.sqrt((extant_var_contrib + deforested_var_contrib) / original_area_ha)\n",
        "\n",
        "                # Original CI: deforested pixels have CI = 0 (exact -100%, percentage uncertainty).\n",
        "                # Only extant pixels contribute to aggregated CI.\n",
        "                if ci95_available and np.any(ci95_valid_mask):\n",
        "                    percent_original_ci = np.sum(ci95_extant_weights * ci95_extant_values, dtype='float64') / original_area_ha\n",
        "                else: percent_original_ci = None\n",
        "            else:\n",
        "                # No deforestation: original stats equal extant stats\n",
        "                percent_original_mean = percent_extant_mean\n",
        "                percent_original_std = percent_extant_std\n",
        "                percent_original_ci = percent_extant_ci\n",
        "        else:\n",
        "            # No extant forest: all deforested\n",
        "            percent_extant_mean = percent_extant_std = percent_extant_ci = None\n",
        "            percent_original_mean = -100.0\n",
        "            percent_original_std = 0.0\n",
        "            percent_original_ci = 0.0\n",
        "\n",
        "        # Calculate intactness statistics\n",
        "        if extant_area_ha > 0:\n",
        "            # Extant: only pixels with intactness > 0\n",
        "            extant_intact_vals = intactness_values[extant_mask]\n",
        "            extant_intact_weights = cell_area_ha[extant_mask]\n",
        "\n",
        "            # Area-weighted mean\n",
        "            intactness_extant_mean = np.sum(extant_intact_vals * extant_intact_weights, dtype='float64') / extant_area_ha\n",
        "\n",
        "            # Area-weighted stdev\n",
        "            variance = np.sum(extant_intact_weights * (extant_intact_vals - intactness_extant_mean)**2, dtype='float64') / extant_area_ha\n",
        "            intactness_extant_std = np.sqrt(variance)\n",
        "\n",
        "            # Original: include deforested pixels as score 0\n",
        "            if deforested_area_ha > 0:\n",
        "                intactness_original_mean = np.sum(extant_intact_vals * extant_intact_weights, dtype='float64') / original_area_ha\n",
        "\n",
        "                extant_var_contrib = np.sum(extant_intact_weights * np.square(extant_intact_vals - intactness_original_mean), dtype='float64')\n",
        "                deforested_var_contrib = deforested_area_ha * np.square(0 - intactness_original_mean)\n",
        "                intactness_original_std = np.sqrt((extant_var_contrib + deforested_var_contrib) / original_area_ha)\n",
        "            else:\n",
        "                # No deforestation: original stats equal extant stats\n",
        "                intactness_original_mean = intactness_extant_mean\n",
        "                intactness_original_std = intactness_extant_std\n",
        "        else:\n",
        "            # No extant forest: all deforested\n",
        "            intactness_extant_mean = intactness_extant_std = None\n",
        "            intactness_original_mean = 0.0\n",
        "            intactness_original_std = 0.0\n",
        "\n",
        "        # Calculate area per intactness score (includes score 0)\n",
        "        score_areas = {}\n",
        "        valid_values = intactness_values[valid_mask]\n",
        "        valid_areas = cell_area_ha[valid_mask]\n",
        "        if len(valid_values) > 0:\n",
        "            unique_scores = np.unique(valid_values)\n",
        "            for score in unique_scores:\n",
        "                score_mask = (valid_values == score)\n",
        "                score_areas[int(score)] = valid_areas[score_mask].sum(dtype='float64')\n",
        "\n",
        "        # Build output row\n",
        "        new_row_dict = {\n",
        "            'Name': polygon_name,\n",
        "            'Percentage change (extant) mean': percent_extant_mean,\n",
        "            'Percentage change (extant) stdev': percent_extant_std,\n",
        "            'Percentage change (extant) CI95': percent_extant_ci,\n",
        "            'Percentage change (original) mean': percent_original_mean,\n",
        "            'Percentage change (original) stdev': percent_original_std,\n",
        "            'Percentage change (original) CI95': percent_original_ci,\n",
        "            f'Intactness (extant) mean / {total_score}': intactness_extant_mean,\n",
        "            f'Intactness (extant) stdev / {total_stdev}': intactness_extant_std,\n",
        "            f'Intactness (original) mean / {total_score}': intactness_original_mean,\n",
        "            f'Intactness (original) stdev / {total_stdev}': intactness_original_std,\n",
        "        }\n",
        "\n",
        "        # Append score area columns dynamically\n",
        "        for score, area in score_areas.items(): new_row_dict[f'Intactness score {score} area (ha)'] = area\n",
        "        new_row = pd.DataFrame([new_row_dict], dtype=object)\n",
        "        df_intactness_stats = pd.concat([df_intactness_stats, new_row], ignore_index=True)\n",
        "\n",
        "    # Save CSV for this intactness raster\n",
        "    df_intactness_stats = df_intactness_stats.set_index('Name')\n",
        "    df_intactness_stats.to_csv(intactness_csv_path)\n",
        "\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Intactness raster progress: {progress_index}/{progress_total}\"\n",
        "\n",
        "print(\"Intactness statistics completed.\")"
      ],
      "metadata": {
        "id": "snELUKMYMZ5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC-rviHPi4q5"
      },
      "source": [
        "# Report statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHpAf6qg1dEf"
      },
      "outputs": [],
      "source": [
        "# Provides customisable summary of all prior statistics for reporting\n",
        "\n",
        "# Load summary stats\n",
        "summary_scenario_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_scenario_stats.csv'), index_col=0)\n",
        "summary_disturbance_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_disturbance_stats.csv'), index_col=0)\n",
        "summary_restoration_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_restoration_stats.csv'), index_col=0)\n",
        "summary_land_forest_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_land_and_forest_cover_stats.csv'), index_col=0)\n",
        "\n",
        "# Print available forest cover scenarios\n",
        "forest_cover_cols = [col for col in summary_land_forest_df.columns if col.endswith('forest cover (ha)')]\n",
        "forest_cover_scenarios = [col.replace(' forest cover (ha)', '') for col in forest_cover_cols]\n",
        "print(\"forest_cover_list = [\")\n",
        "for scenario in forest_cover_scenarios:\n",
        "    print(f\"  '{scenario}',\")\n",
        "print(\"]\\n\")\n",
        "\n",
        "# Print available AGB scenarios\n",
        "print(\"scenario_list = [\")\n",
        "for csv in sorted(os.listdir(scenario_stats_by_scenario_dir)):\n",
        "    if csv.endswith('.csv'): print(f\"  '{csv[:-4]}',\")\n",
        "print(\"]\\n\")\n",
        "\n",
        "# Print available disturbances\n",
        "print(\"disturbance_list = [\")\n",
        "for csv in sorted(os.listdir(disturbance_stats_by_disturbance_dir)):\n",
        "    if csv.endswith('.csv'): print(f\"  '{csv[:-4]}',\")\n",
        "print(\"]\\n\")\n",
        "\n",
        "# Print available restorations\n",
        "print(\"restoration_list = [\")\n",
        "for csv in sorted(os.listdir(restoration_stats_by_restoration_dir)):\n",
        "    if csv.endswith('.csv'): print(f\"  '{csv[:-4]}',\")\n",
        "print(\"]\")\n",
        "\n",
        "# Print available intactness stats\n",
        "print(\"intactness_list = [\")\n",
        "for csv in sorted(os.listdir(intactness_stats_dir)):\n",
        "    if csv.endswith('.csv'): print(f\"  '{csv[:-4]}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "forest_cover_list = [\n",
        "  '2024_undisturbed_since_1996',\n",
        "  '2024_undisturbed_since_oldgrowth',\n",
        "  '2024',\n",
        "]\n",
        "\n",
        "scenario_list = [\n",
        "  '2024_undisturbed_since_1996',\n",
        "  '2024_undisturbed_since_oldgrowth',\n",
        "  '2024',\n",
        "]\n",
        "\n",
        "disturbance_list = [\n",
        "  '2024_deforestation_since_oldgrowth',\n",
        "  '2024_degradation_since_oldgrowth',\n",
        "  '2024_degradation_since_1996',\n",
        "]\n",
        "\n",
        "restoration_list = [\n",
        "  '2024_recovery_potential_with_edge_effects',\n",
        "  '2024_reforestation_potential',\n",
        "  '2024_restoration_potential',\n",
        "]\n",
        "\n",
        "intactness_list = [\n",
        "  'forest_reserves_10_quantiles__2024_undisturbed_since_1996__agbd_v2_1_260206_133525.tif',\n",
        "  'forest_reserves_10_quantiles__2024_undisturbed_since_oldgrowth__agbd_v2_1_260206_133525.tif',\n",
        "]"
      ],
      "metadata": {
        "id": "F9R3CBMyZszw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCMvXcIa2Q0D"
      },
      "outputs": [],
      "source": [
        "# Generate report CSVs from selected statistics\n",
        "\n",
        "polygon_names = summary_scenario_df.index.tolist()\n",
        "\n",
        "# Forest cover\n",
        "forest_cover = pd.DataFrame(index=polygon_names)\n",
        "forest_cover.index.name = 'Name'\n",
        "forest_cover['Area (km^2)'] = summary_land_forest_df['Area (km^2)']\n",
        "for scenario in forest_cover_list:\n",
        "    col = f'{scenario} forest cover (ha)'\n",
        "    if col in summary_land_forest_df.columns:\n",
        "        forest_cover[col] = summary_land_forest_df[col]\n",
        "forest_cover.to_csv(join(report_statistics_dir, 'forest_cover.csv'))\n",
        "\n",
        "# Scenarios - total AGB\n",
        "scenarios_total_agb = pd.DataFrame(index=polygon_names)\n",
        "scenarios_total_agb.index.name = 'Name'\n",
        "for scenario in scenario_list:\n",
        "    col = f'{scenario} AGB (Tg)'\n",
        "    if col in summary_scenario_df.columns:\n",
        "        scenarios_total_agb[col] = summary_scenario_df[col]\n",
        "if source_dir == predictions_dir:\n",
        "    for scenario in scenario_list:\n",
        "        col = f'{scenario} AGB CI95 (Tg)'\n",
        "        if col in summary_scenario_df.columns:\n",
        "            scenarios_total_agb[col] = summary_scenario_df[col]\n",
        "scenarios_total_agb.to_csv(join(report_statistics_dir, 'scenarios_total_agb.csv'))\n",
        "\n",
        "# Scenarios - AGBD\n",
        "scenarios_agbd = pd.DataFrame(index=polygon_names)\n",
        "scenarios_agbd.index.name = 'Name'\n",
        "for scenario in scenario_list:\n",
        "    scenario_df = pd.read_csv(join(scenario_stats_by_scenario_dir, f'{scenario}.csv'), index_col=0)\n",
        "    scenarios_agbd[f'{scenario} AGBD (Mg/ha)'] = scenario_df['AGBD mean (Mg / ha)']\n",
        "if source_dir == predictions_dir:\n",
        "    for scenario in scenario_list:\n",
        "        scenario_df = pd.read_csv(join(scenario_stats_by_scenario_dir, f'{scenario}.csv'), index_col=0)\n",
        "        scenarios_agbd[f'{scenario} AGBD CI95 (Mg/ha)'] = scenario_df['AGBD CI95 (Mg / ha)']\n",
        "scenarios_agbd.to_csv(join(report_statistics_dir, 'scenarios_agbd.csv'))\n",
        "\n",
        "# Disturbance - total AGB\n",
        "disturbance_total_agb = pd.DataFrame(index=polygon_names)\n",
        "disturbance_total_agb.index.name = 'Name'\n",
        "for disturbance in disturbance_list:\n",
        "    col = f'{disturbance} AGB (Tg)'\n",
        "    if col in summary_disturbance_df.columns:\n",
        "        disturbance_total_agb[col] = summary_disturbance_df[col]\n",
        "if source_dir == predictions_dir:\n",
        "    for disturbance in disturbance_list:\n",
        "        col = f'{disturbance} AGB CI95 (Tg)'\n",
        "        if col in summary_disturbance_df.columns:\n",
        "            disturbance_total_agb[col] = summary_disturbance_df[col]\n",
        "disturbance_total_agb.to_csv(join(report_statistics_dir, 'disturbance_total_agb.csv'))\n",
        "\n",
        "# Disturbance - AGBD\n",
        "disturbance_agbd = pd.DataFrame(index=polygon_names)\n",
        "disturbance_agbd.index.name = 'Name'\n",
        "for disturbance in disturbance_list:\n",
        "    disturbance_df = pd.read_csv(join(disturbance_stats_by_disturbance_dir, f'{disturbance}.csv'), index_col=0)\n",
        "    disturbance_agbd[f'{disturbance} AGBD (Mg/ha)'] = disturbance_df['AGBD mean (Mg / ha)']\n",
        "if source_dir == predictions_dir:\n",
        "    for disturbance in disturbance_list:\n",
        "        disturbance_df = pd.read_csv(join(disturbance_stats_by_disturbance_dir, f'{disturbance}.csv'), index_col=0)\n",
        "        disturbance_agbd[f'{disturbance} AGBD CI95 (Mg/ha)'] = disturbance_df['AGBD CI95 (Mg / ha)']\n",
        "disturbance_agbd.to_csv(join(report_statistics_dir, 'disturbance_agbd.csv'))\n",
        "\n",
        "# Restoration - total AGB\n",
        "restoration_total_agb = pd.DataFrame(index=polygon_names)\n",
        "restoration_total_agb.index.name = 'Name'\n",
        "for restoration in restoration_list:\n",
        "    col = f'{restoration} AGB (Tg)'\n",
        "    if col in summary_restoration_df.columns:\n",
        "        restoration_total_agb[col] = summary_restoration_df[col]\n",
        "if source_dir == predictions_dir:\n",
        "    for restoration in restoration_list:\n",
        "        col = f'{restoration} AGB CI95 (Tg)'\n",
        "        if col in summary_restoration_df.columns:\n",
        "            restoration_total_agb[col] = summary_restoration_df[col]\n",
        "restoration_total_agb.to_csv(join(report_statistics_dir, 'restoration_total_agb.csv'))\n",
        "\n",
        "# Restoration - AGBD\n",
        "restoration_agbd = pd.DataFrame(index=polygon_names)\n",
        "restoration_agbd.index.name = 'Name'\n",
        "for restoration in restoration_list:\n",
        "    restoration_df = pd.read_csv(join(restoration_stats_by_restoration_dir, f'{restoration}.csv'), index_col=0)\n",
        "    restoration_agbd[f'{restoration} AGBD (Mg/ha)'] = restoration_df['AGBD mean (Mg / ha)']\n",
        "if source_dir == predictions_dir:\n",
        "    for restoration in restoration_list:\n",
        "        restoration_df = pd.read_csv(join(restoration_stats_by_restoration_dir, f'{restoration}.csv'), index_col=0)\n",
        "        restoration_agbd[f'{restoration} AGBD CI95 (Mg/ha)'] = restoration_df['AGBD CI95 (Mg / ha)']\n",
        "restoration_agbd.to_csv(join(report_statistics_dir, 'restoration_agbd.csv'))\n",
        "\n",
        "# Intactness - percentage change\n",
        "intactness_percentage = pd.DataFrame(index=polygon_names)\n",
        "intactness_percentage.index.name = 'Name'\n",
        "for intactness in intactness_list:\n",
        "    intactness_df = pd.read_csv(join(intactness_stats_dir, f'{intactness}.csv'), index_col=0)\n",
        "    intactness_percentage[f'{intactness} % change (extant)'] = intactness_df['Percentage change (extant) mean']\n",
        "    intactness_percentage[f'{intactness} % change (original)'] = intactness_df['Percentage change (original) mean']\n",
        "if source_dir == predictions_dir:\n",
        "    for intactness in intactness_list:\n",
        "        intactness_df = pd.read_csv(join(intactness_stats_dir, f'{intactness}.csv'), index_col=0)\n",
        "        intactness_percentage[f'{intactness} % change (extant) CI95'] = intactness_df['Percentage change (extant) CI95']\n",
        "        intactness_percentage[f'{intactness} % change (original) CI95'] = intactness_df['Percentage change (original) CI95']\n",
        "intactness_percentage.to_csv(join(report_statistics_dir, 'intactness_percentage.csv'))\n",
        "\n",
        "print(\"Report statistics completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b70oSO_VNtkY"
      },
      "source": [
        "# Sankey plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHEy0mBX2yNj"
      },
      "outputs": [],
      "source": [
        "# Define and create directories\n",
        "sankey_labelled = join(sankey_dir, 'sankey_labelled')\n",
        "sankey_unlabelled = join(sankey_dir, 'sankey_unlabelled')\n",
        "sankey_labelled_svg = join(sankey_dir, 'sankey_labelled_svg')\n",
        "sankey_unlabelled_svg = join(sankey_dir, 'sankey_unlabelled_svg')\n",
        "\n",
        "for dir in [sankey_labelled, sankey_unlabelled, sankey_labelled_svg, sankey_unlabelled_svg]:\n",
        "    makedirs(dir, exist_ok=True)\n",
        "\n",
        "# Load the CSV files\n",
        "summary_scenario_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_scenario_stats.csv'))\n",
        "summary_disturbance_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_disturbance_stats.csv'))\n",
        "\n",
        "# Check that all rows in both .csv files have the same strings (polygon areas) in column A\n",
        "polygon_areas_stats = summary_scenario_stats.iloc[:, 0]\n",
        "polygon_areas_disturbance_stats = summary_disturbance_stats.iloc[:, 0]\n",
        "\n",
        "assert all(polygon_areas_stats == polygon_areas_disturbance_stats), \"Polygon areas do not match between the two CSV files.\"\n",
        "\n",
        "# Print columns relevant for sankey diagram configuration\n",
        "\n",
        "# Filter for AGB columns only (exclude forest cover and CI95 for initial selection)\n",
        "summary_agb_cols = [col for col in summary_scenario_stats.columns[1:] if 'AGB (Tg)' in col and 'CI95' not in col]\n",
        "disturbance_agb_cols = [col for col in summary_disturbance_stats.columns[1:] if 'AGB (Tg)' in col and 'CI95' not in col]\n",
        "\n",
        "print(\"summary_scenario_stats.csv AGB columns\")\n",
        "print(\"(for old_growth_agb_column and current_agb_column)\\n\")\n",
        "\n",
        "# Group by category\n",
        "current_year_cols = [col for col in summary_agb_cols if col.startswith('20') and col[4:].startswith(' ')]\n",
        "oldgrowth_cols = [col for col in summary_agb_cols if 'oldgrowth' in col and not col.endswith('_1 AGB (Tg)') and not col.endswith('_2 AGB (Tg)')]\n",
        "undisturbed_cols = [col for col in summary_agb_cols if 'undisturbed' in col]\n",
        "no_degradation_cols = [col for col in summary_agb_cols if 'no_degradation' in col]\n",
        "\n",
        "print(\"Current year scenarios:\")\n",
        "for i, col in enumerate(current_year_cols, 1):\n",
        "    print(f\"  {col}\")\n",
        "\n",
        "print(\"\\nOld-growth scenarios:\")\n",
        "for i, col in enumerate(oldgrowth_cols, 1):\n",
        "    print(f\"  {col}\")\n",
        "\n",
        "print(\"\\nUndisturbed scenarios:\")\n",
        "for i, col in enumerate(undisturbed_cols, 1):\n",
        "    print(f\"  {col}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"summary_disturbance_stats.csv AGB columns\")\n",
        "print(\"(for degradation/deforestation since/total columns)\\n\")\n",
        "\n",
        "# Group disturbance columns\n",
        "degradation_cols = [col for col in disturbance_agb_cols if 'degradation_since' in col and 'effect' not in col]\n",
        "deforestation_cols = [col for col in disturbance_agb_cols if 'deforestation_since' in col]\n",
        "\n",
        "print(\"Degradation columns:\")\n",
        "for i, col in enumerate(degradation_cols, 1):\n",
        "    print(f\"   {col}\")\n",
        "\n",
        "print(\"\\nDeforestation columns:\")\n",
        "for i, col in enumerate(deforestation_cols, 1):\n",
        "    print(f\"   {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ykformx2-nl"
      },
      "outputs": [],
      "source": [
        "# Plot degradation and deforestation separately\n",
        "separate_disturbance = True\n",
        "# Plot degradation before and since a date separately\n",
        "separate_degradation = True\n",
        "\n",
        "# DPI (default is 96, output image will scale accordingly)\n",
        "dpi = 300\n",
        "# Relative width modifier (ratio, e.g. 0.5 or 2)\n",
        "width_modifier = 0.85\n",
        "\n",
        "# Title (polygon area), density and label variables (weight of 800 ~ bold, 400 ~ normal)\n",
        "show_title = True\n",
        "show_density = True\n",
        "show_labels = True\n",
        "left_axis_label = True\n",
        "svg_transparent_background = True\n",
        "title_font_size = 20\n",
        "title_font_weight = 600\n",
        "density_font_size = 17\n",
        "density_font_weight = 600\n",
        "label_font_size = 17\n",
        "label_font_weight = 600\n",
        "\n",
        "# Base columns and year (summary_scenario_stats)\n",
        "old_growth_agb_column = '2024_undisturbed_since_oldgrowth AGB (Tg)'\n",
        "current_agb_column = '2024 AGB (Tg)'\n",
        "current_year = current_agb_column.split(' ')[0]\n",
        "\n",
        "# Disturbance columns (summary_disturbance_stats)\n",
        "degradation_since_column = '2024_degradation_since_1996 AGB (Tg)'\n",
        "degradation_total_column = '2024_degradation_since_oldgrowth AGB (Tg)'\n",
        "deforestation_total_column = '2024_deforestation_since_oldgrowth AGB (Tg)'\n",
        "\n",
        "# Node labels and colours\n",
        "remaining_name = f'Remaining in {current_year}:'\n",
        "remaining_colour = '#007fff'\n",
        "degradation_before_name = 'Degradation loss before 1996'\n",
        "degradation_before_colour = '#8dc00d'\n",
        "degradation_since_name = 'Degradation loss since 1996'\n",
        "degradation_since_colour = '#ffff00'\n",
        "degradation_total_name = 'Degradation'\n",
        "degradation_total_colour = '#ffff00'\n",
        "deforestation_total_name = 'Deforestation loss'\n",
        "deforestation_total_colour = '#ffffff'\n",
        "disturbance_total_name = 'Disturbance'\n",
        "disturbance_total_colour = '#ffffff'\n",
        "\n",
        "# Validate separation settings\n",
        "assert not separate_degradation or separate_disturbance, \"separate_disturbance must be True if separate_degradation is True.\"\n",
        "\n",
        "# Function to get values from statistics\n",
        "def get_value(df, idx, column_name):\n",
        "    try:\n",
        "        value = df.loc[idx, column_name]\n",
        "        return 0.0 if pd.isnull(value) else float(value)\n",
        "    except KeyError:\n",
        "        print(f\"Column '{column_name}' not found in the dataframe.\")\n",
        "        return 0.0\n",
        "\n",
        "# Loop through each row (polygon area)\n",
        "for idx in summary_scenario_stats.index:\n",
        "    polygon_name = summary_scenario_stats.iloc[idx, 0]\n",
        "\n",
        "    # Get old-growth and current AGB values\n",
        "    old_growth_agb = get_value(summary_scenario_stats, idx, old_growth_agb_column)\n",
        "    current_agb = get_value(summary_scenario_stats, idx, current_agb_column)\n",
        "\n",
        "    # Get disturbance values and calculate before values\n",
        "    degradation_total = get_value(summary_disturbance_stats, idx, degradation_total_column)\n",
        "    if separate_degradation:\n",
        "      degradation_since = get_value(summary_disturbance_stats, idx, degradation_since_column)\n",
        "      degradation_before = degradation_total - degradation_since\n",
        "    deforestation_total = get_value(summary_disturbance_stats, idx, deforestation_total_column)\n",
        "    disturbance_total = degradation_total + deforestation_total\n",
        "\n",
        "    # Check for statistical / precision discrepencies of greater than 10 tonnes (~ a single tree)\n",
        "    discrepancy = abs(current_agb - disturbance_total - old_growth_agb)\n",
        "    if discrepancy >= 1e-5:\n",
        "        print(f\"{polygon_name}: current_agb - disturbance_total != old_growth_agb (discrepancy: {discrepancy:.5e})\")\n",
        "\n",
        "    # Load detailed stats for AGBD and CI95 values\n",
        "    stats_df = pd.read_csv(join(scenario_stats_by_area_dir, f\"{polygon_name}.csv\"))\n",
        "    old_growth_index = stats_df.index[stats_df['scenario'] == f\"{old_growth_agb_column.split(' ')[0]}\"].item()\n",
        "    current_index = stats_df.index[stats_df['scenario'] == f\"{current_agb_column.split(' ')[0]}\"].item()\n",
        "\n",
        "    old_growth_mean_agbd = get_value(stats_df, old_growth_index, \"AGBD mean (Mg / ha)\")\n",
        "    current_mean_agbd = get_value(stats_df, current_index, \"AGBD mean (Mg / ha)\")\n",
        "\n",
        "    uncertainty = 'AGB total CI95 (Tg)' in stats_df.columns\n",
        "    if uncertainty:\n",
        "        old_growth_agb_ci95 = get_value(stats_df, old_growth_index, \"AGB total CI95 (Tg)\")\n",
        "        old_growth_mean_agbd_ci95 = get_value(stats_df, old_growth_index, \"AGBD CI95 (Mg / ha)\")\n",
        "        current_agb_ci95 = get_value(stats_df, current_index, \"AGB total CI95 (Tg)\")\n",
        "        current_mean_agbd_ci95 = get_value(stats_df, current_index, \"AGBD CI95 (Mg / ha)\")\n",
        "\n",
        "    # Build title and subtitle text\n",
        "    title_name = f\"{polygon_name}\"\n",
        "\n",
        "    if uncertainty:\n",
        "        subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} ± {old_growth_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "        subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} ± {current_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "        left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} ± {old_growth_agb_ci95:.2f} Tg\" if left_axis_label else ''\n",
        "        remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} ± {current_agb_ci95:.2f} Tg\"\n",
        "    else:\n",
        "        subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} Mg / ha\"\n",
        "        subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} Mg / ha\"\n",
        "        left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} Tg\" if left_axis_label else ''\n",
        "        remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} Tg\"\n",
        "\n",
        "    if separate_disturbance and separate_degradation:\n",
        "        nodes = [left_axis, degradation_before_name, degradation_since_name, deforestation_total_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0, 0], [1, 2, 3, 4]\n",
        "        values = [-degradation_before, -degradation_since, -deforestation_total, current_agb]\n",
        "        colors = [degradation_before_colour, degradation_since_colour, deforestation_total_colour, remaining_colour]\n",
        "\n",
        "    elif separate_disturbance and not separate_degradation:\n",
        "        nodes = [left_axis, degradation_total_name, deforestation_total_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0], [1, 2, 3]\n",
        "        values = [-degradation_total, -deforestation_total, current_agb]\n",
        "        colors = [degradation_total_colour, deforestation_total_colour, remaining_colour]\n",
        "\n",
        "    else:\n",
        "        nodes = [left_axis, disturbance_total_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0], [1, 2]\n",
        "        values = [-(degradation_total + deforestation_total), current_agb]\n",
        "        colors = [disturbance_total_colour, remaining_colour]\n",
        "\n",
        "    node_colors = [remaining_colour] + colors\n",
        "\n",
        "    # Add percentages to node labels\n",
        "    percentages = [(abs(val) / old_growth_agb * 100) for val in values]\n",
        "    for i in range(1, len(nodes)):\n",
        "        if i - 1 < len(percentages):\n",
        "            nodes[i] += f\" ({percentages[i-1]:.0f}%)\"\n",
        "\n",
        "    # Configure title and density annotations\n",
        "    title_and_density = [\n",
        "        dict(x=0, y=1.28, xref='paper', yref='paper', text=title_name, showarrow=False, xanchor='left', align='left',\n",
        "             font=dict(family=\"arial, sans serif\", size=title_font_size, color=\"black\", weight=title_font_weight)),\n",
        "        dict(x=0, y=1.19, xref='paper', yref='paper', text=subtitle_1_name, showarrow=False, xanchor='left', align='left',\n",
        "             font=dict(family=\"arial, sans serif\", size=density_font_size, color=\"black\", weight=density_font_weight)),\n",
        "        dict(x=0, y=1.11, xref='paper', yref='paper', text=subtitle_2_name, showarrow=False, xanchor='left', align='left',\n",
        "             font=dict(family=\"arial, sans serif\", size=density_font_size, color=\"black\", weight=density_font_weight))\n",
        "    ]\n",
        "\n",
        "    if show_title and not show_density:\n",
        "        title_and_density = title_and_density[0:1]\n",
        "    elif not show_title and show_density:\n",
        "        title_and_density = title_and_density[1:3]\n",
        "    elif not show_title and not show_density:\n",
        "        title_and_density = []\n",
        "\n",
        "    # Remove labels if toggled off\n",
        "    if not show_labels:\n",
        "        nodes = [''] * len(nodes)\n",
        "\n",
        "    # Create sankey diagram\n",
        "    fig = go.Figure(data=[go.Sankey(\n",
        "        arrangement=\"freeform\",\n",
        "        node=dict(label=nodes, color=node_colors, pad=15, thickness=20, line=dict(color=\"black\", width=1)),\n",
        "        link=dict(source=sources, target=targets, value=values, color=colors, line=dict(color=\"black\", width=1))\n",
        "    )])\n",
        "\n",
        "    fig.update_layout(\n",
        "        width=700 * width_modifier, height=500,\n",
        "        font=dict(family=\"arial, sans serif\", size=label_font_size, color=\"black\", weight=label_font_weight),\n",
        "        margin=dict(l=25, r=25, t=115, b=25),\n",
        "        annotations=title_and_density\n",
        "    )\n",
        "\n",
        "    # Save labelled versions\n",
        "    fig.write_image(join(sankey_labelled, f'sankey_diagram_{polygon_name}.png'), scale=dpi / 96)\n",
        "    if svg_transparent_background:\n",
        "        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "    fig.write_image(join(sankey_labelled_svg, f'sankey_diagram_vector_{polygon_name}.svg'), scale=dpi / 96)\n",
        "\n",
        "    # Create and save unlabelled versions\n",
        "    fig_unlabelled = go.Figure(data=[go.Sankey(\n",
        "        arrangement=\"freeform\",\n",
        "        node=dict(label=[''] * len(nodes), color=node_colors, pad=15, thickness=20, line=dict(color=\"black\", width=1)),\n",
        "        link=dict(source=sources, target=targets, value=values, color=colors, line=dict(color=\"black\", width=1))\n",
        "    )])\n",
        "\n",
        "    fig_unlabelled.update_layout(\n",
        "        width=700 * width_modifier, height=500,\n",
        "        font=dict(family=\"arial, sans serif\", size=label_font_size, color=\"black\", weight=label_font_weight),\n",
        "        margin=dict(l=25, r=25, t=115, b=25)\n",
        "    )\n",
        "\n",
        "    fig_unlabelled.write_image(join(sankey_unlabelled, f'sankey_diagram_{polygon_name}.png'), scale=dpi / 96)\n",
        "    if svg_transparent_background:\n",
        "        fig_unlabelled.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "    fig_unlabelled.write_image(join(sankey_unlabelled_svg, f'sankey_diagram_vector_{polygon_name}.svg'), scale=dpi / 96)\n",
        "\n",
        "    # Display figure with white background\n",
        "    fig.update_layout(plot_bgcolor='white', paper_bgcolor='white')\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disturbance trend plots"
      ],
      "metadata": {
        "id": "kbHvNiusiHvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyses yearly forest cover loss and degradation effect trends\n",
        "# relative to a target year and baseline\n",
        "\n",
        "target_year = '2024'\n",
        "start_year = '1996'\n",
        "use_oldgrowth_baseline = True  # if False, uses start_year as baseline\n",
        "\n",
        "# Print available polygons\n",
        "print(\"selected_trend_polygons = [\")\n",
        "for _, row in selected_sample_polygons_gpkg.iterrows(): print(f\"  '{row['name']}',\")\n",
        "print(\"]\")"
      ],
      "metadata": {
        "id": "HNX__bWPiLjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_trend_polygons = [\n",
        "  # 'All protected areas',\n",
        "  # 'Taman Negara National Park',\n",
        "  # 'Tengku Hassanal Wildlife Reserve',\n",
        "  # 'All forest reserves',\n",
        "  # 'Ais forest reserves',\n",
        "  'Berkelah Jerantut forest reserve',\n",
        "  # 'Berkelah Kuantan forest reserves',\n",
        "  # 'Berkelah Temerloh forest reserve',\n",
        "  # 'Remen Chereh forest reserves',\n",
        "  # 'Tekai Tembeling forest reserves',\n",
        "  # 'Tekam forest reserve',\n",
        "  # 'Yong forest reserves',\n",
        "  # 'Yong Lipis forest reserves',\n",
        "  # 'ASARTR phase 1',\n",
        "  'ASARTR phase 2',\n",
        "  'Tekai Tembeling forest reserves new',\n",
        "  # 'All forest reserves new',\n",
        "]"
      ],
      "metadata": {
        "id": "tLKOFeAKiSsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate year range\n",
        "try: start_int, target_int = int(start_year), int(target_year)\n",
        "except ValueError: raise ValueError(\"start_year and target_year must be numeric\")\n",
        "year_range = [str(y) for y in range(start_int, target_int + 1)]\n",
        "\n",
        "# Load summary data\n",
        "summary_land_forest_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_land_and_forest_cover_stats.csv'), index_col=0)\n",
        "summary_disturbance_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_disturbance_stats.csv'), index_col=0)\n",
        "summary_scenario_stats = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_scenario_stats.csv'), index_col=0)\n",
        "\n",
        "# Validate target year data exists\n",
        "target_forest_col = f'{target_year} forest cover (ha)'\n",
        "if target_forest_col not in summary_land_forest_df.columns:\n",
        "    raise FileNotFoundError(f\"Target year forest cover not found: {target_forest_col}\")\n",
        "\n",
        "# Determine baseline names\n",
        "if use_oldgrowth_baseline:\n",
        "    forest_baseline_scenario = f'{target_year}_undisturbed_since_oldgrowth'\n",
        "    degradation_baseline_name = f'{target_year}_degradation_since_oldgrowth'\n",
        "    baseline_label = 'old-growth'\n",
        "else:\n",
        "    forest_baseline_scenario = f'{target_year}_undisturbed_since_{start_year}'\n",
        "    degradation_baseline_name = f'{target_year}_degradation_since_{start_year}'\n",
        "    baseline_label = start_year\n",
        "\n",
        "# Check baseline availability\n",
        "forest_baseline_col = f'{forest_baseline_scenario} forest cover (ha)'\n",
        "forest_baseline_available = forest_baseline_col in summary_land_forest_df.columns\n",
        "if not forest_baseline_available:\n",
        "    raise FileNotFoundError(f\"Forest baseline required: {forest_baseline_col}\")\n",
        "\n",
        "degradation_baseline_path = join(disturbance_stats_by_disturbance_dir, f'{degradation_baseline_name}.csv')\n",
        "if not exists(degradation_baseline_path):\n",
        "    raise FileNotFoundError(f\"Degradation baseline required: {degradation_baseline_path}\")\n",
        "\n",
        "# Validate forest cover years (need year before start for first loss calculation)\n",
        "prev_year_col = f'{start_int - 1} forest cover (ha)'\n",
        "if prev_year_col not in summary_land_forest_df.columns:\n",
        "    raise FileNotFoundError(f\"Year before start required: {prev_year_col}\")\n",
        "missing_forest = [y for y in year_range if f'{y} forest cover (ha)' not in summary_land_forest_df.columns]\n",
        "if missing_forest: raise FileNotFoundError(f\"Missing forest cover years: {missing_forest}\")\n",
        "\n",
        "# Validate degradation effect years\n",
        "missing_degradation, earliest_available = [], None\n",
        "for year in year_range:\n",
        "    effect_path = join(disturbance_stats_by_disturbance_dir, f'{target_year}_effect_of_degradation_in_{year}.csv')\n",
        "    if not exists(effect_path): missing_degradation.append(year)\n",
        "    elif earliest_available is None: earliest_available = year\n",
        "if missing_degradation:\n",
        "    print(f\"Error: Missing degradation years: {missing_degradation}\")\n",
        "    if earliest_available: print(f\"Earliest available: {earliest_available}\")\n",
        "    raise FileNotFoundError(\"Degradation data incomplete for year range\")\n",
        "\n",
        "# Validate polygons\n",
        "available_polygons = summary_land_forest_df.index.tolist()\n",
        "invalid_polygons = [p for p in selected_trend_polygons if p not in available_polygons]\n",
        "if invalid_polygons: raise ValueError(f\"Polygons not found: {invalid_polygons}\")\n",
        "\n",
        "# Load reference data\n",
        "degradation_baseline_df = pd.read_csv(degradation_baseline_path, index_col=0)\n",
        "\n",
        "# Load all degradation effect data\n",
        "degradation_effect_data = {}\n",
        "for year in year_range:\n",
        "    effect_path = join(disturbance_stats_by_disturbance_dir, f'{target_year}_effect_of_degradation_in_{year}.csv')\n",
        "    degradation_effect_data[year] = pd.read_csv(effect_path, index_col=0)\n",
        "\n",
        "# Check uncertainty availability\n",
        "generate_uncertainty_stats = (source_dir == predictions_dir)\n",
        "if generate_uncertainty_stats:\n",
        "    if 'AGB total CI95 (Tg)' not in degradation_effect_data[year_range[0]].columns:\n",
        "        print(\"Warning: CI95 not found, disabling uncertainty\")\n",
        "        generate_uncertainty_stats = False\n",
        "\n",
        "print(f\"Validated: {target_year} target, {start_year}-{target_year} range, {baseline_label} baseline, {len(selected_trend_polygons)} polygons\")"
      ],
      "metadata": {
        "id": "Vzvo3WFPGhvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each polygon\n",
        "progress_total = len(selected_trend_polygons)\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Polygon progress: {progress_index}/{progress_total}\")\n",
        "display(progress_label)\n",
        "\n",
        "\n",
        "# Plot settings\n",
        "dpi = 300\n",
        "width = 1000\n",
        "height = 600\n",
        "svg_transparent_background = True\n",
        "show_title = True\n",
        "forest_cover_colour = '#8dc00d'\n",
        "degradation_colour = '#ffff00'\n",
        "degradation_ci95_colour = 'rgba(255, 255, 0, 0.3)'\n",
        "title_size = 21\n",
        "subtitle_size = 18\n",
        "axis_title_size = 17\n",
        "axis_value_size = 15\n",
        "legend_size = 17\n",
        "\n",
        "# Line labels\n",
        "forest_line_label = 'Forest cover loss'\n",
        "degrad_line_label = 'Degradation effect with CI95' if generate_uncertainty_stats else 'Degradation effect'\n",
        "\n",
        "for polygon_name in selected_trend_polygons:\n",
        "    polygon_trends_dir = join(disturbance_trends_dir, polygon_name)\n",
        "    makedirs(polygon_trends_dir, exist_ok=True)\n",
        "\n",
        "    # Subdirectories for labelled and unlabelled outputs\n",
        "    trends_labelled = join(polygon_trends_dir, 'trends_labelled')\n",
        "    trends_labelled_svg = join(polygon_trends_dir, 'trends_labelled_svg')\n",
        "    trends_unlabelled = join(polygon_trends_dir, 'trends_unlabelled')\n",
        "    trends_unlabelled_svg = join(polygon_trends_dir, 'trends_unlabelled_svg')\n",
        "    for subdir in [trends_labelled, trends_labelled_svg, trends_unlabelled, trends_unlabelled_svg]:\n",
        "        makedirs(subdir, exist_ok=True)\n",
        "\n",
        "    # Reference values for this polygon\n",
        "    baseline_forest = summary_land_forest_df.loc[polygon_name, forest_baseline_col]\n",
        "    target_forest = summary_land_forest_df.loc[polygon_name, target_forest_col]\n",
        "    total_forest_loss = target_forest - baseline_forest\n",
        "    pct_forest_of_baseline = (target_forest / baseline_forest * 100) if baseline_forest != 0 else 0\n",
        "\n",
        "    # Get baseline total AGB from summary disturbance stats\n",
        "    baseline_agb_col = f'{degradation_baseline_name} AGB (Tg)'\n",
        "    baseline_degradation_agb = abs(summary_disturbance_df.loc[polygon_name, baseline_agb_col])\n",
        "\n",
        "    # Forest cover trends (negative = loss)\n",
        "    forest_trends = []\n",
        "    for year in year_range:\n",
        "        prev_col, curr_col = f'{int(year) - 1} forest cover (ha)', f'{year} forest cover (ha)'\n",
        "        abs_loss = summary_land_forest_df.loc[polygon_name, curr_col] - summary_land_forest_df.loc[polygon_name, prev_col]\n",
        "        pct_baseline = (abs_loss / abs(total_forest_loss) * 100) if total_forest_loss != 0 else 0\n",
        "        forest_trends.append({'year': int(year), 'absolute_loss_ha': abs_loss, 'pct_of_total_loss': pct_baseline})\n",
        "    forest_trends_df = pd.DataFrame(forest_trends)\n",
        "    forest_trends_df.to_csv(join(polygon_trends_dir, 'forest_cover_trends.csv'), index=False)\n",
        "\n",
        "    # Degradation trends using total AGB\n",
        "    degradation_trends = []\n",
        "    for year in year_range:\n",
        "        effect_df = degradation_effect_data[year]\n",
        "        abs_effect = effect_df.loc[polygon_name, 'AGB total (Tg)']\n",
        "        pct_baseline = (abs_effect / baseline_degradation_agb * 100) if baseline_degradation_agb != 0 else 0\n",
        "        row = {'year': int(year), 'absolute_effect_tg': abs_effect, 'pct_of_total_degradation': pct_baseline}\n",
        "        if generate_uncertainty_stats:\n",
        "            ci95 = effect_df.loc[polygon_name, 'AGB total CI95 (Tg)']\n",
        "            row['ci95_tg'] = ci95\n",
        "            row['pct_ci95'] = (ci95 / baseline_degradation_agb * 100) if baseline_degradation_agb != 0 else 0\n",
        "        degradation_trends.append(row)\n",
        "    degradation_trends_df = pd.DataFrame(degradation_trends)\n",
        "    degradation_trends_df.to_csv(join(polygon_trends_dir, 'degradation_trends.csv'), index=False)\n",
        "\n",
        "    # Get target and baseline scenario AGB for subtitle\n",
        "    target_agb_col = f'{target_year} AGB (Tg)'\n",
        "    baseline_scenario_agb_col = f'{forest_baseline_scenario} AGB (Tg)'\n",
        "    target_agb = summary_scenario_stats.loc[polygon_name, target_agb_col] if target_agb_col in summary_scenario_stats.columns else None\n",
        "    baseline_scenario_agb = summary_scenario_stats.loc[polygon_name, baseline_scenario_agb_col] if baseline_scenario_agb_col in summary_scenario_stats.columns else None\n",
        "    pct_agb_of_baseline = (target_agb / baseline_scenario_agb * 100) if (target_agb is not None and baseline_scenario_agb is not None and baseline_scenario_agb != 0) else None\n",
        "\n",
        "    # Build title and subtitles\n",
        "    title_text = f\"{polygon_name}, {baseline_label} baseline\"\n",
        "\n",
        "    if baseline_scenario_agb is not None and pct_agb_of_baseline is not None:\n",
        "        subtitle1 = f\"Baseline AGB: {baseline_scenario_agb:.2f} Tg | {target_year} is {pct_agb_of_baseline:.1f}% of baseline\"\n",
        "    else: subtitle1 = \"\"\n",
        "\n",
        "    subtitle2 = f\"Baseline forest cover: {baseline_forest:,.0f} ha | {target_year} is {pct_forest_of_baseline:.1f}% of baseline\"\n",
        "\n",
        "    # Plot configurations\n",
        "    years = forest_trends_df['year'].values\n",
        "\n",
        "    plot_configs = [\n",
        "        ('trends_absolute', 'absolute_loss_ha', 'Forest cover loss (ha)',\n",
        "         'absolute_effect_tg', f'Degradation effect on {target_year} AGB (Tg)', 'ci95_tg'),\n",
        "        ('trends_pct_of_total', 'pct_of_total_loss', 'Forest loss (% of total)',\n",
        "         'pct_of_total_degradation', f'Degradation effect (% of total)', 'pct_ci95')]\n",
        "\n",
        "    for filename, forest_col, forest_axis_label, degrad_col, degrad_axis_label, ci95_col in plot_configs:\n",
        "        fig = go.Figure()\n",
        "        # Get data\n",
        "        forest_vals = forest_trends_df[forest_col].values.astype(float)\n",
        "        degrad_vals = degradation_trends_df[degrad_col].values.astype(float)\n",
        "        ci95_vals = None\n",
        "        if generate_uncertainty_stats and ci95_col in degradation_trends_df.columns:\n",
        "            ci95_vals = degradation_trends_df[ci95_col].values.astype(float)\n",
        "            if np.any(pd.isna(ci95_vals)): ci95_vals = None\n",
        "        # Calculate y-axis ranges (min to 0)\n",
        "        forest_min = np.min(forest_vals) * 1.1 if not np.any(pd.isna(forest_vals)) else -1\n",
        "        degrad_min = np.min(degrad_vals) * 1.1 if not np.any(pd.isna(degrad_vals)) else -1\n",
        "        if ci95_vals is not None: degrad_min = min(degrad_min, np.min(degrad_vals - ci95_vals) * 1.1)\n",
        "        # 1. CI95 shading on y1\n",
        "        if ci95_vals is not None and not np.any(pd.isna(degrad_vals)):\n",
        "            upper, lower = degrad_vals + ci95_vals, degrad_vals - ci95_vals\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=np.concatenate([years, years[::-1]]), y=np.concatenate([upper, lower[::-1]]),\n",
        "                fill='toself', fillcolor=degradation_ci95_colour, line=dict(color=degradation_colour, width=1),\n",
        "                showlegend=False, yaxis='y1', hoverinfo='skip'))\n",
        "        # 2. Degradation line on y1 - buffer then black line\n",
        "        if not np.any(pd.isna(degrad_vals)):\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=years, y=degrad_vals, mode='lines',\n",
        "                line=dict(color=degradation_colour, width=6),\n",
        "                showlegend=False, yaxis='y1', hoverinfo='skip'))\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=years, y=degrad_vals, name=degrad_line_label, mode='lines+markers',\n",
        "                line=dict(color='black', width=2),\n",
        "                marker=dict(size=8, color=degradation_colour, line=dict(color='black', width=1)),\n",
        "                yaxis='y1'))\n",
        "        # 3. Forest cover on y2 - buffer then black line\n",
        "        if not np.any(pd.isna(forest_vals)):\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=years, y=forest_vals, mode='lines',\n",
        "                line=dict(color=forest_cover_colour, width=6),\n",
        "                showlegend=False, yaxis='y2', hoverinfo='skip'))\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=years, y=forest_vals, name=forest_line_label, mode='lines+markers',\n",
        "                line=dict(color='black', width=2),\n",
        "                marker=dict(size=8, color=forest_cover_colour, line=dict(color='black', width=1)),\n",
        "                yaxis='y2'))\n",
        "        # Layout\n",
        "        fig.update_layout(\n",
        "            xaxis=dict(\n",
        "                dtick=1, tickfont=dict(family=\"helvetica\", size=axis_value_size, color=\"black\"),\n",
        "                showgrid=True, gridcolor='lightgrey', gridwidth=0.5),\n",
        "            yaxis=dict(\n",
        "                title=degrad_axis_label,\n",
        "                titlefont=dict(family=\"helvetica\", size=axis_title_size, color=\"black\"),\n",
        "                tickfont=dict(family=\"helvetica\", size=axis_value_size, color=\"black\"),\n",
        "                side='left', range=[degrad_min, 0], showgrid=True, gridcolor='lightgrey', gridwidth=0.5),\n",
        "            yaxis2=dict(\n",
        "                title=forest_axis_label,\n",
        "                titlefont=dict(family=\"helvetica\", size=axis_title_size, color=\"black\"),\n",
        "                tickfont=dict(family=\"helvetica\", size=axis_value_size, color=\"black\"),\n",
        "                side='right', overlaying='y', range=[forest_min, 0], showgrid=False),\n",
        "            legend=dict(x=0.01, y=0.01, bgcolor='rgba(255,255,255,0.8)',\n",
        "                        font=dict(family=\"helvetica\", size=legend_size, color=\"black\")),\n",
        "            width=width, height=height, hovermode='x unified',\n",
        "            plot_bgcolor='white', paper_bgcolor='white',\n",
        "            margin=dict(t=115),\n",
        "            annotations=[\n",
        "                dict(x=0, y=1.28, xref='paper', yref='paper', text=title_text, showarrow=False, xanchor='left', align='left',\n",
        "                     font=dict(family=\"helvetica\", size=title_size, color=\"black\", weight=600)),\n",
        "                dict(x=0, y=1.19, xref='paper', yref='paper', text=subtitle1, showarrow=False, xanchor='left', align='left',\n",
        "                     font=dict(family=\"helvetica\", size=subtitle_size, color=\"black\", weight=600)),\n",
        "                dict(x=0, y=1.11, xref='paper', yref='paper', text=subtitle2, showarrow=False, xanchor='left', align='left',\n",
        "                     font=dict(family=\"helvetica\", size=subtitle_size, color=\"black\", weight=600)),\n",
        "                ])\n",
        "\n",
        "        # Save labelled PNG\n",
        "        fig.write_image(join(trends_labelled, f'{filename}.png'), scale=dpi / 96)\n",
        "\n",
        "        # Save labelled SVG\n",
        "        if svg_transparent_background:\n",
        "            fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "        fig.write_image(join(trends_labelled_svg, f'{filename}.svg'), scale=dpi / 96)\n",
        "\n",
        "        # Unlabelled version: remove title, subtitles, axes titles; make legend text invisible but preserve box width\n",
        "        fig.layout.annotations = []\n",
        "        fig.layout.yaxis.title.text = ''\n",
        "        fig.layout.yaxis2.title.text = ''\n",
        "        fig.update_layout(\n",
        "            legend=dict(font=dict(color='rgba(0,0,0,0)')),\n",
        "            plot_bgcolor='white', paper_bgcolor='white'\n",
        "        )\n",
        "\n",
        "        # Save unlabelled PNG\n",
        "        fig.write_image(join(trends_unlabelled, f'{filename}.png'), scale=dpi / 96)\n",
        "\n",
        "        # Save unlabelled SVG\n",
        "        if svg_transparent_background:\n",
        "            fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "        fig.write_image(join(trends_unlabelled_svg, f'{filename}.svg'), scale=dpi / 96)\n",
        "\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Polygon progress: {progress_index}/{progress_total}\"\n",
        "\n",
        "print(\"Disturbance trends completed.\")"
      ],
      "metadata": {
        "id": "pBfo12S4iYtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXAGIvpk_KWS"
      },
      "source": [
        "# Disconnected runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgzXDe-Fnm3T"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V6E1",
      "toc_visible": true,
      "collapsed_sections": [
        "6j0th2jhtnAR",
        "mFRSKZkpTiqO",
        "zjy-T1TqScbE",
        "TaKqkzjhRtDp",
        "6t45TVipaVma",
        "PC-rviHPi4q5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}