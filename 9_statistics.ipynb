{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/9_statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j0th2jhtnAR"
      },
      "source": [
        "# Imports & Subdirectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9drq4euvG0MS"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGWgfUqto0F"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Imports and upgrades\n",
        "!pip install geopandas\n",
        "!pip install kaleido==0.2.1\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MTERlyxuPd4"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from datetime import datetime\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "import kaleido\n",
        "import math\n",
        "import numpy as np\n",
        "from os.path import exists, join\n",
        "from os import makedirs\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import rasterio\n",
        "from rasterio import mask as msk\n",
        "import re\n",
        "from shutil import copyfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwar9sxbts-3"
      },
      "outputs": [],
      "source": [
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "datasets_dir = join(base_dir, \"4_datasets/final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "mask_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "differences_dir = join(base_dir, \"8_differences\")\n",
        "statistics_dir = join(base_dir, \"9_statistics\")\n",
        "sample_polygons_dir = join(statistics_dir, \"sample_polygons\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(statistics_dir, exist_ok=True)\n",
        "makedirs(sample_polygons_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atvGRSPYgwk9"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjy-T1TqScbE"
      },
      "source": [
        "# Select model, area and sample polygons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj5RbVSuEp-B"
      },
      "outputs": [],
      "source": [
        "# Select if to source predictions from scenarios_dir or uncertainty_dir\n",
        "source_dir = uncertainty_dir\n",
        "# source_dir = scenarios_dir\n",
        "source_dir_name = f\"{source_dir.split('_')[-1]}_dir\"\n",
        "\n",
        "# Select the model\n",
        "for subdir in os.listdir(source_dir):\n",
        "  if 'scenario_masks' not in subdir:\n",
        "    print(f\"selected_model = '{subdir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txTXPYFLpH6v"
      },
      "outputs": [],
      "source": [
        "selected_model = 'agbd_251203_161707'\n",
        "\n",
        "# Define prediction, disturbance and intactness directories\n",
        "selected_model_dir = join(models_dir, selected_model)\n",
        "selected_model_prediction_dir = join(source_dir, selected_model)\n",
        "if source_dir == scenarios_dir: prediction_raster_dir = join(selected_model_prediction_dir, 'scenario_predictions')\n",
        "if source_dir == uncertainty_dir: prediction_raster_dir = join(selected_model_prediction_dir, 'uncertainty_predictions')\n",
        "model_differences_dir = join(differences_dir, f\"{selected_model}_{source_dir_name}\")\n",
        "disturbance_dir = join(model_differences_dir, 'disturbance')\n",
        "intactness_dir = join(model_differences_dir, 'intactness')\n",
        "\n",
        "# Check prediction directory\n",
        "if not exists(prediction_raster_dir):\n",
        "  print(f\"Prediction directory doesn't exist yet: {prediction_raster_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(prediction_raster_dir))} rasters in {prediction_raster_dir}\")\n",
        "# Check disturbance directory\n",
        "if not exists(disturbance_dir):\n",
        "  print(f\"Disturbance directory doesn't exist yet: {disturbance_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(disturbance_dir))} rasters in {disturbance_dir}\")\n",
        "# Check intactness directory\n",
        "if not exists(intactness_dir):\n",
        "  print(f\"Intactness directory doesn't exist yet: {intactness_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(intactness_dir))} rasters in {intactness_dir}\")\n",
        "\n",
        "# Define model stats directory\n",
        "model_statistics_dir = join(statistics_dir, f\"{selected_model}_{source_dir_name}\")\n",
        "makedirs(model_statistics_dir, exist_ok=True)\n",
        "\n",
        "# Select sample area polygons. This should be a single .gpkg with the field 'name' differentiating polygons.\n",
        "sample_polygons = []\n",
        "for geopackage in os.listdir(sample_polygons_dir):\n",
        "  sample_polygons.append(geopackage)\n",
        "if len(sample_polygons) == 0:\n",
        "  print(f\"No sample areas found. Upload .gpkg polygons to {sample_polygons_dir}\")\n",
        "else:\n",
        "  for sample_polygon in sample_polygons: print(f\"selected_sample_polygons = '{sample_polygon}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwWvaX5qDANc"
      },
      "outputs": [],
      "source": [
        "selected_sample_polygons = 'tekai_sample_polygons.gpkg'\n",
        "\n",
        "# Load cell area raster for accurate pixel-by-pixel area calculations\n",
        "cell_area_path = join(areas_dir, \"cell_area.tif\")\n",
        "\n",
        "selected_sample_polygons_dir = join(sample_polygons_dir, selected_sample_polygons)\n",
        "selected_sample_polygons_gpkg = gpd.read_file(selected_sample_polygons_dir)\n",
        "sample_polygons_statistics_dir = join(model_statistics_dir, selected_sample_polygons[:-5])\n",
        "makedirs(sample_polygons_statistics_dir, exist_ok=True)\n",
        "agb_total_raster_dir = join(model_statistics_dir, 'agb_total_rasters')\n",
        "makedirs(agb_total_raster_dir, exist_ok=True)\n",
        "agb_total_scenario_dir = join(agb_total_raster_dir, 'scenarios')\n",
        "makedirs(agb_total_scenario_dir, exist_ok=True)\n",
        "agb_total_dist_dir = join(agb_total_raster_dir, 'disturbance')\n",
        "makedirs(agb_total_dist_dir, exist_ok=True)\n",
        "detailed_stats_by_area_dir = join(sample_polygons_statistics_dir, 'detailed_stats_by_area')\n",
        "makedirs(detailed_stats_by_area_dir, exist_ok=True)\n",
        "detailed_stats_by_scenario_dir = join(sample_polygons_statistics_dir, 'detailed_stats_by_scenario')\n",
        "makedirs(detailed_stats_by_scenario_dir, exist_ok=True)\n",
        "detailed_dist_stats_by_area_dir = join(sample_polygons_statistics_dir, 'detailed_dist_stats_by_area')\n",
        "makedirs(detailed_dist_stats_by_area_dir, exist_ok=True)\n",
        "detailed_dist_stats_by_scenario_dir = join(sample_polygons_statistics_dir, 'detailed_dist_stats_by_scenario')\n",
        "makedirs(detailed_dist_stats_by_scenario_dir, exist_ok=True)\n",
        "intactness_stats_dir = join(sample_polygons_statistics_dir, 'intactness')\n",
        "makedirs(intactness_stats_dir, exist_ok=True)\n",
        "\n",
        "# More intuitive data structure\n",
        "report_statistics_dir = join(sample_polygons_statistics_dir, 'report_statistics')\n",
        "makedirs(report_statistics_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPrMgTxDr3qE"
      },
      "source": [
        "# Convert AGBD to AGB total rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yYVaDPyr3T9"
      },
      "outputs": [],
      "source": [
        "# Converts from measurements/ha to totals using cell area\n",
        "\n",
        "# This can be a higher precision than Mg/ha, as ~30 m pixels are ~0.9 ha,\n",
        "# with smaller values. Precision = 3 = 1 kg\n",
        "agb_mg_precision = 3\n",
        "agb_ci_mg_precision = 3\n",
        "\n",
        "# List all raster files in source directories\n",
        "scenario_mean_rasters = []\n",
        "scenario_ci_rasters = []\n",
        "dist_mean_rasters = []\n",
        "dist_ci_rasters = []\n",
        "\n",
        "# Collect scenario rasters\n",
        "if exists(prediction_raster_dir):\n",
        "    for f in os.listdir(prediction_raster_dir):\n",
        "        if f.endswith('.tif'):\n",
        "            full_path = join(prediction_raster_dir, f)\n",
        "            if source_dir == uncertainty_dir:\n",
        "                if 'mean__' in f: scenario_mean_rasters.append(full_path)\n",
        "                elif f.startswith('ci_'): scenario_ci_rasters.append(full_path)\n",
        "            else: scenario_mean_rasters.append(full_path)\n",
        "\n",
        "# Collect disturbance rasters\n",
        "if exists(disturbance_dir):\n",
        "    for f in os.listdir(disturbance_dir):\n",
        "        if f.endswith('.tif'):\n",
        "            full_path = join(disturbance_dir, f)\n",
        "            if source_dir == uncertainty_dir:\n",
        "                if 'mean__' in f: dist_mean_rasters.append(full_path)\n",
        "                elif f.startswith('ci_'): dist_ci_rasters.append(full_path)\n",
        "            else: dist_mean_rasters.append(full_path)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "scenario_mean_rasters = sorted(scenario_mean_rasters)\n",
        "scenario_ci_rasters = sorted(scenario_ci_rasters)\n",
        "dist_mean_rasters = sorted(dist_mean_rasters)\n",
        "dist_ci_rasters = sorted(dist_ci_rasters)\n",
        "\n",
        "# Create lookup dictionaries for CI matching (ci_{value}__ -> mean__)\n",
        "scenario_ci_lookup = {}\n",
        "for ci_raster in scenario_ci_rasters:\n",
        "    base_name = re.sub(r'ci_[^_]+__', 'mean__', os.path.basename(ci_raster))\n",
        "    scenario_ci_lookup[base_name] = ci_raster\n",
        "dist_ci_lookup = {}\n",
        "for ci_raster in dist_ci_rasters:\n",
        "    base_name = re.sub(r'ci_[^_]+__', 'mean__', os.path.basename(ci_raster))\n",
        "    dist_ci_lookup[base_name] = ci_raster\n",
        "\n",
        "# Load cell area raster\n",
        "cell_area = gdal.Open(cell_area_path)\n",
        "cell_area_array = cell_area.ReadAsArray().astype(np.float64)\n",
        "cell_area = None\n",
        "# Convert cell area from m2 to ha\n",
        "cell_area_ha = cell_area_array / 10000\n",
        "\n",
        "# Convert per-hectare rasters to total values using cell area.\n",
        "def process_rasters(raster_paths, ci_lookup, output_dir, is_disturbance=False):\n",
        "    progress_index = 0\n",
        "    progress_total = len(raster_paths)\n",
        "    raster_type = \"Disturbance\" if is_disturbance else \"Scenario\"\n",
        "    progress_label = widgets.Label(f\"{raster_type} rasters progress: {progress_index}/{progress_total}\")\n",
        "    display(progress_label)\n",
        "    print(f\"Processing {progress_total} {raster_type.lower()} rasters...\")\n",
        "    for raster_path in raster_paths:\n",
        "        base_filename = os.path.basename(raster_path)\n",
        "\n",
        "        # Extract scenario name\n",
        "        if source_dir == uncertainty_dir: raster_name = base_filename.split('__')[1].split('.')[0]\n",
        "        else: raster_name = base_filename.split('__')[0].split('.')[0]\n",
        "        output_agb_mg = join(output_dir, f\"agb_total_mg__{raster_name}.tif\")\n",
        "        agb_exists = exists(output_agb_mg)\n",
        "        agbd_array = nodata = valid_mask = None\n",
        "\n",
        "        # Create total AGB raster: AGB (Mg) = AGBD (Mg/ha) × area (ha)\n",
        "        if not agb_exists:\n",
        "            agbd = gdal.Open(raster_path)\n",
        "            agbd_array = agbd.ReadAsArray()\n",
        "            nodata = int(agbd.GetRasterBand(1).GetNoDataValue())\n",
        "            agbd = None\n",
        "            valid_mask = (agbd_array != nodata)\n",
        "            total_agb_mg = np.zeros_like(agbd_array, dtype='float64')\n",
        "            total_agb_mg[valid_mask] = agbd_array[valid_mask] * cell_area_ha[valid_mask]\n",
        "            total_agb_mg[~valid_mask] = nodata\n",
        "            if agb_mg_precision == 0:\n",
        "              total_agb_mg = np.round(total_agb_mg, agb_mg_precision).astype(np.int16)\n",
        "            else: total_agb_mg = np.round(total_agb_mg, agb_mg_precision)\n",
        "            export_array_as_tif(total_agb_mg, output_agb_mg, template=raster_path)\n",
        "\n",
        "        # Process CI rasters if available\n",
        "        if source_dir == uncertainty_dir:\n",
        "            base_name = os.path.basename(raster_path)\n",
        "            if base_name in ci_lookup:\n",
        "                ci_path = ci_lookup[base_name]\n",
        "                output_agb_ci_mg = join(output_dir, f\"agb_total_ci_95_mg__{raster_name}.tif\")\n",
        "\n",
        "                agb_ci_exists = exists(output_agb_ci_mg)\n",
        "                if not agb_ci_exists:\n",
        "                    ci_raster = gdal.Open(ci_path)\n",
        "                    ci_array = ci_raster.ReadAsArray().astype(np.float64)\n",
        "                    ci_raster = None\n",
        "                    # Load mean raster for nodata mask if needed\n",
        "                    if valid_mask is None:\n",
        "                        agbd = gdal.Open(raster_path)\n",
        "                        agbd_array = agbd.ReadAsArray()\n",
        "                        nodata = int(agbd.GetRasterBand(1).GetNoDataValue())\n",
        "                        agbd = None\n",
        "                        valid_mask = (agbd_array != nodata)\n",
        "                    # Total CI (Mg) = CI (Mg/ha) × area (ha)\n",
        "                    total_agb_ci_mg = np.zeros_like(ci_array, dtype='float64')\n",
        "                    total_agb_ci_mg[valid_mask] = ci_array[valid_mask] * cell_area_ha[valid_mask]\n",
        "                    total_agb_ci_mg[~valid_mask] = nodata\n",
        "                    if agb_ci_mg_precision == 0:\n",
        "                      total_agb_ci_mg = np.round(total_agb_ci_mg, agb_ci_mg_precision).astype(np.int16)\n",
        "                    else: total_agb_ci_mg = np.round(total_agb_ci_mg, agb_ci_mg_precision)\n",
        "                    export_array_as_tif(total_agb_ci_mg, output_agb_ci_mg, template=raster_path)\n",
        "\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"{raster_type} rasters progress: {progress_index}/{progress_total}\"\n",
        "    return progress_total\n",
        "\n",
        "scenario_count = process_rasters(scenario_mean_rasters, scenario_ci_lookup, agb_total_scenario_dir)\n",
        "dist_count = process_rasters(dist_mean_rasters, dist_ci_lookup, agb_total_dist_dir, is_disturbance=True)\n",
        "print(f\"Processed {scenario_count} scenario rasters and {dist_count} disturbance rasters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlQ2iLAiwQoB"
      },
      "source": [
        "# Scenario statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgC9laZXPB9Y"
      },
      "outputs": [],
      "source": [
        "# Create list of available AGB total rasters and extract scenarios\n",
        "scenarios = set()\n",
        "for agb_total_raster in os.listdir(agb_total_scenario_dir):\n",
        "    if agb_total_raster.endswith('.tif') and 'agb_total_mg__' in agb_total_raster:\n",
        "        scenario_name = agb_total_raster.split(\"agb_total_mg__\")[1].split('.')[0]\n",
        "        scenarios.add(scenario_name)\n",
        "\n",
        "scenarios = sorted(list(scenarios))\n",
        "\n",
        "# Select scenario predictions to calculate statistics\n",
        "print('selected_scenarios = [')\n",
        "for scenario in scenarios:\n",
        "    print(f'  \"{scenario}\",')\n",
        "print(']\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utWU_0AEG0UX"
      },
      "outputs": [],
      "source": [
        "selected_scenarios = [\n",
        "  \"2018\",\n",
        "  \"2024\",\n",
        "  \"2024_no_degradation_since_1996\",\n",
        "  \"2024_no_disturbance_since_1996\",\n",
        "  \"2024_oldgrowth\",\n",
        "  \"2024_oldgrowth_all_land\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]\n",
        "\n",
        "\n",
        "# Build lists of AGB total rasters for selected scenarios\n",
        "agb_total_rasters = []\n",
        "\n",
        "for scenario in selected_scenarios:\n",
        "    agb_total_path = join(agb_total_scenario_dir, f\"agb_total_mg__{scenario}.tif\")\n",
        "    if exists(agb_total_path):\n",
        "        agb_total_rasters.append(agb_total_path)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "agb_total_rasters = sorted(agb_total_rasters)\n",
        "\n",
        "# Toggle whether to generate uncertainty stats (only possible with uncertainty_dir)\n",
        "generate_uncertainty_stats = (source_dir == uncertainty_dir)\n",
        "\n",
        "# Pre-allocate arrays for statistics\n",
        "polygon_names = [row[\"name\"] for _, row in selected_sample_polygons_gpkg.iterrows()]\n",
        "n_polygons = len(polygon_names)\n",
        "n_scenarios = len(agb_total_rasters)\n",
        "\n",
        "forest_cover_data = np.zeros((n_scenarios, n_polygons))\n",
        "agbd_mean_data = np.zeros((n_scenarios, n_polygons))\n",
        "agbd_stdev_data = np.zeros((n_scenarios, n_polygons))\n",
        "agb_total_data = np.zeros((n_scenarios, n_polygons))\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    agbd_mean_ci95_data = np.zeros((n_scenarios, n_polygons))\n",
        "    agbd_mean_uncertainty_data = np.zeros((n_scenarios, n_polygons))\n",
        "    agb_total_ci95_data = np.zeros((n_scenarios, n_polygons))\n",
        "\n",
        "# Open AGB total raster datasets\n",
        "agb_total_datasets = {path: rasterio.open(path) for path in agb_total_rasters}\n",
        "\n",
        "# Open AGB total CI95 datasets only if uncertainty stats are generated\n",
        "agb_total_ci95_datasets = {}\n",
        "if generate_uncertainty_stats:\n",
        "    for agb_total_raster in agb_total_rasters:\n",
        "        scenario_name = os.path.basename(agb_total_raster).split('agb_total_mg__')[1].split('.')[0]\n",
        "        agb_total_ci95_path = join(agb_total_scenario_dir, f\"agb_total_ci_95_mg__{scenario_name}.tif\")\n",
        "        if exists(agb_total_ci95_path):\n",
        "            agb_total_ci95_datasets[agb_total_raster] = rasterio.open(agb_total_ci95_path)\n",
        "\n",
        "# Load cell area raster once for all calculations\n",
        "cell_area_dataset = rasterio.open(cell_area_path)\n",
        "\n",
        "# Progress tracking\n",
        "progress_total = n_polygons * n_scenarios\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Raster / polygon pair progress: {progress_index}/{progress_total}\")\n",
        "display(progress_label)\n",
        "\n",
        "try:\n",
        "    # Initialise polygon area dataframe\n",
        "    df_polygon_area_km2 = pd.DataFrame(columns=[\"Name\", \"Area (km^2)\"])\n",
        "\n",
        "    # Loop through each polygon to generate statistics\n",
        "    for poly_idx, (index, row) in enumerate(selected_sample_polygons_gpkg.iterrows()):\n",
        "\n",
        "        # Define the polygon\n",
        "        sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"]\n",
        "        polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "        # Mask the cell area raster to the polygon once\n",
        "        cell_area_masked, transform_1 = msk.mask(cell_area_dataset, polygons, crop=True, filled=False)\n",
        "        cell_area_masked = cell_area_masked.astype('float64')\n",
        "\n",
        "        # Calculate total area of all pixels within polygon in hectares\n",
        "        pixel_area_sum_m2 = np.ma.sum(cell_area_masked, dtype='float64')\n",
        "        pixel_area_sum_ha = pixel_area_sum_m2 / 10000\n",
        "\n",
        "        # Convert cell areas from m2 to ha\n",
        "        cell_area_masked_ha = cell_area_masked / 10000\n",
        "\n",
        "        # Add polygon area to dataframe\n",
        "        new_row = pd.DataFrame([{'Name': sample_polygon_name, 'Area (km^2)': pixel_area_sum_ha / 100}], dtype=object)\n",
        "        df_polygon_area_km2 = pd.concat([df_polygon_area_km2, new_row], ignore_index=True, sort=False)\n",
        "\n",
        "        # Loop through AGB total rasters\n",
        "        for raster_idx, agb_total_raster in enumerate(agb_total_rasters):\n",
        "\n",
        "            # Mask AGB total raster to polygon\n",
        "            agb_total = agb_total_datasets[agb_total_raster]\n",
        "            agb_total_array_masked, transform_2 = msk.mask(agb_total, polygons, crop=True, filled=False)\n",
        "            agb_total_array_masked = agb_total_array_masked.astype('float64')\n",
        "\n",
        "            # Extract forest pixels from valid AGB total pixels\n",
        "            forest_pixels_mask = ~np.ma.getmaskarray(agb_total_array_masked)\n",
        "\n",
        "            # Calculate forest area by summing cell areas of forest pixels\n",
        "            forest_cell_areas_ha = np.ma.array(cell_area_masked_ha.data, mask=~forest_pixels_mask)\n",
        "            forest_cover_ha = np.ma.sum(forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "            # Sum total AGB in Mg\n",
        "            agb_total_mg = np.ma.sum(agb_total_array_masked, dtype='float64')\n",
        "\n",
        "            # Calculate statistics with masked value handling\n",
        "            if np.ma.is_masked(agb_total_mg) or forest_cover_ha <= 0:\n",
        "                agbd_mean_mg_ha = 0.0\n",
        "                agbd_mean_stdev_ha = 0.0\n",
        "                agb_total_tg = 0.0\n",
        "            else:\n",
        "                # Calculate area-weighted mean AGBD\n",
        "                agbd_mean_mg_ha = agb_total_mg / forest_cover_ha\n",
        "\n",
        "                # Back-calculate individual AGBD values for standard deviation\n",
        "                agbd_values = agb_total_array_masked / cell_area_masked_ha\n",
        "                valid_agbd = agbd_values[forest_pixels_mask]\n",
        "                valid_areas = cell_area_masked_ha[forest_pixels_mask]\n",
        "\n",
        "                # Calculate area-weighted standard deviation\n",
        "                variance_weighted = np.sum(valid_areas * (valid_agbd - agbd_mean_mg_ha)**2) / forest_cover_ha\n",
        "                agbd_mean_stdev_ha = np.sqrt(variance_weighted)\n",
        "\n",
        "                # Convert total AGB from Mg to Tg\n",
        "                agb_total_tg = agb_total_mg / 1000000\n",
        "\n",
        "            # Store results in pre-allocated arrays\n",
        "            forest_cover_data[raster_idx, poly_idx] = forest_cover_ha\n",
        "            agbd_mean_data[raster_idx, poly_idx] = agbd_mean_mg_ha\n",
        "            agbd_stdev_data[raster_idx, poly_idx] = agbd_mean_stdev_ha\n",
        "            agb_total_data[raster_idx, poly_idx] = agb_total_tg\n",
        "\n",
        "            if generate_uncertainty_stats and agb_total_raster in agb_total_ci95_datasets:\n",
        "                # Get total AGB CI95 from pre-calculated raster\n",
        "                agb_total_ci95_raster = agb_total_ci95_datasets[agb_total_raster]\n",
        "                agb_total_ci95_array_masked, _ = msk.mask(agb_total_ci95_raster, polygons, crop=True, filled=False)\n",
        "                agb_total_ci95_array_masked = agb_total_ci95_array_masked.astype('float64')\n",
        "\n",
        "                # Sum total AGB CI95 in Mg\n",
        "                agb_total_ci95_mg = abs(np.ma.sum(agb_total_ci95_array_masked, dtype='float64'))\n",
        "\n",
        "                # Calculate uncertainty statistics\n",
        "                if abs(agb_total_mg) > 0:\n",
        "                    agbd_mean_mg_ha_ci95 = agb_total_ci95_mg / forest_cover_ha\n",
        "                    agbd_mean_mg_ha_uncertainty = agb_total_ci95_mg / abs(agb_total_mg) * 100\n",
        "                else:\n",
        "                    agbd_mean_mg_ha_ci95 = 0\n",
        "                    agbd_mean_mg_ha_uncertainty = 0\n",
        "\n",
        "                # Convert total AGB CI95 from Mg to Tg\n",
        "                agb_total_tg_ci95 = agb_total_ci95_mg / 1000000\n",
        "\n",
        "                # Store uncertainty results\n",
        "                agbd_mean_ci95_data[raster_idx, poly_idx] = agbd_mean_mg_ha_ci95\n",
        "                agbd_mean_uncertainty_data[raster_idx, poly_idx] = agbd_mean_mg_ha_uncertainty\n",
        "                agb_total_ci95_data[raster_idx, poly_idx] = agb_total_tg_ci95\n",
        "\n",
        "            # Update progress\n",
        "            progress_index += 1\n",
        "            progress_label.value = f\"Raster / polygon pair progress: {progress_index}/{progress_total}\"\n",
        "\n",
        "finally:\n",
        "    # Close all opened datasets\n",
        "    cell_area_dataset.close()\n",
        "    for dataset in agb_total_datasets.values():\n",
        "        dataset.close()\n",
        "    for dataset in agb_total_ci95_datasets.values():\n",
        "        dataset.close()\n",
        "\n",
        "# Create DataFrames from pre-allocated arrays\n",
        "df_forest_cover_ha = pd.DataFrame(forest_cover_data, index=selected_scenarios, columns=polygon_names)\n",
        "df_forest_cover_ha.rename_axis('scenario', inplace=True)\n",
        "\n",
        "df_agbd_mean_mg_ha = pd.DataFrame(agbd_mean_data, index=selected_scenarios, columns=polygon_names)\n",
        "df_agbd_mean_mg_ha.rename_axis('scenario', inplace=True)\n",
        "\n",
        "df_agbd_stdev_mg_ha = pd.DataFrame(agbd_stdev_data, index=selected_scenarios, columns=polygon_names)\n",
        "df_agbd_stdev_mg_ha.rename_axis('scenario', inplace=True)\n",
        "\n",
        "df_agb_total_tg = pd.DataFrame(agb_total_data, index=selected_scenarios, columns=polygon_names)\n",
        "df_agb_total_tg.rename_axis('scenario', inplace=True)\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    df_agbd_mean_mg_ha_ci95 = pd.DataFrame(agbd_mean_ci95_data, index=selected_scenarios, columns=polygon_names)\n",
        "    df_agbd_mean_mg_ha_ci95.rename_axis('scenario', inplace=True)\n",
        "\n",
        "    df_agbd_mean_mg_ha_uncertainty = pd.DataFrame(agbd_mean_uncertainty_data, index=selected_scenarios, columns=polygon_names)\n",
        "    df_agbd_mean_mg_ha_uncertainty.rename_axis('scenario', inplace=True)\n",
        "\n",
        "    df_agb_total_tg_ci95 = pd.DataFrame(agb_total_ci95_data, index=selected_scenarios, columns=polygon_names)\n",
        "    df_agb_total_tg_ci95.rename_axis('scenario', inplace=True)\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "    df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_mean_mg_ha_ci95,\n",
        "                     df_agbd_mean_mg_ha_uncertainty, df_agbd_stdev_mg_ha, df_agb_total_tg, df_agb_total_tg_ci95]\n",
        "else:\n",
        "    df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Set index of the polygon area km2 dataframe to 'Name' of the polygon\n",
        "df_polygon_area_km2 = df_polygon_area_km2.set_index('Name')\n",
        "\n",
        "# Generate summary stats\n",
        "df_forest_cover_ha_t = df_forest_cover_ha.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest cover (ha)\")\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB (Tg)\")\n",
        "\n",
        "# Use list for efficient concatenation\n",
        "summary_components = [df_polygon_area_km2, df_forest_cover_ha_t, df_agb_total_tg_t]\n",
        "if generate_uncertainty_stats:\n",
        "    df_agb_total_tg_ci95_t = df_agb_total_tg_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB CI95 (Tg)\")\n",
        "    summary_components.append(df_agb_total_tg_ci95_t)\n",
        "\n",
        "summary_stats = pd.concat(summary_components, axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_stats.to_csv(join(sample_polygons_statistics_dir, 'summary_stats.csv'))\n",
        "\n",
        "# Generate empty dataframes for statistics\n",
        "df_base = pd.DataFrame(index=selected_scenarios)\n",
        "df_base.rename_axis('scenario', inplace=True)\n",
        "\n",
        "# Generate detailed stats by area\n",
        "for polygon_area in polygon_names:\n",
        "    polygon_area_km2 = df_polygon_area_km2.loc[polygon_area][\"Area (km^2)\"]\n",
        "    df_detailed_stats = df_base.copy()\n",
        "    df_detailed_stats[\"Area (km^2)\"] = polygon_area_km2\n",
        "    for df_stats in df_stats_list:\n",
        "        if df_stats.equals(df_forest_cover_ha): stat_col = \"Forest cover (ha)\"\n",
        "        if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"Forest AGBD mean (Mg / ha)\"\n",
        "        if df_stats.equals(df_agbd_stdev_mg_ha): stat_col = \"Forest AGBD stdev (Mg / ha)\"\n",
        "        if df_stats.equals(df_agb_total_tg): stat_col = \"Forest AGB total (Tg)\"\n",
        "        if generate_uncertainty_stats:\n",
        "            if df_stats.equals(df_agbd_mean_mg_ha_ci95): stat_col = \"Forest AGBD CI95 (Mg / ha)\"\n",
        "            if df_stats.equals(df_agbd_mean_mg_ha_uncertainty): stat_col = \"Forest AGBD uncertainty (%)\"\n",
        "            if df_stats.equals(df_agb_total_tg_ci95): stat_col = \"Forest AGB total CI95 (Tg)\"\n",
        "        for stats_polygon_area in df_stats:\n",
        "            if stats_polygon_area == polygon_area:\n",
        "                df_stats_renamed = df_stats.rename(columns={stats_polygon_area:stat_col})\n",
        "                df_detailed_stats = pd.concat([df_detailed_stats, df_stats_renamed[stat_col]], axis=1)\n",
        "    df_detailed_stats.to_csv(join(detailed_stats_by_area_dir, f'{polygon_area}.csv'))\n",
        "\n",
        "# Generate detailed stats by scenario - build dictionary once then process\n",
        "scenarios = {}\n",
        "for stats_csv in os.listdir(detailed_stats_by_area_dir):\n",
        "    polygon_name = f\"{stats_csv[:-4]}\"\n",
        "    stats_csv_path = join(detailed_stats_by_area_dir, stats_csv)\n",
        "    stats_csv_df = pd.read_csv(stats_csv_path)\n",
        "    # Process all scenarios for this polygon in one pass\n",
        "    for scenario in stats_csv_df['scenario'].unique():\n",
        "        scenario_df = stats_csv_df[stats_csv_df['scenario'] == scenario].copy()\n",
        "        scenario_df.drop('scenario', axis=1, inplace=True)\n",
        "        scenario_df.insert(0, 'Name', polygon_name)\n",
        "        if scenario in scenarios:\n",
        "            scenarios[scenario] = pd.concat([scenarios[scenario], scenario_df], ignore_index=True)\n",
        "        else:\n",
        "            scenarios[scenario] = scenario_df\n",
        "\n",
        "# Write all scenario CSVs\n",
        "for scenario, scenario_df in scenarios.items():\n",
        "    output_file_path = join(detailed_stats_by_scenario_dir,f'{scenario}.csv')\n",
        "    scenario_df.to_csv(output_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHEWz3QcGh1b"
      },
      "source": [
        "# Disturbance statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPm9wZwJBJnq"
      },
      "outputs": [],
      "source": [
        "# Create list of available AGB total disturbance rasters and extract disturbances\n",
        "dists = set()\n",
        "for agb_total_dist_raster in os.listdir(agb_total_dist_dir):\n",
        "    if agb_total_dist_raster.endswith('.tif') and 'agb_total_mg__' in agb_total_dist_raster:\n",
        "        dist_name = agb_total_dist_raster.split(\"agb_total_mg__\")[1].split('.')[0]\n",
        "        dists.add(dist_name)\n",
        "\n",
        "dists = sorted(list(dists))\n",
        "\n",
        "# Select disturbance rasters to calculate statistics\n",
        "print('selected_dists = [')\n",
        "for dist in dists:\n",
        "    print(f'  \"{dist}\",')\n",
        "print(']')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm_v5xr2G4sV"
      },
      "outputs": [],
      "source": [
        "selected_dists = [\n",
        "  \"2024_deforestation_of_road_mat_daling_2023\",\n",
        "  \"2024_deforestation_since_1996\",\n",
        "  \"2024_deforestation_since_oldgrowth\",\n",
        "  \"2024_degradation_since_1996\",\n",
        "  \"2024_degradation_since_oldgrowth\",\n",
        "  \"2024_disturbance_since_1996\",\n",
        "  \"2024_disturbance_since_oldgrowth\",\n",
        "]\n",
        "\n",
        "# Build lists of AGB total disturbance rasters for selected disturbances\n",
        "agb_total_dist_rasters = []\n",
        "\n",
        "for dist in selected_dists:\n",
        "    agb_total_path = join(agb_total_dist_dir, f\"agb_total_mg__{dist}.tif\")\n",
        "    if exists(agb_total_path):\n",
        "        agb_total_dist_rasters.append(agb_total_path)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "agb_total_dist_rasters = sorted(agb_total_dist_rasters)\n",
        "\n",
        "# Toggle whether to generate uncertainty stats (only possible with uncertainty_dir)\n",
        "generate_uncertainty_stats = (source_dir == uncertainty_dir)\n",
        "\n",
        "# Pre-allocate arrays for statistics\n",
        "polygon_names = [row[\"name\"] for _, row in selected_sample_polygons_gpkg.iterrows()]\n",
        "n_polygons = len(polygon_names)\n",
        "n_dists = len(agb_total_dist_rasters)\n",
        "\n",
        "agbd_mean_data = np.zeros((n_dists, n_polygons))\n",
        "agbd_stdev_data = np.zeros((n_dists, n_polygons))\n",
        "agb_total_data = np.zeros((n_dists, n_polygons))\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    agbd_mean_ci95_data = np.zeros((n_dists, n_polygons))\n",
        "    agbd_mean_uncertainty_data = np.zeros((n_dists, n_polygons))\n",
        "    agb_total_ci95_data = np.zeros((n_dists, n_polygons))\n",
        "\n",
        "# Open AGB total disturbance raster datasets\n",
        "agb_total_dist_datasets = {path: rasterio.open(path) for path in agb_total_dist_rasters}\n",
        "\n",
        "# Open AGB total CI95 disturbance datasets only if uncertainty stats are generated\n",
        "agb_total_ci95_dist_datasets = {}\n",
        "if generate_uncertainty_stats:\n",
        "    for agb_total_dist_raster in agb_total_dist_rasters:\n",
        "        dist_name = os.path.basename(agb_total_dist_raster).split('agb_total_mg__')[1].split('.')[0]\n",
        "        agb_total_ci95_path = join(agb_total_dist_dir, f\"agb_total_ci_95_mg__{dist_name}.tif\")\n",
        "        if exists(agb_total_ci95_path):\n",
        "            agb_total_ci95_dist_datasets[agb_total_dist_raster] = rasterio.open(agb_total_ci95_path)\n",
        "\n",
        "# Load cell area raster once for all calculations\n",
        "cell_area_dataset = rasterio.open(cell_area_path)\n",
        "\n",
        "# Progress tracking\n",
        "progress_total = n_polygons * n_dists\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Raster / polygon pair progress: {progress_index}/{progress_total}\")\n",
        "display(progress_label)\n",
        "\n",
        "try:\n",
        "    # Initialise polygon area dataframe\n",
        "    df_polygon_area_km2 = pd.DataFrame(columns=[\"Name\", \"Area (km^2)\"])\n",
        "\n",
        "    # Loop through each polygon to generate statistics\n",
        "    for poly_idx, (index, row) in enumerate(selected_sample_polygons_gpkg.iterrows()):\n",
        "\n",
        "        # Define the polygon\n",
        "        sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"]\n",
        "        polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "        # Mask the cell area raster to the polygon once\n",
        "        cell_area_masked, transform_1 = msk.mask(cell_area_dataset, polygons, crop=True, filled=False)\n",
        "        cell_area_masked = cell_area_masked.astype('float64')\n",
        "\n",
        "        # Calculate total area of all pixels within polygon in hectares\n",
        "        pixel_area_sum_m2 = np.ma.sum(cell_area_masked, dtype='float64')\n",
        "        pixel_area_sum_ha = pixel_area_sum_m2 / 10000\n",
        "\n",
        "        # Convert cell areas from m2 to ha\n",
        "        cell_area_masked_ha = cell_area_masked / 10000\n",
        "\n",
        "        # Add polygon area to dataframe\n",
        "        new_row = pd.DataFrame([{'Name': sample_polygon_name, 'Area (km^2)': pixel_area_sum_ha / 100}], dtype=object)\n",
        "        df_polygon_area_km2 = pd.concat([df_polygon_area_km2, new_row], ignore_index=True, sort=False)\n",
        "\n",
        "        # Loop through AGB total disturbance rasters\n",
        "        for raster_idx, agb_total_dist_raster in enumerate(agb_total_dist_rasters):\n",
        "\n",
        "            # Mask AGB total disturbance raster to polygon\n",
        "            agb_total_dist = agb_total_dist_datasets[agb_total_dist_raster]\n",
        "            agb_total_array_masked, transform_2 = msk.mask(agb_total_dist, polygons, crop=True, filled=False)\n",
        "            agb_total_array_masked = agb_total_array_masked.astype('float64')\n",
        "\n",
        "            # Extract forest pixels from valid AGB total pixels\n",
        "            forest_pixels_mask = ~np.ma.getmaskarray(agb_total_array_masked)\n",
        "\n",
        "            # Calculate forest area by summing cell areas of forest pixels\n",
        "            forest_cell_areas_ha = np.ma.array(cell_area_masked_ha.data, mask=~forest_pixels_mask)\n",
        "            forest_cover_ha = np.ma.sum(forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "            # Sum total AGB in Mg\n",
        "            agb_total_mg = np.ma.sum(agb_total_array_masked, dtype='float64')\n",
        "\n",
        "            # Calculate statistics with masked value handling\n",
        "            if np.ma.is_masked(agb_total_mg) or forest_cover_ha <= 0:\n",
        "                agbd_mean_mg_ha = 0.0\n",
        "                agbd_mean_stdev_ha = 0.0\n",
        "                agb_total_tg = 0.0\n",
        "            else:\n",
        "                # Calculate area-weighted mean AGBD\n",
        "                agbd_mean_mg_ha = agb_total_mg / forest_cover_ha\n",
        "\n",
        "                # Back-calculate individual AGBD values for standard deviation\n",
        "                agbd_values = agb_total_array_masked / cell_area_masked_ha\n",
        "                valid_agbd = agbd_values[forest_pixels_mask]\n",
        "                valid_areas = cell_area_masked_ha[forest_pixels_mask]\n",
        "\n",
        "                # Calculate area-weighted standard deviation\n",
        "                variance_weighted = np.sum(valid_areas * (valid_agbd - agbd_mean_mg_ha)**2) / forest_cover_ha\n",
        "                agbd_mean_stdev_ha = np.sqrt(variance_weighted)\n",
        "\n",
        "                # Convert total AGB from Mg to Tg\n",
        "                agb_total_tg = agb_total_mg / 1000000\n",
        "\n",
        "            # Store results in pre-allocated arrays\n",
        "            agbd_mean_data[raster_idx, poly_idx] = agbd_mean_mg_ha\n",
        "            agbd_stdev_data[raster_idx, poly_idx] = agbd_mean_stdev_ha\n",
        "            agb_total_data[raster_idx, poly_idx] = agb_total_tg\n",
        "\n",
        "            if generate_uncertainty_stats and agb_total_dist_raster in agb_total_ci95_dist_datasets:\n",
        "                # Get total AGB CI95 from pre-calculated raster\n",
        "                agb_total_ci95_raster = agb_total_ci95_dist_datasets[agb_total_dist_raster]\n",
        "                agb_total_ci95_array_masked, _ = msk.mask(agb_total_ci95_raster, polygons, crop=True, filled=False)\n",
        "                agb_total_ci95_array_masked = agb_total_ci95_array_masked.astype('float64')\n",
        "\n",
        "                # Sum total AGB CI95 in Mg\n",
        "                agb_total_ci95_mg = abs(np.ma.sum(agb_total_ci95_array_masked, dtype='float64'))\n",
        "\n",
        "                # Calculate uncertainty statistics\n",
        "                if abs(agb_total_mg) > 0:\n",
        "                    agbd_mean_mg_ha_ci95 = agb_total_ci95_mg / forest_cover_ha\n",
        "                    agbd_mean_mg_ha_uncertainty = agb_total_ci95_mg / abs(agb_total_mg) * 100\n",
        "                else:\n",
        "                    agbd_mean_mg_ha_ci95 = 0\n",
        "                    agbd_mean_mg_ha_uncertainty = 0\n",
        "\n",
        "                # Convert total AGB CI95 from Mg to Tg\n",
        "                agb_total_tg_ci95 = agb_total_ci95_mg / 1000000\n",
        "\n",
        "                # Store uncertainty results\n",
        "                agbd_mean_ci95_data[raster_idx, poly_idx] = agbd_mean_mg_ha_ci95\n",
        "                agbd_mean_uncertainty_data[raster_idx, poly_idx] = agbd_mean_mg_ha_uncertainty\n",
        "                agb_total_ci95_data[raster_idx, poly_idx] = agb_total_tg_ci95\n",
        "\n",
        "            # Update progress\n",
        "            progress_index += 1\n",
        "            progress_label.value = f\"Raster / polygon pair progress: {progress_index}/{progress_total}\"\n",
        "\n",
        "finally:\n",
        "    # Close all opened datasets\n",
        "    cell_area_dataset.close()\n",
        "    for dataset in agb_total_dist_datasets.values():\n",
        "        dataset.close()\n",
        "    for dataset in agb_total_ci95_dist_datasets.values():\n",
        "        dataset.close()\n",
        "\n",
        "# Create DataFrames from pre-allocated arrays\n",
        "df_agbd_mean_mg_ha = pd.DataFrame(agbd_mean_data, index=selected_dists, columns=polygon_names)\n",
        "df_agbd_mean_mg_ha.rename_axis('dist', inplace=True)\n",
        "\n",
        "df_agbd_stdev_mg_ha = pd.DataFrame(agbd_stdev_data, index=selected_dists, columns=polygon_names)\n",
        "df_agbd_stdev_mg_ha.rename_axis('dist', inplace=True)\n",
        "\n",
        "df_agb_total_tg = pd.DataFrame(agb_total_data, index=selected_dists, columns=polygon_names)\n",
        "df_agb_total_tg.rename_axis('dist', inplace=True)\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    df_agbd_mean_mg_ha_ci95 = pd.DataFrame(agbd_mean_ci95_data, index=selected_dists, columns=polygon_names)\n",
        "    df_agbd_mean_mg_ha_ci95.rename_axis('dist', inplace=True)\n",
        "\n",
        "    df_agbd_mean_mg_ha_uncertainty = pd.DataFrame(agbd_mean_uncertainty_data, index=selected_dists, columns=polygon_names)\n",
        "    df_agbd_mean_mg_ha_uncertainty.rename_axis('dist', inplace=True)\n",
        "\n",
        "    df_agb_total_tg_ci95 = pd.DataFrame(agb_total_ci95_data, index=selected_dists, columns=polygon_names)\n",
        "    df_agb_total_tg_ci95.rename_axis('dist', inplace=True)\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "    df_stats_list = [df_agbd_mean_mg_ha, df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agbd_stdev_mg_ha, df_agb_total_tg, df_agb_total_tg_ci95]\n",
        "else:\n",
        "    df_stats_list = [df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Set index of the polygon area km2 dataframe to 'Name' of the polygon\n",
        "df_polygon_area_km2 = df_polygon_area_km2.set_index('Name')\n",
        "\n",
        "# Generate summary stats\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB (Tg)\")\n",
        "\n",
        "# Use list for efficient concatenation\n",
        "summary_components = [df_polygon_area_km2, df_agb_total_tg_t]\n",
        "if generate_uncertainty_stats:\n",
        "    df_agb_total_tg_ci95_t = df_agb_total_tg_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB CI95 (Tg)\")\n",
        "    summary_components.append(df_agb_total_tg_ci95_t)\n",
        "\n",
        "summary_stats = pd.concat(summary_components, axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_stats.to_csv(join(sample_polygons_statistics_dir, 'summary_dist_stats.csv'))\n",
        "\n",
        "# Generate empty dataframes for statistics\n",
        "df_base = pd.DataFrame(index=selected_dists)\n",
        "df_base.rename_axis('dist', inplace=True)\n",
        "\n",
        "# Generate detailed stats by polygon\n",
        "for polygon_area in polygon_names:\n",
        "    polygon_area_km2 = df_polygon_area_km2.loc[polygon_area][\"Area (km^2)\"]\n",
        "    df_detailed_dist_stats = df_base.copy()\n",
        "    df_detailed_dist_stats[\"Area (km^2)\"] = polygon_area_km2\n",
        "    for df_stats in df_stats_list:\n",
        "        if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"Forest AGBD mean (Mg / ha)\"\n",
        "        if df_stats.equals(df_agbd_stdev_mg_ha): stat_col = \"Forest AGBD stdev (Mg / ha)\"\n",
        "        if df_stats.equals(df_agb_total_tg): stat_col = \"Forest AGB total (Tg)\"\n",
        "        if generate_uncertainty_stats:\n",
        "            if df_stats.equals(df_agbd_mean_mg_ha_ci95): stat_col = \"Forest AGBD CI95 (Mg / ha)\"\n",
        "            if df_stats.equals(df_agbd_mean_mg_ha_uncertainty): stat_col = \"Forest AGBD uncertainty (%)\"\n",
        "            if df_stats.equals(df_agb_total_tg_ci95): stat_col = \"Forest AGB total CI95 (Tg)\"\n",
        "        for stats_polygon_area in df_stats:\n",
        "            if stats_polygon_area == polygon_area:\n",
        "                df_stats_renamed = df_stats.rename(columns={stats_polygon_area:stat_col})\n",
        "                df_detailed_dist_stats = pd.concat([df_detailed_dist_stats, df_stats_renamed[stat_col]], axis=1)\n",
        "    df_detailed_dist_stats.to_csv(join(detailed_dist_stats_by_area_dir, f'{polygon_area}.csv'))\n",
        "\n",
        "# Generate detailed stats by disturbance type - build dictionary once then process\n",
        "dists = {}\n",
        "for stats_csv in os.listdir(detailed_dist_stats_by_area_dir):\n",
        "    polygon_name = f\"{stats_csv[:-4]}\"\n",
        "    stats_csv_path = join(detailed_dist_stats_by_area_dir, stats_csv)\n",
        "    stats_csv_df = pd.read_csv(stats_csv_path)\n",
        "    # Process all disturbance types for this polygon in one pass\n",
        "    for dist in stats_csv_df['dist'].unique():\n",
        "        dist_df = stats_csv_df[stats_csv_df['dist'] == dist].copy()\n",
        "        dist_df.drop('dist', axis=1, inplace=True)\n",
        "        dist_df.insert(0, 'Name', polygon_name)\n",
        "        if dist in dists:\n",
        "            dists[dist] = pd.concat([dists[dist], dist_df], ignore_index=True)\n",
        "        else:\n",
        "            dists[dist] = dist_df\n",
        "\n",
        "# Write all disturbance CSVs\n",
        "for dist, dist_df in dists.items():\n",
        "    output_file_path = join(detailed_dist_stats_by_scenario_dir,f'{dist}.csv')\n",
        "    dist_df.to_csv(output_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t45TVipaVma"
      },
      "source": [
        "# Intactness statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6p_6d-0adF4"
      },
      "outputs": [],
      "source": [
        "# Create list of available intactness rasters\n",
        "intactness_rasters = []\n",
        "for root, dirs, files in os.walk(intactness_dir):\n",
        "    for file in files:\n",
        "        if \"intactness__\" in file and file.endswith('tif'):\n",
        "            relative_path = os.path.relpath(join(root, file), intactness_dir)\n",
        "            intactness_rasters.append(relative_path)\n",
        "\n",
        "# Select intactness rasters to calculate statistics\n",
        "print(\"# Select intactness raster to calculate statistics\")\n",
        "print(\"intactness_rasters = [\")\n",
        "for raster in intactness_rasters:\n",
        "    print(f\"'{raster}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8mA9ax8IjCj"
      },
      "outputs": [],
      "source": [
        "# Select intactness raster to calculate statistics\n",
        "intactness_rasters = [\n",
        "'2024_oldgrowth_all_land__2024_disturbance_since_oldgrowth/intactness__forest_reserves_10_quantiles__2024_oldgrowth_all_land__2024_disturbance_since_oldgrowth__agbd_251203_161707.tif',\n",
        "'2024_oldgrowth_all_land__2024_disturbance_since_oldgrowth/intactness__gedi_area_10_quantiles__2024_oldgrowth_all_land__2024_disturbance_since_oldgrowth__agbd_251203_161707.tif',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6Q51AedNXu6"
      },
      "outputs": [],
      "source": [
        "# Load cell area raster for accurate pixel-by-pixel area calculations\n",
        "cell_area_path = join(areas_dir, \"cell_area.tif\")\n",
        "\n",
        "# Toggle for non-forest definition\n",
        "# True: non-forest = land pixels with intactness == 0 (excludes water)\n",
        "# False: non-forest = any pixels with intactness == 0 (includes water)\n",
        "# Note: True requires appropriate land mask created in 'Oldgrowth scenarios' section of 6_scenarios.ipynb\n",
        "restrict_non_forest_to_land_only = True\n",
        "\n",
        "# Match percentage change rasters\n",
        "intactness_percentage_raster_paths = {}\n",
        "for intactness_raster in intactness_rasters:\n",
        "    intactness_raster_path = join(intactness_dir, intactness_raster)\n",
        "    intactness_baseline_dist_dir = intactness_raster.split('/')[0]\n",
        "    percentage_change_filename = f\"percentage_change__{intactness_baseline_dist_dir}__{selected_model}.tif\"\n",
        "    percentage_change_path = join(intactness_dir, intactness_baseline_dist_dir, percentage_change_filename)\n",
        "    intactness_percentage_raster_paths[intactness_raster_path] = percentage_change_path\n",
        "\n",
        "# Function to calculate area-weighted statistics\n",
        "def weighted_stats(values, weights):\n",
        "    # Handle empty arrays\n",
        "    if len(values) == 0:\n",
        "        return None, None\n",
        "    # Calculate weighted mean\n",
        "    weighted_sum = np.sum(values * weights)\n",
        "    sum_of_weights = np.sum(weights)\n",
        "    weighted_mean = weighted_sum / sum_of_weights if sum_of_weights > 0 else 0\n",
        "    # Calculate weighted standard deviation\n",
        "    if sum_of_weights > 0:\n",
        "        variance = np.sum(weights * (values - weighted_mean) ** 2) / sum_of_weights\n",
        "        weighted_std = np.sqrt(variance)\n",
        "    else: weighted_std = 0\n",
        "    return weighted_mean, weighted_std\n",
        "\n",
        "# Function to calculate area for each intactness score\n",
        "def calculate_score_areas(intactness_masked, cell_area_masked_ha):\n",
        "    score_areas = {}\n",
        "    if np.ma.count(intactness_masked) > 0:\n",
        "        # Get unique values from valid (unmasked) data\n",
        "        unique_scores = np.unique(intactness_masked.compressed())\n",
        "        for score in unique_scores:\n",
        "            # Find pixels with this score (not masked)\n",
        "            score_mask = (intactness_masked == score) & (~intactness_masked.mask)\n",
        "            if np.any(score_mask): score_area_ha = np.sum(cell_area_masked_ha[score_mask], dtype='float64')\n",
        "            else: score_area_ha = 0.0\n",
        "            score_areas[int(score)] = score_area_ha\n",
        "    return score_areas\n",
        "\n",
        "# Pre-open cell area dataset\n",
        "cell_area_dataset = rasterio.open(cell_area_path)\n",
        "\n",
        "# Progress tracking\n",
        "n_intactness_rasters = len(intactness_percentage_raster_paths)\n",
        "n_polygons = len(selected_sample_polygons_gpkg)\n",
        "progress_total = n_intactness_rasters * n_polygons\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Raster / polygon pair progress: {progress_index}/{progress_total}\")\n",
        "display(progress_label)\n",
        "\n",
        "try:\n",
        "    # Loop through each polygon stored in GPKG to generate statistics\n",
        "    for intactness_raster, percentage_raster in intactness_percentage_raster_paths.items():\n",
        "        polygon_quantiles = intactness_raster.split('/')[-1].split('__')[1]\n",
        "        baseline = intactness_raster.split('/')[-1].split('__')[2]\n",
        "        disturbance = intactness_raster.split('/')[-1].split('__')[3]\n",
        "        current_year = disturbance.split('_')[0]\n",
        "\n",
        "        # Dynamic land mask path based on intactness raster year\n",
        "        if restrict_non_forest_to_land_only:\n",
        "            all_land_mask_path = join(mask_dir, f\"mask_forest_{current_year}_oldgrowth_all_land.tif\")\n",
        "            if not exists(all_land_mask_path):\n",
        "                raise FileNotFoundError(f\"Land mask not found: {all_land_mask_path}. \"\n",
        "                                      f\"Please create this mask in the 'Oldgrowth scenarios' section of 6_scenarios.ipynb\")\n",
        "            all_land_mask_dataset = rasterio.open(all_land_mask_path)\n",
        "\n",
        "        intactness_csv_name = f\"{polygon_quantiles}__{baseline}__{disturbance}.csv\"\n",
        "        intactness_csv_path = join(intactness_stats_dir, intactness_csv_name)\n",
        "        total_score = int(intactness_raster.split('/')[-1].split('__')[1].split('_')[-2])\n",
        "        total_stdev = int(total_score / 2)\n",
        "        df_intactness_stats = pd.DataFrame(columns=[\n",
        "            \"Name\",\n",
        "            \"Percentage change (remaining forest) mean\",\n",
        "            \"Percentage change (remaining forest) stdev\",\n",
        "            \"Percentage change (non-forest = -100) mean\",\n",
        "            \"Percentage change (non-forest = -100) stdev\",\n",
        "            f\"Intactness (remaining forest) mean / {total_score}\",\n",
        "            f\"Intactness (remaining forest) stdev / {total_stdev}\",\n",
        "            f\"Intactness (non-forest = 0) mean / {total_score}\",\n",
        "            f\"Intactness (non-forest = 0) stdev / {total_stdev}\"\n",
        "        ])\n",
        "\n",
        "        for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "\n",
        "            # Define the polygon\n",
        "            sample_polygon_geometry = row[\"geometry\"]\n",
        "            sample_polygon_name = row[\"name\"]\n",
        "            polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "            # Calculate sample_polygon_geometry area (ellipsoidal as opposed to planimetric)\n",
        "            sample_polygons_crs = selected_sample_polygons_gpkg.crs\n",
        "            temp_gdf = gpd.GeoDataFrame({'name': [sample_polygon_name], 'geometry': sample_polygon_geometry}, crs=sample_polygons_crs)\n",
        "            temp_gdf_utm = temp_gdf.estimate_utm_crs()\n",
        "            polygon_area_ha = np.divide(temp_gdf.to_crs(temp_gdf_utm).area[0], 10000, dtype='float64')\n",
        "\n",
        "            # Read & mask intactness to polygon\n",
        "            with rasterio.open(intactness_raster) as src:\n",
        "                nodata_value = src.nodata\n",
        "                intactness_masked, transform_2 = msk.mask(src, polygons, crop=True, filled=False)\n",
        "                intactness_masked = intactness_masked.astype('float64')\n",
        "\n",
        "            # Check if all values are masked (outside polygon)\n",
        "            if np.ma.count(intactness_masked) == 0:\n",
        "                # No valid intactness data - set all stats to None and continue to next polygon\n",
        "                new_row = pd.DataFrame([{\n",
        "                    'Name': sample_polygon_name,\n",
        "                    'Percentage change (remaining forest) mean': None,\n",
        "                    'Percentage change (remaining forest) stdev': None,\n",
        "                    'Percentage change (non-forest = -100) mean': None,\n",
        "                    'Percentage change (non-forest = -100) stdev': None,\n",
        "                    f'Intactness (remaining forest) mean / {total_score}': None,\n",
        "                    f'Intactness (remaining forest) stdev / {total_stdev}': None,\n",
        "                    f'Intactness (non-forest = 0) mean / {total_score}': None,\n",
        "                    f'Intactness (non-forest = 0) stdev / {total_stdev}': None,\n",
        "                }], dtype=object)\n",
        "\n",
        "                df_intactness_stats = pd.concat([df_intactness_stats, new_row], ignore_index=True)\n",
        "\n",
        "                # Update progress\n",
        "                progress_index += 1\n",
        "                progress_label.value = f\"Raster / polygon pair progress: {progress_index}/{progress_total}\"\n",
        "                continue\n",
        "\n",
        "            # Mask the cell area raster to the polygon using pre-opened dataset\n",
        "            cell_area_masked, transform_ca = msk.mask(cell_area_dataset, polygons, crop=True, filled=False)\n",
        "            cell_area_masked = cell_area_masked.astype('float64')\n",
        "\n",
        "            # Convert adjusted cell areas from m2 to ha for easier calculations\n",
        "            cell_area_masked_ha = cell_area_masked / 10000\n",
        "\n",
        "            # Create forest mask from intactness raster (forest = intactness > 0)\n",
        "            forest_mask = (~intactness_masked.mask) & (intactness_masked > 0)\n",
        "\n",
        "            # Create non-forest mask based on toggle setting\n",
        "            potential_non_forest_mask = (~intactness_masked.mask) & (intactness_masked == 0)\n",
        "\n",
        "            if restrict_non_forest_to_land_only:\n",
        "                # Get land mask and restrict non-forest to land pixels only (excludes water)\n",
        "                all_land_mask_data, transform_alm = msk.mask(all_land_mask_dataset, polygons, crop=True, filled=False)\n",
        "                all_land_mask_data = all_land_mask_data.astype('float64')\n",
        "                all_land_mask = ~np.ma.getmaskarray(all_land_mask_data) & (all_land_mask_data == 1)\n",
        "                non_forest_mask = potential_non_forest_mask & all_land_mask\n",
        "            else:\n",
        "                # Include all pixels with intactness == 0 (including water)\n",
        "                non_forest_mask = potential_non_forest_mask\n",
        "\n",
        "            # Extract forest and non-forest land areas\n",
        "            forest_cell_areas_ha = np.ma.array(cell_area_masked_ha.data, mask=~forest_mask)\n",
        "            forest_area_ha_sum = np.ma.sum(forest_cell_areas_ha)\n",
        "\n",
        "            non_forest_cell_areas_ha = np.ma.array(cell_area_masked_ha.data, mask=~non_forest_mask)\n",
        "            non_forest_area_ha_sum = np.ma.sum(non_forest_cell_areas_ha)\n",
        "\n",
        "            # Mask percentage change raster to the polygon\n",
        "            with rasterio.open(percentage_raster) as percent_change:\n",
        "                percent_change_masked, transform_pc = msk.mask(percent_change, polygons, crop=True, filled=False)\n",
        "                percent_change_masked = percent_change_masked.astype('float64')\n",
        "\n",
        "            # Apply forest mask to percentage change values (for forest-only statistics)\n",
        "            percent_change_forest_only = np.ma.array(percent_change_masked.data, mask=~forest_mask)\n",
        "\n",
        "            if forest_area_ha_sum > 0:\n",
        "                # Extract percentage change values for forest pixels\n",
        "                forest_percent_values = np.ma.compressed(percent_change_forest_only)\n",
        "                forest_percent_weights = np.ma.compressed(forest_cell_areas_ha)\n",
        "\n",
        "                # Calculate area-weighted percentage change statistics for remaining forest\n",
        "                percent_change_forest_mean, percent_change_forest_std = weighted_stats(\n",
        "                    forest_percent_values, forest_percent_weights\n",
        "                )\n",
        "\n",
        "                # For all land including non-forest land (treated as -100% change)\n",
        "                if non_forest_area_ha_sum > 0:\n",
        "                    # Calculate the weighted mean directly\n",
        "                    all_mean_numerator = np.sum(forest_percent_values * forest_percent_weights)\n",
        "                    all_mean_denominator = forest_area_ha_sum + non_forest_area_ha_sum\n",
        "                    all_mean_numerator += non_forest_area_ha_sum * (-100.0)\n",
        "                    percent_change_all_mean = all_mean_numerator / all_mean_denominator\n",
        "\n",
        "                    # Calculate the weighted variance directly\n",
        "                    forest_variance_contribution = np.sum(\n",
        "                        forest_percent_weights * np.square(forest_percent_values - percent_change_all_mean)\n",
        "                    )\n",
        "                    non_forest_variance_contribution = non_forest_area_ha_sum * np.square((-100.0) - percent_change_all_mean)\n",
        "                    all_variance = (forest_variance_contribution + non_forest_variance_contribution) / all_mean_denominator\n",
        "                    percent_change_all_std = np.sqrt(all_variance)\n",
        "                else:\n",
        "                    # If no non-forest land area, all-land stats are the same as forest stats\n",
        "                    percent_change_all_mean = percent_change_forest_mean\n",
        "                    percent_change_all_std = percent_change_forest_std\n",
        "            else:\n",
        "                # If no forest, set forest stats to None and all-land stats to -100% change\n",
        "                percent_change_forest_mean = percent_change_forest_std = None\n",
        "                percent_change_all_mean = -100.0\n",
        "                percent_change_all_std = 0.0\n",
        "\n",
        "            # Compute intactness stats\n",
        "            if forest_area_ha_sum > 0:\n",
        "                # Remaining-forest intactness (only forest pixels)\n",
        "                forest_intact_vals = intactness_masked.data[forest_mask]\n",
        "                forest_intact_weights = cell_area_masked_ha.data[forest_mask]\n",
        "                intactness_remaining_mean, intactness_remaining_std = weighted_stats(\n",
        "                    forest_intact_vals, forest_intact_weights\n",
        "                )\n",
        "\n",
        "                # All-land intactness (non-forest land = 0)\n",
        "                total_land = forest_area_ha_sum + non_forest_area_ha_sum\n",
        "                num = np.sum(forest_intact_vals * forest_intact_weights)\n",
        "                den = total_land\n",
        "                intactness_all_mean = num / den\n",
        "\n",
        "                # Variance: forest + non-forest land contributions\n",
        "                var_forest = np.sum(forest_intact_weights * np.square(forest_intact_vals - intactness_all_mean))\n",
        "                var_nonforest = non_forest_area_ha_sum * np.square(0 - intactness_all_mean)\n",
        "                intactness_all_std = np.sqrt((var_forest + var_nonforest) / den)\n",
        "            else:\n",
        "                # No forest present\n",
        "                intactness_remaining_mean = intactness_remaining_std = None\n",
        "                intactness_all_mean = 0.0\n",
        "                intactness_all_std = 0.0\n",
        "\n",
        "            # Calculate area for each intactness score\n",
        "            score_areas = calculate_score_areas(intactness_masked, cell_area_masked_ha)\n",
        "\n",
        "            # Create new row with all statistics including score areas\n",
        "            new_row_dict = {\n",
        "                'Name': sample_polygon_name,\n",
        "                'Percentage change (remaining forest) mean': percent_change_forest_mean,\n",
        "                'Percentage change (remaining forest) stdev': percent_change_forest_std,\n",
        "                'Percentage change (non-forest = -100) mean': percent_change_all_mean,\n",
        "                'Percentage change (non-forest = -100) stdev': percent_change_all_std,\n",
        "                f'Intactness (remaining forest) mean / {total_score}': intactness_remaining_mean,\n",
        "                f'Intactness (remaining forest) stdev / {total_stdev}': intactness_remaining_std,\n",
        "                f'Intactness (non-forest = 0) mean / {total_score}': intactness_all_mean,\n",
        "                f'Intactness (non-forest = 0) stdev / {total_stdev}': intactness_all_std,\n",
        "            }\n",
        "\n",
        "            # Add score area columns\n",
        "            for score, area in score_areas.items():\n",
        "                new_row_dict[f'Intactness score {score} area (ha)'] = area\n",
        "\n",
        "            new_row = pd.DataFrame([new_row_dict], dtype=object)\n",
        "\n",
        "            # Append to main dataframe\n",
        "            df_intactness_stats = pd.concat([df_intactness_stats, new_row], ignore_index=True)\n",
        "\n",
        "            # Update progress\n",
        "            progress_index += 1\n",
        "            progress_label.value = f\"Raster / polygon pair progress: {progress_index}/{progress_total}\"\n",
        "\n",
        "        # Set index to Name and save to CSV\n",
        "        df_intactness_stats = df_intactness_stats.set_index('Name')\n",
        "        df_intactness_stats.to_csv(intactness_csv_path)\n",
        "        print(f\"Saved statistics to {intactness_csv_path}\")\n",
        "\n",
        "        # Close land mask dataset if opened\n",
        "        if restrict_non_forest_to_land_only:\n",
        "            all_land_mask_dataset.close()\n",
        "\n",
        "finally: cell_area_dataset.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC-rviHPi4q5"
      },
      "source": [
        "# Report statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHpAf6qg1dEf"
      },
      "outputs": [],
      "source": [
        "# Reduces statistics to a more specific and intuitive format.\n",
        "\n",
        "# Define scenarios for report\n",
        "print(\"# Remember that order matters\\n\")\n",
        "print(\"scenario_list = [\")\n",
        "for csv in os.listdir(detailed_stats_by_scenario_dir):\n",
        "  print(f\"'{csv[:-4]}',\")\n",
        "print(\"]\")\n",
        "print(\"\")\n",
        "\n",
        "disturbance_csv_files = [f[:-4] for f in os.listdir(detailed_dist_stats_by_scenario_dir) if f.endswith('.csv')]\n",
        "\n",
        "def get_disturbance_type(filename):\n",
        "    if 'degradation_deforestation' in filename: return 3  # comes third\n",
        "    elif 'deforestation' in filename: return 2  # comes second\n",
        "    else: return 1  # comes first (degradation)\n",
        "\n",
        "# Sort by year, then disturbance type\n",
        "files_by_category = {}\n",
        "for file in disturbance_csv_files:\n",
        "    year = file.split('_')[0]\n",
        "    dist_type = get_disturbance_type(file)\n",
        "    key = (year, dist_type)\n",
        "\n",
        "    if key not in files_by_category:\n",
        "        files_by_category[key] = []\n",
        "    files_by_category[key].append(file)\n",
        "print(\"disturbance_list = [\")\n",
        "current_year = None\n",
        "\n",
        "# Process each category in order\n",
        "for key in sorted(files_by_category.keys(), key=lambda k: (int(k[0]), k[1])):\n",
        "    files = files_by_category[key]\n",
        "    # First add 'total' files\n",
        "    total_files = [f for f in files if '_total' in f]\n",
        "    for file in total_files:\n",
        "        print(f\"    '{file}',\")\n",
        "    # Group remaining files by reference year\n",
        "    ref_year_files = {}\n",
        "    for file in files:\n",
        "        if '_total' in file:\n",
        "            continue\n",
        "        ref_year = file.split('_')[-1]\n",
        "        if ref_year not in ref_year_files:\n",
        "            ref_year_files[ref_year] = []\n",
        "        ref_year_files[ref_year].append(file)\n",
        "    # Process each reference year, placing 'since' before 'before'\n",
        "    for ref_year in sorted(ref_year_files.keys(), key=int, reverse=True):\n",
        "        year_files = ref_year_files[ref_year]\n",
        "        since_files = [f for f in year_files if '_since_' in f]\n",
        "        before_files = [f for f in year_files if '_before_' in f]\n",
        "        # Add 'since' files normally\n",
        "        for file in since_files:\n",
        "            print(f\"    '{file}',\")\n",
        "        # Add 'before' files commented out\n",
        "        for file in before_files:\n",
        "            print(f\"    # '{file}',\")\n",
        "\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCMvXcIa2Q0D"
      },
      "outputs": [],
      "source": [
        "# Remember that order matters\n",
        "\n",
        "scenario_list = [\n",
        "'2018',\n",
        "'2024',\n",
        "# '2024_oldgrowth',\n",
        "# '2024_oldgrowth_all_land',\n",
        "# '2024_alternate_degradation_2018',\n",
        "# '2024_no_degradation_since_1996',\n",
        "# '2024_no_degradation_since_2019',\n",
        "]\n",
        "\n",
        "disturbance_list = [\n",
        "    # '2024_degradation_total',\n",
        "    '2024_degradation_since_2019',\n",
        "    # '2024_degradation_before_2019',\n",
        "    # '2024_degradation_since_1996',\n",
        "    # '2024_degradation_before_1996',\n",
        "    # '2024_deforestation_total',\n",
        "    '2024_deforestation_since_2019',\n",
        "    # '2024_deforestation_before_2019',\n",
        "    # '2024_degradation_deforestation_total',\n",
        "    '2024_degradation_deforestation_since_2019',\n",
        "    # '2024_degradation_deforestation_before_2019',\n",
        "]\n",
        "\n",
        "report_year = '2024'\n",
        "\n",
        "all_land_scenario = None\n",
        "for scenario in scenario_list:\n",
        "  if 'all_land' in scenario:\n",
        "    all_land_scenario = scenario\n",
        "if all_land_scenario == None: print(\"No all land scenario exists in the detailed stats.\")\n",
        "\n",
        "# Read summary stats\n",
        "summary_stats_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_stats.csv'))\n",
        "summary_dist_stats_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_dist_stats.csv'))\n",
        "\n",
        "# Create attributes CSV\n",
        "attributes = pd.DataFrame()\n",
        "attributes['Name'] = summary_stats_df.iloc[:, 0]\n",
        "attributes['Area (km^2)'] = summary_stats_df['Area (km^2)']\n",
        "attributes[f'{report_year} forest cover (ha)'] = summary_stats_df[f'{report_year} forest cover (ha)']\n",
        "if all_land_scenario:\n",
        "  attributes[f'{all_land_scenario} forest cover (ha)'] = summary_stats_df[f'{all_land_scenario} forest cover (ha)']\n",
        "attributes.to_csv(join(report_statistics_dir, f'{report_year}_attributes.csv'), index=False)\n",
        "\n",
        "# Create scenarios total AGB CSV\n",
        "scenarios_total_agb = pd.DataFrame()\n",
        "scenarios_total_agb['Name'] = summary_stats_df['Unnamed: 0']\n",
        "for scenario in scenario_list:\n",
        "  scenarios_total_agb[f'{scenario} forest AGB (Tg)'] = summary_stats_df[f'{scenario} forest AGB (Tg)']\n",
        "if source_dir == uncertainty_dir:\n",
        "  for scenario in scenario_list:\n",
        "    scenarios_total_agb[f'{scenario} forest AGB CI95 (Tg)'] = summary_stats_df[f'{scenario} forest AGB CI95 (Tg)']\n",
        "scenarios_total_agb.to_csv(join(report_statistics_dir, f'{report_year}_scenarios_total_agb.csv'), index=False)\n",
        "\n",
        "# Create scenarios AGBD CSV\n",
        "scenarios_agbd = pd.DataFrame()\n",
        "scenarios_agbd['Name'] = summary_stats_df['Unnamed: 0']\n",
        "for scenario in scenario_list:\n",
        "  scenario_detailed_stats_df = pd.read_csv(join(detailed_stats_by_scenario_dir, f'{scenario}.csv'))\n",
        "  scenarios_agbd[f'{scenario} forest AGBD (Mg / ha)'] = scenario_detailed_stats_df['Forest AGBD mean (Mg / ha)']\n",
        "if source_dir == uncertainty_dir:\n",
        "  for scenario in scenario_list:\n",
        "    scenario_detailed_stats_df = pd.read_csv(join(detailed_stats_by_scenario_dir, f'{scenario}.csv'))\n",
        "    scenarios_agbd[f'{scenario} forest AGBD CI95 (Mg / ha)'] = scenario_detailed_stats_df['Forest AGBD CI95 (Mg / ha)']\n",
        "scenarios_agbd.to_csv(join(report_statistics_dir, f'{report_year}_scenarios_agbd.csv'), index=False)\n",
        "\n",
        "# Create disturbance total AGB CSV\n",
        "disturbance_total_agb = pd.DataFrame()\n",
        "disturbance_total_agb['Name'] = summary_dist_stats_df['Unnamed: 0']\n",
        "for disturbance in disturbance_list:\n",
        "  disturbance_total_agb[f'{disturbance} forest AGB (Tg)'] = summary_dist_stats_df[f'{disturbance} forest AGB (Tg)']\n",
        "if source_dir == uncertainty_dir:\n",
        "  for disturbance in disturbance_list:\n",
        "    disturbance_total_agb[f'{disturbance} forest AGB CI95 (Tg)'] = summary_dist_stats_df[f'{disturbance} forest AGB CI95 (Tg)']\n",
        "disturbance_total_agb.to_csv(join(report_statistics_dir, f'{report_year}_disturbance_total_agb.csv'), index=False)\n",
        "\n",
        "# Create disturbance AGBD CSV\n",
        "disturbance_agbd = pd.DataFrame()\n",
        "disturbance_agbd['Name'] = summary_stats_df['Unnamed: 0']\n",
        "for disturbance in disturbance_list:\n",
        "  disturbance_detailed_stats_df = pd.read_csv(join(detailed_dist_stats_by_scenario_dir, f'{disturbance}.csv'))\n",
        "  disturbance_agbd[f'{disturbance} forest AGBD (Mg / ha)'] = disturbance_detailed_stats_df['Forest AGBD mean (Mg / ha)']\n",
        "if source_dir == uncertainty_dir:\n",
        "  for disturbance in disturbance_list:\n",
        "    disturbance_detailed_stats_df = pd.read_csv(join(detailed_dist_stats_by_scenario_dir, f'{disturbance}.csv'))\n",
        "    disturbance_agbd[f'{disturbance} forest AGBD CI95 (Mg / ha)'] = disturbance_detailed_stats_df['Forest AGBD CI95 (Mg / ha)']\n",
        "disturbance_agbd.to_csv(join(report_statistics_dir, f'{report_year}_disturbance_agbd.csv'), index=False)\n",
        "\n",
        "print(\"Report statistics completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b70oSO_VNtkY"
      },
      "source": [
        "# Sankey plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHEy0mBX2yNj"
      },
      "outputs": [],
      "source": [
        "# Define and create directories\n",
        "sankey_labelled = join(sample_polygons_statistics_dir, 'sankey_labelled')\n",
        "sankey_unlabelled = join(sample_polygons_statistics_dir, 'sankey_unlabelled')\n",
        "sankey_labelled_svg = join(sample_polygons_statistics_dir, 'sankey_labelled_svg')\n",
        "sankey_unlabelled_svg = join(sample_polygons_statistics_dir, 'sankey_unlabelled_svg')\n",
        "\n",
        "for dir in [sankey_labelled, sankey_unlabelled, sankey_labelled_svg, sankey_unlabelled_svg]:\n",
        "    makedirs(dir, exist_ok=True)\n",
        "\n",
        "# Load the CSV files\n",
        "summary_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_stats.csv'))\n",
        "summary_dist_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_dist_stats.csv'))\n",
        "\n",
        "# Check that all rows in both .csv files have the same strings (polygon areas) in column A\n",
        "polygon_areas_stats = summary_stats.iloc[:, 0]\n",
        "polygon_areas_dist_stats = summary_dist_stats.iloc[:, 0]\n",
        "\n",
        "assert all(polygon_areas_stats == polygon_areas_dist_stats), \"Polygon areas do not match between the two CSV files.\"\n",
        "\n",
        "# Print columns relevant for sankey diagram configuration\n",
        "\n",
        "# Filter for AGB columns only (exclude forest cover and CI95 for initial selection)\n",
        "summary_agb_cols = [col for col in summary_stats.columns[1:] if 'forest AGB (Tg)' in col and 'CI95' not in col]\n",
        "dist_agb_cols = [col for col in summary_dist_stats.columns[1:] if 'forest AGB (Tg)' in col and 'CI95' not in col]\n",
        "\n",
        "print(\"=== summary_stats.csv AGB columns ===\")\n",
        "print(\"(for old_growth_agb_column and current_agb_column)\\n\")\n",
        "\n",
        "# Group by category\n",
        "current_year_cols = [col for col in summary_agb_cols if col.startswith('2024 ') or col.startswith('2023 ') or col.startswith('2022 ')]\n",
        "oldgrowth_cols = [col for col in summary_agb_cols if 'oldgrowth' in col and not col.endswith('_1 forest AGB (Tg)') and not col.endswith('_2 forest AGB (Tg)')]\n",
        "no_disturbance_cols = [col for col in summary_agb_cols if 'no_disturbance' in col]\n",
        "no_degradation_cols = [col for col in summary_agb_cols if 'no_degradation' in col]\n",
        "\n",
        "print(\"Current year scenarios:\")\n",
        "for i, col in enumerate(current_year_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\nOld-growth scenarios:\")\n",
        "for i, col in enumerate(oldgrowth_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\nNo disturbance scenarios:\")\n",
        "for i, col in enumerate(no_disturbance_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\nNo degradation scenarios:\")\n",
        "for i, col in enumerate(no_degradation_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"=== summary_dist_stats.csv AGB columns ===\")\n",
        "print(\"(for degradation/deforestation since/total columns)\\n\")\n",
        "\n",
        "# Group disturbance columns\n",
        "degradation_cols = [col for col in dist_agb_cols if 'degradation' in col]\n",
        "deforestation_cols = [col for col in dist_agb_cols if 'deforestation' in col]\n",
        "disturbance_cols = [col for col in dist_agb_cols if 'disturbance' in col and 'effect' not in col]\n",
        "\n",
        "print(\"Degradation columns:\")\n",
        "for i, col in enumerate(degradation_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\nDeforestation columns:\")\n",
        "for i, col in enumerate(deforestation_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\nDisturbance (combined) columns:\")\n",
        "for i, col in enumerate(disturbance_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ykformx2-nl"
      },
      "outputs": [],
      "source": [
        "# Plot degradation and deforestation separately\n",
        "separate_disturbance = True\n",
        "# Plot degradation before and since a date separately\n",
        "separate_degradation = True\n",
        "# Plot deforestation before and since a date separately\n",
        "separate_deforestation = True\n",
        "# Plot total disturbance before and since a date separately (when separate_disturbance is False)\n",
        "separate_disturbance_temporal = False\n",
        "\n",
        "# DPI (default is 96, output image will scale accordingly)\n",
        "dpi = 300\n",
        "# Relative width modifier (ratio, e.g. 0.5 or 2)\n",
        "width_modifier = 0.85\n",
        "\n",
        "# Title (polygon area), density and label variables (weight of 800 ~ bold, 400 ~ normal)\n",
        "show_title = True\n",
        "show_density = True\n",
        "show_labels = True\n",
        "left_axis_label = True\n",
        "svg_transparent_background = True\n",
        "title_font_size = 20\n",
        "title_font_weight = 600\n",
        "density_font_size = 17\n",
        "density_font_weight = 600\n",
        "label_font_size = 17\n",
        "label_font_weight = 600\n",
        "\n",
        "# Base columns and year (summary_stats)\n",
        "old_growth_agb_column = '2024_oldgrowth_all_land forest AGB (Tg)'\n",
        "current_agb_column = '2024 forest AGB (Tg)'\n",
        "current_year = current_agb_column.split(' ')[0]\n",
        "\n",
        "# Disturbance columns (summary_dist_stats)\n",
        "degradation_since_column = '2024_degradation_since_1996 forest AGB (Tg)'\n",
        "degradation_total_column = '2024_degradation_since_oldgrowth forest AGB (Tg)'\n",
        "deforestation_since_column = '2024_deforestation_since_1996 forest AGB (Tg)'\n",
        "deforestation_total_column = '2024_deforestation_since_oldgrowth forest AGB (Tg)'\n",
        "disturbance_since_column = '2024_disturbance_since_1996 forest AGB (Tg)'\n",
        "disturbance_total_column = '2024_disturbance_since_oldgrowth forest AGB (Tg)'\n",
        "\n",
        "# Node labels and colours\n",
        "remaining_name = f'Remaining in {current_year}:'\n",
        "remaining_colour = '#007fff'\n",
        "degradation_before_name = 'Degradation loss before 1996'\n",
        "degradation_before_colour = '#1a801a'\n",
        "degradation_since_name = 'Degradation loss since 1996'\n",
        "degradation_since_colour = '#8dc00d'\n",
        "degradation_total_name = 'Degradation loss'\n",
        "degradation_total_colour = '#8dc00d'\n",
        "deforestation_before_name = 'Deforestation loss before 1996'\n",
        "deforestation_before_colour = '#ffffff'\n",
        "deforestation_since_name = 'Deforestation loss since 1996'\n",
        "deforestation_since_colour = '#ffff00'\n",
        "deforestation_total_name = 'Deforestation loss'\n",
        "deforestation_total_colour = '#ffffff'\n",
        "disturbance_before_name = 'Disturbance loss before 1996'\n",
        "disturbance_before_colour = '#cccccc'\n",
        "disturbance_since_name = 'Disturbance loss since 1996'\n",
        "disturbance_since_colour = '#999999'\n",
        "disturbance_total_name = 'Disturbance loss'\n",
        "disturbance_total_colour = '#ffffff'\n",
        "\n",
        "# Validate separation settings\n",
        "assert not separate_degradation or separate_disturbance, \"separate_disturbance must be True if separate_degradation is True.\"\n",
        "assert not separate_deforestation or separate_disturbance, \"separate_disturbance must be True if separate_deforestation is True.\"\n",
        "assert not separate_disturbance_temporal or not separate_disturbance, \"separate_disturbance must be False if separate_disturbance_temporal is True.\"\n",
        "\n",
        "# Function to get values from statistics\n",
        "def get_value(df, idx, column_name):\n",
        "    try:\n",
        "        value = df.loc[idx, column_name]\n",
        "        return 0.0 if pd.isnull(value) else float(value)\n",
        "    except KeyError:\n",
        "        print(f\"Column '{column_name}' not found in the dataframe.\")\n",
        "        return 0.0\n",
        "\n",
        "# Loop through each row (polygon area)\n",
        "for idx in summary_stats.index:\n",
        "    polygon_name = summary_stats.iloc[idx, 0]\n",
        "\n",
        "    # Get old-growth and current AGB values\n",
        "    old_growth_agb = get_value(summary_stats, idx, old_growth_agb_column)\n",
        "    current_agb = get_value(summary_stats, idx, current_agb_column)\n",
        "\n",
        "    # Get disturbance values and calculate before values\n",
        "    degradation_since = get_value(summary_dist_stats, idx, degradation_since_column)\n",
        "    degradation_total = get_value(summary_dist_stats, idx, degradation_total_column)\n",
        "    degradation_before = degradation_total - degradation_since\n",
        "\n",
        "    deforestation_since = get_value(summary_dist_stats, idx, deforestation_since_column)\n",
        "    deforestation_total = get_value(summary_dist_stats, idx, deforestation_total_column)\n",
        "    deforestation_before = deforestation_total - deforestation_since\n",
        "\n",
        "    disturbance_since = get_value(summary_dist_stats, idx, disturbance_since_column)\n",
        "    disturbance_total = get_value(summary_dist_stats, idx, disturbance_total_column)\n",
        "    disturbance_before = disturbance_total - disturbance_since\n",
        "\n",
        "    # Statistical assertions\n",
        "    if separate_degradation:\n",
        "        discrepancy = abs(degradation_before + degradation_since - degradation_total)\n",
        "        if discrepancy >= 1e-6:\n",
        "            print(f\"{polygon_name}: degradation_before + degradation_since != degradation_total (discrepancy: {discrepancy:.6e})\")\n",
        "    if separate_deforestation:\n",
        "        discrepancy = abs(deforestation_before + deforestation_since - deforestation_total)\n",
        "        if discrepancy >= 1e-6:\n",
        "            print(f\"{polygon_name}: deforestation_before + deforestation_since != deforestation_total (discrepancy: {discrepancy:.6e})\")\n",
        "    if separate_disturbance:\n",
        "        discrepancy = abs(degradation_total + deforestation_total - disturbance_total)\n",
        "        if discrepancy >= 1e-6:\n",
        "            print(f\"{polygon_name}: degradation_total + deforestation_total != disturbance_total (discrepancy: {discrepancy:.6e})\")\n",
        "    discrepancy = abs(current_agb - disturbance_total - old_growth_agb)\n",
        "    if discrepancy >= 1e-6:\n",
        "        print(f\"{polygon_name}: current_agb - disturbance_total != old_growth_agb (discrepancy: {discrepancy:.6e})\")\n",
        "        print(\"Note: Constraining degradation floor to disturbance or capping disturbances to 0 can break equality when amalgamating across areas\")\n",
        "\n",
        "    # Load detailed stats for AGBD and CI95 values\n",
        "    detailed_stats_df = pd.read_csv(join(detailed_stats_by_area_dir, f\"{polygon_name}.csv\"))\n",
        "    old_growth_index = detailed_stats_df.index[detailed_stats_df['scenario'] == f\"{old_growth_agb_column.split(' ')[0]}\"].item()\n",
        "    current_index = detailed_stats_df.index[detailed_stats_df['scenario'] == f\"{current_agb_column.split(' ')[0]}\"].item()\n",
        "\n",
        "    old_growth_mean_agbd = get_value(detailed_stats_df, old_growth_index, \"Forest AGBD mean (Mg / ha)\")\n",
        "    current_mean_agbd = get_value(detailed_stats_df, current_index, \"Forest AGBD mean (Mg / ha)\")\n",
        "\n",
        "    uncertainty = 'Forest AGB total CI95 (Tg)' in detailed_stats_df.columns\n",
        "    if uncertainty:\n",
        "        old_growth_agb_ci95 = get_value(detailed_stats_df, old_growth_index, \"Forest AGB total CI95 (Tg)\")\n",
        "        old_growth_mean_agbd_ci95 = get_value(detailed_stats_df, old_growth_index, \"Forest AGBD CI95 (Mg / ha)\")\n",
        "        current_agb_ci95 = get_value(detailed_stats_df, current_index, \"Forest AGB total CI95 (Tg)\")\n",
        "        current_mean_agbd_ci95 = get_value(detailed_stats_df, current_index, \"Forest AGBD CI95 (Mg / ha)\")\n",
        "\n",
        "    # Build title and subtitle text\n",
        "    title_name = f\"{polygon_name}\"\n",
        "\n",
        "    if uncertainty:\n",
        "        subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} ± {old_growth_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "        subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} ± {current_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "        left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} ± {old_growth_agb_ci95:.2f} Tg\" if left_axis_label else ''\n",
        "        remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} ± {current_agb_ci95:.2f} Tg\"\n",
        "    else:\n",
        "        subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} Mg / ha\"\n",
        "        subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} Mg / ha\"\n",
        "        left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} Tg\" if left_axis_label else ''\n",
        "        remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} Tg\"\n",
        "\n",
        "    # Configure nodes and links based on separation settings\n",
        "    if separate_disturbance and separate_degradation and separate_deforestation:\n",
        "        nodes = [left_axis, degradation_before_name, degradation_since_name, deforestation_before_name, deforestation_since_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0, 0, 0], [1, 2, 3, 4, 5]\n",
        "        values = [-degradation_before, -degradation_since, -deforestation_before, -deforestation_since, current_agb]\n",
        "        colors = [degradation_before_colour, degradation_since_colour, deforestation_before_colour, deforestation_since_colour, remaining_colour]\n",
        "\n",
        "    elif separate_disturbance and separate_degradation and not separate_deforestation:\n",
        "        nodes = [left_axis, degradation_before_name, degradation_since_name, deforestation_total_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0, 0], [1, 2, 3, 4]\n",
        "        values = [-degradation_before, -degradation_since, -deforestation_total, current_agb]\n",
        "        colors = [degradation_before_colour, degradation_since_colour, deforestation_total_colour, remaining_colour]\n",
        "\n",
        "    elif separate_disturbance and not separate_degradation and separate_deforestation:\n",
        "        nodes = [left_axis, degradation_total_name, deforestation_before_name, deforestation_since_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0, 0], [1, 2, 3, 4]\n",
        "        values = [-degradation_total, -deforestation_before, -deforestation_since, current_agb]\n",
        "        colors = [degradation_total_colour, deforestation_before_colour, deforestation_since_colour, remaining_colour]\n",
        "\n",
        "    elif separate_disturbance and not separate_degradation and not separate_deforestation:\n",
        "        nodes = [left_axis, degradation_total_name, deforestation_total_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0], [1, 2, 3]\n",
        "        values = [-degradation_total, -deforestation_total, current_agb]\n",
        "        colors = [degradation_total_colour, deforestation_total_colour, remaining_colour]\n",
        "\n",
        "    elif not separate_disturbance and separate_disturbance_temporal:\n",
        "        nodes = [left_axis, disturbance_before_name, disturbance_since_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0], [1, 2, 3]\n",
        "        values = [-disturbance_before, -disturbance_since, current_agb]\n",
        "        colors = [disturbance_before_colour, disturbance_since_colour, remaining_colour]\n",
        "\n",
        "    else:\n",
        "        nodes = [left_axis, disturbance_total_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0], [1, 2]\n",
        "        values = [-disturbance_total, current_agb]\n",
        "        colors = [disturbance_total_colour, remaining_colour]\n",
        "\n",
        "    node_colors = [remaining_colour] + colors\n",
        "\n",
        "    # Add percentages to node labels\n",
        "    percentages = [(abs(val) / old_growth_agb * 100) for val in values]\n",
        "    for i in range(1, len(nodes)):\n",
        "        if i - 1 < len(percentages):\n",
        "            nodes[i] += f\" ({percentages[i-1]:.0f}%)\"\n",
        "\n",
        "    # Configure title and density annotations\n",
        "    title_and_density = [\n",
        "        dict(x=0, y=1.28, xref='paper', yref='paper', text=title_name, showarrow=False, xanchor='left', align='left',\n",
        "             font=dict(family=\"arial, sans serif\", size=title_font_size, color=\"black\", weight=title_font_weight)),\n",
        "        dict(x=0, y=1.19, xref='paper', yref='paper', text=subtitle_1_name, showarrow=False, xanchor='left', align='left',\n",
        "             font=dict(family=\"arial, sans serif\", size=density_font_size, color=\"black\", weight=density_font_weight)),\n",
        "        dict(x=0, y=1.11, xref='paper', yref='paper', text=subtitle_2_name, showarrow=False, xanchor='left', align='left',\n",
        "             font=dict(family=\"arial, sans serif\", size=density_font_size, color=\"black\", weight=density_font_weight))\n",
        "    ]\n",
        "\n",
        "    if show_title and not show_density:\n",
        "        title_and_density = title_and_density[0:1]\n",
        "    elif not show_title and show_density:\n",
        "        title_and_density = title_and_density[1:3]\n",
        "    elif not show_title and not show_density:\n",
        "        title_and_density = []\n",
        "\n",
        "    # Remove labels if toggled off\n",
        "    if not show_labels:\n",
        "        nodes = [''] * len(nodes)\n",
        "\n",
        "    # Create sankey diagram\n",
        "    fig = go.Figure(data=[go.Sankey(\n",
        "        arrangement=\"freeform\",\n",
        "        node=dict(label=nodes, color=node_colors, pad=15, thickness=20, line=dict(color=\"black\", width=1)),\n",
        "        link=dict(source=sources, target=targets, value=values, color=colors, line=dict(color=\"black\", width=1))\n",
        "    )])\n",
        "\n",
        "    fig.update_layout(\n",
        "        width=700 * width_modifier, height=500,\n",
        "        font=dict(family=\"arial, sans serif\", size=label_font_size, color=\"black\", weight=label_font_weight),\n",
        "        margin=dict(l=25, r=25, t=115, b=25),\n",
        "        annotations=title_and_density\n",
        "    )\n",
        "\n",
        "    # Save labelled versions\n",
        "    fig.write_image(join(sankey_labelled, f'sankey_diagram_{polygon_name}.png'), scale=dpi / 96)\n",
        "    if svg_transparent_background:\n",
        "        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "    fig.write_image(join(sankey_labelled_svg, f'sankey_diagram_vector_{polygon_name}.svg'), scale=dpi / 96)\n",
        "\n",
        "    # Create and save unlabelled versions\n",
        "    fig_unlabelled = go.Figure(data=[go.Sankey(\n",
        "        arrangement=\"freeform\",\n",
        "        node=dict(label=[''] * len(nodes), color=node_colors, pad=15, thickness=20, line=dict(color=\"black\", width=1)),\n",
        "        link=dict(source=sources, target=targets, value=values, color=colors, line=dict(color=\"black\", width=1))\n",
        "    )])\n",
        "\n",
        "    fig_unlabelled.update_layout(\n",
        "        width=700 * width_modifier, height=500,\n",
        "        font=dict(family=\"arial, sans serif\", size=label_font_size, color=\"black\", weight=label_font_weight),\n",
        "        margin=dict(l=25, r=25, t=115, b=25)\n",
        "    )\n",
        "\n",
        "    fig_unlabelled.write_image(join(sankey_unlabelled, f'sankey_diagram_{polygon_name}.png'), scale=dpi / 96)\n",
        "    if svg_transparent_background:\n",
        "        fig_unlabelled.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "    fig_unlabelled.write_image(join(sankey_unlabelled_svg, f'sankey_diagram_vector_{polygon_name}.svg'), scale=dpi / 96)\n",
        "\n",
        "    # Display figure with white background\n",
        "    fig.update_layout(plot_bgcolor='white', paper_bgcolor='white')\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXAGIvpk_KWS"
      },
      "source": [
        "# Disconnected runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgzXDe-Fnm3T"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}