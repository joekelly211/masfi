{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/3_features_lcluc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yk8CnJRCYVQ"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5USlWSaxqv9Y"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOfy6gWFwHEC"
      },
      "outputs": [],
      "source": [
        "# Installs\n",
        "%%capture\n",
        "!pip install astropy\n",
        "!pip install earthengine-api\n",
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTHaRSv8wICv"
      },
      "outputs": [],
      "source": [
        "!# Reload imports, replacing those in the cache\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "import csv\n",
        "import ee\n",
        "import geopandas as gpd\n",
        "import glob\n",
        "from google.colab import runtime, userdata\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from os import makedirs, remove\n",
        "from os.path import exists, join\n",
        "from osgeo import gdal, gdalconst, ogr\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "from scipy.ndimage import label, sum as ndi_sum\n",
        "from shutil import copyfile\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMCq50kg36Br"
      },
      "outputs": [],
      "source": [
        "# 1_areas directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "template_dir = join(areas_dir, \"template.tif\")\n",
        "\n",
        "# 3_features directories\n",
        "features_dir = join(base_dir, \"3_features\")\n",
        "ee_dir = join(features_dir, \"earth_engine\")\n",
        "user_upload_dir = join(features_dir, \"user_upload\")\n",
        "glad_lcluc_dir = join(features_dir, 'glad_lcluc')\n",
        "resampled_dir = join(features_dir, \"resampled\")\n",
        "continuous_final_dir = join(features_dir, \"continuous_final\")\n",
        "binary_dir = join(features_dir, 'binary')\n",
        "edge_effects_dir = join(features_dir, 'binary_edge_effects')\n",
        "\n",
        "# 6_scenarios directories\n",
        "scenario_dir = join(base_dir, \"6_scenarios\")\n",
        "scenario_mask_dir = join(scenario_dir, \"scenario_masks\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(ee_dir, exist_ok=True)\n",
        "makedirs(user_upload_dir, exist_ok=True)\n",
        "makedirs(glad_lcluc_dir, exist_ok=True)\n",
        "makedirs(resampled_dir, exist_ok=True)\n",
        "makedirs(continuous_final_dir, exist_ok=True)\n",
        "makedirs(binary_dir, exist_ok=True)\n",
        "makedirs(edge_effects_dir, exist_ok=True)\n",
        "makedirs(scenario_dir, exist_ok=True)\n",
        "makedirs(scenario_mask_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhkgXF4foXhx"
      },
      "outputs": [],
      "source": [
        "# export_array_as_tif function\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_dir, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)\n",
        "\n",
        "# Burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "    vector = ogr.Open(polygon_path)\n",
        "    layer = vector.GetLayer()\n",
        "    if all_touched: options = [\"ALL_TOUCHED=TRUE\"]\n",
        "    else: options = []\n",
        "    if not fixed: options.append(f\"ATTRIBUTE={column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()}\")\n",
        "    gdal.RasterizeLayer(raster, [1], layer,\n",
        "                        burn_values=[fixed_value] if fixed else None,\n",
        "                        options=options)\n",
        "    raster.FlushCache()\n",
        "    raster = vector = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSuevzw7xyil"
      },
      "source": [
        "# Download Earth Engine rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_wg4IqHpFXT"
      },
      "outputs": [],
      "source": [
        "# Enable Google Earth Engine API at Google Cloud https://console.cloud.google.com/apis/dashboard\n",
        "# See here for walkthrough: https://github.com/googlecolab/colabtools/issues/4228#issuecomment-1859068706\n",
        "# Set project ID under 'secrets' tab on the left with the name 'google_cloud_project'\n",
        "ee_project = userdata.get('google_cloud_project')\n",
        "\n",
        "# Authenticate Earth Engine\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project=ee_project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddVd9lPMbLOL"
      },
      "outputs": [],
      "source": [
        "# Edit this section to change which Earth Engine datasets are downloaded.\n",
        "\n",
        "# Warning: Earth Engine uses 'nearest neighbour' to resample rasters to the desired extent and resolution before exporting.\n",
        "# This creates artifacts if the data is continuous, such as DEMs (elevation) or other topographic metrics.\n",
        "# These should be downloaded from the original source, uploaded to '/user_upload' and resampled in the next section, checking the option for 'bilinear'.\n",
        "\n",
        "# Check datasets in https://code.earthengine.google.com/ with:\n",
        "# var assetList = ee.data.listAssets(\"projects/JRC/TMF/v1_2022/\");\n",
        "# print(assetList);\n",
        "\n",
        "ee_datasets = [\n",
        "\n",
        "    {\n",
        "        \"ee_dataset_name\": \"tmf\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"projects/JRC/TMF/v1_2023/AnnualChanges\",\n",
        "            \"projects/JRC/TMF/v1_2023/AnnualDisruptionObs2023\",\n",
        "            \"projects/JRC/TMF/v1_2023/TransitionMap_MainClasses\",\n",
        "            \"projects/JRC/TMF/v1_2023/TransitionMap_Subtypes\",\n",
        "            \"projects/JRC/TMF/v1_2023/Ndisturb_C2_1982_2022\",\n",
        "        ],\n",
        "    }\n",
        "    # {\n",
        "    #     \"ee_dataset_name\": \"glad\",\n",
        "    #     \"ee_dataset_type\": \"Image\",\n",
        "    #     \"ee_paths\": [\n",
        "    #                 # 'projects/glad/GLCLU2020/Forest_gain',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_disturbance',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_netgain',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_netloss',\n",
        "    #                 # 'projects/glad/GLCLU2020/Forest_loss',\n",
        "    #                 # 'projects/glad/GLCLU2020/Forest_type',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC_2000',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_gain',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_loss',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2020',\n",
        "    #                 # 'projects/glad/GLCLU2020/Water_dynamics',\n",
        "    #                 # 'projects/glad/GLCLU2020/Water_dynamics_classes',\n",
        "    #     ]\n",
        "    # }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIlnNYhQcb10"
      },
      "outputs": [],
      "source": [
        "# Verify Earth Engine rasters that will be downloaded\n",
        "ee_raster_list = []\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "    for ee_path in ee_paths:\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image = ee.ImageCollection(ee_path)\n",
        "            ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        else:\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "        for ee_band in ee_bands:\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_raster_list.append(ee_tif_filename)\n",
        "\n",
        "ee_raster_list = list(reversed(ee_raster_list))\n",
        "ee_raster_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qislgFIYUnqi"
      },
      "outputs": [],
      "source": [
        "# Earth Engine download progress\n",
        "ee_progress_index = 0\n",
        "ee_progress_label = widgets.Label(f\"Earth Engine download progress: {ee_progress_index}/{len(ee_raster_list)}\")\n",
        "display(ee_progress_label)\n",
        "\n",
        "# Load template and set Earth Engine geometry\n",
        "template_polygon_dir = join(polygons_dir, 'template.gpkg')\n",
        "template_area = gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0]\n",
        "template_coords = list(gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0].exterior.coords)\n",
        "ee_geometry = ee.Geometry.Polygon(template_coords)\n",
        "\n",
        "# Download Earth Engine datasets\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "    # Loop through Earth Engine paths\n",
        "    for ee_path in ee_paths:\n",
        "        # identify bands\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image = ee.ImageCollection(ee_path)\n",
        "            ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "        # Loop through bands\n",
        "        for ee_band in reversed(ee_bands):\n",
        "            # Set filename and directory of downloaded raster and check if exists\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_tif_dir = join(ee_dir, ee_tif_filename)\n",
        "            ee_temp_dir = join(\"/gdrive/MyDrive\", ee_tif_filename)\n",
        "            # Different temporary directory required if MyDrive is mounted\n",
        "            if base_dir.startswith('/content/drive/MyDrive/'):\n",
        "              ee_temp_dir = join(\"/content/drive/MyDrive/\", ee_tif_filename)\n",
        "            # Check if temporary raster exists and needs copying\n",
        "            if exists(ee_temp_dir):\n",
        "              copyfile(ee_temp_dir, ee_tif_dir)\n",
        "              remove(ee_temp_dir)\n",
        "            # Check if copied raster exists, and if not download from Earth Engine.\n",
        "            if not exists(ee_tif_dir):\n",
        "              if ee_dataset_type == 'ImageCollection':\n",
        "                image_selected = ee_image.qualityMosaic(ee_band).select([ee_band])\n",
        "                resolution = ee_image.first().projection().nominalScale().getInfo()\n",
        "              if ee_dataset_type == 'Image':\n",
        "                image_selected = ee_image.select([ee_band])\n",
        "                resolution = ee_image.select(0).projection().nominalScale().getInfo()\n",
        "              ee_task = ee.batch.Export.image.toDrive(image=image_selected.toFloat(),\n",
        "                                                    description=ee_tif_filename[:-4],\n",
        "                                                    scale=resolution,\n",
        "                                                    region=ee_geometry,\n",
        "                                                    maxPixels=10000000000,\n",
        "                                                    fileNamePrefix=ee_tif_filename[:-4],\n",
        "                                                    crs='EPSG:4326',\n",
        "                                                    fileFormat='GeoTIFF')\n",
        "              ee_task.start()\n",
        "              # Check whether the raster has downloaded yet\n",
        "              while not exists(ee_temp_dir):\n",
        "                  ee_task_status = ee_task.status()\n",
        "                  # If the task is completed, continue\n",
        "                  if ee_task_status[\"state\"] == 'COMPLETED': break\n",
        "                  # If it has failed or been cancelled, show an error\n",
        "                  elif ee_task_status['state'] == 'FAILED' or ee_task_status['state'] == 'CANCELLED':\n",
        "                      print(f\"{ee_tif_filename}:{ee_task_status['error_message']}\")\n",
        "                      try: remove(ee_temp_dir)\n",
        "                      except: pass\n",
        "                      break\n",
        "                  sleep(1)\n",
        "              # Copy the raster to intended directory and remove the temporary raster\n",
        "              while not exists(ee_temp_dir):\n",
        "                sleep(1)\n",
        "              copyfile(ee_temp_dir, ee_tif_dir)\n",
        "              remove(ee_temp_dir)\n",
        "            # Update Earth Engine download progress\n",
        "            ee_progress_index += 1\n",
        "            ee_progress_label.value = f\"Earth Engine download progress: {ee_progress_index}/{len(ee_raster_list)}\"\n",
        "\n",
        "# Check Earth Engine tasks here: https://code.earthengine.google.com/tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIBbo-gsS3nD"
      },
      "source": [
        "# GLAD LCLUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W90YInS-S5I5"
      },
      "outputs": [],
      "source": [
        "# GLAD data can be used in-place of TMF data for testing non-TMF areas.\n",
        "# LCLUC contains several land cover and land use types, each with continuous metrics.\n",
        "# This splits them into categories for better modelling, based on the legend:\n",
        "# https://glad.umd.edu/sites/default/files/legend_0.xlsx\n",
        "# Should do before resampling.\n",
        "\n",
        "lcluc_dict = {\n",
        "    'terra_vegetation_cover_percent': (0, 24),\n",
        "    'terra_stable_tree_m': (25, 48),\n",
        "    'wetland_vegetation_cover_percent': (100, 124),\n",
        "    'wetland_stable_tree_m': (125, 148),\n",
        "    'open_surface_water_percent_of_year': (200, 207),\n",
        "    'snow_ice': (241, 241),\n",
        "    'cropland': (244, 244),\n",
        "    'built_up': (250, 250),\n",
        "    'ocean': (254, 254),\n",
        "}\n",
        "\n",
        "lcluc_exists = False\n",
        "for lcluc_raster in os.listdir(ee_dir):\n",
        "  if 'LCLUC' in lcluc_raster:\n",
        "    lcluc_exists = True\n",
        "    lcluc_path = join(ee_dir, lcluc_raster)\n",
        "    luluc_array = gdal.Open(lcluc_path).ReadAsArray()\n",
        "    for key, (lower, upper) in lcluc_dict.items():\n",
        "        split_luluc_filename = f\"{lcluc_raster[:-4]}_{key}.tif\"\n",
        "        split_luluc_filename_binary = f\"{lcluc_raster[:-4]}_{key}_binary.tif\"\n",
        "        split_luluc_dir = join(glad_lcluc_dir, split_luluc_filename)\n",
        "        split_luluc_dir_binary = join(glad_lcluc_dir, split_luluc_filename_binary)\n",
        "        if not exists(split_luluc_dir) and not exists(split_luluc_dir_binary):\n",
        "          split_luluc_mask = np.logical_and(luluc_array >= lower, luluc_array <= upper)\n",
        "          split_luluc_array = np.where(split_luluc_mask, luluc_array, 0) # outside the range set to 0\n",
        "          non_zero_percentage = np.count_nonzero(split_luluc_array) / split_luluc_array.size * 100\n",
        "          if non_zero_percentage >= 0.1:\n",
        "            # Check if there's only one unique non-zero value, and convert to a 1-0 binary raster if true\n",
        "            unique_non_zero_values = np.unique(split_luluc_array[split_luluc_array > 0])\n",
        "            if len(unique_non_zero_values) == 1:\n",
        "                split_luluc_array = np.where(split_luluc_array > 0, 1, 0)\n",
        "                split_luluc_dir = split_luluc_dir_binary\n",
        "            export_array_as_tif(split_luluc_array, split_luluc_dir, template=lcluc_path)\n",
        "            print(f\"{lcluc_raster} raster has been processed\")\n",
        "\n",
        "if not lcluc_exists: print(\"There are no GLAD LCLUC rasters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-UOVsvRuHNS"
      },
      "source": [
        "# Resample EE rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5yEcU40E8eD"
      },
      "outputs": [],
      "source": [
        "# Create dictionary of all tifs in Earth Engine and user upload directory\n",
        "resample_dict = {}\n",
        "for resample_raster in os.listdir(ee_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'categorical'\"})\n",
        "for resample_raster in os.listdir(user_upload_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'categorical'\"})\n",
        "for resample_raster in os.listdir(glad_lcluc_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'continuous'\"})\n",
        "resample_dict = {key: value for key, value in sorted(resample_dict.items())}\n",
        "\n",
        "# Select rasters for resampling and verify data type (categorical or continuous)\n",
        "print(\"selected_original_rasters = {\")\n",
        "for key, value in resample_dict.items():\n",
        "    print(f'\"{key}\": {value},')\n",
        "print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfxPFqlxE0Ok"
      },
      "outputs": [],
      "source": [
        "selected_original_rasters = {\n",
        "\"tmf_AnnualChanges_Dec1990.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1991.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1992.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1993.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1994.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1995.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1996.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1997.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1998.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1999.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2000.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2001.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2002.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2003.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2004.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2005.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2006.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2007.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2008.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2009.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2010.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2011.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2012.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2013.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2014.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2015.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2016.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2017.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2018.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2019.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2020.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2021.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2022.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2023.tif\": 'categorical',\n",
        "\"tmf_AnnualDisruptionObs2023_y2023.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1982.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1983.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1984.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1985.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1986.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1987.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1988.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1989.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1990.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1991.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1992.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1993.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1994.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1995.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1996.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1997.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1998.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1999.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2000.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2001.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2002.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2003.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2004.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2005.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2006.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2007.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2008.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2009.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2010.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2011.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2012.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2013.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2014.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2015.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2016.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2017.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2018.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2019.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2020.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2021.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2022.tif\": 'categorical',\n",
        "\"tmf_TransitionMap_MainClasses_TransitionMap_MainClasses.tif\": 'categorical',\n",
        "\"tmf_TransitionMap_Subtypes_TransitionMap_Subtypes.tif\": 'categorical',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEZwd6XYXqRk"
      },
      "outputs": [],
      "source": [
        "# Set resample algorithms for different raster types\n",
        "# See https://gdal.org/programs/gdalwarp.html\n",
        "categorical_alg = 'near'\n",
        "continuous_alg = 'bilinear'\n",
        "\n",
        "template = gdal.Open(template_dir)\n",
        "template_dimensions = template.GetGeoTransform()\n",
        "xres, yres = template_dimensions[1], -template_dimensions[5]\n",
        "xmin = template_dimensions[0]\n",
        "ymin = template_dimensions[3] - template.RasterYSize * yres\n",
        "xmax = xmin + template.RasterXSize * xres\n",
        "ymax = template_dimensions[3]\n",
        "\n",
        "# Resample progress\n",
        "resample_progress_index = 0\n",
        "resample_progress_label = widgets.Label(f\"Resample progress: {resample_progress_index}/{len(selected_original_rasters.items())}\")\n",
        "display(resample_progress_label)\n",
        "\n",
        "# Iterate over selected rasters\n",
        "for original_raster_name, data_type in selected_original_rasters.items():\n",
        "  resampled_raster_dir = join(resampled_dir, original_raster_name)\n",
        "  if not exists(resampled_raster_dir):\n",
        "    original_raster_dir = join(ee_dir, original_raster_name)\n",
        "    if not exists(original_raster_dir): original_raster_dir = join(user_upload_dir, original_raster_name)\n",
        "    if not exists(original_raster_dir): original_raster_dir = join(glad_lcluc_dir, original_raster_name)\n",
        "    # Set resample type\n",
        "    if data_type == 'categorical': resample_alg = categorical_alg\n",
        "    if data_type == 'continuous': resample_alg = continuous_alg\n",
        "    src = gdal.Warp(\n",
        "        resampled_raster_dir,\n",
        "        original_raster_dir,\n",
        "        xRes=xres, yRes=yres,\n",
        "        outputBounds=(xmin, ymin, xmax, ymax),\n",
        "        resampleAlg=resample_alg,\n",
        "        outputType=gdalconst.GDT_Float32)\n",
        "    # Compress and close\n",
        "    driver = gdal.GetDriverByName(\"GTiff\")\n",
        "    src = driver.CreateCopy(resampled_raster_dir, src, 0, options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "    src = None\n",
        "  # Update resample progress\n",
        "  resample_progress_index += 1\n",
        "  resample_progress_label.value = f\"Resample progress: {resample_progress_index}/{len(selected_original_rasters.items())}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D35niESIwR9"
      },
      "outputs": [],
      "source": [
        "# Determine continous feature precision\n",
        "\n",
        "override_max_unique_values = False\n",
        "max_unique_values = 5000 # Should be >=10\n",
        "\n",
        "if override_max_unique_values == False:\n",
        "  dem_base_path = join(areas_dir, \"base_dem.tif\")\n",
        "  dem_base_array = gdal.Open(dem_base_path).ReadAsArray()\n",
        "  max_unique_values = int(np.ptp(dem_base_array)) # Precision based on elevation variance\n",
        "resampled_precision_dict = {}\n",
        "\n",
        "for resampled_feature, resample_type in selected_original_rasters.items():\n",
        "  if resample_type == 'continuous':\n",
        "    resampled_feature_path = join(resampled_dir, resampled_feature)\n",
        "    print(f\"Reading {resampled_feature}...\")\n",
        "    # Read raster as array\n",
        "    resampled_feature_array = gdal.Open(resampled_feature_path).ReadAsArray()\n",
        "    # Convert 'nodata' values to nan\n",
        "    resampled_feature_array[resampled_feature_array == nodatavalue] = np.nan\n",
        "    resampled_feature_array_masked = np.ma.array(resampled_feature_array, mask=np.isnan(resampled_feature_array))\n",
        "    # Count unique values in raster\n",
        "    unique_values = len(np.unique(resampled_feature_array_masked))\n",
        "    print(f\"There are {unique_values} unique values in {resampled_feature}\")\n",
        "    # Generate histogram from 100,000 random points\n",
        "    random_selection = np.random.choice(resampled_feature_array_masked.ravel(), size = 100_000, replace = False)\n",
        "    _ = plt.hist(random_selection, bins='auto')  # arguments are passed to np.histogram\n",
        "    plt.title(f\"{resampled_feature}\")\n",
        "    plt.show()\n",
        "    # Remove 0 values for log10\n",
        "    resampled_feature_array_masked[resampled_feature_array_masked == 0] = np.nan\n",
        "    resampled_feature_array_masked = np.ma.array(resampled_feature_array, mask=np.isnan(resampled_feature_array))\n",
        "    # Create log10 array for determining positions for rounding\n",
        "    array_log10 = np.log10(abs(resampled_feature_array_masked))\n",
        "    place_value_decimal = int(abs(np.min(array_log10)))\n",
        "    place_value_integer = int(0 - np.max(array_log10))\n",
        "    # Iterate down precision levels to determine optimal number of unique values\n",
        "    min_starting_precision = len(str(max_unique_values))\n",
        "    for precision in reversed(range(place_value_integer, max(min_starting_precision, place_value_decimal +1))):\n",
        "      rounded_array = np.round(resampled_feature_array, decimals=precision)\n",
        "      round_unique_values = len(np.unique(rounded_array))\n",
        "      optimal_precision = None\n",
        "      if round_unique_values <= max_unique_values:\n",
        "        optimal_precision = precision\n",
        "        print(f\"The optimal precison for {resampled_feature} is {optimal_precision}, with {round_unique_values} unique values.\")\n",
        "        resampled_precision_dict.update({f'{resampled_feature}':f'{optimal_precision}'})\n",
        "        break\n",
        "    if optimal_precision == None: print(\"There's a problem with setting precision.\")\n",
        "    print(\"___________________\\n\")\n",
        "\n",
        "print(\"Dictionary for optimal rounding values:\")\n",
        "resampled_precision_dict\n",
        "\n",
        "precision_dict_csv_path = join(resampled_dir, 'rounding_dictionary.csv')\n",
        "# Save rounding dictionary to CSV\n",
        "with open(precision_dict_csv_path, 'w', newline='') as precision_dict_csv:\n",
        "    writer = csv.writer(precision_dict_csv)\n",
        "    writer.writerow(resampled_precision_dict.keys())\n",
        "    writer.writerow(resampled_precision_dict.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UjcjQ52NoT7"
      },
      "outputs": [],
      "source": [
        "# Open rounding dictionary and verify\n",
        "with open(precision_dict_csv_path, 'r') as file:\n",
        "    keys, values = list(csv.reader(file))\n",
        "    topo_precision_dict = dict(zip(keys, values))\n",
        "\n",
        "# Verify precision and correct if necessary\n",
        "print(\"topo_precision_dict = {\")\n",
        "for key, value in topo_precision_dict.items():\n",
        "    print(f'\"{key}\": {value},')\n",
        "print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2kquWfYOeS-"
      },
      "outputs": [],
      "source": [
        "topo_precision_dict = {\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn-nfKEPOi9d"
      },
      "outputs": [],
      "source": [
        "# Set smoothing kernel\n",
        "kernel = Gaussian2DKernel(x_stddev=1, y_stddev=1)\n",
        "\n",
        "# Continuous progress\n",
        "continuous_progress_index = 0\n",
        "continuous_progress_label = widgets.Label(f\"Continuous progress: {continuous_progress_index}/{len(topo_precision_dict.items())}\")\n",
        "display(continuous_progress_label)\n",
        "\n",
        "# Iterate over selected continuous rasters\n",
        "for continuous, precision in topo_precision_dict.items():\n",
        "  cont_raster_resampled_path = join(resampled_dir, continuous)\n",
        "  cont_raster_resampled_array = gdal.Open(cont_raster_resampled_path).ReadAsArray()\n",
        "  # Convert nodata values to 0\n",
        "  cont_raster_resampled_array[cont_raster_resampled_array == nodatavalue] = 0\n",
        "  # Set path and check if exists\n",
        "  cont_raster_unsmoothed_filename = f\"{continuous[:-4]}_unsmooth.tif\"\n",
        "  cont_raster_unsmoothed_path = join(continuous_final_dir, cont_raster_unsmoothed_filename)\n",
        "  if not exists(cont_raster_unsmoothed_path):\n",
        "    # Round and export unsmoothed continuous raster\n",
        "    cont_raster_unsmoothed_rounded = np.round(cont_raster_resampled_array, decimals=int(precision))\n",
        "    export_array_as_tif(cont_raster_unsmoothed_rounded, cont_raster_unsmoothed_path)\n",
        "  # Smooth using 2D spatial convolution\n",
        "  cont_raster_smoothed_filename = f\"{continuous[:-4]}_smooth.tif\"\n",
        "  cont_raster_smoothed_path = join(continuous_final_dir, cont_raster_smoothed_filename)\n",
        "  if not exists(cont_raster_smoothed_path):\n",
        "    cont_raster_smoothed = convolve(cont_raster_resampled_array, kernel, boundary='extend')\n",
        "    # Round and export smoothed continuous raster\n",
        "    cont_raster_smoothed_rounded = np.round(cont_raster_smoothed, decimals=int(precision))\n",
        "    export_array_as_tif(cont_raster_smoothed_rounded, cont_raster_smoothed_path)\n",
        "  # Update continuous progress\n",
        "  continuous_progress_index += 1\n",
        "  continuous_progress_label.value = f\"Continuous progress: {continuous_progress_index}/{len(topo_precision_dict.items())}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhukvPP02gBm"
      },
      "source": [
        "# TMF binary features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdGYp7HAXkD4"
      },
      "outputs": [],
      "source": [
        "# Check TMF data users guide for classification. https://forobs.jrc.ec.europa.eu/static/tmf/TMF_DataUsersGuide.pdf\n",
        "\n",
        "cell_size_x = gdal.Open(join(areas_dir, 'cell_size_x.tif')).ReadAsArray()\n",
        "cell_size_y = gdal.Open(join(areas_dir, 'cell_size_y.tif')).ReadAsArray()\n",
        "cell_size_ha = np.mean(cell_size_x) * np.mean(cell_size_y) / 10_000\n",
        "sieve_size = int(np.ceil(0.5/cell_size_ha)) # Removes all forest patches smaller than 0.5 ha\n",
        "print(f\"Forest binary sieve size (>0.5 ha) is {sieve_size} pixels.\")\n",
        "\n",
        "# Generate list of valid TMF rasters to convert to binary\n",
        "binary_list = []\n",
        "for resampled_raster in os.listdir(resampled_dir):\n",
        "  # Verify these are in the filenames\n",
        "  if 'DisruptionObs' in resampled_raster or 'AnnualChanges' in resampled_raster or 'Ndisturb' in resampled_raster:\n",
        "    # Verify this is the position of the year in the filename\n",
        "    year = resampled_raster[-8:-4]  # Data prior to 1990 is poor\n",
        "    if int(year) >= 1990: binary_list.append(resampled_raster)\n",
        "\n",
        "# Binary progress\n",
        "binary_progress_index = 0\n",
        "binary_progress_label = widgets.Label(f\"Binary progress: {binary_progress_index}/{len(binary_list)}\")\n",
        "display(binary_progress_label)\n",
        "\n",
        "for resampled_raster in binary_list:\n",
        "  year = resampled_raster[-8:-4]\n",
        "  # Forest binary\n",
        "  if 'AnnualChanges' in resampled_raster:\n",
        "    forest_binary_path = join(binary_dir, f\"forest_binary_{year}.tif\")\n",
        "    if not exists(forest_binary_path):\n",
        "      ac_raster_path = join(resampled_dir, resampled_raster)\n",
        "      ac_array = gdal.Open(ac_raster_path).ReadAsArray()\n",
        "      # Set classes 1 & 2 as 1, all else as 0\n",
        "      forest_binary_array = np.where((ac_array == 1) | (ac_array == 2), 1, 0)\n",
        "\n",
        "      # Sieve to 0.5 ha, using 8-connectedness (3, 3)\n",
        "      fb_array_labelled, fb_array_features = label(forest_binary_array, structure=np.ones((3, 3)))\n",
        "      # Determine the size of each patch\n",
        "      fb_array_sizes = ndi_sum(forest_binary_array, fb_array_labelled, range(fb_array_features + 1))\n",
        "      # Create a mask to remove patches smaller than the threshold\n",
        "      fb_array_mask_sizes = fb_array_sizes >= sieve_size\n",
        "      fb_array_mask_sizes[0] = 0 # Ensure non-forest (0) is excluded\n",
        "      fb_array_mask = fb_array_mask_sizes[fb_array_labelled]\n",
        "      # Apply the mask to the forest binary array and export\n",
        "      fb_array_sieved = forest_binary_array * fb_array_mask\n",
        "      export_array_as_tif(fb_array_sieved, forest_binary_path)\n",
        "\n",
        "  # Disturbance binary\n",
        "  if 'DisruptionObs' in resampled_raster or 'Ndisturb' in resampled_raster:\n",
        "    disturbance_binary_path = join(binary_dir, f\"disturbance_binary_{year}.tif\")\n",
        "    if not exists(disturbance_binary_path):\n",
        "      ac_raster_path = glob.glob(f\"{resampled_dir}/*AnnualChanges*{year}*\")\n",
        "      ac_array = gdal.Open(ac_raster_path[0]).ReadAsArray()\n",
        "      do_raster_path = join(resampled_dir, resampled_raster)\n",
        "      do_array = gdal.Open(do_raster_path).ReadAsArray()\n",
        "      # Set all disruption events to '1' if they're not classed as undisturbed forest or water in AnnualChanges\n",
        "      disturbance_binary_array = np.where((do_array >= 1) & ((ac_array != 1) & (ac_array != 5)), 1, 0)\n",
        "      export_array_as_tif(disturbance_binary_array, disturbance_binary_path)\n",
        "\n",
        "  # Update binary progress\n",
        "  binary_progress_index += 1\n",
        "  binary_progress_label.value = f\"Binary progress: {binary_progress_index}/{len(binary_list)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHmFlgGylGFL"
      },
      "outputs": [],
      "source": [
        "# Extract mangrove binary (optional)\n",
        "extract_mangrove_binary = True\n",
        "\n",
        "if extract_mangrove_binary:\n",
        "  mangrove_binary_path = join(binary_dir, \"mangrove_binary.tif\")\n",
        "  if not exists(mangrove_binary_path):\n",
        "    # Open subtypes array\n",
        "    for resampled_raster in os.listdir(resampled_dir):\n",
        "      if 'Subtypes' in resampled_raster:\n",
        "        subtypes_raster_path = join(resampled_dir, resampled_raster)\n",
        "        subtypes_raster_array = gdal.Open(subtypes_raster_path).ReadAsArray()\n",
        "\n",
        "    # Open oldest available forest binary raster for full mangrove extent (1990)\n",
        "    forest_binary_1990_path = join(binary_dir, \"forest_binary_1990.tif\")\n",
        "    forest_binary_1990_array = gdal.Open(forest_binary_1990_path).ReadAsArray()\n",
        "\n",
        "    mangrove_binary_array = np.logical_and(forest_binary_1990_array == 1,\n",
        "        np.logical_or(subtypes_raster_array == 12,(subtypes_raster_array >= 61) & (subtypes_raster_array <= 69))\n",
        "    )\n",
        "\n",
        "    # Calculate the percentage of forest pixels that are mangrove\n",
        "    forest_1990_pixels = np.sum(forest_binary_1990_array)\n",
        "    mangrove_pixels = np.sum(mangrove_binary_array)\n",
        "    if mangrove_pixels > 0:\n",
        "      mangrove_percent = (mangrove_pixels / forest_1990_pixels) * 100\n",
        "      export_array_as_tif(mangrove_binary_array, mangrove_binary_path)\n",
        "      print(f\"Number of mangrove pixels: {mangrove_pixels}\")\n",
        "      print(f\"Percentage of 1990 forest pixels that are mangrove: {mangrove_percent:.2f}%\")\n",
        "    else: print(\"There are no mangrove pixels in the template area.\")\n",
        "  else: print(\"A mangrove binary raster already exists. Delete it to generate a new one.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJnV0l75FpPN"
      },
      "source": [
        "# LU polygon binary features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vigY_9XqXhCQ"
      },
      "outputs": [],
      "source": [
        "# Selected 'land use' polygons.\n",
        "# Creating a 'complete recovery' or 'complete restoration' scenario requires ONE of these as a proxy.\n",
        "# This can be multiple combined PAs / polygons that have no or minimal history of human disturbance.\n",
        "\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area.gpkg', 'project_area_buffered_bbox.gpkg', 'gedi_area.gpkg', 'project_area_inverse.gpkg', 'gedi_area_inverse.gpkg']\n",
        "print(\"lu_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    print(f\"'{polygon}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w62_C-Vt0Emj"
      },
      "outputs": [],
      "source": [
        "lu_polygons = [\n",
        "# 'peninsular_malaysia.gpkg',\n",
        "'lu_oldgrowth.gpkg',\n",
        "]\n",
        "\n",
        "# Convert all template values to 'nodata' in preparation\n",
        "template_tif = gdal.Open(template_dir)\n",
        "template_mask_array = gdal.Open(template_dir).ReadAsArray()\n",
        "template_mask_array[template_mask_array != None] = 0\n",
        "\n",
        "for lu_polygon in lu_polygons:\n",
        "  lu_binary_name = f\"{lu_polygon[:-5]}_binary.tif\"\n",
        "  lu_binary_path = join(binary_dir, lu_binary_name)\n",
        "  if not exists(lu_binary_path):\n",
        "    lu_polygon_path = join(polygons_dir, lu_polygon)\n",
        "    export_array_as_tif(template_mask_array, lu_binary_path)\n",
        "    # Burn the value '1' where it overlaps with the project area polygon\n",
        "    burn_polygon_to_raster(lu_binary_path, lu_polygon_path, fixed=True, fixed_value=1, all_touched=False)\n",
        "    print(f\"{lu_binary_name} has been created.\")\n",
        "  else: print(f\"{lu_binary_name} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKSLJnR4SE5H"
      },
      "source": [
        "# Binary masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3rRcG0_ZOdd"
      },
      "outputs": [],
      "source": [
        "# Generate masks for later scenario predictions, e.g. so outputs only show forest.\n",
        "mask_type_list = []\n",
        "for binary in os.listdir(binary_dir):\n",
        "    mask_type = binary.split('_')[0]\n",
        "    if mask_type not in mask_type_list:\n",
        "        mask_type_list.append(mask_type)\n",
        "\n",
        "print(\"mask_types = [\")\n",
        "for mask_type in mask_type_list:\n",
        "    print(f\"'{mask_type}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F0klPciSEHh"
      },
      "outputs": [],
      "source": [
        "mask_types = [\n",
        "'forest',\n",
        "# 'lu',\n",
        "# 'disturbance',\n",
        "]\n",
        "\n",
        "# Create list of binary rasters to mask\n",
        "binary_mask_list = []\n",
        "for mask_type in mask_types:\n",
        "  for binary in os.listdir(binary_dir):\n",
        "    if mask_type in binary:\n",
        "      binary_mask_list.append(binary)\n",
        "\n",
        "# Binary progress\n",
        "mask_progress_index = 0\n",
        "mask_progress_label = widgets.Label(f\"Binary progress: {mask_progress_index}/{len(binary_mask_list)}\")\n",
        "display(mask_progress_label)\n",
        "\n",
        "# Create masks from the selected binary raster type\n",
        "for mask_type in mask_types:\n",
        "  for binary in binary_mask_list:\n",
        "    binary_path = join(binary_dir, binary)\n",
        "    try: year = str(int(binary[-8:-4])) # Check for year\n",
        "    except: year = None\n",
        "    mask_raster_path = join(scenario_mask_dir, f\"mask_{mask_type}_{year}.tif\")\n",
        "    if not exists(mask_raster_path):\n",
        "        binary_raster = gdal.Open(binary_path)\n",
        "        binary_array = gdal.Open(binary_path).ReadAsArray()\n",
        "        mask_array = np.where(binary_array == 0, nodatavalue, 1)\n",
        "        export_array_as_tif(mask_array, mask_raster_path)\n",
        "        print(f\"A mask raster has been created: {mask_raster_path}\")\n",
        "    else: print(f\"A mask raster already exists at: {mask_raster_path}\")\n",
        "    # Update mask progress\n",
        "    mask_progress_index += 1\n",
        "    mask_progress_label.value = f\"Binary progress: {mask_progress_index}/{len(binary_mask_list)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCv0mNkb2pIY"
      },
      "source": [
        "# Binary feature edge effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqkninr4qy-g"
      },
      "outputs": [],
      "source": [
        "# Set smoothing kernel\n",
        "kernel = Gaussian2DKernel(x_stddev=3, y_stddev=3)\n",
        "# Set precision\n",
        "precision = 2\n",
        "\n",
        "binary_list = []\n",
        "for binary_raster in os.listdir(binary_dir) + os.listdir(resampled_dir):\n",
        "  if \"binary\" in binary_raster:\n",
        "    binary_list.append(binary_raster)\n",
        "\n",
        "# Edge effect progress\n",
        "edge_effect_progress_index = 0\n",
        "edge_effect_progress_label = widgets.Label(f\"Edge effect progress: {edge_effect_progress_index}/{len(binary_list)}\")\n",
        "display(edge_effect_progress_label)\n",
        "\n",
        "for binary_raster in binary_list:\n",
        "  if \"binary\" in binary_raster:\n",
        "    edge_effects_filename = binary_raster.replace('binary', 'with_edge_effects')\n",
        "    edge_effects_path = join(edge_effects_dir, edge_effects_filename)\n",
        "    if not exists(edge_effects_path):\n",
        "      binary_raster_path = join(binary_dir, binary_raster)\n",
        "      if not exists(binary_raster_path): binary_raster_path = join(resampled_dir, binary_raster)\n",
        "      binary_array = gdal.Open(binary_raster_path).ReadAsArray()\n",
        "      # Reclassify for binary differentiation after proximity conversion\n",
        "      differentiator_array = binary_array.copy()\n",
        "      differentiator_array[differentiator_array == 1] = 10\n",
        "      # Positive proximity\n",
        "      positive_distances = distance_transform_edt(binary_array == 0) # target pixels\n",
        "      positive_proximity_array = np.where(positive_distances > 2, 0, positive_distances) # max distance 2\n",
        "      # Negative proximity\n",
        "      negative_distances = distance_transform_edt(binary_array == 1) # target pixels\n",
        "      negative_proximity_array = np.where(negative_distances > 2, 0, negative_distances) # max distance 2\n",
        "      # Sum proximities and differentiator\n",
        "      pixel_prox_summed =  differentiator_array + positive_proximity_array + negative_proximity_array\n",
        "      # Reclassify for better semantic understanding of pixel proximity\n",
        "      pixel_prox_reclassed = pixel_prox_summed.copy()\n",
        "      pixel_prox_reclass_table = [(0, 0, -4), (1, 1, -1), (1.4, 1.5, -2), (2, 2, -3), (10, 10, 3), (11, 11, 0), (11.4, 11.5, 1), (12, 12, 2)]\n",
        "      for min_value, max_value, new_value in pixel_prox_reclass_table:\n",
        "        pixel_prox_reclassed[(pixel_prox_reclassed >= min_value) & (pixel_prox_reclassed <= max_value)] = new_value\n",
        "      # Smooth binary array using 2D convolution\n",
        "      binary_smoothed = convolve(binary_array, kernel, boundary='extend')\n",
        "      # Sum pixel proximity and smoothed binary array\n",
        "      edge_effects_array = np.round(pixel_prox_reclassed + binary_smoothed, precision)\n",
        "      # Export edge effects features\n",
        "      export_array_as_tif(edge_effects_array, edge_effects_path)\n",
        "\n",
        "  # Update binary progress\n",
        "  edge_effect_progress_index += 1\n",
        "  edge_effect_progress_label.value = f\"Edge effect progress: {edge_effect_progress_index}/{len(binary_list)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6r7JXbijM50"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU_AC6MjNfTN"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "-yk8CnJRCYVQ",
        "kSuevzw7xyil",
        "5-UOVsvRuHNS",
        "EhukvPP02gBm",
        "yKSLJnR4SE5H",
        "GCv0mNkb2pIY"
      ],
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}