{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/3_features_lcluc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yk8CnJRCYVQ"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5USlWSaxqv9Y"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOfy6gWFwHEC"
      },
      "outputs": [],
      "source": [
        "# Installs\n",
        "%%capture\n",
        "!pip install astropy\n",
        "!pip install earthengine-api\n",
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTHaRSv8wICv"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "import concurrent.futures\n",
        "import csv\n",
        "import ee\n",
        "import geopandas as gpd\n",
        "import glob\n",
        "from google.colab import runtime, userdata\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from os import makedirs, remove\n",
        "from os.path import exists, join\n",
        "from osgeo import gdal, gdalconst, ogr\n",
        "gdal.UseExceptions()\n",
        "import re\n",
        "import requests\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "from scipy.ndimage import label, sum as ndi_sum\n",
        "from shutil import copyfile, move, rmtree\n",
        "import threading\n",
        "from time import sleep\n",
        "from urllib.request import urlretrieve\n",
        "import uuid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMCq50kg36Br"
      },
      "outputs": [],
      "source": [
        "# 1_areas directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "template_dir = join(areas_dir, \"template.tif\")\n",
        "\n",
        "# 3_features directories\n",
        "features_dir = join(base_dir, \"3_features\")\n",
        "ee_dir = join(features_dir, \"earth_engine\")\n",
        "user_upload_dir = join(features_dir, \"user_upload\")\n",
        "alpha_earth_dir = join(features_dir, \"alpha_earth\")\n",
        "glad_lcluc_dir = join(features_dir, 'glad_lcluc')\n",
        "resampled_dir = join(features_dir, \"resampled\")\n",
        "continuous_final_dir = join(features_dir, \"continuous_final\")\n",
        "binary_dir = join(features_dir, 'binary')\n",
        "edge_effects_dir = join(features_dir, 'binary_edge_effects')\n",
        "\n",
        "# 6_scenarios directories\n",
        "scenario_dir = join(base_dir, \"6_scenarios\")\n",
        "scenario_mask_dir = join(scenario_dir, \"scenario_masks\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(ee_dir, exist_ok=True)\n",
        "makedirs(user_upload_dir, exist_ok=True)\n",
        "makedirs(alpha_earth_dir, exist_ok=True)\n",
        "makedirs(glad_lcluc_dir, exist_ok=True)\n",
        "makedirs(resampled_dir, exist_ok=True)\n",
        "makedirs(continuous_final_dir, exist_ok=True)\n",
        "makedirs(binary_dir, exist_ok=True)\n",
        "makedirs(edge_effects_dir, exist_ok=True)\n",
        "makedirs(scenario_dir, exist_ok=True)\n",
        "makedirs(scenario_mask_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhkgXF4foXhx"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = [\"COMPRESS=DEFLATE\", \"PREDICTOR=3\", \"ZLEVEL=9\"]\n",
        "    else: options = []\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = vector = None\n",
        "    try:\n",
        "        raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "        vector = ogr.Open(polygon_path)\n",
        "        if not raster or not vector:\n",
        "            raise ValueError(\"Cannot open input files\")\n",
        "        layer = vector.GetLayer()\n",
        "        options = [\"ALL_TOUCHED=TRUE\"] if all_touched else []\n",
        "        if fixed:\n",
        "            gdal.RasterizeLayer(raster, [1], layer, burn_values=[fixed_value], options=options)\n",
        "        else:\n",
        "            attr_name = column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()\n",
        "            options.append(f\"ATTRIBUTE={attr_name}\")\n",
        "            gdal.RasterizeLayer(raster, [1], layer, options=options)\n",
        "    finally:\n",
        "        if raster: raster.FlushCache()\n",
        "        raster = vector = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSuevzw7xyil"
      },
      "source": [
        "# Download Earth Engine rasters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define datasets"
      ],
      "metadata": {
        "id": "m5Y3xYz-Va9p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_wg4IqHpFXT"
      },
      "outputs": [],
      "source": [
        "# Enable Google Earth Engine API at Google Cloud https://console.cloud.google.com/apis/dashboard\n",
        "# See here for walkthrough: https://github.com/googlecolab/colabtools/issues/4228#issuecomment-1859068706\n",
        "# Set project ID under 'secrets' tab on the left with the name 'google_cloud_project'\n",
        "ee_project = userdata.get('google_cloud_project')\n",
        "\n",
        "# Authenticate Earth Engine\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project=ee_project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddVd9lPMbLOL"
      },
      "outputs": [],
      "source": [
        "# Edit this section to change which Earth Engine datasets are downloaded.\n",
        "# Do not modify the Alpha Earth entries for them to work in the download scripts\n",
        "\n",
        "# Check datasets in https://code.earthengine.google.com/ with:\n",
        "# var assetList = ee.data.listAssets(\"projects/JRC/TMF/v1_2022/\");\n",
        "# print(assetList);\n",
        "\n",
        "ee_datasets = [\n",
        "\n",
        "    {\n",
        "        \"ee_dataset_name\": \"tmf\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"projects/JRC/TMF/v1_2024/AnnualChanges\",\n",
        "            \"projects/JRC/TMF/v1_2024/TransitionMap_MainClasses\",\n",
        "            \"projects/JRC/TMF/v1_2024/TransitionMap_Subtypes\",\n",
        "            \"projects/JRC/TMF/v1_2024/AnnualDisruptionObs2024\",\n",
        "            \"projects/JRC/TMF/v1_2023/AnnualDisruptionObs2023\",\n",
        "            \"projects/JRC/TMF/v1_2023/Ndisturb_C2_1982_2022\",\n",
        "\n",
        "        ],\n",
        "    },\n",
        "    # {\n",
        "    #     \"ee_dataset_name\": \"glad\",\n",
        "    #     \"ee_dataset_type\": \"Image\",\n",
        "    #     \"ee_paths\": [\n",
        "    #                 'projects/glad/GLCLU2020/Forest_gain',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_disturbance',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_netgain',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_netloss',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_loss',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_type',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC_2000',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_gain',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_loss',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Water_dynamics',\n",
        "    #                 'projects/glad/GLCLU2020/Water_dynamics_classes',\n",
        "    #     ]\n",
        "    # }\n",
        "\n",
        "    {\n",
        "        \"ee_dataset_name\": \"alpha_earth_2017\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "        ],\n",
        "    },\n",
        "    # {\n",
        "    #     \"ee_dataset_name\": \"alpha_earth_2018\",\n",
        "    #     \"ee_dataset_type\": \"ImageCollection\",\n",
        "    #     \"ee_paths\": [\n",
        "    #         \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "    #     ],\n",
        "    # },\n",
        "    # {\n",
        "    #     \"ee_dataset_name\": \"alpha_earth_2019\",\n",
        "    #     \"ee_dataset_type\": \"ImageCollection\",\n",
        "    #     \"ee_paths\": [\n",
        "    #         \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "    #     ],\n",
        "    # },\n",
        "    # {\n",
        "    #     \"ee_dataset_name\": \"alpha_earth_2020\",\n",
        "    #     \"ee_dataset_type\": \"ImageCollection\",\n",
        "    #     \"ee_paths\": [\n",
        "    #         \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "    #     ],\n",
        "    # },\n",
        "    # {\n",
        "    #     \"ee_dataset_name\": \"alpha_earth_2021\",\n",
        "    #     \"ee_dataset_type\": \"ImageCollection\",\n",
        "    #     \"ee_paths\": [\n",
        "    #         \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "    #     ],\n",
        "    # },\n",
        "    # {\n",
        "    #     \"ee_dataset_name\": \"alpha_earth_2022\",\n",
        "    #     \"ee_dataset_type\": \"ImageCollection\",\n",
        "    #     \"ee_paths\": [\n",
        "    #         \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "    #     ],\n",
        "    # },\n",
        "    # {\n",
        "    #     \"ee_dataset_name\": \"alpha_earth_2023\",\n",
        "    #     \"ee_dataset_type\": \"ImageCollection\",\n",
        "    #     \"ee_paths\": [\n",
        "    #         \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "    #     ],\n",
        "    # },\n",
        "    # {\n",
        "    #     \"ee_dataset_name\": \"alpha_earth_2024\",\n",
        "    #     \"ee_dataset_type\": \"ImageCollection\",\n",
        "    #     \"ee_paths\": [\n",
        "    #         \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "    #     ],\n",
        "    # },\n",
        "\n",
        "]\n",
        "\n",
        "# Verify Earth Engine rasters that will be downloaded\n",
        "ee_raster_list = []\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "\n",
        "    # Check if this is an AlphaEarth dataset.\n",
        "    # Needs different approach to avoid crashing.\n",
        "    is_alpha_earth = 'alpha_earth' in ee_dataset_name.lower()\n",
        "\n",
        "    for ee_path in ee_paths:\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            if is_alpha_earth:\n",
        "                # Memory-efficient approach for AlphaEarth\n",
        "                ee_image_collection = ee.ImageCollection(ee_path)\n",
        "                first_image = ee_image_collection.first()\n",
        "                ee_bands = first_image.bandNames().getInfo()\n",
        "            else:\n",
        "                # Standard approach for regular ImageCollections\n",
        "                ee_image = ee.ImageCollection(ee_path)\n",
        "                ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        else:\n",
        "            # Silent processing for Image datasets\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "\n",
        "        for ee_band in ee_bands:\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_raster_list.append(ee_tif_filename)\n",
        "\n",
        "# Original simple output\n",
        "ee_image_number = len(ee_raster_list)\n",
        "print(f\"There are {ee_image_number} rasters in the list.\")\n",
        "ee_raster_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caWuFWxx22QT"
      },
      "source": [
        "## Simple queue method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qislgFIYUnqi"
      },
      "outputs": [],
      "source": [
        "# Slow but stable\n",
        "\n",
        "# Earth Engine download progress\n",
        "ee_progress_index = 0\n",
        "ee_progress_label = widgets.Label(f\"Earth Engine download progress: {ee_progress_index}/{len(ee_raster_list)}\")\n",
        "display(ee_progress_label)\n",
        "\n",
        "# Load template and set Earth Engine geometry\n",
        "template_polygon_dir = join(polygons_dir, 'template.gpkg')\n",
        "template_area = gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0]\n",
        "template_coords = list(gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0].exterior.coords)\n",
        "ee_geometry = ee.Geometry.Polygon(template_coords)\n",
        "\n",
        "# Download Earth Engine datasets\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "\n",
        "    # Check if this is an AlphaEarth dataset\n",
        "    is_alpha_earth = 'alpha_earth' in ee_dataset_name.lower()\n",
        "\n",
        "    # Loop through Earth Engine paths\n",
        "    for ee_path in ee_paths:\n",
        "        # identify bands\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image = ee.ImageCollection(ee_path)\n",
        "            if is_alpha_earth:\n",
        "                # Memory-efficient approach for AlphaEarth\n",
        "                first_image = ee_image.first()\n",
        "                ee_bands = first_image.bandNames().getInfo()\n",
        "            else:\n",
        "                # Standard approach for regular datasets\n",
        "                ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "        # Loop through bands\n",
        "        for ee_band in reversed(ee_bands):\n",
        "            # Set filename and directory of downloaded raster and check if exists\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_tif_dir = join(ee_dir, ee_tif_filename)\n",
        "            ee_temp_dir = join(\"/gdrive/MyDrive\", ee_tif_filename)\n",
        "            # Different temporary directory required if MyDrive is mounted\n",
        "            if base_dir.startswith('/content/drive/MyDrive/'):\n",
        "              ee_temp_dir = join(\"/content/drive/MyDrive/\", ee_tif_filename)\n",
        "            # Check if temporary raster exists and needs copying\n",
        "            if exists(ee_temp_dir):\n",
        "              copyfile(ee_temp_dir, ee_tif_dir)\n",
        "              remove(ee_temp_dir)\n",
        "            # Check if copied raster exists, and if not download from Earth Engine.\n",
        "            if not exists(ee_tif_dir):\n",
        "              if ee_dataset_type == 'ImageCollection':\n",
        "                if is_alpha_earth:\n",
        "                    # Special AlphaEarth processing: extract year and filter\n",
        "                    year = ee_tif_filename.split('_')[2]  # Extract year from filename\n",
        "                    ee_filtered = ee_image.filterDate(f'{year}-01-01', f'{int(year)+1}-01-01')\n",
        "                    image_selected = ee_filtered.mosaic().select([ee_band])\n",
        "                    resolution = ee_filtered.first().projection().nominalScale().getInfo()\n",
        "                else:\n",
        "                    # Standard processing for regular ImageCollections\n",
        "                    image_selected = ee_image.qualityMosaic(ee_band).select([ee_band])\n",
        "                    resolution = ee_image.first().projection().nominalScale().getInfo()\n",
        "              if ee_dataset_type == 'Image':\n",
        "                image_selected = ee_image.select([ee_band])\n",
        "                resolution = ee_image.select(0).projection().nominalScale().getInfo()\n",
        "              ee_task = ee.batch.Export.image.toDrive(image=image_selected.toFloat(),\n",
        "                                                    description=ee_tif_filename[:-4],\n",
        "                                                    scale=resolution,\n",
        "                                                    region=ee_geometry,\n",
        "                                                    maxPixels=10000000000,\n",
        "                                                    fileNamePrefix=ee_tif_filename[:-4],\n",
        "                                                    crs='EPSG:4326',\n",
        "                                                    fileFormat='GeoTIFF')\n",
        "              ee_task.start()\n",
        "              # Check whether the raster has downloaded yet\n",
        "              while not exists(ee_temp_dir):\n",
        "                  ee_task_status = ee_task.status()\n",
        "                  # If the task is completed, continue\n",
        "                  if ee_task_status[\"state\"] == 'COMPLETED': break\n",
        "                  # If it has failed or been cancelled, show an error\n",
        "                  elif ee_task_status['state'] == 'FAILED' or ee_task_status['state'] == 'CANCELLED':\n",
        "                      print(f\"{ee_tif_filename}:{ee_task_status['error_message']}\")\n",
        "                      try: remove(ee_temp_dir)\n",
        "                      except: pass\n",
        "                      break\n",
        "                  sleep(1)\n",
        "              # Copy the raster to intended directory and remove the temporary raster\n",
        "              while not exists(ee_temp_dir):\n",
        "                sleep(1)\n",
        "              copyfile(ee_temp_dir, ee_tif_dir)\n",
        "              remove(ee_temp_dir)\n",
        "            # Update Earth Engine download progress\n",
        "            ee_progress_index += 1\n",
        "            ee_progress_label.value = f\"Earth Engine download progress: {ee_progress_index}/{len(ee_raster_list)}\"\n",
        "\n",
        "# Check Earth Engine tasks here: https://code.earthengine.google.com/tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zwwDicI25Nq"
      },
      "source": [
        "## Concurrent queue method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYxiD4UUw-zC"
      },
      "outputs": [],
      "source": [
        "# Should be faster and resilient to interruptions, but may be issues Earth Engine side.\n",
        "\n",
        "# Maximum concurrent tasks in Earth Engine\n",
        "ee_max_concurrent_tasks = 2\n",
        "\n",
        "# Load template and set Earth Engine geometry\n",
        "template_polygon_dir = join(polygons_dir, 'template.gpkg')\n",
        "template_area = gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0]\n",
        "template_coords = list(gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0].exterior.coords)\n",
        "ee_geometry = ee.Geometry.Polygon(template_coords)\n",
        "\n",
        "# Create a dictionary of all rasters to download\n",
        "raster_dictionary = {}\n",
        "\n",
        "# Populate the dictionary with information about each raster\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "\n",
        "    # Check if this is an AlphaEarth dataset\n",
        "    is_alpha_earth = 'alpha_earth' in ee_dataset_name.lower()\n",
        "\n",
        "    for ee_path in ee_paths:\n",
        "        # Identify bands\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image = ee.ImageCollection(ee_path)\n",
        "            if is_alpha_earth:\n",
        "                # Memory-efficient approach for AlphaEarth\n",
        "                first_image = ee_image.first()\n",
        "                ee_bands = first_image.bandNames().getInfo()\n",
        "            else:\n",
        "                # Standard approach for regular datasets\n",
        "                ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "\n",
        "        # Loop through bands and create entries in dictionary\n",
        "        for ee_band in ee_bands:\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_tif_dir = join(ee_dir, ee_tif_filename)\n",
        "\n",
        "            # Different temporary directory required if MyDrive is mounted\n",
        "            if base_dir.startswith('/content/drive/MyDrive/'):\n",
        "                ee_temp_dir = join(\"/content/drive/MyDrive/\", ee_tif_filename)\n",
        "            else:\n",
        "                ee_temp_dir = join(\"/gdrive/MyDrive\", ee_tif_filename)\n",
        "\n",
        "            description = ee_tif_filename[:-4]\n",
        "\n",
        "            raster_dictionary[description] = {\n",
        "                'ee_dataset_type': ee_dataset_type,\n",
        "                'ee_dataset_name': ee_dataset_name,\n",
        "                'ee_path': ee_path,\n",
        "                'ee_band': ee_band,\n",
        "                'image_path': ee_tif_dir,\n",
        "                'image_path_temp': ee_temp_dir,\n",
        "                'image_description': description,\n",
        "                'image_status': '',\n",
        "                'ee_task_id': '',\n",
        "                'ee_task': None,\n",
        "                'task_current_execution': False\n",
        "            }\n",
        "\n",
        "# Count total number of rasters\n",
        "raster_number = len(raster_dictionary)\n",
        "\n",
        "# Progress widgets\n",
        "ee_counted_tasks = set()\n",
        "ee_task_progress_index = 0\n",
        "ee_task_progress_label = widgets.Label(\n",
        "    f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        ")\n",
        "display(ee_task_progress_label)\n",
        "\n",
        "raster_progress_index = 0\n",
        "raster_progress_label = widgets.Label(\n",
        "    f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        ")\n",
        "display(raster_progress_label)\n",
        "\n",
        "# Thorough initialization to check all files\n",
        "print(\"Initialising: Checking for completed downloads...\")\n",
        "for raster_info in raster_dictionary.values():\n",
        "    processed = False\n",
        "\n",
        "    # First check if the final file exists\n",
        "    if exists(raster_info['image_path']):\n",
        "        processed = True\n",
        "    # Then check if the temp file exists and move it\n",
        "    elif exists(raster_info['image_path_temp']):\n",
        "        print(f\"Moving temp file for {raster_info['image_description']}\")\n",
        "        move(raster_info['image_path_temp'], raster_info['image_path'])\n",
        "        processed = True\n",
        "\n",
        "    if processed:\n",
        "        raster_info.update({\n",
        "            'image_status': 'processed',\n",
        "            'ee_task_id': '',\n",
        "            'ee_task': None,\n",
        "            'task_current_execution': False\n",
        "        })\n",
        "        # Count it once for the progress bars\n",
        "        if raster_info['image_description'] not in ee_counted_tasks:\n",
        "            ee_counted_tasks.add(raster_info['image_description'])\n",
        "            ee_task_progress_index += 1\n",
        "            ee_task_progress_label.value = (\n",
        "                f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        "            )\n",
        "        raster_progress_index += 1\n",
        "        raster_progress_label.value = (\n",
        "            f\"Raster download progress: {raster_progress_index}/{raster_number}\")\n",
        "    else:\n",
        "        # Clear any state so it'll be requeued later\n",
        "        raster_info.update({\n",
        "            'image_status': '',\n",
        "            'ee_task_id': '',\n",
        "            'ee_task': None,\n",
        "            'task_current_execution': False\n",
        "        })\n",
        "        ee_counted_tasks.discard(raster_info['image_description'])\n",
        "\n",
        "# Detect tasks that were already running before this session\n",
        "print(\"Checking for running Earth Engine tasks...\")\n",
        "ee_current_task_count = 0\n",
        "for task in ee.batch.Task.list():\n",
        "    task_state = task.status()['state']\n",
        "    task_id = task.id\n",
        "    task_description = task.config['description']\n",
        "\n",
        "    if task_state in ['READY', 'RUNNING', 'QUEUED']:\n",
        "        ee_current_task_count += 1\n",
        "        for v in raster_dictionary.values():\n",
        "            if v['image_description'] == task_description and v['image_status'] != 'processed':\n",
        "                v.update({\n",
        "                    'image_status': 'task',\n",
        "                    'ee_task_id': task_id,\n",
        "                    'ee_task': task,\n",
        "                    'task_current_execution': True\n",
        "                })\n",
        "                if task_description not in ee_counted_tasks:\n",
        "                    ee_counted_tasks.add(task_description)\n",
        "                    ee_task_progress_index += 1\n",
        "                    ee_task_progress_label.value = (\n",
        "                        f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        "                    )\n",
        "                break\n",
        "\n",
        "# Main processing loop\n",
        "print(\"Starting main processing loop...\")\n",
        "while True:\n",
        "    # Re-count active EE tasks each pass\n",
        "    active_states = ['READY', 'RUNNING', 'QUEUED']\n",
        "    ee_current_task_count = len([t for t in ee.batch.Task.list()\n",
        "                              if t.status()['state'] in active_states])\n",
        "\n",
        "    # Break when every raster is either processed or failed\n",
        "    if all(v['image_status'] in ['processed', 'failed']\n",
        "           for v in raster_dictionary.values()):\n",
        "        break\n",
        "\n",
        "    # Iterate over rasters\n",
        "    for raster_info in raster_dictionary.values():\n",
        "        # Skip finished / failed rasters\n",
        "        if raster_info['image_status'] in ['processed', 'failed']:\n",
        "            continue\n",
        "\n",
        "        # Final file already exists - double check\n",
        "        if exists(raster_info['image_path']):\n",
        "            raster_info.update({\n",
        "                'image_status': 'processed',\n",
        "                'ee_task_id': '',\n",
        "                'ee_task': None,\n",
        "                'task_current_execution': False\n",
        "            })\n",
        "\n",
        "            if raster_info['image_description'] not in ee_counted_tasks:\n",
        "                ee_counted_tasks.add(raster_info['image_description'])\n",
        "                ee_task_progress_index += 1\n",
        "                ee_task_progress_label.value = (\n",
        "                    f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        "                )\n",
        "\n",
        "            raster_progress_index += 1\n",
        "            raster_progress_label.value = (\n",
        "                f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # Temporary file exists – move to downloads directory\n",
        "        if exists(raster_info['image_path_temp']):\n",
        "            move(raster_info['image_path_temp'], raster_info['image_path'])\n",
        "\n",
        "            raster_info.update({\n",
        "                'image_status': 'processed',\n",
        "                'ee_task_id': '',\n",
        "                'ee_task': None,\n",
        "                'task_current_execution': False\n",
        "            })\n",
        "\n",
        "            if raster_info['image_description'] not in ee_counted_tasks:\n",
        "                ee_counted_tasks.add(raster_info['image_description'])\n",
        "                ee_task_progress_index += 1\n",
        "                ee_task_progress_label.value = (\n",
        "                    f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        "                )\n",
        "\n",
        "            raster_progress_index += 1\n",
        "            raster_progress_label.value = (\n",
        "                f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # Task is running – poll its status\n",
        "        if raster_info['image_status'] == 'task':\n",
        "            ee_task = raster_info['ee_task']\n",
        "            if ee_task is None:\n",
        "                # Fallback: find it again by ID\n",
        "                matches = [t for t in ee.batch.Task.list() if t.id == raster_info['ee_task_id']]\n",
        "                ee_task = matches[0] if matches else None\n",
        "                raster_info['ee_task'] = ee_task\n",
        "\n",
        "            if ee_task is not None:\n",
        "                task_state = ee_task.status()['state']\n",
        "\n",
        "                if task_state in ('FAILED', 'CANCELLED'):\n",
        "                    raster_info.update({\n",
        "                        'image_status': 'failed',\n",
        "                        'ee_task_id': '',\n",
        "                        'ee_task': None,\n",
        "                        'task_current_execution': False\n",
        "                    })\n",
        "                    print(f\"{raster_info['image_description']} failed. Skipping.\")\n",
        "\n",
        "                elif task_state == 'COMPLETED':\n",
        "                    # Check if final file exists\n",
        "                    if exists(raster_info['image_path']):\n",
        "                        raster_info.update({\n",
        "                            'image_status': 'processed',\n",
        "                            'ee_task_id': '',\n",
        "                            'ee_task': None,\n",
        "                            'task_current_execution': False\n",
        "                        })\n",
        "                        raster_progress_index += 1\n",
        "                        raster_progress_label.value = (\n",
        "                            f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        "                        )\n",
        "                    # Check if temp file exists\n",
        "                    elif exists(raster_info['image_path_temp']):\n",
        "                        move(raster_info['image_path_temp'], raster_info['image_path'])\n",
        "                        raster_info.update({\n",
        "                            'image_status': 'processed',\n",
        "                            'ee_task_id': '',\n",
        "                            'ee_task': None,\n",
        "                            'task_current_execution': False\n",
        "                        })\n",
        "                        raster_progress_index += 1\n",
        "                        raster_progress_label.value = (\n",
        "                            f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        "                        )\n",
        "                    # Neither file exists, reset status to trigger download\n",
        "                    else:\n",
        "                        raster_info.update({\n",
        "                            'image_status': '',  # Reset to empty to queue task again\n",
        "                            'ee_task_id': '',\n",
        "                            'ee_task': None,\n",
        "                            'task_current_execution': False\n",
        "                        })\n",
        "                        print(f\"Earth Engine task {raster_info['image_description']} completed but file not found. Retrying download.\")\n",
        "\n",
        "            continue  # READY / RUNNING / QUEUED, keep polling\n",
        "\n",
        "        # Need to queue a new task\n",
        "        if raster_info['image_status'] == '':\n",
        "            # Wait for a free slot\n",
        "            if ee_current_task_count >= ee_max_concurrent_tasks:\n",
        "                continue  # try again next outer loop pass\n",
        "\n",
        "            ee_path = raster_info['ee_path']\n",
        "            ee_band = raster_info['ee_band']\n",
        "            ee_dataset_type = raster_info['ee_dataset_type']\n",
        "            ee_dataset_name = raster_info['ee_dataset_name']\n",
        "\n",
        "            # Check if this is an AlphaEarth dataset\n",
        "            is_alpha_earth = 'alpha_earth' in ee_dataset_name.lower()\n",
        "\n",
        "            # Select the appropriate image and band\n",
        "            if ee_dataset_type == 'ImageCollection':\n",
        "                ee_image = ee.ImageCollection(ee_path)\n",
        "                if is_alpha_earth:\n",
        "                    # Special AlphaEarth processing: extract year and filter\n",
        "                    year = raster_info['image_description'].split('_')[2]  # Extract year from description\n",
        "                    ee_filtered = ee_image.filterDate(f'{year}-01-01', f'{int(year)+1}-01-01')\n",
        "                    image_selected = ee_filtered.mosaic().select([ee_band])\n",
        "                    resolution = ee_filtered.first().projection().nominalScale().getInfo()\n",
        "                else:\n",
        "                    # Standard processing for regular ImageCollections\n",
        "                    image_selected = ee_image.qualityMosaic(ee_band).select([ee_band])\n",
        "                    resolution = ee_image.first().projection().nominalScale().getInfo()\n",
        "            elif ee_dataset_type == 'Image':\n",
        "                ee_image = ee.Image(ee_path)\n",
        "                image_selected = ee_image.select([ee_band])\n",
        "                resolution = ee_image.select(0).projection().nominalScale().getInfo()\n",
        "\n",
        "            task = ee.batch.Export.image.toDrive(\n",
        "                image=image_selected.toFloat(),\n",
        "                description=raster_info['image_description'],\n",
        "                fileNamePrefix=raster_info['image_description'],\n",
        "                fileFormat='GeoTIFF',\n",
        "                region=ee_geometry,\n",
        "                scale=resolution,\n",
        "                maxPixels=10000000000,\n",
        "                crs='EPSG:4326'\n",
        "            )\n",
        "            task.start()\n",
        "\n",
        "            ee_current_task_count += 1\n",
        "            raster_info.update({\n",
        "                'image_status': 'task',\n",
        "                'ee_task_id': task.id,\n",
        "                'ee_task': task,\n",
        "                'task_current_execution': True\n",
        "            })\n",
        "\n",
        "    sleep(5)\n",
        "\n",
        "processed_count = sum(1 for v in raster_dictionary.values()\n",
        "                     if v['image_status'] == 'processed')\n",
        "failed_count = sum(1 for v in raster_dictionary.values()\n",
        "                  if v['image_status'] == 'failed')\n",
        "\n",
        "print(f\"Final Status Check:\\nProcessed Rasters: {processed_count}\\nFailed Rasters: {failed_count}\")\n",
        "print(\"Check Earth Engine tasks here: https://code.earthengine.google.com/tasks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiled method (experimental)"
      ],
      "metadata": {
        "id": "td73nZmzUws6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"USE AT OWN RISK.\")\n",
        "print(\"Last resort if queue methods are taking impossibly long to complete.\")\n",
        "print(\"Attempts to iteratively tile the request until it's small enough for direct download.\")\n",
        "print(\"May cause the user to hit daily or total Earth Engine request/usage limits.\")\n",
        "\n",
        "user_input = input(\"Type 'OK' to proceed: \")\n",
        "if user_input.upper() == 'OK': pass\n",
        "else: print(\"Not OK.\")\n",
        "\n",
        "# Set to False to suppress detailed messages about image splitting\n",
        "verbose = False\n",
        "\n",
        "compression = [\n",
        "    'COMPRESS=LZW',  # Good speed / size ratio\n",
        "    # 'ZSTD_LEVEL=1',\n",
        "]\n",
        "\n",
        "clip_geometry = False  # If True, clips the download geometry to the dataset footprint\n",
        "# If False, the empty geometry will be filled with nodata values.\n",
        "\n",
        "# Create a temporary directory for tiles\n",
        "temp_tiles_dir = join(ee_dir, 'temp_tiles')\n",
        "makedirs(temp_tiles_dir, exist_ok=True)\n",
        "\n",
        "# Load template and set Earth Engine geometry\n",
        "template_polygon_dir = join(polygons_dir, 'template.gpkg')\n",
        "template_area = gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0]\n",
        "template_coords = list(gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0].exterior.coords)\n",
        "ee_geometry = ee.Geometry.Polygon(template_coords)\n",
        "\n",
        "# Create a dictionary of all rasters to download\n",
        "raster_dictionary = {}\n",
        "\n",
        "# Populate the dictionary with information about each raster\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "    for ee_path in ee_paths:\n",
        "        # Identify bands\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image_collection = ee.ImageCollection(ee_path)\n",
        "            # Special handling for AlphaEarth to avoid memory limits\n",
        "            if 'SATELLITE_EMBEDDING' in ee_path:\n",
        "                # We know AlphaEarth has bands A00-A63, avoid getInfo() call\n",
        "                ee_bands = [f\"A{i:02d}\" for i in range(64)]\n",
        "            else: ee_bands = [b['id'] for b in ee_image_collection.getInfo()['features'][0]['bands']]\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "        # Loop through bands and create entries in dictionary\n",
        "        for ee_band in ee_bands:\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_tif_dir = join(ee_dir, ee_tif_filename)\n",
        "            # Different temporary directory required if MyDrive is mounted\n",
        "            if base_dir.startswith('/content/drive/MyDrive/'):\n",
        "                ee_temp_dir = join(\"/content/drive/MyDrive/\", ee_tif_filename)\n",
        "            else: ee_temp_dir = join(\"/gdrive/MyDrive\", ee_tif_filename)\n",
        "            description = ee_tif_filename[:-4]\n",
        "            raster_dictionary[description] = {\n",
        "                'ee_dataset_type': ee_dataset_type,\n",
        "                'ee_path': ee_path,\n",
        "                'ee_band': ee_band,\n",
        "                'image_path': ee_tif_dir,\n",
        "                'image_path_temp': ee_temp_dir,\n",
        "                'image_description': description,\n",
        "                'image_status': '',\n",
        "                'ee_object_id': ee_path\n",
        "            }\n",
        "\n",
        "# Control parallel processing\n",
        "max_concurrent_images = 10\n",
        "\n",
        "# Lock for updating progress\n",
        "progress_lock = threading.Lock()\n",
        "\n",
        "# Total raster count\n",
        "raster_number = len(raster_dictionary)\n",
        "\n",
        "# Global variable for tracking progress\n",
        "global_progress_index = 0\n",
        "\n",
        "# Function to display custom progress bar\n",
        "def display_progress():\n",
        "    percent = int((global_progress_index / raster_number) * 100) if raster_number > 0 else 0\n",
        "    bar_width = 80\n",
        "    filled_length = int(bar_width * global_progress_index // raster_number)\n",
        "    bar = '=' * filled_length + ' ' * (bar_width - filled_length)\n",
        "    progress_html = f\"\"\"\n",
        "    <div style=\"width:100%; margin-top:10px; margin-bottom:10px;\">\n",
        "        <div style=\"color:#CCCCCC; font-family:monospace;\">\n",
        "            Raster download progress: {percent}% [{bar}] {global_progress_index}/{raster_number}\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    clear_output(wait=True)\n",
        "    display(HTML(progress_html))\n",
        "\n",
        "# Display initial progress\n",
        "display_progress()\n",
        "\n",
        "def download_tile(raster_band, geometry, scale, output_path, max_retries=3):\n",
        "    \"\"\"Try to download a tile with the given geometry, handling EE-specific errors\"\"\"\n",
        "    for retry in range(max_retries):\n",
        "        try:\n",
        "            # Get the download URL\n",
        "            url = raster_band.getDownloadURL({\n",
        "                'scale': scale,\n",
        "                'region': geometry,\n",
        "                'format': 'GEO_TIFF',\n",
        "                'crs': 'EPSG:4326'\n",
        "            })\n",
        "            # Download the file\n",
        "            urlretrieve(url, output_path)\n",
        "            return True, None\n",
        "        except ee.EEException as e:\n",
        "            error_msg = str(e)\n",
        "            # Check for size-related errors specifically\n",
        "            if \"Total request size\" in error_msg and \"must be less than or equal to\" in error_msg:\n",
        "                return False, \"SIZE_LIMIT\"\n",
        "            else:\n",
        "                if retry < max_retries - 1:\n",
        "                    sleep(5)\n",
        "        except Exception as e:\n",
        "            if retry < max_retries - 1:\n",
        "                sleep(5)\n",
        "    return False, \"OTHER_ERROR\"\n",
        "\n",
        "def split_tile_vertically(geometry, n_parts=2):\n",
        "    \"\"\"Split a rectangular geometry into n_parts vertically\"\"\"\n",
        "    bounds = geometry.bounds().getInfo()['coordinates'][0]\n",
        "    min_x = min(coord[0] for coord in bounds)\n",
        "    min_y = min(coord[1] for coord in bounds)\n",
        "    max_x = max(coord[0] for coord in bounds)\n",
        "    max_y = max(coord[1] for coord in bounds)\n",
        "    height = max_y - min_y\n",
        "    part_height = height / n_parts\n",
        "    parts = []\n",
        "    for i in range(n_parts):\n",
        "        part_min_y = min_y + (i * part_height)\n",
        "        part_max_y = min_y + ((i + 1) * part_height)\n",
        "        parts.append(ee.Geometry.Rectangle([min_x, part_min_y, max_x, part_max_y]))\n",
        "    return parts\n",
        "\n",
        "def process_image(image_description, raster_info):\n",
        "    \"\"\"Process a single raster image - to be run in parallel\"\"\"\n",
        "    global global_progress_index\n",
        "    # Skip if already processed or failed\n",
        "    if exists(raster_info['image_path']) or raster_info.get('image_status') == 'failed':\n",
        "        with progress_lock:\n",
        "            global_progress_index += 1\n",
        "            # No direct widget update here - handled by update_progress_display thread\n",
        "        return True\n",
        "    # Get the band\n",
        "    ee_object = None\n",
        "    raster_band = None\n",
        "    tile_paths = []\n",
        "    try:\n",
        "        ee_path = raster_info['ee_path']\n",
        "        ee_band = raster_info['ee_band']\n",
        "        ee_dataset_type = raster_info['ee_dataset_type']\n",
        "        # Select the appropriate image and band\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image_collection = ee.ImageCollection(ee_path)\n",
        "            # Special handling for AlphaEarth to avoid memory limits\n",
        "            if 'SATELLITE_EMBEDDING' in ee_path:\n",
        "                # Extract year from image_description (e.g., \"alpha_earth_2024_ANNUAL_A00\" -> 2024)\n",
        "                year = image_description.split('_')[2]\n",
        "                # Filter to specific year but DON'T filter by bounds yet - let it mosaic first\n",
        "                ee_filtered = ee_image_collection.filterDate(f'{year}-01-01', f'{int(year)+1}-01-01')\n",
        "                ee_object = ee_filtered.mosaic()\n",
        "                # Now clip to your geometry AFTER mosaicking\n",
        "                # ee_object = ee_object.clip(ee_geometry)\n",
        "                projection = ee_filtered.first().projection()\n",
        "            else:\n",
        "                # Original logic for all other datasets - UNCHANGED\n",
        "                ee_object = ee_image_collection.qualityMosaic(ee_band)\n",
        "                projection = ee_image_collection.first().projection()\n",
        "            raster_band = ee_object.select([ee_band]).toFloat()\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_object = ee.Image(ee_path)\n",
        "            raster_band = ee_object.select([ee_band]).toFloat()\n",
        "            projection = ee_object.select(0).projection()\n",
        "        if clip_geometry:\n",
        "            # Get the footprint\n",
        "            footprint = ee_object.geometry()\n",
        "            # Calculate intersection with the footprint\n",
        "            download_geometry = ee_geometry.intersection(footprint)\n",
        "        else: download_geometry = ee_geometry\n",
        "        # Get image projection and scale\n",
        "        scale = projection.nominalScale().getInfo()\n",
        "        # Create a folder for this image's tiles\n",
        "        image_tiles_dir = join(temp_tiles_dir, image_description)\n",
        "        makedirs(image_tiles_dir, exist_ok=True)\n",
        "        # Track downloaded tiles\n",
        "        tile_paths = []\n",
        "        # First try to download the whole image at once\n",
        "        whole_image_path = join(image_tiles_dir, f\"{image_description}_whole.tif\")\n",
        "        success, error_type = download_tile(raster_band, download_geometry, scale, whole_image_path)\n",
        "        if success: tile_paths = [whole_image_path]\n",
        "        else:\n",
        "            # Log failure only if verbose\n",
        "            if verbose:\n",
        "                if error_type == \"SIZE_LIMIT\": print(f\"[{image_description}] Full image download failed due to size limit, starting adaptive tiling...\")\n",
        "                else: print(f\"[{image_description}] Full image download failed, starting adaptive tiling...\")\n",
        "            # Initial split factor depends on error type\n",
        "            initial_parts = 2 if error_type == \"SIZE_LIMIT\" else 2\n",
        "            # Start with initial split of the geometry\n",
        "            parts = split_tile_vertically(download_geometry, initial_parts)\n",
        "            tiles_to_process = []\n",
        "            for i in range(len(parts)):\n",
        "                part_id = str(uuid.uuid4()).replace('-', '')\n",
        "                tiles_to_process.append((parts[i], part_id, f\"{image_description}_part_{part_id}.tif\"))\n",
        "            successful_tile_height = None\n",
        "            # Process tiles until none are left\n",
        "            while tiles_to_process:\n",
        "                current_geometry, part_num, tile_filename = tiles_to_process.pop(0)\n",
        "                tile_path = join(image_tiles_dir, tile_filename)\n",
        "                # Try to download with current dimensions\n",
        "                success, error_type = download_tile(raster_band, current_geometry, scale, tile_path)\n",
        "                if success:\n",
        "                    tile_paths.append(tile_path)\n",
        "                    # If this is our first successful tile, remember its height\n",
        "                    if successful_tile_height is None:\n",
        "                        bounds = current_geometry.bounds().getInfo()['coordinates'][0]\n",
        "                        min_y = min(coord[1] for coord in bounds)\n",
        "                        max_y = max(coord[1] for coord in bounds)\n",
        "                        successful_tile_height = max_y - min_y\n",
        "                else: # If download failed, log it if verbose\n",
        "                    if verbose:\n",
        "                        print(f\"[{image_description}] Part {part_num} download failed: {error_type}\")\n",
        "                    # If size limit error, split more aggressively\n",
        "                    split_factor = 3 if error_type == \"SIZE_LIMIT\" else 2\n",
        "                    # If we have a successful tile height, try to use it\n",
        "                    if successful_tile_height is not None:\n",
        "                        bounds = current_geometry.bounds().getInfo()['coordinates'][0]\n",
        "                        min_y = min(coord[1] for coord in bounds)\n",
        "                        max_y = max(coord[1] for coord in bounds)\n",
        "                        current_height = max_y - min_y\n",
        "                        # Calculate how many parts we need to match the successful height\n",
        "                        needed_parts = max(split_factor, math.ceil(current_height / successful_tile_height))\n",
        "                        split_parts = split_tile_vertically(current_geometry, needed_parts)\n",
        "                    else: split_parts = split_tile_vertically(current_geometry, split_factor)\n",
        "                    # Add new parts to the processing queue\n",
        "                    for i, geom in enumerate(split_parts):\n",
        "                        new_part_num = str(uuid.uuid4()).replace('-', '')\n",
        "                        tiles_to_process.append((geom, new_part_num, f\"{image_description}_part_{new_part_num}.tif\"))\n",
        "                    # Remove the failed attempt file if it exists\n",
        "                    if os.path.exists(tile_path):\n",
        "                        os.remove(tile_path)\n",
        "        # Merge tiles using GDAL\n",
        "        if len(tile_paths) > 0:\n",
        "            if len(tile_paths) == 1:\n",
        "                # Just one tile, compress and copy directly\n",
        "                merged_temp_path = raster_info['image_path_temp']\n",
        "                gdal_translate_options = gdal.TranslateOptions(\n",
        "                    format=\"GTiff\",\n",
        "                    creationOptions=compression)\n",
        "                gdal.Translate(merged_temp_path, tile_paths[0], options=gdal_translate_options)\n",
        "            else:\n",
        "                # Multiple tiles need merging\n",
        "                vrt_path = join(image_tiles_dir, f\"{image_description}_mosaic.vrt\")\n",
        "                merged_temp_path = raster_info['image_path_temp']\n",
        "                # Create VRT from tiles\n",
        "                gdal.BuildVRT(vrt_path, tile_paths)\n",
        "                # Translate VRT to GeoTIFF with compression\n",
        "                gdal_translate_options = gdal.TranslateOptions(\n",
        "                    format=\"GTiff\",\n",
        "                    creationOptions=compression)\n",
        "                gdal.Translate(merged_temp_path, vrt_path, options=gdal_translate_options)\n",
        "            # Move to final location\n",
        "            move(merged_temp_path, raster_info['image_path'])\n",
        "            # Update progress tracking with thread safety\n",
        "            with progress_lock:\n",
        "                global_progress_index += 1\n",
        "                # No direct widget update here - handled by update_progress_display thread\n",
        "            # Clean up tile folder after successful merge\n",
        "            rmtree(image_tiles_dir)\n",
        "            return True\n",
        "        else:\n",
        "           if verbose: print(f\"[{image_description}] Failed: No tiles were successfully downloaded\")\n",
        "           raster_info['image_status'] = 'failed'\n",
        "           return False\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"Error processing {image_description}: {str(e)}\")\n",
        "        raster_info['image_status'] = 'failed'\n",
        "        return False\n",
        "    finally:\n",
        "        # Explicitly clear any large objects\n",
        "        ee_object = None\n",
        "        raster_band = None\n",
        "\n",
        "# Function to update progress display periodically\n",
        "def update_progress_display():\n",
        "    last_count = 0\n",
        "    while global_progress_index < raster_number:\n",
        "        sleep(0.5)  # Update every half second\n",
        "        current_count = 0\n",
        "        with progress_lock:\n",
        "            current_count = global_progress_index\n",
        "        if current_count != last_count:\n",
        "            display_progress()\n",
        "            last_count = current_count\n",
        "    # Final update to ensure 100% is shown\n",
        "    display_progress()\n",
        "\n",
        "# Count initially processed images\n",
        "global_progress_index = 0\n",
        "for v in raster_dictionary.values():\n",
        "    if v.get('image_status') == 'processed' or exists(v['image_path']):\n",
        "        global_progress_index += 1\n",
        "\n",
        "# Display initial progress\n",
        "display_progress()\n",
        "\n",
        "# Create a list of pending images to process\n",
        "pending_images = [(desc, img) for desc, img in raster_dictionary.items()\n",
        "                  if not exists(img['image_path']) and img.get('image_status') != 'failed']\n",
        "if verbose: print(f\"Starting processing of {len(pending_images)} rasters with {max_concurrent_images} parallel workers\")\n",
        "\n",
        "# Start the progress monitoring thread\n",
        "progress_thread = threading.Thread(target=update_progress_display)\n",
        "progress_thread.daemon = True\n",
        "progress_thread.start()\n",
        "\n",
        "try:\n",
        "    # Use ThreadPoolExecutor for parallel processing\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent_images) as executor:\n",
        "        futures = {executor.submit(process_image, desc, img): (desc, img) for desc, img in pending_images}\n",
        "        # Wait for completion and process results\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            desc, _ = futures[future]\n",
        "            try:\n",
        "                success = future.result()\n",
        "                # Only log failures if verbose\n",
        "                if not success and verbose: print(f\"Raster {desc} processing failed\")\n",
        "            except Exception as e:\n",
        "                if verbose: print(f\"Raster {desc} processing generated an exception: {e}\")\n",
        "                # Mark as failed\n",
        "                raster_dictionary[desc]['image_status'] = 'failed'\n",
        "except Exception as e:\n",
        "    print(f\"Error in thread pool execution: {e}\")\n",
        "finally:\n",
        "    # Make sure we wait for the progress thread to update one last time\n",
        "    if progress_thread.is_alive(): sleep(0.6)  # Give time for one last update\n",
        "print(f\"Processing complete\")\n",
        "\n",
        "# Count and display results\n",
        "processed_count = sum(1 for v in raster_dictionary.values()\n",
        "                      if exists(v['image_path']))\n",
        "failed_count = sum(1 for v in raster_dictionary.values()\n",
        "                   if v.get('image_status') == 'failed')\n",
        "\n",
        "print(f\"Final Status: {processed_count} rasters processed, {failed_count} rasters failed\")\n",
        "print(\"Check Earth Engine tasks here: https://code.earthengine.google.com/tasks\")"
      ],
      "metadata": {
        "id": "a3vShCduUzGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIBbo-gsS3nD"
      },
      "source": [
        "# GLAD LCLUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W90YInS-S5I5"
      },
      "outputs": [],
      "source": [
        "# GLAD data can be used in-place of TMF data for testing non-TMF areas.\n",
        "# LCLUC contains several land cover and land use types, each with continuous metrics.\n",
        "# This splits them into categories for better modelling, based on the legend:\n",
        "# https://glad.umd.edu/sites/default/files/legend_0.xlsx\n",
        "# This block should be run before resampling.\n",
        "\n",
        "lcluc_dict = {\n",
        "    'terra_vegetation_cover_percent': (0, 24),\n",
        "    'terra_stable_tree_m': (25, 48),\n",
        "    'wetland_vegetation_cover_percent': (100, 124),\n",
        "    'wetland_stable_tree_m': (125, 148),\n",
        "    'open_surface_water_percent_of_year': (200, 207),\n",
        "    'snow_ice': (241, 241),\n",
        "    'cropland': (244, 244),\n",
        "    'built_up': (250, 250),\n",
        "    'ocean': (254, 254),\n",
        "}\n",
        "\n",
        "lcluc_exists = False\n",
        "for lcluc_raster in os.listdir(ee_dir):\n",
        "  if 'LCLUC' in lcluc_raster:\n",
        "    lcluc_exists = True\n",
        "    lcluc_path = join(ee_dir, lcluc_raster)\n",
        "    luluc_array = gdal.Open(lcluc_path).ReadAsArray()\n",
        "    for key, (lower, upper) in lcluc_dict.items():\n",
        "        split_luluc_filename = f\"{lcluc_raster[:-4]}_{key}.tif\"\n",
        "        split_luluc_filename_binary = f\"{lcluc_raster[:-4]}_{key}_binary.tif\"\n",
        "        split_luluc_dir = join(glad_lcluc_dir, split_luluc_filename)\n",
        "        split_luluc_dir_binary = join(glad_lcluc_dir, split_luluc_filename_binary)\n",
        "        if not exists(split_luluc_dir) and not exists(split_luluc_dir_binary):\n",
        "          split_luluc_mask = np.logical_and(luluc_array >= lower, luluc_array <= upper)\n",
        "          split_luluc_array = np.where(split_luluc_mask, luluc_array, 0) # outside the range set to 0\n",
        "          non_zero_percentage = np.count_nonzero(split_luluc_array) / split_luluc_array.size * 100\n",
        "          if non_zero_percentage >= 0.1:\n",
        "            # Check if there's only one unique non-zero value, and convert to a 1-0 binary raster if true\n",
        "            unique_non_zero_values = np.unique(split_luluc_array[split_luluc_array > 0])\n",
        "            if len(unique_non_zero_values) == 1:\n",
        "                split_luluc_array = np.where(split_luluc_array > 0, 1, 0)\n",
        "                split_luluc_dir = split_luluc_dir_binary\n",
        "            export_array_as_tif(split_luluc_array, split_luluc_dir, template=lcluc_path)\n",
        "            print(f\"{lcluc_raster} raster has been processed\")\n",
        "\n",
        "if not lcluc_exists: print(\"There are no GLAD LCLUC rasters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alpha Earth"
      ],
      "metadata": {
        "id": "Ya6zMBMdbFPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename and move to Alpha Earth directory\n",
        "\n",
        "for filename in os.listdir(ee_dir):\n",
        "    if 'alpha_earth' in filename and filename.endswith('.tif'):\n",
        "        if match := re.search(r'_(\\d{4})_', filename):\n",
        "            year = match.group(1)\n",
        "            new_name = re.sub(r'_\\d{4}_', '_', filename).replace('ANNUAL_', '').replace('.tif', f'_{year}.tif')\n",
        "            move(join(ee_dir, filename), f\"{alpha_earth_dir}/{new_name}\")\n",
        "            print(f\"{filename} -> {new_name}\")"
      ],
      "metadata": {
        "id": "Ts2FamVtbHER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-UOVsvRuHNS"
      },
      "source": [
        "# Resample EE rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5yEcU40E8eD"
      },
      "outputs": [],
      "source": [
        "# Create dictionary of all tifs in Earth Engine and user upload directory\n",
        "resample_dict = {}\n",
        "for resample_raster in os.listdir(ee_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'categorical'\"})\n",
        "for resample_raster in os.listdir(user_upload_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'categorical'\"})\n",
        "for resample_raster in os.listdir(glad_lcluc_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'continuous'\"})\n",
        "resample_dict = {key: value for key, value in sorted(resample_dict.items())}\n",
        "\n",
        "# Select rasters for resampling and verify data type (categorical or continuous)\n",
        "print(\"selected_original_rasters = {\")\n",
        "for key, value in resample_dict.items():\n",
        "  if key.endswith('.tif'): print(f'\"{key}\": {value},')\n",
        "print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfxPFqlxE0Ok"
      },
      "outputs": [],
      "source": [
        "selected_original_rasters = {\n",
        "\"tmf_AnnualChanges_Dec1990.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1991.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1992.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1993.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1994.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1995.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1996.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1997.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1998.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1999.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2000.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2001.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2002.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2003.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2004.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2005.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2006.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2007.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2008.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2009.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2010.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2011.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2012.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2013.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2014.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2015.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2016.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2017.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2018.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2019.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2020.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2021.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2022.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2023.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2024.tif\": 'categorical',\n",
        "\"tmf_AnnualDisruptionObs2023_y2023.tif\": 'categorical',\n",
        "\"tmf_AnnualDisruptionObs2024_SumNonForest.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1982.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1983.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1984.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1985.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1986.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1987.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1988.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1989.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1990.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1991.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1992.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1993.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1994.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1995.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1996.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1997.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1998.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1999.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2000.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2001.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2002.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2003.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2004.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2005.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2006.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2007.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2008.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2009.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2010.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2011.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2012.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2013.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2014.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2015.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2016.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2017.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2018.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2019.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2020.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2021.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2022.tif\": 'categorical',\n",
        "\"tmf_TransitionMap_MainClasses_TransitionMap_MainClasses.tif\": 'categorical',\n",
        "\"tmf_TransitionMap_Subtypes_TransitionMap_Subtypes.tif\": 'categorical',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEZwd6XYXqRk"
      },
      "outputs": [],
      "source": [
        "# Set resample algorithms for different raster types\n",
        "# See https://gdal.org/programs/gdalwarp.html\n",
        "categorical_alg = 'near'\n",
        "continuous_alg = 'bilinear'\n",
        "\n",
        "template = gdal.Open(template_dir)\n",
        "template_dimensions = template.GetGeoTransform()\n",
        "xres, yres = template_dimensions[1], -template_dimensions[5]\n",
        "xmin = template_dimensions[0]\n",
        "ymin = template_dimensions[3] - template.RasterYSize * yres\n",
        "xmax = xmin + template.RasterXSize * xres\n",
        "ymax = template_dimensions[3]\n",
        "\n",
        "# Resample progress\n",
        "resample_progress_index = 0\n",
        "resample_progress_label = widgets.Label(f\"Resample progress: {resample_progress_index}/{len(selected_original_rasters.items())}\")\n",
        "display(resample_progress_label)\n",
        "\n",
        "# Iterate over selected rasters\n",
        "for original_raster_name, data_type in selected_original_rasters.items():\n",
        "  resampled_raster_dir = join(resampled_dir, original_raster_name)\n",
        "  if not exists(resampled_raster_dir):\n",
        "    original_raster_dir = join(ee_dir, original_raster_name)\n",
        "    if not exists(original_raster_dir): original_raster_dir = join(user_upload_dir, original_raster_name)\n",
        "    if not exists(original_raster_dir): original_raster_dir = join(glad_lcluc_dir, original_raster_name)\n",
        "    # Set resample type\n",
        "    if data_type == 'categorical': resample_alg = categorical_alg\n",
        "    if data_type == 'continuous': resample_alg = continuous_alg\n",
        "    src = gdal.Warp(\n",
        "        resampled_raster_dir,\n",
        "        original_raster_dir,\n",
        "        xRes=xres, yRes=yres,\n",
        "        outputBounds=(xmin, ymin, xmax, ymax),\n",
        "        resampleAlg=resample_alg,\n",
        "        outputType=gdalconst.GDT_Float32)\n",
        "    # Compress and close\n",
        "    driver = gdal.GetDriverByName(\"GTiff\")\n",
        "    src = driver.CreateCopy(resampled_raster_dir, src, 0, options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "    src = None\n",
        "  # Update resample progress\n",
        "  resample_progress_index += 1\n",
        "  resample_progress_label.value = f\"Resample progress: {resample_progress_index}/{len(selected_original_rasters.items())}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhukvPP02gBm"
      },
      "source": [
        "# TMF binary features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdGYp7HAXkD4"
      },
      "outputs": [],
      "source": [
        "# Check TMF data users guide for classification. https://forobs.jrc.ec.europa.eu/static/tmf/TMF_DataUsersGuide.pdf\n",
        "\n",
        "cell_size_x = gdal.Open(join(areas_dir, 'cell_size_x.tif')).ReadAsArray()\n",
        "cell_size_y = gdal.Open(join(areas_dir, 'cell_size_y.tif')).ReadAsArray()\n",
        "cell_size_ha = np.mean(cell_size_x) * np.mean(cell_size_y) / 10_000\n",
        "sieve_size = int(np.ceil(0.5/cell_size_ha)) # Removes all forest patches smaller than 0.5 ha\n",
        "print(f\"Forest binary sieve size (>0.5 ha) is {sieve_size} pixels.\")\n",
        "\n",
        "# Generate list of valid TMF rasters to convert to binary\n",
        "binary_list = []\n",
        "for resampled_raster in os.listdir(resampled_dir):\n",
        "  # Verify these are in the filenames\n",
        "  if 'DisruptionObs' in resampled_raster or 'AnnualChanges' in resampled_raster or 'Ndisturb' in resampled_raster:\n",
        "    # Verify this is the position of the year in the filename\n",
        "    if '2024' in resampled_raster: year = 2024 # This one has a funny name\n",
        "    else: year = resampled_raster[-8:-4]\n",
        "    if int(year) >= 1990: binary_list.append(resampled_raster) # Data prior to 1990 is poor\n",
        "\n",
        "# Binary progress\n",
        "binary_progress_index = 0\n",
        "binary_progress_label = widgets.Label(f\"Binary progress: {binary_progress_index}/{len(binary_list)}\")\n",
        "display(binary_progress_label)\n",
        "\n",
        "for resampled_raster in binary_list:\n",
        "  if '2024' in resampled_raster: year = 2024 # This one has a funny name\n",
        "  else: year = resampled_raster[-8:-4]\n",
        "  # Forest binary\n",
        "  if 'AnnualChanges' in resampled_raster:\n",
        "    forest_binary_path = join(binary_dir, f\"forest_binary_{year}.tif\")\n",
        "    if not exists(forest_binary_path):\n",
        "      ac_raster_path = join(resampled_dir, resampled_raster)\n",
        "      ac_array = gdal.Open(ac_raster_path).ReadAsArray()\n",
        "      # Set classes 1 & 2 as 1, all else as 0\n",
        "      forest_binary_array = np.where((ac_array == 1) | (ac_array == 2), 1, 0)\n",
        "\n",
        "      # Sieve to 0.5 ha, using 8-connectedness (3, 3)\n",
        "      fb_array_labelled, fb_array_features = label(forest_binary_array, structure=np.ones((3, 3)))\n",
        "      # Determine the size of each patch\n",
        "      fb_array_sizes = ndi_sum(forest_binary_array, fb_array_labelled, range(fb_array_features + 1))\n",
        "      # Create a mask to remove patches smaller than the threshold\n",
        "      fb_array_mask_sizes = fb_array_sizes >= sieve_size\n",
        "      fb_array_mask_sizes[0] = 0 # Ensure non-forest (0) is excluded\n",
        "      fb_array_mask = fb_array_mask_sizes[fb_array_labelled]\n",
        "      # Apply the mask to the forest binary array and export\n",
        "      fb_array_sieved = forest_binary_array * fb_array_mask\n",
        "      export_array_as_tif(fb_array_sieved, forest_binary_path)\n",
        "\n",
        "  # Disturbance binary\n",
        "  if 'DisruptionObs' in resampled_raster or 'Ndisturb' in resampled_raster:\n",
        "    disturbance_binary_path = join(binary_dir, f\"disturbance_binary_{year}.tif\")\n",
        "    if not exists(disturbance_binary_path):\n",
        "      ac_raster_path = glob.glob(f\"{resampled_dir}/*AnnualChanges*{year}*\")\n",
        "      ac_array = gdal.Open(ac_raster_path[0]).ReadAsArray()\n",
        "      do_raster_path = join(resampled_dir, resampled_raster)\n",
        "      do_array = gdal.Open(do_raster_path).ReadAsArray()\n",
        "      # Set all disruption events to '1' if they're not classed as undisturbed forest or water in AnnualChanges\n",
        "      disturbance_binary_array = np.where((do_array >= 1) & ((ac_array != 1) & (ac_array != 5)), 1, 0)\n",
        "      export_array_as_tif(disturbance_binary_array, disturbance_binary_path)\n",
        "\n",
        "  # Update binary progress\n",
        "  binary_progress_index += 1\n",
        "  binary_progress_label.value = f\"Binary progress: {binary_progress_index}/{len(binary_list)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHmFlgGylGFL"
      },
      "outputs": [],
      "source": [
        "# Extract mangrove binary (optional)\n",
        "extract_mangrove_binary = True\n",
        "\n",
        "if extract_mangrove_binary:\n",
        "  mangrove_binary_path = join(binary_dir, \"mangrove_binary.tif\")\n",
        "  if not exists(mangrove_binary_path):\n",
        "    # Open subtypes array\n",
        "    for resampled_raster in os.listdir(resampled_dir):\n",
        "      if 'Subtypes' in resampled_raster:\n",
        "        subtypes_raster_path = join(resampled_dir, resampled_raster)\n",
        "        subtypes_raster_array = gdal.Open(subtypes_raster_path).ReadAsArray()\n",
        "\n",
        "    # Open oldest available forest binary raster for full mangrove extent (1990)\n",
        "    forest_binary_1990_path = join(binary_dir, \"forest_binary_1990.tif\")\n",
        "    forest_binary_1990_array = gdal.Open(forest_binary_1990_path).ReadAsArray()\n",
        "\n",
        "    mangrove_binary_array = np.logical_and(forest_binary_1990_array == 1,\n",
        "        np.logical_or(subtypes_raster_array == 12,(subtypes_raster_array >= 61) & (subtypes_raster_array <= 69))\n",
        "    )\n",
        "\n",
        "    # Calculate the percentage of forest pixels that are mangrove\n",
        "    forest_1990_pixels = np.sum(forest_binary_1990_array)\n",
        "    mangrove_pixels = np.sum(mangrove_binary_array)\n",
        "    if mangrove_pixels > 0:\n",
        "      mangrove_percent = (mangrove_pixels / forest_1990_pixels) * 100\n",
        "      export_array_as_tif(mangrove_binary_array, mangrove_binary_path)\n",
        "      print(f\"Number of mangrove pixels: {mangrove_pixels}\")\n",
        "      print(f\"Percentage of 1990 forest pixels that are mangrove: {mangrove_percent:.2f}%\")\n",
        "    else: print(\"There are no mangrove pixels in the template area.\")\n",
        "  else: print(\"A mangrove binary raster already exists. Delete it to generate a new one.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJnV0l75FpPN"
      },
      "source": [
        "# LU polygon binary features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vigY_9XqXhCQ"
      },
      "outputs": [],
      "source": [
        "# Selected 'land use' polygons.\n",
        "# Creating a 'complete recovery' or 'complete restoration' scenario requires ONE of these as a proxy.\n",
        "# This can be multiple combined PAs / polygons that have no or minimal history of human disturbance.\n",
        "\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area.gpkg', 'project_area_buffered_bbox.gpkg', 'gedi_area.gpkg', 'project_area_inverse.gpkg', 'gedi_area_inverse.gpkg']\n",
        "print(\"lu_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    print(f\"'{polygon}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w62_C-Vt0Emj"
      },
      "outputs": [],
      "source": [
        "lu_polygons = [\n",
        "# 'peninsular_malaysia.gpkg',\n",
        "'lu_yong.gpkg',\n",
        "'lu_yong_lipis.gpkg',\n",
        "'lu_berkelah_jerantut.gpkg',\n",
        "'lu_tekai_tembeling.gpkg',\n",
        "'lu_ais.gpkg',\n",
        "'lu_tekam.gpkg',\n",
        "'lu_berkelah_temerloh.gpkg',\n",
        "'lu_remen_chereh.gpkg',\n",
        "'lu_berkelah_kuantan.gpkg',\n",
        "# 'forest_reserves.gpkg',\n",
        "# 'forest_reserves_inverse.gpkg',\n",
        "'lu_old-growth_protected_areas.gpkg',\n",
        "]\n",
        "\n",
        "# Convert all template values to 'nodata' in preparation\n",
        "template_tif = gdal.Open(template_dir)\n",
        "template_mask_array = gdal.Open(template_dir).ReadAsArray()\n",
        "template_mask_array[template_mask_array != None] = 0\n",
        "\n",
        "for lu_polygon in lu_polygons:\n",
        "  lu_binary_name = f\"{lu_polygon[:-5]}_binary.tif\"\n",
        "  lu_binary_path = join(binary_dir, lu_binary_name)\n",
        "  if not exists(lu_binary_path):\n",
        "    lu_polygon_path = join(polygons_dir, lu_polygon)\n",
        "    export_array_as_tif(template_mask_array, lu_binary_path)\n",
        "    # Burn the value '1' where it overlaps with the project area polygon\n",
        "    burn_polygon_to_raster(lu_binary_path, lu_polygon_path, fixed=True, fixed_value=1, all_touched=False)\n",
        "    print(f\"{lu_binary_name} has been created.\")\n",
        "  else: print(f\"{lu_binary_name} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKSLJnR4SE5H"
      },
      "source": [
        "# Binary masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3rRcG0_ZOdd"
      },
      "outputs": [],
      "source": [
        "# Generate masks for later scenario predictions, e.g. so outputs only show forest.\n",
        "mask_type_list = []\n",
        "for binary in os.listdir(binary_dir):\n",
        "    mask_type = binary.split('_')[0]\n",
        "    if mask_type not in mask_type_list:\n",
        "        mask_type_list.append(mask_type)\n",
        "\n",
        "print(\"mask_types = [\")\n",
        "for mask_type in mask_type_list:\n",
        "    print(f\"'{mask_type}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F0klPciSEHh"
      },
      "outputs": [],
      "source": [
        "mask_types = [\n",
        "'forest',\n",
        "# 'lu',\n",
        "# 'disturbance',\n",
        "]\n",
        "\n",
        "# Create list of binary rasters to mask\n",
        "binary_mask_list = []\n",
        "for mask_type in mask_types:\n",
        "  for binary in os.listdir(binary_dir):\n",
        "    if mask_type in binary:\n",
        "      binary_mask_list.append(binary)\n",
        "\n",
        "# Binary progress\n",
        "mask_progress_index = 0\n",
        "mask_progress_label = widgets.Label(f\"Binary progress: {mask_progress_index}/{len(binary_mask_list)}\")\n",
        "display(mask_progress_label)\n",
        "\n",
        "# Create masks from the selected binary raster type\n",
        "for mask_type in mask_types:\n",
        "  for binary in binary_mask_list:\n",
        "    binary_path = join(binary_dir, binary)\n",
        "    try: year = str(int(binary[-8:-4])) # Check for year\n",
        "    except: year = None\n",
        "    mask_raster_path = join(scenario_mask_dir, f\"mask_{mask_type}_{year}.tif\")\n",
        "    if not exists(mask_raster_path):\n",
        "        binary_raster = gdal.Open(binary_path)\n",
        "        binary_array = gdal.Open(binary_path).ReadAsArray()\n",
        "        mask_array = np.where(binary_array == 0, nodatavalue, 1)\n",
        "        export_array_as_tif(mask_array, mask_raster_path)\n",
        "        print(f\"A mask raster has been created: {mask_raster_path}\")\n",
        "    else: print(f\"A mask raster already exists at: {mask_raster_path}\")\n",
        "    # Update mask progress\n",
        "    mask_progress_index += 1\n",
        "    mask_progress_label.value = f\"Binary progress: {mask_progress_index}/{len(binary_mask_list)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCv0mNkb2pIY"
      },
      "source": [
        "# Binary feature edge effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqkninr4qy-g"
      },
      "outputs": [],
      "source": [
        "# Edge effect feature generation for binary rasters\n",
        "# Adds spatial awareness to tabular machine learning algorithms (e.g. XGBoost)\n",
        "# without performance and optimisation issues of deep learning models (e.g. CNN)\n",
        "# Encodes euclidean distance bands as integers and local density as decimals\n",
        "# Positive integers for binary 1, negative for binary 0\n",
        "# Addresses satellite geolocation uncertainty and ecological edge effects\n",
        "\n",
        "# Configuration\n",
        "target_limit = 120  # metres edge effect distance\n",
        "\n",
        "# Determine pixel size from cell size rasters\n",
        "cell_size_x = gdal.Open(join(areas_dir, 'cell_size_x.tif')).ReadAsArray()\n",
        "cell_size_y = gdal.Open(join(areas_dir, 'cell_size_y.tif')).ReadAsArray()\n",
        "cell_area = np.mean([np.mean(cell_size_x), np.mean(cell_size_y)])  # metres per pixel\n",
        "\n",
        "# Calculate maximum pixel distance\n",
        "# Include any pixel whose closest point (edge or corner) is within target_limit\n",
        "# For a square pixel with side length cell_area:\n",
        "# - distance from center to edge midpoint = cell_area/2\n",
        "# - distance from center to corner = sqrt(2) * cell_area/2 (diagonal of half-square)\n",
        "# A pixel at grid distance d has its center at d * cell_area metres\n",
        "# Its closest corner is at (d * cell_area) - sqrt(2) * cell_area/2\n",
        "# Include pixel if: (d * cell_area) - sqrt(2) * cell_area/2 <= target_limit\n",
        "# Rearranging: d <= (target_limit + sqrt(2) * cell_area/2) / cell_area\n",
        "corner_offset = np.sqrt(2) * cell_area / 2\n",
        "max_pixel_distance = (target_limit + corner_offset) / cell_area\n",
        "\n",
        "# Find all unique euclidean distances within range\n",
        "distances = set()\n",
        "max_dist_int = int(np.ceil(max_pixel_distance))\n",
        "for i in range(-max_dist_int, max_dist_int + 1):\n",
        "    for j in range(-max_dist_int, max_dist_int + 1):\n",
        "        dist = np.sqrt(i**2 + j**2)\n",
        "        if dist <= max_pixel_distance:\n",
        "            distances.add(dist)\n",
        "\n",
        "sorted_distances = sorted(distances)\n",
        "num_bands = len(sorted_distances) - 1  # exclude 0 for counting bands\n",
        "\n",
        "# Gaussian kernel standard deviation\n",
        "# Setting max_pixel_distance = 1.96 * sigma captures ~95% of kernel weight\n",
        "# Minimizes spillover beyond ecological boundary while preserving variance\n",
        "gaussian_stdev = max_pixel_distance / 1.96\n",
        "# Kernel radius uses ceiling to include all pixels that might influence smoothing\n",
        "# Slight mismatch with distance bands is ecologically valid: pixels just beyond\n",
        "# boundary receive small smoothed values but no distance band\n",
        "kernel_radius = int(np.ceil(max_pixel_distance))\n",
        "kernel_size = 2 * kernel_radius + 1\n",
        "\n",
        "# Create normalised kernel (sums to 1)\n",
        "# Normalisation ensures consistent smoothing regardless of kernel truncation\n",
        "kernel = Gaussian2DKernel(x_stddev=gaussian_stdev, y_stddev=gaussian_stdev,\n",
        "                          x_size=kernel_size, y_size=kernel_size)\n",
        "\n",
        "precision = 2\n",
        "\n",
        "binary_list = []\n",
        "for binary_raster in os.listdir(binary_dir) + os.listdir(resampled_dir):\n",
        "    if \"binary\" in binary_raster:\n",
        "        binary_list.append(binary_raster)\n",
        "\n",
        "edge_effect_progress_index = 0\n",
        "edge_effect_progress_label = widgets.Label(f\"Edge effect progress: {edge_effect_progress_index}/{len(binary_list)}\")\n",
        "display(edge_effect_progress_label)\n",
        "\n",
        "for binary_raster in binary_list:\n",
        "    if \"binary\" in binary_raster:\n",
        "        edge_effects_filename = binary_raster.replace('binary', 'with_edge_effects')\n",
        "        edge_effects_path = join(edge_effects_dir, edge_effects_filename)\n",
        "        if not exists(edge_effects_path):\n",
        "            binary_raster_path = join(binary_dir, binary_raster)\n",
        "            if not exists(binary_raster_path):\n",
        "                binary_raster_path = join(resampled_dir, binary_raster)\n",
        "            binary_array = gdal.Open(binary_raster_path).ReadAsArray()\n",
        "\n",
        "            # Calculate euclidean distance transforms\n",
        "            # Distance from nearest 1-valued pixel for all pixels\n",
        "            dist_from_ones = distance_transform_edt(binary_array == 0)\n",
        "            # Distance from nearest 0-valued pixel for all pixels\n",
        "            dist_from_zeros = distance_transform_edt(binary_array == 1)\n",
        "\n",
        "            # Initialise integer bands array\n",
        "            integer_bands = np.zeros_like(binary_array)\n",
        "\n",
        "            # Assign bands for class 1 pixels\n",
        "            class_one_mask = binary_array == 1\n",
        "            for i, dist in enumerate(sorted_distances):\n",
        "                # Determine band boundaries using midpoints between unique distances\n",
        "                if i == 0:\n",
        "                    lower = -0.01  # include exact 0\n",
        "                    upper = (sorted_distances[0] + sorted_distances[1]) / 2 if len(sorted_distances) > 1 else 0.5\n",
        "                elif i == len(sorted_distances) - 1:\n",
        "                    lower = (sorted_distances[i-1] + dist) / 2\n",
        "                    upper = max_pixel_distance + 0.01\n",
        "                else:\n",
        "                    lower = (sorted_distances[i-1] + dist) / 2\n",
        "                    upper = (dist + sorted_distances[i+1]) / 2\n",
        "\n",
        "                band_mask = class_one_mask & (dist_from_zeros > lower) & (dist_from_zeros <= upper)\n",
        "                integer_bands[band_mask] = i  # 0, 1, 2, 3...\n",
        "\n",
        "            # Pixels beyond ecological boundary become background\n",
        "            beyond_mask = class_one_mask & (dist_from_zeros > max_pixel_distance)\n",
        "            integer_bands[beyond_mask] = num_bands\n",
        "\n",
        "            # Assign bands for class 0 pixels\n",
        "            class_zero_mask = binary_array == 0\n",
        "            for i, dist in enumerate(sorted_distances):\n",
        "                # Determine band boundaries using midpoints between unique distances\n",
        "                if i == 0:\n",
        "                    lower = -0.01  # include exact 0\n",
        "                    upper = (sorted_distances[0] + sorted_distances[1]) / 2 if len(sorted_distances) > 1 else 0.5\n",
        "                elif i == len(sorted_distances) - 1:\n",
        "                    lower = (sorted_distances[i-1] + dist) / 2\n",
        "                    upper = max_pixel_distance + 0.01\n",
        "                else:\n",
        "                    lower = (sorted_distances[i-1] + dist) / 2\n",
        "                    upper = (dist + sorted_distances[i+1]) / 2\n",
        "\n",
        "                band_mask = class_zero_mask & (dist_from_ones > lower) & (dist_from_ones <= upper)\n",
        "                # Use -i for consistency with positive bands (both start at 1)\n",
        "                if i == 0:\n",
        "                    continue  # skip distance 0 as pixels can't be at distance 0 from opposite class\n",
        "                else:\n",
        "                    integer_bands[band_mask] = -i  # -1, -2, -3...\n",
        "\n",
        "            # Pixels beyond ecological boundary become background\n",
        "            beyond_mask = class_zero_mask & (dist_from_ones > max_pixel_distance)\n",
        "            integer_bands[beyond_mask] = -num_bands\n",
        "\n",
        "            # Apply gaussian smoothing\n",
        "            # Kernel may reach pixels beyond max_pixel_distance due to ceiling operation\n",
        "            # Ecologically valid: represents weak spatial influence beyond direct edge effects\n",
        "            binary_smoothed = convolve(binary_array.astype(float), kernel, boundary='extend')\n",
        "\n",
        "            # Scale smoothed values from 0.01 to 0.99\n",
        "            # Ensures all pixels within boundary have decimal component for local density\n",
        "            decimal_component = 0.01 + (binary_smoothed * 0.98)\n",
        "\n",
        "            # Identify pixels beyond edge effect distance\n",
        "            pure_integer_mask = (integer_bands == num_bands) | (integer_bands == -num_bands)\n",
        "\n",
        "            # Combine integer bands with decimal weights\n",
        "            # Add for positive integers, subtract for negative to preserve bands\n",
        "            edge_effects_array = integer_bands.astype(float)\n",
        "            positive_mask = integer_bands >= 0\n",
        "            negative_mask = integer_bands < 0\n",
        "\n",
        "            edge_effects_array[positive_mask & ~pure_integer_mask] += decimal_component[positive_mask & ~pure_integer_mask]\n",
        "            edge_effects_array[negative_mask & ~pure_integer_mask] -= decimal_component[negative_mask & ~pure_integer_mask]\n",
        "\n",
        "            # Round to specified precision\n",
        "            edge_effects_array = np.round(edge_effects_array, precision)\n",
        "\n",
        "            # Restore pure integer values\n",
        "            edge_effects_array[pure_integer_mask] = integer_bands[pure_integer_mask]\n",
        "\n",
        "            export_array_as_tif(edge_effects_array, edge_effects_path)\n",
        "\n",
        "    edge_effect_progress_index += 1\n",
        "    edge_effect_progress_label.value = f\"Edge effect progress: {edge_effect_progress_index}/{len(binary_list)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuous feature precision"
      ],
      "metadata": {
        "id": "mPSRn0UB-8TI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D35niESIwR9"
      },
      "outputs": [],
      "source": [
        "# Creates a dictionary of optiomal precision based on number of desired unique values.\n",
        "# Limiting the number unique values avoids overfitting and reduces training time.\n",
        "\n",
        "continuous_features = False\n",
        "\n",
        "override_max_unique_values = False\n",
        "max_unique_values = 5000 # Should be >=10\n",
        "\n",
        "if continuous_features:\n",
        "  if override_max_unique_values == False:\n",
        "    dem_base_path = join(areas_dir, \"base_dem.tif\")\n",
        "    dem_base_array = gdal.Open(dem_base_path).ReadAsArray()\n",
        "    max_unique_values = int(np.ptp(dem_base_array)) # Precision based on elevation variance\n",
        "  resampled_precision_dict = {}\n",
        "\n",
        "  for resampled_feature, resample_type in selected_original_rasters.items():\n",
        "    if resample_type == 'continuous':\n",
        "      resampled_feature_path = join(resampled_dir, resampled_feature)\n",
        "      print(f\"Reading {resampled_feature}...\")\n",
        "      # Read raster as array\n",
        "      resampled_feature_array = gdal.Open(resampled_feature_path).ReadAsArray()\n",
        "      # Convert 'nodata' values to nan\n",
        "      resampled_feature_array[resampled_feature_array == nodatavalue] = np.nan\n",
        "      resampled_feature_array_masked = np.ma.array(resampled_feature_array, mask=np.isnan(resampled_feature_array))\n",
        "      # Count unique values in raster\n",
        "      unique_values = len(np.unique(resampled_feature_array_masked))\n",
        "      print(f\"There are {unique_values} unique values in {resampled_feature}\")\n",
        "      # Generate histogram from 100,000 random points\n",
        "      random_selection = np.random.choice(resampled_feature_array_masked.ravel(), size = 100_000, replace = False)\n",
        "      _ = plt.hist(random_selection, bins='auto')  # arguments are passed to np.histogram\n",
        "      plt.title(f\"{resampled_feature}\")\n",
        "      plt.show()\n",
        "      # Remove 0 values for log10\n",
        "      resampled_feature_array_masked[resampled_feature_array_masked == 0] = np.nan\n",
        "      resampled_feature_array_masked = np.ma.array(resampled_feature_array, mask=np.isnan(resampled_feature_array))\n",
        "      # Create log10 array for determining positions for rounding\n",
        "      array_log10 = np.log10(abs(resampled_feature_array_masked))\n",
        "      place_value_decimal = int(abs(np.min(array_log10)))\n",
        "      place_value_integer = int(0 - np.max(array_log10))\n",
        "      # Iterate down precision levels to determine optimal number of unique values\n",
        "      min_starting_precision = len(str(max_unique_values))\n",
        "      for precision in reversed(range(place_value_integer, max(min_starting_precision, place_value_decimal +1))):\n",
        "        rounded_array = np.round(resampled_feature_array, decimals=precision)\n",
        "        round_unique_values = len(np.unique(rounded_array))\n",
        "        optimal_precision = None\n",
        "        if round_unique_values <= max_unique_values:\n",
        "          optimal_precision = precision\n",
        "          print(f\"The optimal precison for {resampled_feature} is {optimal_precision}, with {round_unique_values} unique values.\")\n",
        "          resampled_precision_dict.update({f'{resampled_feature}':f'{optimal_precision}'})\n",
        "          break\n",
        "      if optimal_precision == None: print(\"There's a problem with setting precision.\")\n",
        "      print(\"___________________\\n\")\n",
        "\n",
        "  print(\"Dictionary for optimal rounding values:\")\n",
        "  resampled_precision_dict\n",
        "\n",
        "  precision_dict_csv_path = join(resampled_dir, 'rounding_dictionary.csv')\n",
        "  # Save rounding dictionary to CSV\n",
        "  with open(precision_dict_csv_path, 'w', newline='') as precision_dict_csv:\n",
        "      writer = csv.writer(precision_dict_csv)\n",
        "      writer.writerow(resampled_precision_dict.keys())\n",
        "      writer.writerow(resampled_precision_dict.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UjcjQ52NoT7"
      },
      "outputs": [],
      "source": [
        "if continuous_features:\n",
        "  # Open rounding dictionary and verify\n",
        "  with open(precision_dict_csv_path, 'r') as file:\n",
        "      keys, values = list(csv.reader(file))\n",
        "      topo_precision_dict = dict(zip(keys, values))\n",
        "\n",
        "  # Verify precision and correct if necessary\n",
        "  print(\"topo_precision_dict = {\")\n",
        "  for key, value in topo_precision_dict.items():\n",
        "      print(f'\"{key}\": {value},')\n",
        "  print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2kquWfYOeS-"
      },
      "outputs": [],
      "source": [
        "topo_precision_dict = {\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn-nfKEPOi9d"
      },
      "outputs": [],
      "source": [
        "# Creates both unsmoothed and smoothed feature versions\n",
        "# Adds spatial awareness to tabular machine learning algorithms (e.g. XGBoost)\n",
        "# Without performance and optimisation issues of deep learning models (e.g. CNN)\n",
        "# Smoothed version captures local patterns and gradients within ecological context\n",
        "# Also helps account for satellite geolocation inaccuracies\n",
        "\n",
        "# Configuration\n",
        "target_limit = 120  # metres for spatial context radius\n",
        "\n",
        "# Determine pixel size from cell size rasters\n",
        "cell_size_x = gdal.Open(join(areas_dir, 'cell_size_x.tif')).ReadAsArray()\n",
        "cell_size_y = gdal.Open(join(areas_dir, 'cell_size_y.tif')).ReadAsArray()\n",
        "cell_area = np.mean([np.mean(cell_size_x), np.mean(cell_size_y)])  # metres per pixel\n",
        "\n",
        "# Calculate maximum pixel distance for spatial context\n",
        "# Include any pixel whose closest point (edge or corner) is within target_limit\n",
        "# For a square pixel with side length cell_area:\n",
        "# - distance from center to edge midpoint = cell_area/2\n",
        "# - distance from center to corner = sqrt(2) * cell_area/2 (diagonal of half-square)\n",
        "# A pixel at grid distance d has its center at d * cell_area metres\n",
        "# Its closest corner is at (d * cell_area) - sqrt(2) * cell_area/2\n",
        "# Include pixel if: (d * cell_area) - sqrt(2) * cell_area/2 <= target_limit\n",
        "# Rearranging: d <= (target_limit + sqrt(2) * cell_area/2) / cell_area\n",
        "corner_offset = np.sqrt(2) * cell_area / 2\n",
        "max_pixel_distance = (target_limit + corner_offset) / cell_area\n",
        "\n",
        "# Gaussian kernel standard deviation\n",
        "# Setting max_pixel_distance = 1.96 * sigma captures ~95% of kernel weight\n",
        "# Ensures spatial influence decays to near-zero at target_limit boundary\n",
        "# Preserves meaningful variance across the spatial context zone\n",
        "gaussian_stdev = max_pixel_distance / 1.96\n",
        "# Kernel radius uses ceiling to include all pixels within spatial influence\n",
        "kernel_radius = int(np.ceil(max_pixel_distance))\n",
        "kernel_size = 2 * kernel_radius + 1\n",
        "\n",
        "# Create normalised kernel (sums to 1)\n",
        "# Normalisation ensures consistent smoothing regardless of kernel truncation\n",
        "# Provides weighted average of surrounding pixel values\n",
        "kernel = Gaussian2DKernel(x_stddev=gaussian_stdev, y_stddev=gaussian_stdev,\n",
        "                          x_size=kernel_size, y_size=kernel_size)\n",
        "\n",
        "# Continuous progress\n",
        "continuous_progress_index = 0\n",
        "continuous_progress_label = widgets.Label(f\"Continuous progress: {continuous_progress_index}/{len(topo_precision_dict.items())}\")\n",
        "display(continuous_progress_label)\n",
        "\n",
        "# Iterate over selected continuous rasters\n",
        "for continuous, precision in topo_precision_dict.items():\n",
        "    cont_raster_resampled_path = join(resampled_dir, continuous)\n",
        "    cont_raster_resampled_array = gdal.Open(cont_raster_resampled_path).ReadAsArray()\n",
        "    # Convert nodata values to 0\n",
        "    cont_raster_resampled_array[cont_raster_resampled_array == nodatavalue] = 0\n",
        "\n",
        "    # Set path and check if exists\n",
        "    cont_raster_unsmoothed_filename = f\"{continuous[:-4]}_unsmooth.tif\"\n",
        "    cont_raster_unsmoothed_path = join(continuous_final_dir, cont_raster_unsmoothed_filename)\n",
        "    if not exists(cont_raster_unsmoothed_path):\n",
        "        # Round and export unsmoothed continuous raster\n",
        "        # Preserves original values at pixel level for model comparison\n",
        "        cont_raster_unsmoothed_rounded = np.round(cont_raster_resampled_array, decimals=int(precision))\n",
        "        export_array_as_tif(cont_raster_unsmoothed_rounded, cont_raster_unsmoothed_path)\n",
        "\n",
        "    # Smooth using 2d spatial convolution\n",
        "    cont_raster_smoothed_filename = f\"{continuous[:-4]}_smooth.tif\"\n",
        "    cont_raster_smoothed_path = join(continuous_final_dir, cont_raster_smoothed_filename)\n",
        "    if not exists(cont_raster_smoothed_path):\n",
        "        # Apply gaussian smoothing via 2d spatial convolution\n",
        "        # Creates spatially-aware version capturing local patterns and gradients\n",
        "        # Model receives both smoothed and unsmoothed versions for enhanced feature learning\n",
        "        # Boundary='extend' prevents edge artifacts by extending edge values\n",
        "        cont_raster_smoothed = convolve(cont_raster_resampled_array, kernel, boundary='extend')\n",
        "        # Round and export smoothed continuous raster\n",
        "        # Precision maintained from original dictionary specification\n",
        "        cont_raster_smoothed_rounded = np.round(cont_raster_smoothed, decimals=int(precision))\n",
        "        export_array_as_tif(cont_raster_smoothed_rounded, cont_raster_smoothed_path)\n",
        "\n",
        "    # Update continuous progress\n",
        "    continuous_progress_index += 1\n",
        "    continuous_progress_label.value = f\"Continuous progress: {continuous_progress_index}/{len(topo_precision_dict.items())}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6r7JXbijM50"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU_AC6MjNfTN"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-yk8CnJRCYVQ",
        "kSuevzw7xyil",
        "XIBbo-gsS3nD",
        "Ya6zMBMdbFPR",
        "5-UOVsvRuHNS",
        "EhukvPP02gBm",
        "XJnV0l75FpPN",
        "yKSLJnR4SE5H"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}