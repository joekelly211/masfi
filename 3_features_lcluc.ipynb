{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/dev/3_features_lcluc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yk8CnJRCYVQ"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5USlWSaxqv9Y"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOfy6gWFwHEC"
      },
      "outputs": [],
      "source": [
        "# Installs\n",
        "%%capture\n",
        "!pip install astropy\n",
        "!pip install earthengine-api\n",
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTHaRSv8wICv"
      },
      "outputs": [],
      "source": [
        "!# Reload imports, replacing those in the cache\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "import csv\n",
        "import ee\n",
        "import geopandas as gpd\n",
        "import glob\n",
        "from google.colab import runtime, userdata\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from os import makedirs, remove\n",
        "from os.path import exists, join\n",
        "from osgeo import gdal, gdalconst, ogr\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "from scipy.ndimage import label, sum as ndi_sum\n",
        "from shutil import copyfile, move\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMCq50kg36Br"
      },
      "outputs": [],
      "source": [
        "# 1_areas directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "template_dir = join(areas_dir, \"template.tif\")\n",
        "\n",
        "# 3_features directories\n",
        "features_dir = join(base_dir, \"3_features\")\n",
        "ee_dir = join(features_dir, \"earth_engine\")\n",
        "user_upload_dir = join(features_dir, \"user_upload\")\n",
        "glad_lcluc_dir = join(features_dir, 'glad_lcluc')\n",
        "resampled_dir = join(features_dir, \"resampled\")\n",
        "continuous_final_dir = join(features_dir, \"continuous_final\")\n",
        "binary_dir = join(features_dir, 'binary')\n",
        "edge_effects_dir = join(features_dir, 'binary_edge_effects')\n",
        "\n",
        "# 6_scenarios directories\n",
        "scenario_dir = join(base_dir, \"6_scenarios\")\n",
        "scenario_mask_dir = join(scenario_dir, \"scenario_masks\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(ee_dir, exist_ok=True)\n",
        "makedirs(user_upload_dir, exist_ok=True)\n",
        "makedirs(glad_lcluc_dir, exist_ok=True)\n",
        "makedirs(resampled_dir, exist_ok=True)\n",
        "makedirs(continuous_final_dir, exist_ok=True)\n",
        "makedirs(binary_dir, exist_ok=True)\n",
        "makedirs(edge_effects_dir, exist_ok=True)\n",
        "makedirs(scenario_dir, exist_ok=True)\n",
        "makedirs(scenario_mask_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhkgXF4foXhx"
      },
      "outputs": [],
      "source": [
        "# export_array_as_tif function\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_dir, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)\n",
        "\n",
        "# Burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "    vector = ogr.Open(polygon_path)\n",
        "    layer = vector.GetLayer()\n",
        "    if all_touched: options = [\"ALL_TOUCHED=TRUE\"]\n",
        "    else: options = []\n",
        "    if not fixed: options.append(f\"ATTRIBUTE={column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()}\")\n",
        "    gdal.RasterizeLayer(raster, [1], layer,\n",
        "                        burn_values=[fixed_value] if fixed else None,\n",
        "                        options=options)\n",
        "    raster.FlushCache()\n",
        "    raster = vector = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSuevzw7xyil"
      },
      "source": [
        "# Download Earth Engine rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_wg4IqHpFXT"
      },
      "outputs": [],
      "source": [
        "# Enable Google Earth Engine API at Google Cloud https://console.cloud.google.com/apis/dashboard\n",
        "# See here for walkthrough: https://github.com/googlecolab/colabtools/issues/4228#issuecomment-1859068706\n",
        "# Set project ID under 'secrets' tab on the left with the name 'google_cloud_project'\n",
        "ee_project = userdata.get('google_cloud_project')\n",
        "\n",
        "# Authenticate Earth Engine\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project=ee_project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddVd9lPMbLOL"
      },
      "outputs": [],
      "source": [
        "# Edit this section to change which Earth Engine datasets are downloaded.\n",
        "\n",
        "# Warning: Earth Engine uses 'nearest neighbour' to resample rasters to the desired extent and resolution before exporting.\n",
        "# This creates artifacts if the data is continuous, such as DEMs (elevation) or other topographic metrics.\n",
        "# These should be downloaded from the original source, uploaded to '/user_upload' and resampled in the next section, checking the option for 'bilinear'.\n",
        "\n",
        "# Check datasets in https://code.earthengine.google.com/ with:\n",
        "# var assetList = ee.data.listAssets(\"projects/JRC/TMF/v1_2022/\");\n",
        "# print(assetList);\n",
        "\n",
        "ee_datasets = [\n",
        "\n",
        "    {\n",
        "        \"ee_dataset_name\": \"tmf\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"projects/JRC/TMF/v1_2024/AnnualChanges\",\n",
        "            \"projects/JRC/TMF/v1_2024/TransitionMap_MainClasses\",\n",
        "            \"projects/JRC/TMF/v1_2024/TransitionMap_Subtypes\",\n",
        "            \"projects/JRC/TMF/v1_2024/AnnualDisruptionObs2024\",\n",
        "            \"projects/JRC/TMF/v1_2023/AnnualDisruptionObs2023\",\n",
        "            \"projects/JRC/TMF/v1_2023/Ndisturb_C2_1982_2022\",\n",
        "\n",
        "        ],\n",
        "    }\n",
        "    # {\n",
        "    #     \"ee_dataset_name\": \"glad\",\n",
        "    #     \"ee_dataset_type\": \"Image\",\n",
        "    #     \"ee_paths\": [\n",
        "    #                 # 'projects/glad/GLCLU2020/Forest_gain',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_disturbance',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_netgain',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_netloss',\n",
        "    #                 # 'projects/glad/GLCLU2020/Forest_loss',\n",
        "    #                 # 'projects/glad/GLCLU2020/Forest_type',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC_2000',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_gain',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_loss',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2020',\n",
        "    #                 # 'projects/glad/GLCLU2020/Water_dynamics',\n",
        "    #                 # 'projects/glad/GLCLU2020/Water_dynamics_classes',\n",
        "    #     ]\n",
        "    # }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIlnNYhQcb10"
      },
      "outputs": [],
      "source": [
        "# Verify Earth Engine rasters that will be downloaded\n",
        "ee_raster_list = []\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "    for ee_path in ee_paths:\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image = ee.ImageCollection(ee_path)\n",
        "            ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        else:\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "        for ee_band in ee_bands:\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_raster_list.append(ee_tif_filename)\n",
        "\n",
        "ee_raster_list = list(reversed(ee_raster_list))\n",
        "ee_raster_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original queue method"
      ],
      "metadata": {
        "id": "caWuFWxx22QT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qislgFIYUnqi"
      },
      "outputs": [],
      "source": [
        "# Earth Engine download progress\n",
        "ee_progress_index = 0\n",
        "ee_progress_label = widgets.Label(f\"Earth Engine download progress: {ee_progress_index}/{len(ee_raster_list)}\")\n",
        "display(ee_progress_label)\n",
        "\n",
        "# Load template and set Earth Engine geometry\n",
        "template_polygon_dir = join(polygons_dir, 'template.gpkg')\n",
        "template_area = gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0]\n",
        "template_coords = list(gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0].exterior.coords)\n",
        "ee_geometry = ee.Geometry.Polygon(template_coords)\n",
        "\n",
        "# Download Earth Engine datasets\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "    # Loop through Earth Engine paths\n",
        "    for ee_path in ee_paths:\n",
        "        # identify bands\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image = ee.ImageCollection(ee_path)\n",
        "            ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "        # Loop through bands\n",
        "        for ee_band in reversed(ee_bands):\n",
        "            # Set filename and directory of downloaded raster and check if exists\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_tif_dir = join(ee_dir, ee_tif_filename)\n",
        "            ee_temp_dir = join(\"/gdrive/MyDrive\", ee_tif_filename)\n",
        "            # Different temporary directory required if MyDrive is mounted\n",
        "            if base_dir.startswith('/content/drive/MyDrive/'):\n",
        "              ee_temp_dir = join(\"/content/drive/MyDrive/\", ee_tif_filename)\n",
        "            # Check if temporary raster exists and needs copying\n",
        "            if exists(ee_temp_dir):\n",
        "              copyfile(ee_temp_dir, ee_tif_dir)\n",
        "              remove(ee_temp_dir)\n",
        "            # Check if copied raster exists, and if not download from Earth Engine.\n",
        "            if not exists(ee_tif_dir):\n",
        "              if ee_dataset_type == 'ImageCollection':\n",
        "                image_selected = ee_image.qualityMosaic(ee_band).select([ee_band])\n",
        "                resolution = ee_image.first().projection().nominalScale().getInfo()\n",
        "              if ee_dataset_type == 'Image':\n",
        "                image_selected = ee_image.select([ee_band])\n",
        "                resolution = ee_image.select(0).projection().nominalScale().getInfo()\n",
        "              ee_task = ee.batch.Export.image.toDrive(image=image_selected.toFloat(),\n",
        "                                                    description=ee_tif_filename[:-4],\n",
        "                                                    scale=resolution,\n",
        "                                                    region=ee_geometry,\n",
        "                                                    maxPixels=10000000000,\n",
        "                                                    fileNamePrefix=ee_tif_filename[:-4],\n",
        "                                                    crs='EPSG:4326',\n",
        "                                                    fileFormat='GeoTIFF')\n",
        "              ee_task.start()\n",
        "              # Check whether the raster has downloaded yet\n",
        "              while not exists(ee_temp_dir):\n",
        "                  ee_task_status = ee_task.status()\n",
        "                  # If the task is completed, continue\n",
        "                  if ee_task_status[\"state\"] == 'COMPLETED': break\n",
        "                  # If it has failed or been cancelled, show an error\n",
        "                  elif ee_task_status['state'] == 'FAILED' or ee_task_status['state'] == 'CANCELLED':\n",
        "                      print(f\"{ee_tif_filename}:{ee_task_status['error_message']}\")\n",
        "                      try: remove(ee_temp_dir)\n",
        "                      except: pass\n",
        "                      break\n",
        "                  sleep(1)\n",
        "              # Copy the raster to intended directory and remove the temporary raster\n",
        "              while not exists(ee_temp_dir):\n",
        "                sleep(1)\n",
        "              copyfile(ee_temp_dir, ee_tif_dir)\n",
        "              remove(ee_temp_dir)\n",
        "            # Update Earth Engine download progress\n",
        "            ee_progress_index += 1\n",
        "            ee_progress_label.value = f\"Earth Engine download progress: {ee_progress_index}/{len(ee_raster_list)}\"\n",
        "\n",
        "# Check Earth Engine tasks here: https://code.earthengine.google.com/tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhanced queue method"
      ],
      "metadata": {
        "id": "0zwwDicI25Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum concurrent tasks in Earth Engine\n",
        "ee_max_concurrent_tasks = 30\n",
        "\n",
        "# Load template and set Earth Engine geometry\n",
        "template_polygon_dir = join(polygons_dir, 'template.gpkg')\n",
        "template_area = gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0]\n",
        "template_coords = list(gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0].exterior.coords)\n",
        "ee_geometry = ee.Geometry.Polygon(template_coords)\n",
        "\n",
        "# Create a dictionary of all rasters to download\n",
        "raster_dictionary = {}\n",
        "\n",
        "# Populate the dictionary with information about each raster\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "\n",
        "    for ee_path in ee_paths:\n",
        "        # Identify bands\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image = ee.ImageCollection(ee_path)\n",
        "            ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "\n",
        "        # Loop through bands and create entries in dictionary\n",
        "        for ee_band in ee_bands:\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_tif_dir = join(ee_dir, ee_tif_filename)\n",
        "\n",
        "            # Different temporary directory required if MyDrive is mounted\n",
        "            if base_dir.startswith('/content/drive/MyDrive/'):\n",
        "                ee_temp_dir = join(\"/content/drive/MyDrive/\", ee_tif_filename)\n",
        "            else:\n",
        "                ee_temp_dir = join(\"/gdrive/MyDrive\", ee_tif_filename)\n",
        "\n",
        "            description = ee_tif_filename[:-4]\n",
        "\n",
        "            raster_dictionary[description] = {\n",
        "                'ee_dataset_type': ee_dataset_type,\n",
        "                'ee_path': ee_path,\n",
        "                'ee_band': ee_band,\n",
        "                'image_path': ee_tif_dir,\n",
        "                'image_path_temp': ee_temp_dir,\n",
        "                'image_description': description,\n",
        "                'image_status': '',\n",
        "                'ee_task_id': '',\n",
        "                'ee_task': None,\n",
        "                'task_current_execution': False\n",
        "            }\n",
        "\n",
        "# Count total number of rasters\n",
        "raster_number = len(raster_dictionary)\n",
        "\n",
        "# Progress widgets\n",
        "ee_counted_tasks = set()\n",
        "ee_task_progress_index = 0\n",
        "ee_task_progress_label = widgets.Label(\n",
        "    f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        ")\n",
        "display(ee_task_progress_label)\n",
        "\n",
        "raster_progress_index = 0\n",
        "raster_progress_label = widgets.Label(\n",
        "    f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        ")\n",
        "display(raster_progress_label)\n",
        "\n",
        "# Initialize per-raster monitoring fields\n",
        "for raster_info in raster_dictionary.values():\n",
        "    # 1) move any stray temp file into downloads/\n",
        "    if exists(raster_info['image_path_temp']):\n",
        "        move(raster_info['image_path_temp'], raster_info['image_path'])\n",
        "\n",
        "    # 2) if the final TIFF now exists, mark it processed\n",
        "    if exists(raster_info['image_path']):\n",
        "        raster_info.update({\n",
        "            'image_status': 'processed',\n",
        "            'ee_task_id': '',\n",
        "            'ee_task': None,\n",
        "            'task_current_execution': False\n",
        "        })\n",
        "        # count it once for the EE-task progress bar\n",
        "        ee_counted_tasks.add(raster_info['image_description'])\n",
        "        ee_task_progress_index += 1\n",
        "        ee_task_progress_label.value = (\n",
        "            f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        "        )\n",
        "        raster_progress_index += 1\n",
        "        raster_progress_label.value = (\n",
        "            f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        "        )\n",
        "    else:\n",
        "        # 3) otherwise clear any state so it'll be re-queued\n",
        "        raster_info.update({\n",
        "            'image_status': '',\n",
        "            'ee_task_id': '',\n",
        "            'ee_task': None,\n",
        "            'task_current_execution': False\n",
        "        })\n",
        "        ee_counted_tasks.discard(raster_info['image_description'])\n",
        "\n",
        "# Detect tasks that were already running before this session\n",
        "ee_current_task_count = 0\n",
        "for task in ee.batch.Task.list():\n",
        "    task_state = task.status()['state']\n",
        "    task_id = task.id\n",
        "    task_description = task.config['description']\n",
        "\n",
        "    if task_state in ['READY', 'RUNNING', 'QUEUED']:\n",
        "        ee_current_task_count += 1\n",
        "        for v in raster_dictionary.values():\n",
        "            if v['image_description'] == task_description:\n",
        "                v.update({\n",
        "                    'image_status': 'task',\n",
        "                    'ee_task_id': task_id,\n",
        "                    'ee_task': task,\n",
        "                    'task_current_execution': True\n",
        "                })\n",
        "                if task_description not in ee_counted_tasks:\n",
        "                    ee_counted_tasks.add(task_description)\n",
        "                    ee_task_progress_index += 1\n",
        "                    ee_task_progress_label.value = (\n",
        "                        f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        "                    )\n",
        "                break\n",
        "\n",
        "# Main processing loop\n",
        "while True:\n",
        "    # Re-count active EE tasks each pass\n",
        "    active_states = ['READY', 'RUNNING', 'QUEUED']\n",
        "    ee_current_task_count = len([t for t in ee.batch.Task.list()\n",
        "                              if t.status()['state'] in active_states])\n",
        "\n",
        "    # Break when every raster is either processed or failed\n",
        "    if all(v['image_status'] in ['processed', 'failed']\n",
        "           for v in raster_dictionary.values()):\n",
        "        break\n",
        "\n",
        "    # Iterate over rasters\n",
        "    for raster_info in raster_dictionary.values():\n",
        "        # Skip finished / failed rasters\n",
        "        if raster_info['image_status'] in ['processed', 'failed']:\n",
        "            continue\n",
        "\n",
        "        # Final file already exists\n",
        "        if exists(raster_info['image_path']):\n",
        "            raster_info.update({\n",
        "                'image_status': 'processed',\n",
        "                'ee_task_id': '',\n",
        "                'ee_task': None,\n",
        "                'task_current_execution': False\n",
        "            })\n",
        "\n",
        "            if raster_info['image_description'] not in ee_counted_tasks:\n",
        "                ee_counted_tasks.add(raster_info['image_description'])\n",
        "                ee_task_progress_index += 1\n",
        "                ee_task_progress_label.value = (\n",
        "                    f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        "                )\n",
        "\n",
        "            raster_progress_index += 1\n",
        "            raster_progress_label.value = (\n",
        "                f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # Temporary file exists – move to downloads directory\n",
        "        if exists(raster_info['image_path_temp']):\n",
        "            move(raster_info['image_path_temp'], raster_info['image_path'])\n",
        "\n",
        "            raster_info.update({\n",
        "                'image_status': 'processed',\n",
        "                'ee_task_id': '',\n",
        "                'ee_task': None,\n",
        "                'task_current_execution': False\n",
        "            })\n",
        "\n",
        "            if raster_info['image_description'] not in ee_counted_tasks:\n",
        "                ee_counted_tasks.add(raster_info['image_description'])\n",
        "                ee_task_progress_index += 1\n",
        "                ee_task_progress_label.value = (\n",
        "                    f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        "                )\n",
        "\n",
        "            raster_progress_index += 1\n",
        "            raster_progress_label.value = (\n",
        "                f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # Task is running – poll its status\n",
        "        if raster_info['image_status'] == 'task':\n",
        "            ee_task = raster_info['ee_task']\n",
        "            if ee_task is None:\n",
        "                # Fallback: find it again by ID\n",
        "                matches = [t for t in ee.batch.Task.list() if t.id == raster_info['ee_task_id']]\n",
        "                ee_task = matches[0] if matches else None\n",
        "                raster_info['ee_task'] = ee_task\n",
        "\n",
        "            if ee_task is not None:\n",
        "                task_state = ee_task.status()['state']\n",
        "\n",
        "                if task_state in ('FAILED', 'CANCELLED'):\n",
        "                    raster_info.update({\n",
        "                        'image_status': 'failed',\n",
        "                        'ee_task_id': '',\n",
        "                        'ee_task': None,\n",
        "                        'task_current_execution': False\n",
        "                    })\n",
        "                    print(f\"{raster_info['image_description']} failed. Skipping.\")\n",
        "\n",
        "                elif task_state == 'COMPLETED':\n",
        "                    raster_info.update({\n",
        "                        'task_current_execution': False\n",
        "                    })\n",
        "\n",
        "            continue  # READY / RUNNING / QUEUED, keep polling\n",
        "\n",
        "        # Need to queue a new task\n",
        "        if raster_info['image_status'] == '':\n",
        "            # Wait for a free slot\n",
        "            if ee_current_task_count >= ee_max_concurrent_tasks:\n",
        "                continue  # try again next outer loop pass\n",
        "\n",
        "            ee_path = raster_info['ee_path']\n",
        "            ee_band = raster_info['ee_band']\n",
        "            ee_dataset_type = raster_info['ee_dataset_type']\n",
        "\n",
        "            # Select the appropriate image and band\n",
        "            if ee_dataset_type == 'ImageCollection':\n",
        "                ee_image = ee.ImageCollection(ee_path)\n",
        "                image_selected = ee_image.qualityMosaic(ee_band).select([ee_band])\n",
        "                resolution = ee_image.first().projection().nominalScale().getInfo()\n",
        "            elif ee_dataset_type == 'Image':\n",
        "                ee_image = ee.Image(ee_path)\n",
        "                image_selected = ee_image.select([ee_band])\n",
        "                resolution = ee_image.select(0).projection().nominalScale().getInfo()\n",
        "\n",
        "            task = ee.batch.Export.image.toDrive(\n",
        "                image=image_selected.toFloat(),\n",
        "                description=raster_info['image_description'],\n",
        "                fileNamePrefix=raster_info['image_description'],\n",
        "                fileFormat='GeoTIFF',\n",
        "                region=ee_geometry,\n",
        "                scale=resolution,\n",
        "                maxPixels=10000000000,\n",
        "                crs='EPSG:4326'\n",
        "            )\n",
        "            task.start()\n",
        "\n",
        "            ee_current_task_count += 1\n",
        "            raster_info.update({\n",
        "                'image_status': 'task',\n",
        "                'ee_task_id': task.id,\n",
        "                'ee_task': task,\n",
        "                'task_current_execution': True\n",
        "            })\n",
        "\n",
        "    sleep(5)\n",
        "\n",
        "processed_count = sum(1 for v in raster_dictionary.values()\n",
        "                     if v['image_status'] == 'processed')\n",
        "failed_count = sum(1 for v in raster_dictionary.values()\n",
        "                  if v['image_status'] == 'failed')\n",
        "\n",
        "print(f\"Final Status Check:\\nProcessed Rasters: {processed_count}\\nFailed Rasters: {failed_count}\")\n",
        "print(\"Check Earth Engine tasks here: https://code.earthengine.google.com/tasks\")"
      ],
      "metadata": {
        "id": "XYxiD4UUw-zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Direct download method"
      ],
      "metadata": {
        "id": "wHPltI0z2-UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import requests\n",
        "import threading\n",
        "from shutil import move, rmtree\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "# Set to False to suppress detailed messages about image splitting\n",
        "verbose = False\n",
        "\n",
        "compression = [\n",
        "    'COMPRESS=LZW',  # Good speed / size ratio\n",
        "    # 'ZSTD_LEVEL=1',\n",
        "]\n",
        "\n",
        "clip_geometry = False  # If True, clips the download geometry to the dataset footprint\n",
        "# If False, the empty geometry will be filled with nodata values.\n",
        "\n",
        "# Create a temporary directory for tiles\n",
        "temp_tiles_dir = join(ee_dir, 'temp_tiles')\n",
        "makedirs(temp_tiles_dir, exist_ok=True)\n",
        "\n",
        "# Load template and set Earth Engine geometry\n",
        "template_polygon_dir = join(polygons_dir, 'template.gpkg')\n",
        "template_area = gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0]\n",
        "template_coords = list(gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0].exterior.coords)\n",
        "ee_geometry = ee.Geometry.Polygon(template_coords)\n",
        "\n",
        "# Create a dictionary of all rasters to download\n",
        "raster_dictionary = {}\n",
        "\n",
        "# Populate the dictionary with information about each raster\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "\n",
        "    for ee_path in ee_paths:\n",
        "        # Identify bands\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image = ee.ImageCollection(ee_path)\n",
        "            ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "\n",
        "        # Loop through bands and create entries in dictionary\n",
        "        for ee_band in ee_bands:\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_tif_dir = join(ee_dir, ee_tif_filename)\n",
        "\n",
        "            # Different temporary directory required if MyDrive is mounted\n",
        "            if base_dir.startswith('/content/drive/MyDrive/'):\n",
        "                ee_temp_dir = join(\"/content/drive/MyDrive/\", ee_tif_filename)\n",
        "            else:\n",
        "                ee_temp_dir = join(\"/gdrive/MyDrive\", ee_tif_filename)\n",
        "\n",
        "            description = ee_tif_filename[:-4]\n",
        "\n",
        "            raster_dictionary[description] = {\n",
        "                'ee_dataset_type': ee_dataset_type,\n",
        "                'ee_path': ee_path,\n",
        "                'ee_band': ee_band,\n",
        "                'image_path': ee_tif_dir,\n",
        "                'image_path_temp': ee_temp_dir,\n",
        "                'image_description': description,\n",
        "                'image_status': '',\n",
        "                'ee_object_id': ee_path\n",
        "            }\n",
        "\n",
        "# Control parallel processing\n",
        "MAX_CONCURRENT_IMAGES = 10\n",
        "\n",
        "# Earth Engine size limit in bytes (approximately 50MB)\n",
        "EE_SIZE_LIMIT = 50331648\n",
        "\n",
        "# Lock for updating progress\n",
        "progress_lock = threading.Lock()\n",
        "\n",
        "# Total raster count\n",
        "raster_number = len(raster_dictionary)\n",
        "\n",
        "# Global variable for tracking progress\n",
        "global_progress_index = 0\n",
        "\n",
        "# Function to display custom progress bar\n",
        "def display_progress():\n",
        "    percent = int((global_progress_index / raster_number) * 100) if raster_number > 0 else 0\n",
        "    bar_width = 80\n",
        "    filled_length = int(bar_width * global_progress_index // raster_number)\n",
        "    bar = '=' * filled_length + ' ' * (bar_width - filled_length)\n",
        "\n",
        "    progress_html = f\"\"\"\n",
        "    <div style=\"width:100%; margin-top:10px; margin-bottom:10px;\">\n",
        "        <div style=\"color:#CCCCCC; font-family:monospace;\">\n",
        "            Raster download progress: {percent}% [{bar}] {global_progress_index}/{raster_number}\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    display(HTML(progress_html))\n",
        "\n",
        "# Display initial progress\n",
        "display_progress()\n",
        "\n",
        "def download_tile(raster_band, geometry, scale, output_path, max_retries=3):\n",
        "    \"\"\"Try to download a tile with the given geometry, handling EE-specific errors\"\"\"\n",
        "    for retry in range(max_retries):\n",
        "        try:\n",
        "            # Get the download URL\n",
        "            url = raster_band.getDownloadURL({\n",
        "                'scale': scale,\n",
        "                'region': geometry,\n",
        "                'format': 'GEO_TIFF',\n",
        "                'crs': 'EPSG:4326'\n",
        "            })\n",
        "\n",
        "            # Download the file\n",
        "            urlretrieve(url, output_path)\n",
        "            return True, None\n",
        "\n",
        "        except ee.EEException as e:\n",
        "            error_msg = str(e)\n",
        "            # Check for size-related errors specifically\n",
        "            if \"Total request size\" in error_msg and \"must be less than or equal to\" in error_msg:\n",
        "                return False, \"SIZE_LIMIT\"\n",
        "            else:\n",
        "                if retry < max_retries - 1:\n",
        "                    sleep(5)\n",
        "        except Exception as e:\n",
        "            if retry < max_retries - 1:\n",
        "                sleep(5)\n",
        "\n",
        "    return False, \"OTHER_ERROR\"\n",
        "\n",
        "def split_tile_vertically(geometry, n_parts=2):\n",
        "    \"\"\"Split a rectangular geometry into n_parts vertically\"\"\"\n",
        "    bounds = geometry.bounds().getInfo()['coordinates'][0]\n",
        "    min_x = min(coord[0] for coord in bounds)\n",
        "    min_y = min(coord[1] for coord in bounds)\n",
        "    max_x = max(coord[0] for coord in bounds)\n",
        "    max_y = max(coord[1] for coord in bounds)\n",
        "\n",
        "    height = max_y - min_y\n",
        "    part_height = height / n_parts\n",
        "\n",
        "    parts = []\n",
        "    for i in range(n_parts):\n",
        "        part_min_y = min_y + (i * part_height)\n",
        "        part_max_y = min_y + ((i + 1) * part_height)\n",
        "        parts.append(ee.Geometry.Rectangle([min_x, part_min_y, max_x, part_max_y]))\n",
        "\n",
        "    return parts\n",
        "\n",
        "def process_image(image_description, raster_info):\n",
        "    \"\"\"Process a single raster image - to be run in parallel\"\"\"\n",
        "    global global_progress_index\n",
        "\n",
        "    # Skip if already processed or failed\n",
        "    if exists(raster_info['image_path']) or raster_info.get('image_status') == 'failed':\n",
        "        with progress_lock:\n",
        "            global_progress_index += 1\n",
        "            # No direct widget update here - handled by update_progress_display thread\n",
        "        return True\n",
        "\n",
        "    # Get the band\n",
        "    ee_object = None\n",
        "    raster_band = None\n",
        "    tile_paths = []\n",
        "\n",
        "    try:\n",
        "        ee_path = raster_info['ee_path']\n",
        "        ee_band = raster_info['ee_band']\n",
        "        ee_dataset_type = raster_info['ee_dataset_type']\n",
        "\n",
        "        # Select the appropriate image and band\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image_collection = ee.ImageCollection(ee_path)\n",
        "            ee_object = ee_image_collection.qualityMosaic(ee_band)\n",
        "            raster_band = ee_object.select([ee_band]).toFloat()\n",
        "            projection = ee_image_collection.first().projection()\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_object = ee.Image(ee_path)\n",
        "            raster_band = ee_object.select([ee_band]).toFloat()\n",
        "            projection = ee_object.select(0).projection()\n",
        "\n",
        "        if clip_geometry:\n",
        "            # Get the footprint\n",
        "            footprint = ee_object.geometry()\n",
        "            # Calculate intersection with the footprint\n",
        "            download_geometry = ee_geometry.intersection(footprint)\n",
        "        else:\n",
        "            download_geometry = ee_geometry\n",
        "\n",
        "        # Get image projection and scale\n",
        "        scale = projection.nominalScale().getInfo()\n",
        "\n",
        "        # Create a folder for this image's tiles\n",
        "        image_tiles_dir = join(temp_tiles_dir, image_description)\n",
        "        makedirs(image_tiles_dir, exist_ok=True)\n",
        "\n",
        "        # Track downloaded tiles\n",
        "        tile_paths = []\n",
        "\n",
        "        # First try to download the whole image at once\n",
        "        whole_image_path = join(image_tiles_dir, f\"{image_description}_whole.tif\")\n",
        "\n",
        "        success, error_type = download_tile(raster_band, download_geometry, scale, whole_image_path)\n",
        "\n",
        "        if success:\n",
        "            tile_paths = [whole_image_path]\n",
        "        else:\n",
        "            # Log failure only if verbose\n",
        "            if verbose:\n",
        "                if error_type == \"SIZE_LIMIT\":\n",
        "                    print(f\"[{image_description}] Full image download failed due to size limit, starting adaptive tiling...\")\n",
        "                else:\n",
        "                    print(f\"[{image_description}] Full image download failed, starting adaptive tiling...\")\n",
        "\n",
        "            # Initial split factor depends on error type\n",
        "            initial_parts = 2 if error_type == \"SIZE_LIMIT\" else 2\n",
        "\n",
        "            # Start with initial split of the geometry\n",
        "            parts = split_tile_vertically(download_geometry, initial_parts)\n",
        "            tiles_to_process = [(parts[i], i+1, f\"{image_description}_part_{i+1}.tif\")\n",
        "                            for i in range(len(parts))]\n",
        "\n",
        "            successful_tile_height = None\n",
        "\n",
        "            # Process tiles until none are left\n",
        "            while tiles_to_process:\n",
        "                current_geometry, part_num, tile_filename = tiles_to_process.pop(0)\n",
        "                tile_path = join(image_tiles_dir, tile_filename)\n",
        "\n",
        "                # Try to download with current dimensions\n",
        "                success, error_type = download_tile(raster_band, current_geometry, scale, tile_path)\n",
        "\n",
        "                if success:\n",
        "                    tile_paths.append(tile_path)\n",
        "\n",
        "                    # If this is our first successful tile, remember its height\n",
        "                    if successful_tile_height is None:\n",
        "                        bounds = current_geometry.bounds().getInfo()['coordinates'][0]\n",
        "                        min_y = min(coord[1] for coord in bounds)\n",
        "                        max_y = max(coord[1] for coord in bounds)\n",
        "                        successful_tile_height = max_y - min_y\n",
        "                else:\n",
        "                    # If download failed, log it if verbose\n",
        "                    if verbose:\n",
        "                        print(f\"[{image_description}] Part {part_num} download failed: {error_type}\")\n",
        "\n",
        "                    # If size limit error, split more aggressively\n",
        "                    split_factor = 3 if error_type == \"SIZE_LIMIT\" else 2\n",
        "\n",
        "                    # If we have a successful tile height, try to use it\n",
        "                    if successful_tile_height is not None:\n",
        "                        bounds = current_geometry.bounds().getInfo()['coordinates'][0]\n",
        "                        min_y = min(coord[1] for coord in bounds)\n",
        "                        max_y = max(coord[1] for coord in bounds)\n",
        "                        current_height = max_y - min_y\n",
        "\n",
        "                        # Calculate how many parts we need to match the successful height\n",
        "                        needed_parts = max(split_factor, math.ceil(current_height / successful_tile_height))\n",
        "                        split_parts = split_tile_vertically(current_geometry, needed_parts)\n",
        "                    else:\n",
        "                        split_parts = split_tile_vertically(current_geometry, split_factor)\n",
        "\n",
        "                    # Add new parts to the processing queue\n",
        "                    next_part_num = max([p[1] for p in tiles_to_process]) + 1 if tiles_to_process else part_num + 1\n",
        "                    for i, geom in enumerate(split_parts):\n",
        "                        new_part_num = next_part_num + i\n",
        "                        tiles_to_process.insert(0, (geom, new_part_num, f\"{image_description}_part_{new_part_num}.tif\"))\n",
        "\n",
        "                    # Remove the failed attempt file if it exists\n",
        "                    if os.path.exists(tile_path):\n",
        "                        os.remove(tile_path)\n",
        "\n",
        "        # Merge tiles using GDAL\n",
        "        if len(tile_paths) > 0:\n",
        "            if len(tile_paths) == 1:\n",
        "                # Just one tile, compress and copy directly\n",
        "                merged_temp_path = raster_info['image_path_temp']\n",
        "\n",
        "                gdal_translate_options = gdal.TranslateOptions(\n",
        "                    format=\"GTiff\",\n",
        "                    creationOptions=compression\n",
        "                )\n",
        "                gdal.Translate(merged_temp_path, tile_paths[0], options=gdal_translate_options)\n",
        "            else:\n",
        "                # Multiple tiles need merging\n",
        "                vrt_path = join(image_tiles_dir, f\"{image_description}_mosaic.vrt\")\n",
        "                merged_temp_path = raster_info['image_path_temp']\n",
        "\n",
        "                # Create VRT from tiles\n",
        "                gdal.BuildVRT(vrt_path, tile_paths)\n",
        "\n",
        "                # Translate VRT to GeoTIFF with compression\n",
        "                gdal_translate_options = gdal.TranslateOptions(\n",
        "                    format=\"GTiff\",\n",
        "                    creationOptions=compression\n",
        "                )\n",
        "                gdal.Translate(merged_temp_path, vrt_path, options=gdal_translate_options)\n",
        "\n",
        "            # Move to final location\n",
        "            move(merged_temp_path, raster_info['image_path'])\n",
        "\n",
        "            # Update progress tracking with thread safety\n",
        "            with progress_lock:\n",
        "                global_progress_index += 1\n",
        "                # No direct widget update here - handled by update_progress_display thread\n",
        "\n",
        "            # Clean up tile folder after successful merge\n",
        "            rmtree(image_tiles_dir)\n",
        "            return True\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"[{image_description}] Failed: No tiles were successfully downloaded\")\n",
        "            raster_info['image_status'] = 'failed'\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"Error processing {image_description}: {str(e)}\")\n",
        "        raster_info['image_status'] = 'failed'\n",
        "        return False\n",
        "    finally:\n",
        "        # Explicitly clear any large objects\n",
        "        ee_object = None\n",
        "        raster_band = None\n",
        "\n",
        "# Function to update progress display periodically\n",
        "def update_progress_display():\n",
        "    last_count = 0\n",
        "    while global_progress_index < raster_number:\n",
        "        sleep(0.5)  # Update every half second\n",
        "\n",
        "        current_count = 0\n",
        "        with progress_lock:\n",
        "            current_count = global_progress_index\n",
        "\n",
        "        if current_count != last_count:\n",
        "            display_progress()\n",
        "            last_count = current_count\n",
        "\n",
        "    # Final update to ensure 100% is shown\n",
        "    display_progress()\n",
        "\n",
        "# Count initially processed images\n",
        "global_progress_index = 0\n",
        "for v in raster_dictionary.values():\n",
        "    if v.get('image_status') == 'processed' or exists(v['image_path']):\n",
        "        global_progress_index += 1\n",
        "\n",
        "# Display initial progress\n",
        "display_progress()\n",
        "\n",
        "# Create a list of pending images to process\n",
        "pending_images = [(desc, img) for desc, img in raster_dictionary.items()\n",
        "                  if not exists(img['image_path']) and img.get('image_status') != 'failed']\n",
        "\n",
        "if verbose:\n",
        "    print(f\"Starting processing of {len(pending_images)} rasters with {MAX_CONCURRENT_IMAGES} parallel workers\")\n",
        "\n",
        "# Start the progress monitoring thread\n",
        "progress_thread = threading.Thread(target=update_progress_display)\n",
        "progress_thread.daemon = True\n",
        "progress_thread.start()\n",
        "\n",
        "try:\n",
        "    # Use ThreadPoolExecutor for parallel processing\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_CONCURRENT_IMAGES) as executor:\n",
        "        futures = {executor.submit(process_image, desc, img): (desc, img) for desc, img in pending_images}\n",
        "\n",
        "        # Wait for completion and process results\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            desc, _ = futures[future]\n",
        "            try:\n",
        "                success = future.result()\n",
        "                # Only log failures if verbose\n",
        "                if not success and verbose:\n",
        "                    print(f\"Raster {desc} processing failed\")\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"Raster {desc} processing generated an exception: {e}\")\n",
        "                # Mark as failed\n",
        "                raster_dictionary[desc]['image_status'] = 'failed'\n",
        "except Exception as e:\n",
        "    print(f\"Error in thread pool execution: {e}\")\n",
        "finally:\n",
        "    # Make sure we wait for the progress thread to update one last time\n",
        "    if progress_thread.is_alive():\n",
        "        sleep(0.6)  # Give time for one last update\n",
        "\n",
        "print(f\"Processing complete\")\n",
        "\n",
        "# Count and display results\n",
        "processed_count = sum(1 for v in raster_dictionary.values()\n",
        "                      if exists(v['image_path']))\n",
        "failed_count = sum(1 for v in raster_dictionary.values()\n",
        "                   if v.get('image_status') == 'failed')\n",
        "\n",
        "print(f\"Final Status: {processed_count} rasters processed, {failed_count} rasters failed\")\n",
        "print(\"Check Earth Engine tasks here: https://code.earthengine.google.com/tasks\")"
      ],
      "metadata": {
        "id": "OzB4fZsOyO4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIBbo-gsS3nD"
      },
      "source": [
        "# GLAD LCLUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W90YInS-S5I5"
      },
      "outputs": [],
      "source": [
        "# GLAD data can be used in-place of TMF data for testing non-TMF areas.\n",
        "# LCLUC contains several land cover and land use types, each with continuous metrics.\n",
        "# This splits them into categories for better modelling, based on the legend:\n",
        "# https://glad.umd.edu/sites/default/files/legend_0.xlsx\n",
        "# Should do before resampling.\n",
        "\n",
        "lcluc_dict = {\n",
        "    'terra_vegetation_cover_percent': (0, 24),\n",
        "    'terra_stable_tree_m': (25, 48),\n",
        "    'wetland_vegetation_cover_percent': (100, 124),\n",
        "    'wetland_stable_tree_m': (125, 148),\n",
        "    'open_surface_water_percent_of_year': (200, 207),\n",
        "    'snow_ice': (241, 241),\n",
        "    'cropland': (244, 244),\n",
        "    'built_up': (250, 250),\n",
        "    'ocean': (254, 254),\n",
        "}\n",
        "\n",
        "lcluc_exists = False\n",
        "for lcluc_raster in os.listdir(ee_dir):\n",
        "  if 'LCLUC' in lcluc_raster:\n",
        "    lcluc_exists = True\n",
        "    lcluc_path = join(ee_dir, lcluc_raster)\n",
        "    luluc_array = gdal.Open(lcluc_path).ReadAsArray()\n",
        "    for key, (lower, upper) in lcluc_dict.items():\n",
        "        split_luluc_filename = f\"{lcluc_raster[:-4]}_{key}.tif\"\n",
        "        split_luluc_filename_binary = f\"{lcluc_raster[:-4]}_{key}_binary.tif\"\n",
        "        split_luluc_dir = join(glad_lcluc_dir, split_luluc_filename)\n",
        "        split_luluc_dir_binary = join(glad_lcluc_dir, split_luluc_filename_binary)\n",
        "        if not exists(split_luluc_dir) and not exists(split_luluc_dir_binary):\n",
        "          split_luluc_mask = np.logical_and(luluc_array >= lower, luluc_array <= upper)\n",
        "          split_luluc_array = np.where(split_luluc_mask, luluc_array, 0) # outside the range set to 0\n",
        "          non_zero_percentage = np.count_nonzero(split_luluc_array) / split_luluc_array.size * 100\n",
        "          if non_zero_percentage >= 0.1:\n",
        "            # Check if there's only one unique non-zero value, and convert to a 1-0 binary raster if true\n",
        "            unique_non_zero_values = np.unique(split_luluc_array[split_luluc_array > 0])\n",
        "            if len(unique_non_zero_values) == 1:\n",
        "                split_luluc_array = np.where(split_luluc_array > 0, 1, 0)\n",
        "                split_luluc_dir = split_luluc_dir_binary\n",
        "            export_array_as_tif(split_luluc_array, split_luluc_dir, template=lcluc_path)\n",
        "            print(f\"{lcluc_raster} raster has been processed\")\n",
        "\n",
        "if not lcluc_exists: print(\"There are no GLAD LCLUC rasters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-UOVsvRuHNS"
      },
      "source": [
        "# Resample EE rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5yEcU40E8eD"
      },
      "outputs": [],
      "source": [
        "# Create dictionary of all tifs in Earth Engine and user upload directory\n",
        "resample_dict = {}\n",
        "for resample_raster in os.listdir(ee_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'categorical'\"})\n",
        "for resample_raster in os.listdir(user_upload_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'categorical'\"})\n",
        "for resample_raster in os.listdir(glad_lcluc_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'continuous'\"})\n",
        "resample_dict = {key: value for key, value in sorted(resample_dict.items())}\n",
        "\n",
        "# Select rasters for resampling and verify data type (categorical or continuous)\n",
        "print(\"selected_original_rasters = {\")\n",
        "for key, value in resample_dict.items():\n",
        "    print(f'\"{key}\": {value},')\n",
        "print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfxPFqlxE0Ok"
      },
      "outputs": [],
      "source": [
        "selected_original_rasters = {\n",
        "# \"temp_tiles\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1990.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1991.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1992.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1993.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1994.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1995.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1996.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1997.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1998.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1999.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2000.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2001.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2002.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2003.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2004.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2005.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2006.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2007.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2008.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2009.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2010.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2011.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2012.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2013.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2014.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2015.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2016.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2017.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2018.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2019.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2020.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2021.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2022.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2023.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2024.tif\": 'categorical',\n",
        "\"tmf_AnnualDisruptionObs2023_y2023.tif\": 'categorical',\n",
        "\"tmf_AnnualDisruptionObs2024_SumNonForest.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1982.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1983.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1984.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1985.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1986.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1987.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1988.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1989.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1990.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1991.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1992.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1993.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1994.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1995.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1996.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1997.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1998.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1999.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2000.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2001.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2002.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2003.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2004.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2005.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2006.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2007.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2008.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2009.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2010.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2011.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2012.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2013.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2014.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2015.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2016.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2017.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2018.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2019.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2020.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2021.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2022.tif\": 'categorical',\n",
        "\"tmf_TransitionMap_MainClasses_TransitionMap_MainClasses.tif\": 'categorical',\n",
        "\"tmf_TransitionMap_Subtypes_TransitionMap_Subtypes.tif\": 'categorical',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEZwd6XYXqRk"
      },
      "outputs": [],
      "source": [
        "# Set resample algorithms for different raster types\n",
        "# See https://gdal.org/programs/gdalwarp.html\n",
        "categorical_alg = 'near'\n",
        "continuous_alg = 'bilinear'\n",
        "\n",
        "template = gdal.Open(template_dir)\n",
        "template_dimensions = template.GetGeoTransform()\n",
        "xres, yres = template_dimensions[1], -template_dimensions[5]\n",
        "xmin = template_dimensions[0]\n",
        "ymin = template_dimensions[3] - template.RasterYSize * yres\n",
        "xmax = xmin + template.RasterXSize * xres\n",
        "ymax = template_dimensions[3]\n",
        "\n",
        "# Resample progress\n",
        "resample_progress_index = 0\n",
        "resample_progress_label = widgets.Label(f\"Resample progress: {resample_progress_index}/{len(selected_original_rasters.items())}\")\n",
        "display(resample_progress_label)\n",
        "\n",
        "# Iterate over selected rasters\n",
        "for original_raster_name, data_type in selected_original_rasters.items():\n",
        "  resampled_raster_dir = join(resampled_dir, original_raster_name)\n",
        "  if not exists(resampled_raster_dir):\n",
        "    original_raster_dir = join(ee_dir, original_raster_name)\n",
        "    if not exists(original_raster_dir): original_raster_dir = join(user_upload_dir, original_raster_name)\n",
        "    if not exists(original_raster_dir): original_raster_dir = join(glad_lcluc_dir, original_raster_name)\n",
        "    # Set resample type\n",
        "    if data_type == 'categorical': resample_alg = categorical_alg\n",
        "    if data_type == 'continuous': resample_alg = continuous_alg\n",
        "    src = gdal.Warp(\n",
        "        resampled_raster_dir,\n",
        "        original_raster_dir,\n",
        "        xRes=xres, yRes=yres,\n",
        "        outputBounds=(xmin, ymin, xmax, ymax),\n",
        "        resampleAlg=resample_alg,\n",
        "        outputType=gdalconst.GDT_Float32)\n",
        "    # Compress and close\n",
        "    driver = gdal.GetDriverByName(\"GTiff\")\n",
        "    src = driver.CreateCopy(resampled_raster_dir, src, 0, options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "    src = None\n",
        "  # Update resample progress\n",
        "  resample_progress_index += 1\n",
        "  resample_progress_label.value = f\"Resample progress: {resample_progress_index}/{len(selected_original_rasters.items())}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D35niESIwR9"
      },
      "outputs": [],
      "source": [
        "# Determine continous feature precision\n",
        "\n",
        "override_max_unique_values = False\n",
        "max_unique_values = 5000 # Should be >=10\n",
        "\n",
        "if override_max_unique_values == False:\n",
        "  dem_base_path = join(areas_dir, \"base_dem.tif\")\n",
        "  dem_base_array = gdal.Open(dem_base_path).ReadAsArray()\n",
        "  max_unique_values = int(np.ptp(dem_base_array)) # Precision based on elevation variance\n",
        "resampled_precision_dict = {}\n",
        "\n",
        "for resampled_feature, resample_type in selected_original_rasters.items():\n",
        "  if resample_type == 'continuous':\n",
        "    resampled_feature_path = join(resampled_dir, resampled_feature)\n",
        "    print(f\"Reading {resampled_feature}...\")\n",
        "    # Read raster as array\n",
        "    resampled_feature_array = gdal.Open(resampled_feature_path).ReadAsArray()\n",
        "    # Convert 'nodata' values to nan\n",
        "    resampled_feature_array[resampled_feature_array == nodatavalue] = np.nan\n",
        "    resampled_feature_array_masked = np.ma.array(resampled_feature_array, mask=np.isnan(resampled_feature_array))\n",
        "    # Count unique values in raster\n",
        "    unique_values = len(np.unique(resampled_feature_array_masked))\n",
        "    print(f\"There are {unique_values} unique values in {resampled_feature}\")\n",
        "    # Generate histogram from 100,000 random points\n",
        "    random_selection = np.random.choice(resampled_feature_array_masked.ravel(), size = 100_000, replace = False)\n",
        "    _ = plt.hist(random_selection, bins='auto')  # arguments are passed to np.histogram\n",
        "    plt.title(f\"{resampled_feature}\")\n",
        "    plt.show()\n",
        "    # Remove 0 values for log10\n",
        "    resampled_feature_array_masked[resampled_feature_array_masked == 0] = np.nan\n",
        "    resampled_feature_array_masked = np.ma.array(resampled_feature_array, mask=np.isnan(resampled_feature_array))\n",
        "    # Create log10 array for determining positions for rounding\n",
        "    array_log10 = np.log10(abs(resampled_feature_array_masked))\n",
        "    place_value_decimal = int(abs(np.min(array_log10)))\n",
        "    place_value_integer = int(0 - np.max(array_log10))\n",
        "    # Iterate down precision levels to determine optimal number of unique values\n",
        "    min_starting_precision = len(str(max_unique_values))\n",
        "    for precision in reversed(range(place_value_integer, max(min_starting_precision, place_value_decimal +1))):\n",
        "      rounded_array = np.round(resampled_feature_array, decimals=precision)\n",
        "      round_unique_values = len(np.unique(rounded_array))\n",
        "      optimal_precision = None\n",
        "      if round_unique_values <= max_unique_values:\n",
        "        optimal_precision = precision\n",
        "        print(f\"The optimal precison for {resampled_feature} is {optimal_precision}, with {round_unique_values} unique values.\")\n",
        "        resampled_precision_dict.update({f'{resampled_feature}':f'{optimal_precision}'})\n",
        "        break\n",
        "    if optimal_precision == None: print(\"There's a problem with setting precision.\")\n",
        "    print(\"___________________\\n\")\n",
        "\n",
        "print(\"Dictionary for optimal rounding values:\")\n",
        "resampled_precision_dict\n",
        "\n",
        "precision_dict_csv_path = join(resampled_dir, 'rounding_dictionary.csv')\n",
        "# Save rounding dictionary to CSV\n",
        "with open(precision_dict_csv_path, 'w', newline='') as precision_dict_csv:\n",
        "    writer = csv.writer(precision_dict_csv)\n",
        "    writer.writerow(resampled_precision_dict.keys())\n",
        "    writer.writerow(resampled_precision_dict.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UjcjQ52NoT7"
      },
      "outputs": [],
      "source": [
        "# Open rounding dictionary and verify\n",
        "with open(precision_dict_csv_path, 'r') as file:\n",
        "    keys, values = list(csv.reader(file))\n",
        "    topo_precision_dict = dict(zip(keys, values))\n",
        "\n",
        "# Verify precision and correct if necessary\n",
        "print(\"topo_precision_dict = {\")\n",
        "for key, value in topo_precision_dict.items():\n",
        "    print(f'\"{key}\": {value},')\n",
        "print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2kquWfYOeS-"
      },
      "outputs": [],
      "source": [
        "topo_precision_dict = {\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn-nfKEPOi9d"
      },
      "outputs": [],
      "source": [
        "# Set smoothing kernel\n",
        "kernel = Gaussian2DKernel(x_stddev=1, y_stddev=1)\n",
        "\n",
        "# Continuous progress\n",
        "continuous_progress_index = 0\n",
        "continuous_progress_label = widgets.Label(f\"Continuous progress: {continuous_progress_index}/{len(topo_precision_dict.items())}\")\n",
        "display(continuous_progress_label)\n",
        "\n",
        "# Iterate over selected continuous rasters\n",
        "for continuous, precision in topo_precision_dict.items():\n",
        "  cont_raster_resampled_path = join(resampled_dir, continuous)\n",
        "  cont_raster_resampled_array = gdal.Open(cont_raster_resampled_path).ReadAsArray()\n",
        "  # Convert nodata values to 0\n",
        "  cont_raster_resampled_array[cont_raster_resampled_array == nodatavalue] = 0\n",
        "  # Set path and check if exists\n",
        "  cont_raster_unsmoothed_filename = f\"{continuous[:-4]}_unsmooth.tif\"\n",
        "  cont_raster_unsmoothed_path = join(continuous_final_dir, cont_raster_unsmoothed_filename)\n",
        "  if not exists(cont_raster_unsmoothed_path):\n",
        "    # Round and export unsmoothed continuous raster\n",
        "    cont_raster_unsmoothed_rounded = np.round(cont_raster_resampled_array, decimals=int(precision))\n",
        "    export_array_as_tif(cont_raster_unsmoothed_rounded, cont_raster_unsmoothed_path)\n",
        "  # Smooth using 2D spatial convolution\n",
        "  cont_raster_smoothed_filename = f\"{continuous[:-4]}_smooth.tif\"\n",
        "  cont_raster_smoothed_path = join(continuous_final_dir, cont_raster_smoothed_filename)\n",
        "  if not exists(cont_raster_smoothed_path):\n",
        "    cont_raster_smoothed = convolve(cont_raster_resampled_array, kernel, boundary='extend')\n",
        "    # Round and export smoothed continuous raster\n",
        "    cont_raster_smoothed_rounded = np.round(cont_raster_smoothed, decimals=int(precision))\n",
        "    export_array_as_tif(cont_raster_smoothed_rounded, cont_raster_smoothed_path)\n",
        "  # Update continuous progress\n",
        "  continuous_progress_index += 1\n",
        "  continuous_progress_label.value = f\"Continuous progress: {continuous_progress_index}/{len(topo_precision_dict.items())}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhukvPP02gBm"
      },
      "source": [
        "# TMF binary features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdGYp7HAXkD4"
      },
      "outputs": [],
      "source": [
        "# Check TMF data users guide for classification. https://forobs.jrc.ec.europa.eu/static/tmf/TMF_DataUsersGuide.pdf\n",
        "\n",
        "cell_size_x = gdal.Open(join(areas_dir, 'cell_size_x.tif')).ReadAsArray()\n",
        "cell_size_y = gdal.Open(join(areas_dir, 'cell_size_y.tif')).ReadAsArray()\n",
        "cell_size_ha = np.mean(cell_size_x) * np.mean(cell_size_y) / 10_000\n",
        "sieve_size = int(np.ceil(0.5/cell_size_ha)) # Removes all forest patches smaller than 0.5 ha\n",
        "print(f\"Forest binary sieve size (>0.5 ha) is {sieve_size} pixels.\")\n",
        "\n",
        "# Generate list of valid TMF rasters to convert to binary\n",
        "binary_list = []\n",
        "for resampled_raster in os.listdir(resampled_dir):\n",
        "  # Verify these are in the filenames\n",
        "  if 'DisruptionObs' in resampled_raster or 'AnnualChanges' in resampled_raster or 'Ndisturb' in resampled_raster:\n",
        "    # Verify this is the position of the year in the filename\n",
        "    if '2024' in resampled_raster: year = 2024 # This one has a funny name\n",
        "    else: year = resampled_raster[-8:-4]\n",
        "    if int(year) >= 1990: binary_list.append(resampled_raster) # Data prior to 1990 is poor\n",
        "\n",
        "# Binary progress\n",
        "binary_progress_index = 0\n",
        "binary_progress_label = widgets.Label(f\"Binary progress: {binary_progress_index}/{len(binary_list)}\")\n",
        "display(binary_progress_label)\n",
        "\n",
        "for resampled_raster in binary_list:\n",
        "  if '2024' in resampled_raster: year = 2024 # This one has a funny name\n",
        "  else: year = resampled_raster[-8:-4]\n",
        "  # Forest binary\n",
        "  if 'AnnualChanges' in resampled_raster:\n",
        "    forest_binary_path = join(binary_dir, f\"forest_binary_{year}.tif\")\n",
        "    if not exists(forest_binary_path):\n",
        "      ac_raster_path = join(resampled_dir, resampled_raster)\n",
        "      ac_array = gdal.Open(ac_raster_path).ReadAsArray()\n",
        "      # Set classes 1 & 2 as 1, all else as 0\n",
        "      forest_binary_array = np.where((ac_array == 1) | (ac_array == 2), 1, 0)\n",
        "\n",
        "      # Sieve to 0.5 ha, using 8-connectedness (3, 3)\n",
        "      fb_array_labelled, fb_array_features = label(forest_binary_array, structure=np.ones((3, 3)))\n",
        "      # Determine the size of each patch\n",
        "      fb_array_sizes = ndi_sum(forest_binary_array, fb_array_labelled, range(fb_array_features + 1))\n",
        "      # Create a mask to remove patches smaller than the threshold\n",
        "      fb_array_mask_sizes = fb_array_sizes >= sieve_size\n",
        "      fb_array_mask_sizes[0] = 0 # Ensure non-forest (0) is excluded\n",
        "      fb_array_mask = fb_array_mask_sizes[fb_array_labelled]\n",
        "      # Apply the mask to the forest binary array and export\n",
        "      fb_array_sieved = forest_binary_array * fb_array_mask\n",
        "      export_array_as_tif(fb_array_sieved, forest_binary_path)\n",
        "\n",
        "  # Disturbance binary\n",
        "  if 'DisruptionObs' in resampled_raster or 'Ndisturb' in resampled_raster:\n",
        "    disturbance_binary_path = join(binary_dir, f\"disturbance_binary_{year}.tif\")\n",
        "    if not exists(disturbance_binary_path):\n",
        "      ac_raster_path = glob.glob(f\"{resampled_dir}/*AnnualChanges*{year}*\")\n",
        "      ac_array = gdal.Open(ac_raster_path[0]).ReadAsArray()\n",
        "      do_raster_path = join(resampled_dir, resampled_raster)\n",
        "      do_array = gdal.Open(do_raster_path).ReadAsArray()\n",
        "      # Set all disruption events to '1' if they're not classed as undisturbed forest or water in AnnualChanges\n",
        "      disturbance_binary_array = np.where((do_array >= 1) & ((ac_array != 1) & (ac_array != 5)), 1, 0)\n",
        "      export_array_as_tif(disturbance_binary_array, disturbance_binary_path)\n",
        "\n",
        "  # Update binary progress\n",
        "  binary_progress_index += 1\n",
        "  binary_progress_label.value = f\"Binary progress: {binary_progress_index}/{len(binary_list)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHmFlgGylGFL"
      },
      "outputs": [],
      "source": [
        "# Extract mangrove binary (optional)\n",
        "extract_mangrove_binary = True\n",
        "\n",
        "if extract_mangrove_binary:\n",
        "  mangrove_binary_path = join(binary_dir, \"mangrove_binary.tif\")\n",
        "  if not exists(mangrove_binary_path):\n",
        "    # Open subtypes array\n",
        "    for resampled_raster in os.listdir(resampled_dir):\n",
        "      if 'Subtypes' in resampled_raster:\n",
        "        subtypes_raster_path = join(resampled_dir, resampled_raster)\n",
        "        subtypes_raster_array = gdal.Open(subtypes_raster_path).ReadAsArray()\n",
        "\n",
        "    # Open oldest available forest binary raster for full mangrove extent (1990)\n",
        "    forest_binary_1990_path = join(binary_dir, \"forest_binary_1990.tif\")\n",
        "    forest_binary_1990_array = gdal.Open(forest_binary_1990_path).ReadAsArray()\n",
        "\n",
        "    mangrove_binary_array = np.logical_and(forest_binary_1990_array == 1,\n",
        "        np.logical_or(subtypes_raster_array == 12,(subtypes_raster_array >= 61) & (subtypes_raster_array <= 69))\n",
        "    )\n",
        "\n",
        "    # Calculate the percentage of forest pixels that are mangrove\n",
        "    forest_1990_pixels = np.sum(forest_binary_1990_array)\n",
        "    mangrove_pixels = np.sum(mangrove_binary_array)\n",
        "    if mangrove_pixels > 0:\n",
        "      mangrove_percent = (mangrove_pixels / forest_1990_pixels) * 100\n",
        "      export_array_as_tif(mangrove_binary_array, mangrove_binary_path)\n",
        "      print(f\"Number of mangrove pixels: {mangrove_pixels}\")\n",
        "      print(f\"Percentage of 1990 forest pixels that are mangrove: {mangrove_percent:.2f}%\")\n",
        "    else: print(\"There are no mangrove pixels in the template area.\")\n",
        "  else: print(\"A mangrove binary raster already exists. Delete it to generate a new one.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJnV0l75FpPN"
      },
      "source": [
        "# LU polygon binary features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vigY_9XqXhCQ"
      },
      "outputs": [],
      "source": [
        "# Selected 'land use' polygons.\n",
        "# Creating a 'complete recovery' or 'complete restoration' scenario requires ONE of these as a proxy.\n",
        "# This can be multiple combined PAs / polygons that have no or minimal history of human disturbance.\n",
        "\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area.gpkg', 'project_area_buffered_bbox.gpkg', 'gedi_area.gpkg', 'project_area_inverse.gpkg', 'gedi_area_inverse.gpkg']\n",
        "print(\"lu_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    print(f\"'{polygon}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w62_C-Vt0Emj"
      },
      "outputs": [],
      "source": [
        "lu_polygons = [\n",
        "# 'peninsular_malaysia.gpkg',\n",
        "'lu_oldgrowth.gpkg',\n",
        "]\n",
        "\n",
        "# Convert all template values to 'nodata' in preparation\n",
        "template_tif = gdal.Open(template_dir)\n",
        "template_mask_array = gdal.Open(template_dir).ReadAsArray()\n",
        "template_mask_array[template_mask_array != None] = 0\n",
        "\n",
        "for lu_polygon in lu_polygons:\n",
        "  lu_binary_name = f\"{lu_polygon[:-5]}_binary.tif\"\n",
        "  lu_binary_path = join(binary_dir, lu_binary_name)\n",
        "  if not exists(lu_binary_path):\n",
        "    lu_polygon_path = join(polygons_dir, lu_polygon)\n",
        "    export_array_as_tif(template_mask_array, lu_binary_path)\n",
        "    # Burn the value '1' where it overlaps with the project area polygon\n",
        "    burn_polygon_to_raster(lu_binary_path, lu_polygon_path, fixed=True, fixed_value=1, all_touched=False)\n",
        "    print(f\"{lu_binary_name} has been created.\")\n",
        "  else: print(f\"{lu_binary_name} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKSLJnR4SE5H"
      },
      "source": [
        "# Binary masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3rRcG0_ZOdd"
      },
      "outputs": [],
      "source": [
        "# Generate masks for later scenario predictions, e.g. so outputs only show forest.\n",
        "mask_type_list = []\n",
        "for binary in os.listdir(binary_dir):\n",
        "    mask_type = binary.split('_')[0]\n",
        "    if mask_type not in mask_type_list:\n",
        "        mask_type_list.append(mask_type)\n",
        "\n",
        "print(\"mask_types = [\")\n",
        "for mask_type in mask_type_list:\n",
        "    print(f\"'{mask_type}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F0klPciSEHh"
      },
      "outputs": [],
      "source": [
        "mask_types = [\n",
        "'forest',\n",
        "# 'lu',\n",
        "# 'disturbance',\n",
        "]\n",
        "\n",
        "# Create list of binary rasters to mask\n",
        "binary_mask_list = []\n",
        "for mask_type in mask_types:\n",
        "  for binary in os.listdir(binary_dir):\n",
        "    if mask_type in binary:\n",
        "      binary_mask_list.append(binary)\n",
        "\n",
        "# Binary progress\n",
        "mask_progress_index = 0\n",
        "mask_progress_label = widgets.Label(f\"Binary progress: {mask_progress_index}/{len(binary_mask_list)}\")\n",
        "display(mask_progress_label)\n",
        "\n",
        "# Create masks from the selected binary raster type\n",
        "for mask_type in mask_types:\n",
        "  for binary in binary_mask_list:\n",
        "    binary_path = join(binary_dir, binary)\n",
        "    try: year = str(int(binary[-8:-4])) # Check for year\n",
        "    except: year = None\n",
        "    mask_raster_path = join(scenario_mask_dir, f\"mask_{mask_type}_{year}.tif\")\n",
        "    if not exists(mask_raster_path):\n",
        "        binary_raster = gdal.Open(binary_path)\n",
        "        binary_array = gdal.Open(binary_path).ReadAsArray()\n",
        "        mask_array = np.where(binary_array == 0, nodatavalue, 1)\n",
        "        export_array_as_tif(mask_array, mask_raster_path)\n",
        "        print(f\"A mask raster has been created: {mask_raster_path}\")\n",
        "    else: print(f\"A mask raster already exists at: {mask_raster_path}\")\n",
        "    # Update mask progress\n",
        "    mask_progress_index += 1\n",
        "    mask_progress_label.value = f\"Binary progress: {mask_progress_index}/{len(binary_mask_list)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCv0mNkb2pIY"
      },
      "source": [
        "# Binary feature edge effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqkninr4qy-g"
      },
      "outputs": [],
      "source": [
        "# Set smoothing kernel\n",
        "kernel = Gaussian2DKernel(x_stddev=3, y_stddev=3)\n",
        "# Set precision\n",
        "precision = 2\n",
        "\n",
        "binary_list = []\n",
        "for binary_raster in os.listdir(binary_dir) + os.listdir(resampled_dir):\n",
        "  if \"binary\" in binary_raster:\n",
        "    binary_list.append(binary_raster)\n",
        "\n",
        "# Edge effect progress\n",
        "edge_effect_progress_index = 0\n",
        "edge_effect_progress_label = widgets.Label(f\"Edge effect progress: {edge_effect_progress_index}/{len(binary_list)}\")\n",
        "display(edge_effect_progress_label)\n",
        "\n",
        "for binary_raster in binary_list:\n",
        "  if \"binary\" in binary_raster:\n",
        "    edge_effects_filename = binary_raster.replace('binary', 'with_edge_effects')\n",
        "    edge_effects_path = join(edge_effects_dir, edge_effects_filename)\n",
        "    if not exists(edge_effects_path):\n",
        "      binary_raster_path = join(binary_dir, binary_raster)\n",
        "      if not exists(binary_raster_path): binary_raster_path = join(resampled_dir, binary_raster)\n",
        "      binary_array = gdal.Open(binary_raster_path).ReadAsArray()\n",
        "      # Reclassify for binary differentiation after proximity conversion\n",
        "      differentiator_array = binary_array.copy()\n",
        "      differentiator_array[differentiator_array == 1] = 10\n",
        "      # Positive proximity\n",
        "      positive_distances = distance_transform_edt(binary_array == 0) # target pixels\n",
        "      positive_proximity_array = np.where(positive_distances > 2, 0, positive_distances) # max distance 2\n",
        "      # Negative proximity\n",
        "      negative_distances = distance_transform_edt(binary_array == 1) # target pixels\n",
        "      negative_proximity_array = np.where(negative_distances > 2, 0, negative_distances) # max distance 2\n",
        "      # Sum proximities and differentiator\n",
        "      pixel_prox_summed =  differentiator_array + positive_proximity_array + negative_proximity_array\n",
        "      # Reclassify for better semantic understanding of pixel proximity\n",
        "      pixel_prox_reclassed = pixel_prox_summed.copy()\n",
        "      pixel_prox_reclass_table = [(0, 0, -4), (1, 1, -1), (1.4, 1.5, -2), (2, 2, -3), (10, 10, 3), (11, 11, 0), (11.4, 11.5, 1), (12, 12, 2)]\n",
        "      for min_value, max_value, new_value in pixel_prox_reclass_table:\n",
        "        pixel_prox_reclassed[(pixel_prox_reclassed >= min_value) & (pixel_prox_reclassed <= max_value)] = new_value\n",
        "      # Smooth binary array using 2D convolution\n",
        "      binary_smoothed = convolve(binary_array, kernel, boundary='extend')\n",
        "      # Sum pixel proximity and smoothed binary array\n",
        "      edge_effects_array = np.round(pixel_prox_reclassed + binary_smoothed, precision)\n",
        "      # Export edge effects features\n",
        "      export_array_as_tif(edge_effects_array, edge_effects_path)\n",
        "\n",
        "  # Update binary progress\n",
        "  edge_effect_progress_index += 1\n",
        "  edge_effect_progress_label.value = f\"Edge effect progress: {edge_effect_progress_index}/{len(binary_list)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6r7JXbijM50"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU_AC6MjNfTN"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "-yk8CnJRCYVQ"
      ],
      "gpuType": "V5E1",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
