{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/3_features_lcluc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yk8CnJRCYVQ"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5USlWSaxqv9Y"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOfy6gWFwHEC"
      },
      "outputs": [],
      "source": [
        "# Installs\n",
        "%%capture\n",
        "!pip install astropy\n",
        "!pip install earthengine-api\n",
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTHaRSv8wICv"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "import concurrent.futures\n",
        "import csv\n",
        "import ee\n",
        "import geopandas as gpd\n",
        "import glob\n",
        "from google.colab import runtime, userdata\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from os import makedirs, remove\n",
        "from os.path import exists, join\n",
        "from osgeo import gdal, gdalconst, ogr\n",
        "gdal.UseExceptions()\n",
        "import re\n",
        "import requests\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "from scipy.ndimage import label, sum as ndi_sum\n",
        "from shutil import copyfile, move, rmtree\n",
        "import threading\n",
        "from time import sleep\n",
        "from urllib.request import urlretrieve\n",
        "import uuid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMCq50kg36Br"
      },
      "outputs": [],
      "source": [
        "# 1_areas directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "\n",
        "# 3_features directories\n",
        "features_dir = join(base_dir, \"3_features\")\n",
        "ee_dir = join(features_dir, \"earth_engine\")\n",
        "user_upload_dir = join(features_dir, \"user_upload\")\n",
        "alpha_earth_dir = join(features_dir, \"alpha_earth\")\n",
        "glad_lcluc_dir = join(features_dir, 'glad_lcluc')\n",
        "resampled_dir = join(features_dir, \"resampled\")\n",
        "continuous_final_dir = join(features_dir, \"continuous_final\")\n",
        "binary_dir = join(features_dir, 'binary')\n",
        "edge_effects_dir = join(features_dir, 'binary_edge_effects')\n",
        "\n",
        "# 6_scenarios directories\n",
        "scenario_dir = join(base_dir, \"6_scenarios\")\n",
        "scenario_mask_dir = join(scenario_dir, \"scenario_masks\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(ee_dir, exist_ok=True)\n",
        "makedirs(user_upload_dir, exist_ok=True)\n",
        "makedirs(alpha_earth_dir, exist_ok=True)\n",
        "makedirs(glad_lcluc_dir, exist_ok=True)\n",
        "makedirs(resampled_dir, exist_ok=True)\n",
        "makedirs(continuous_final_dir, exist_ok=True)\n",
        "makedirs(binary_dir, exist_ok=True)\n",
        "makedirs(edge_effects_dir, exist_ok=True)\n",
        "makedirs(scenario_dir, exist_ok=True)\n",
        "makedirs(scenario_mask_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhkgXF4foXhx"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = vector = None\n",
        "    try:\n",
        "        raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "        vector = ogr.Open(polygon_path)\n",
        "        if not raster or not vector:\n",
        "            raise ValueError(\"Cannot open input files\")\n",
        "        layer = vector.GetLayer()\n",
        "        options = [\"ALL_TOUCHED=TRUE\"] if all_touched else []\n",
        "        if fixed:\n",
        "            gdal.RasterizeLayer(raster, [1], layer, burn_values=[fixed_value], options=options)\n",
        "        else:\n",
        "            attr_name = column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()\n",
        "            options.append(f\"ATTRIBUTE={attr_name}\")\n",
        "            gdal.RasterizeLayer(raster, [1], layer, options=options)\n",
        "    finally:\n",
        "        if raster: raster.FlushCache()\n",
        "        raster = vector = None\n",
        "\n",
        "# Global function: edge effects\n",
        "# Provides spatial awareness analogous to CNN receptive fields for tabular models\n",
        "# Data_type: 'binary' or 'continuous'.\n",
        "cell_size_y_path = join(areas_dir, 'cell_size_y.tif')\n",
        "cell_size_x_path = join(areas_dir, 'cell_size_x.tif')\n",
        "threshold_metres = 120 # Distance to account for edge effects. Should match original features.\n",
        "def edge_effects(array, data_type, cell_size_x_path, cell_size_y_path, threshold_metres=threshold_metres):\n",
        "    # Determine pixel size from cell size rasters.\n",
        "    cell_size_x = gdal.Open(cell_size_x_path).ReadAsArray()\n",
        "    cell_size_y = gdal.Open(cell_size_y_path).ReadAsArray()\n",
        "    cell_area = np.mean([np.mean(cell_size_x), np.mean(cell_size_y)])\n",
        "    # Maximum pixel distance for kernel extent.\n",
        "    max_pixel_distance = threshold_metres / cell_area\n",
        "    # 2D Gaussian weight distribution follows chi-squared with df=2.\n",
        "    # Cumulative probability within radius r: P = 1 - exp(-r² / 2σ²).\n",
        "    # Solving for r at P=0.95: r = σ * sqrt(-2 * ln(0.05)) ≈ 2.45σ.\n",
        "    # Setting r = max_pixel_distance ensures 95% of kernel weight falls within threshold.\n",
        "    gaussian_stdev = max_pixel_distance / 2.45\n",
        "    kernel_radius = int(np.ceil(max_pixel_distance))\n",
        "    kernel_size = 2 * kernel_radius + 1\n",
        "    # Gaussian kernel for spatial weighting.\n",
        "    kernel = Gaussian2DKernel(x_stddev=gaussian_stdev, y_stddev=gaussian_stdev,\n",
        "                              x_size=kernel_size, y_size=kernel_size)\n",
        "    # Circular mask enforces ecological threshold as hard boundary.\n",
        "    # Square kernels would include pixels beyond threshold at corners.\n",
        "    y, x = np.ogrid[:kernel_size, :kernel_size]\n",
        "    centre = kernel_radius\n",
        "    distance_from_centre = np.sqrt((x - centre)**2 + (y - centre)**2)\n",
        "    circular_mask = distance_from_centre <= max_pixel_distance\n",
        "    # Apply mask and renormalise to sum to 1.\n",
        "    # Renormalisation ensures consistent weighting after truncation.\n",
        "    kernel_array = kernel.array.copy()\n",
        "    kernel_array[~circular_mask] = 0\n",
        "    kernel_array /= kernel_array.sum()\n",
        "    # Gaussian smoothing captures local spatial context.\n",
        "    # For binary: represents local class density within threshold.\n",
        "    # For continuous: represents local weighted mean within threshold.\n",
        "    # boundary='extend' extrapolates edge values beyond raster extent.\n",
        "    smoothed = convolve(array.astype(float), kernel_array, boundary='extend')\n",
        "    if data_type == 'continuous': return smoothed # Without rounding\n",
        "    if data_type == 'binary': smoothed = np.round(smoothed, 2) # Round\n",
        "    # Binary data: compute signed distance to class boundary.\n",
        "    # Euclidean distance transform gives centre-to-centre pixel distance.\n",
        "    dist_from_ones = distance_transform_edt(array == 0)\n",
        "    dist_from_zeros = distance_transform_edt(array == 1)\n",
        "    # Convert to distance from pixel centre to class boundary.\n",
        "    # Class boundary lies between adjacent pixels of different classes.\n",
        "    # Subtracting 0.5 pixels approximates centre-to-boundary distance.\n",
        "    # Sign encodes class membership: positive = class 1, negative = class 0.\n",
        "    # Magnitude encodes proximity to boundary (edge effects zone).\n",
        "    signed_distance = np.where(\n",
        "        array == 1,\n",
        "        np.maximum(dist_from_zeros - 0.5, 0) * cell_area,\n",
        "        -np.maximum(dist_from_ones - 0.5, 0) * cell_area\n",
        "    )\n",
        "    # Cap at threshold: pixels beyond are interior, not edge-influenced.\n",
        "    # Round to integer metres for cleaner feature representation.\n",
        "    signed_distance = np.round(np.clip(signed_distance, -threshold_metres, threshold_metres)).astype(np.int16)\n",
        "    return signed_distance, smoothed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSuevzw7xyil"
      },
      "source": [
        "# Download Earth Engine rasters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Y3xYz-Va9p"
      },
      "source": [
        "## Define datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_wg4IqHpFXT"
      },
      "outputs": [],
      "source": [
        "# Enable Google Earth Engine API at Google Cloud https://console.cloud.google.com/apis/dashboard\n",
        "# See here for walkthrough: https://github.com/googlecolab/colabtools/issues/4228#issuecomment-1859068706\n",
        "# Set project ID under 'secrets' tab on the left with the name 'google_cloud_project'\n",
        "ee_project = userdata.get('google_cloud_project')\n",
        "\n",
        "# Authenticate Earth Engine\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project=ee_project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddVd9lPMbLOL"
      },
      "outputs": [],
      "source": [
        "# Edit this section to change which Earth Engine datasets are downloaded.\n",
        "# Do not modify the Alpha Earth entries for them to work in the download scripts\n",
        "\n",
        "# Check datasets in https://code.earthengine.google.com/ with:\n",
        "# var assetList = ee.data.listAssets(\"projects/JRC/TMF/v1_2022/\");\n",
        "# print(assetList);\n",
        "\n",
        "ee_datasets = [\n",
        "\n",
        "    {\n",
        "        \"ee_dataset_name\": \"tmf\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"projects/JRC/TMF/v1_2024/AnnualChanges\",\n",
        "            \"projects/JRC/TMF/v1_2024/TransitionMap_MainClasses\",\n",
        "            \"projects/JRC/TMF/v1_2024/TransitionMap_Subtypes\",\n",
        "            \"projects/JRC/TMF/v1_2024/AnnualDisruptionObs2024\",\n",
        "            \"projects/JRC/TMF/v1_2023/AnnualDisruptionObs2023\",\n",
        "            \"projects/JRC/TMF/v1_2023/Ndisturb_C2_1982_2022\",\n",
        "\n",
        "        ],\n",
        "    },\n",
        "    # {\n",
        "    #     \"ee_dataset_name\": \"glad\",\n",
        "    #     \"ee_dataset_type\": \"Image\",\n",
        "    #     \"ee_paths\": [\n",
        "    #                 'projects/glad/GLCLU2020/Forest_gain',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_disturbance',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_netgain',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_netloss',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_loss',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_type',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC_2000',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_gain',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_loss',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Water_dynamics',\n",
        "    #                 'projects/glad/GLCLU2020/Water_dynamics_classes',\n",
        "    #     ]\n",
        "    # }\n",
        "\n",
        "    {\n",
        "        \"ee_dataset_name\": \"alpha_earth_2017\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"ee_dataset_name\": \"alpha_earth_2018\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"ee_dataset_name\": \"alpha_earth_2019\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"ee_dataset_name\": \"alpha_earth_2020\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"ee_dataset_name\": \"alpha_earth_2021\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"ee_dataset_name\": \"alpha_earth_2022\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"ee_dataset_name\": \"alpha_earth_2023\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"ee_dataset_name\": \"alpha_earth_2024\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\",\n",
        "        ],\n",
        "    },\n",
        "\n",
        "]\n",
        "\n",
        "# Verify Earth Engine rasters that will be downloaded\n",
        "ee_raster_list = []\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "\n",
        "    # Check if this is an AlphaEarth dataset.\n",
        "    # Needs different approach to avoid crashing.\n",
        "    is_alpha_earth = 'alpha_earth' in ee_dataset_name.lower()\n",
        "\n",
        "    for ee_path in ee_paths:\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            if is_alpha_earth:\n",
        "                # Memory-efficient approach for AlphaEarth\n",
        "                ee_image_collection = ee.ImageCollection(ee_path)\n",
        "                first_image = ee_image_collection.first()\n",
        "                ee_bands = first_image.bandNames().getInfo()\n",
        "            else:\n",
        "                # Standard approach for regular ImageCollections\n",
        "                ee_image = ee.ImageCollection(ee_path)\n",
        "                ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        else:\n",
        "            # Silent processing for Image datasets\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "\n",
        "        for ee_band in ee_bands:\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_raster_list.append(ee_tif_filename)\n",
        "\n",
        "# Original simple output\n",
        "ee_image_number = len(ee_raster_list)\n",
        "print(f\"There are {ee_image_number} rasters in the list.\")\n",
        "ee_raster_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caWuFWxx22QT"
      },
      "source": [
        "## Simple queue method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qislgFIYUnqi"
      },
      "outputs": [],
      "source": [
        "# Slow but stable\n",
        "\n",
        "# Earth Engine download progress\n",
        "ee_progress_index = 0\n",
        "ee_progress_label = widgets.Label(f\"Earth Engine download progress: {ee_progress_index}/{len(ee_raster_list)}\")\n",
        "display(ee_progress_label)\n",
        "\n",
        "# Load template and set Earth Engine geometry\n",
        "template_polygon_dir = join(polygons_dir, 'template.gpkg')\n",
        "template_area = gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0]\n",
        "template_coords = list(gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0].exterior.coords)\n",
        "ee_geometry = ee.Geometry.Polygon(template_coords)\n",
        "\n",
        "# Download Earth Engine datasets\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "\n",
        "    # Check if this is an AlphaEarth dataset\n",
        "    is_alpha_earth = 'alpha_earth' in ee_dataset_name.lower()\n",
        "\n",
        "    # Loop through Earth Engine paths\n",
        "    for ee_path in ee_paths:\n",
        "        # identify bands\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image = ee.ImageCollection(ee_path)\n",
        "            if is_alpha_earth:\n",
        "                # Memory-efficient approach for AlphaEarth\n",
        "                first_image = ee_image.first()\n",
        "                ee_bands = first_image.bandNames().getInfo()\n",
        "            else:\n",
        "                # Standard approach for regular datasets\n",
        "                ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "        # Loop through bands\n",
        "        for ee_band in reversed(ee_bands):\n",
        "            # Set filename and directory of downloaded raster and check if exists\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_tif_dir = join(ee_dir, ee_tif_filename)\n",
        "            ee_temp_dir = join(\"/gdrive/MyDrive\", ee_tif_filename)\n",
        "            # Different temporary directory required if MyDrive is mounted\n",
        "            if base_dir.startswith('/content/drive/MyDrive/'):\n",
        "              ee_temp_dir = join(\"/content/drive/MyDrive/\", ee_tif_filename)\n",
        "            # Check if temporary raster exists and needs copying\n",
        "            if exists(ee_temp_dir):\n",
        "              copyfile(ee_temp_dir, ee_tif_dir)\n",
        "              remove(ee_temp_dir)\n",
        "            # Check if copied raster exists, and if not download from Earth Engine.\n",
        "            if not exists(ee_tif_dir):\n",
        "              if ee_dataset_type == 'ImageCollection':\n",
        "                if is_alpha_earth:\n",
        "                    # Special AlphaEarth processing: extract year and filter\n",
        "                    year = ee_tif_filename.split('_')[2]  # Extract year from filename\n",
        "                    ee_filtered = ee_image.filterDate(f'{year}-01-01', f'{int(year)+1}-01-01')\n",
        "                    image_selected = ee_filtered.mosaic().select([ee_band])\n",
        "                    resolution = ee_filtered.first().projection().nominalScale().getInfo()\n",
        "                else:\n",
        "                    # Standard processing for regular ImageCollections\n",
        "                    image_selected = ee_image.qualityMosaic(ee_band).select([ee_band])\n",
        "                    resolution = ee_image.first().projection().nominalScale().getInfo()\n",
        "              if ee_dataset_type == 'Image':\n",
        "                image_selected = ee_image.select([ee_band])\n",
        "                resolution = ee_image.select(0).projection().nominalScale().getInfo()\n",
        "              ee_task = ee.batch.Export.image.toDrive(image=image_selected.toFloat(),\n",
        "                                                    description=ee_tif_filename[:-4],\n",
        "                                                    scale=resolution,\n",
        "                                                    region=ee_geometry,\n",
        "                                                    maxPixels=10000000000,\n",
        "                                                    fileNamePrefix=ee_tif_filename[:-4],\n",
        "                                                    crs='EPSG:4326',\n",
        "                                                    fileFormat='GeoTIFF')\n",
        "              ee_task.start()\n",
        "              # Check whether the raster has downloaded yet\n",
        "              while not exists(ee_temp_dir):\n",
        "                  ee_task_status = ee_task.status()\n",
        "                  # If the task is completed, continue\n",
        "                  if ee_task_status[\"state\"] == 'COMPLETED': break\n",
        "                  # If it has failed or been cancelled, show an error\n",
        "                  elif ee_task_status['state'] == 'FAILED' or ee_task_status['state'] == 'CANCELLED':\n",
        "                      print(f\"{ee_tif_filename}:{ee_task_status['error_message']}\")\n",
        "                      try: remove(ee_temp_dir)\n",
        "                      except: pass\n",
        "                      break\n",
        "                  sleep(1)\n",
        "              # Copy the raster to intended directory and remove the temporary raster\n",
        "              while not exists(ee_temp_dir):\n",
        "                sleep(1)\n",
        "              copyfile(ee_temp_dir, ee_tif_dir)\n",
        "              remove(ee_temp_dir)\n",
        "            # Update Earth Engine download progress\n",
        "            ee_progress_index += 1\n",
        "            ee_progress_label.value = f\"Earth Engine download progress: {ee_progress_index}/{len(ee_raster_list)}\"\n",
        "\n",
        "# Check Earth Engine tasks here: https://code.earthengine.google.com/tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zwwDicI25Nq"
      },
      "source": [
        "## Concurrent queue method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYxiD4UUw-zC"
      },
      "outputs": [],
      "source": [
        "# Should be faster and resilient to interruptions, but may be issues Earth Engine side.\n",
        "\n",
        "# Maximum concurrent tasks in Earth Engine\n",
        "ee_max_concurrent_tasks = 2\n",
        "\n",
        "# Load template and set Earth Engine geometry\n",
        "template_polygon_dir = join(polygons_dir, 'template.gpkg')\n",
        "template_area = gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0]\n",
        "template_coords = list(gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0].exterior.coords)\n",
        "ee_geometry = ee.Geometry.Polygon(template_coords)\n",
        "\n",
        "# Create a dictionary of all rasters to download\n",
        "raster_dictionary = {}\n",
        "\n",
        "# Populate the dictionary with information about each raster\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "\n",
        "    # Check if this is an AlphaEarth dataset\n",
        "    is_alpha_earth = 'alpha_earth' in ee_dataset_name.lower()\n",
        "\n",
        "    for ee_path in ee_paths:\n",
        "        # Identify bands\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image = ee.ImageCollection(ee_path)\n",
        "            if is_alpha_earth:\n",
        "                # Memory-efficient approach for AlphaEarth\n",
        "                first_image = ee_image.first()\n",
        "                ee_bands = first_image.bandNames().getInfo()\n",
        "            else:\n",
        "                # Standard approach for regular datasets\n",
        "                ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "\n",
        "        # Loop through bands and create entries in dictionary\n",
        "        for ee_band in ee_bands:\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_tif_dir = join(ee_dir, ee_tif_filename)\n",
        "\n",
        "            # Different temporary directory required if MyDrive is mounted\n",
        "            if base_dir.startswith('/content/drive/MyDrive/'):\n",
        "                ee_temp_dir = join(\"/content/drive/MyDrive/\", ee_tif_filename)\n",
        "            else:\n",
        "                ee_temp_dir = join(\"/gdrive/MyDrive\", ee_tif_filename)\n",
        "\n",
        "            description = ee_tif_filename[:-4]\n",
        "\n",
        "            raster_dictionary[description] = {\n",
        "                'ee_dataset_type': ee_dataset_type,\n",
        "                'ee_dataset_name': ee_dataset_name,\n",
        "                'ee_path': ee_path,\n",
        "                'ee_band': ee_band,\n",
        "                'image_path': ee_tif_dir,\n",
        "                'image_path_temp': ee_temp_dir,\n",
        "                'image_description': description,\n",
        "                'image_status': '',\n",
        "                'ee_task_id': '',\n",
        "                'ee_task': None,\n",
        "                'task_current_execution': False\n",
        "            }\n",
        "\n",
        "# Count total number of rasters\n",
        "raster_number = len(raster_dictionary)\n",
        "\n",
        "# Progress widgets\n",
        "ee_counted_tasks = set()\n",
        "ee_task_progress_index = 0\n",
        "ee_task_progress_label = widgets.Label(\n",
        "    f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        ")\n",
        "display(ee_task_progress_label)\n",
        "\n",
        "raster_progress_index = 0\n",
        "raster_progress_label = widgets.Label(\n",
        "    f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        ")\n",
        "display(raster_progress_label)\n",
        "\n",
        "# Thorough initialization to check all files\n",
        "print(\"Initialising: Checking for completed downloads...\")\n",
        "for raster_info in raster_dictionary.values():\n",
        "    processed = False\n",
        "\n",
        "    # First check if the final file exists\n",
        "    if exists(raster_info['image_path']):\n",
        "        processed = True\n",
        "    # Then check if the temp file exists and move it\n",
        "    elif exists(raster_info['image_path_temp']):\n",
        "        print(f\"Moving temp file for {raster_info['image_description']}\")\n",
        "        move(raster_info['image_path_temp'], raster_info['image_path'])\n",
        "        processed = True\n",
        "\n",
        "    if processed:\n",
        "        raster_info.update({\n",
        "            'image_status': 'processed',\n",
        "            'ee_task_id': '',\n",
        "            'ee_task': None,\n",
        "            'task_current_execution': False\n",
        "        })\n",
        "        # Count it once for the progress bars\n",
        "        if raster_info['image_description'] not in ee_counted_tasks:\n",
        "            ee_counted_tasks.add(raster_info['image_description'])\n",
        "            ee_task_progress_index += 1\n",
        "            ee_task_progress_label.value = (\n",
        "                f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        "            )\n",
        "        raster_progress_index += 1\n",
        "        raster_progress_label.value = (\n",
        "            f\"Raster download progress: {raster_progress_index}/{raster_number}\")\n",
        "    else:\n",
        "        # Clear any state so it'll be requeued later\n",
        "        raster_info.update({\n",
        "            'image_status': '',\n",
        "            'ee_task_id': '',\n",
        "            'ee_task': None,\n",
        "            'task_current_execution': False\n",
        "        })\n",
        "        ee_counted_tasks.discard(raster_info['image_description'])\n",
        "\n",
        "# Detect tasks that were already running before this session\n",
        "print(\"Checking for running Earth Engine tasks...\")\n",
        "ee_current_task_count = 0\n",
        "for task in ee.batch.Task.list():\n",
        "    task_state = task.status()['state']\n",
        "    task_id = task.id\n",
        "    task_description = task.config['description']\n",
        "\n",
        "    if task_state in ['READY', 'RUNNING', 'QUEUED']:\n",
        "        ee_current_task_count += 1\n",
        "        for v in raster_dictionary.values():\n",
        "            if v['image_description'] == task_description and v['image_status'] != 'processed':\n",
        "                v.update({\n",
        "                    'image_status': 'task',\n",
        "                    'ee_task_id': task_id,\n",
        "                    'ee_task': task,\n",
        "                    'task_current_execution': True\n",
        "                })\n",
        "                if task_description not in ee_counted_tasks:\n",
        "                    ee_counted_tasks.add(task_description)\n",
        "                    ee_task_progress_index += 1\n",
        "                    ee_task_progress_label.value = (\n",
        "                        f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        "                    )\n",
        "                break\n",
        "\n",
        "# Main processing loop\n",
        "print(\"Starting main processing loop...\")\n",
        "while True:\n",
        "    # Re-count active EE tasks each pass\n",
        "    active_states = ['READY', 'RUNNING', 'QUEUED']\n",
        "    ee_current_task_count = len([t for t in ee.batch.Task.list()\n",
        "                              if t.status()['state'] in active_states])\n",
        "\n",
        "    # Break when every raster is either processed or failed\n",
        "    if all(v['image_status'] in ['processed', 'failed']\n",
        "           for v in raster_dictionary.values()):\n",
        "        break\n",
        "\n",
        "    # Iterate over rasters\n",
        "    for raster_info in raster_dictionary.values():\n",
        "        # Skip finished / failed rasters\n",
        "        if raster_info['image_status'] in ['processed', 'failed']:\n",
        "            continue\n",
        "\n",
        "        # Final file already exists - double check\n",
        "        if exists(raster_info['image_path']):\n",
        "            raster_info.update({\n",
        "                'image_status': 'processed',\n",
        "                'ee_task_id': '',\n",
        "                'ee_task': None,\n",
        "                'task_current_execution': False\n",
        "            })\n",
        "\n",
        "            if raster_info['image_description'] not in ee_counted_tasks:\n",
        "                ee_counted_tasks.add(raster_info['image_description'])\n",
        "                ee_task_progress_index += 1\n",
        "                ee_task_progress_label.value = (\n",
        "                    f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        "                )\n",
        "\n",
        "            raster_progress_index += 1\n",
        "            raster_progress_label.value = (\n",
        "                f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # Temporary file exists – move to downloads directory\n",
        "        if exists(raster_info['image_path_temp']):\n",
        "            move(raster_info['image_path_temp'], raster_info['image_path'])\n",
        "\n",
        "            raster_info.update({\n",
        "                'image_status': 'processed',\n",
        "                'ee_task_id': '',\n",
        "                'ee_task': None,\n",
        "                'task_current_execution': False\n",
        "            })\n",
        "\n",
        "            if raster_info['image_description'] not in ee_counted_tasks:\n",
        "                ee_counted_tasks.add(raster_info['image_description'])\n",
        "                ee_task_progress_index += 1\n",
        "                ee_task_progress_label.value = (\n",
        "                    f\"Earth Engine task progress: {ee_task_progress_index}/{raster_number}\"\n",
        "                )\n",
        "\n",
        "            raster_progress_index += 1\n",
        "            raster_progress_label.value = (\n",
        "                f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # Task is running – poll its status\n",
        "        if raster_info['image_status'] == 'task':\n",
        "            ee_task = raster_info['ee_task']\n",
        "            if ee_task is None:\n",
        "                # Fallback: find it again by ID\n",
        "                matches = [t for t in ee.batch.Task.list() if t.id == raster_info['ee_task_id']]\n",
        "                ee_task = matches[0] if matches else None\n",
        "                raster_info['ee_task'] = ee_task\n",
        "\n",
        "            if ee_task is not None:\n",
        "                task_state = ee_task.status()['state']\n",
        "\n",
        "                if task_state in ('FAILED', 'CANCELLED'):\n",
        "                    raster_info.update({\n",
        "                        'image_status': 'failed',\n",
        "                        'ee_task_id': '',\n",
        "                        'ee_task': None,\n",
        "                        'task_current_execution': False\n",
        "                    })\n",
        "                    print(f\"{raster_info['image_description']} failed. Skipping.\")\n",
        "\n",
        "                elif task_state == 'COMPLETED':\n",
        "                    # Check if final file exists\n",
        "                    if exists(raster_info['image_path']):\n",
        "                        raster_info.update({\n",
        "                            'image_status': 'processed',\n",
        "                            'ee_task_id': '',\n",
        "                            'ee_task': None,\n",
        "                            'task_current_execution': False\n",
        "                        })\n",
        "                        raster_progress_index += 1\n",
        "                        raster_progress_label.value = (\n",
        "                            f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        "                        )\n",
        "                    # Check if temp file exists\n",
        "                    elif exists(raster_info['image_path_temp']):\n",
        "                        move(raster_info['image_path_temp'], raster_info['image_path'])\n",
        "                        raster_info.update({\n",
        "                            'image_status': 'processed',\n",
        "                            'ee_task_id': '',\n",
        "                            'ee_task': None,\n",
        "                            'task_current_execution': False\n",
        "                        })\n",
        "                        raster_progress_index += 1\n",
        "                        raster_progress_label.value = (\n",
        "                            f\"Raster download progress: {raster_progress_index}/{raster_number}\"\n",
        "                        )\n",
        "                    # Neither file exists, reset status to trigger download\n",
        "                    else:\n",
        "                        raster_info.update({\n",
        "                            'image_status': '',  # Reset to empty to queue task again\n",
        "                            'ee_task_id': '',\n",
        "                            'ee_task': None,\n",
        "                            'task_current_execution': False\n",
        "                        })\n",
        "                        print(f\"Earth Engine task {raster_info['image_description']} completed but file not found. Retrying download.\")\n",
        "\n",
        "            continue  # READY / RUNNING / QUEUED, keep polling\n",
        "\n",
        "        # Need to queue a new task\n",
        "        if raster_info['image_status'] == '':\n",
        "            # Wait for a free slot\n",
        "            if ee_current_task_count >= ee_max_concurrent_tasks:\n",
        "                continue  # try again next outer loop pass\n",
        "\n",
        "            ee_path = raster_info['ee_path']\n",
        "            ee_band = raster_info['ee_band']\n",
        "            ee_dataset_type = raster_info['ee_dataset_type']\n",
        "            ee_dataset_name = raster_info['ee_dataset_name']\n",
        "\n",
        "            # Check if this is an AlphaEarth dataset\n",
        "            is_alpha_earth = 'alpha_earth' in ee_dataset_name.lower()\n",
        "\n",
        "            # Select the appropriate image and band\n",
        "            if ee_dataset_type == 'ImageCollection':\n",
        "                ee_image = ee.ImageCollection(ee_path)\n",
        "                if is_alpha_earth:\n",
        "                    # Special AlphaEarth processing: extract year and filter\n",
        "                    year = raster_info['image_description'].split('_')[2]  # Extract year from description\n",
        "                    ee_filtered = ee_image.filterDate(f'{year}-01-01', f'{int(year)+1}-01-01')\n",
        "                    image_selected = ee_filtered.mosaic().select([ee_band])\n",
        "                    resolution = ee_filtered.first().projection().nominalScale().getInfo()\n",
        "                else:\n",
        "                    # Standard processing for regular ImageCollections\n",
        "                    image_selected = ee_image.qualityMosaic(ee_band).select([ee_band])\n",
        "                    resolution = ee_image.first().projection().nominalScale().getInfo()\n",
        "            elif ee_dataset_type == 'Image':\n",
        "                ee_image = ee.Image(ee_path)\n",
        "                image_selected = ee_image.select([ee_band])\n",
        "                resolution = ee_image.select(0).projection().nominalScale().getInfo()\n",
        "\n",
        "            task = ee.batch.Export.image.toDrive(\n",
        "                image=image_selected.toFloat(),\n",
        "                description=raster_info['image_description'],\n",
        "                fileNamePrefix=raster_info['image_description'],\n",
        "                fileFormat='GeoTIFF',\n",
        "                region=ee_geometry,\n",
        "                scale=resolution,\n",
        "                maxPixels=10000000000,\n",
        "                crs='EPSG:4326'\n",
        "            )\n",
        "            task.start()\n",
        "\n",
        "            ee_current_task_count += 1\n",
        "            raster_info.update({\n",
        "                'image_status': 'task',\n",
        "                'ee_task_id': task.id,\n",
        "                'ee_task': task,\n",
        "                'task_current_execution': True\n",
        "            })\n",
        "\n",
        "    sleep(5)\n",
        "\n",
        "processed_count = sum(1 for v in raster_dictionary.values()\n",
        "                     if v['image_status'] == 'processed')\n",
        "failed_count = sum(1 for v in raster_dictionary.values()\n",
        "                  if v['image_status'] == 'failed')\n",
        "\n",
        "print(f\"Final Status Check:\\nProcessed Rasters: {processed_count}\\nFailed Rasters: {failed_count}\")\n",
        "print(\"Check Earth Engine tasks here: https://code.earthengine.google.com/tasks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td73nZmzUws6"
      },
      "source": [
        "## Tiled method (experimental)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a3vShCduUzGZ"
      },
      "outputs": [],
      "source": [
        "print(\"USE AT OWN RISK.\")\n",
        "print(\"Last resort if queue methods are taking impossibly long to complete.\")\n",
        "print(\"Attempts to iteratively tile the request until it's small enough for direct download.\")\n",
        "print(\"May cause the user to hit daily or total Earth Engine request/usage limits.\")\n",
        "\n",
        "user_input = input(\"Type 'OK' to proceed: \")\n",
        "if user_input.upper() == 'OK': pass\n",
        "else: print(\"Not OK.\")\n",
        "\n",
        "# Set to False to suppress detailed messages about image splitting\n",
        "verbose = False\n",
        "\n",
        "compression = [\n",
        "    'COMPRESS=ZSTD', # Good speed / size ratio\n",
        "    'ZSTD_LEVEL=1',\n",
        "]\n",
        "\n",
        "clip_geometry = False  # If True, clips the download geometry to the dataset footprint\n",
        "# If False, the empty geometry will be filled with nodata values.\n",
        "\n",
        "# Create a temporary directory for tiles\n",
        "temp_tiles_dir = join(ee_dir, 'temp_tiles')\n",
        "makedirs(temp_tiles_dir, exist_ok=True)\n",
        "\n",
        "# Load template and set Earth Engine geometry\n",
        "template_polygon_dir = join(polygons_dir, 'template.gpkg')\n",
        "template_area = gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0]\n",
        "template_coords = list(gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0].exterior.coords)\n",
        "ee_geometry = ee.Geometry.Polygon(template_coords)\n",
        "\n",
        "# Create a dictionary of all rasters to download\n",
        "raster_dictionary = {}\n",
        "\n",
        "# Populate the dictionary with information about each raster\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "    for ee_path in ee_paths:\n",
        "        # Identify bands\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image_collection = ee.ImageCollection(ee_path)\n",
        "            # Special handling for AlphaEarth to avoid memory limits\n",
        "            if 'SATELLITE_EMBEDDING' in ee_path:\n",
        "                # We know AlphaEarth has bands A00-A63, avoid getInfo() call\n",
        "                ee_bands = [f\"A{i:02d}\" for i in range(64)]\n",
        "            else: ee_bands = [b['id'] for b in ee_image_collection.getInfo()['features'][0]['bands']]\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "        # Loop through bands and create entries in dictionary\n",
        "        for ee_band in ee_bands:\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_tif_dir = join(ee_dir, ee_tif_filename)\n",
        "            # Different temporary directory required if MyDrive is mounted\n",
        "            if base_dir.startswith('/content/drive/MyDrive/'):\n",
        "                ee_temp_dir = join(\"/content/drive/MyDrive/\", ee_tif_filename)\n",
        "            else: ee_temp_dir = join(\"/gdrive/MyDrive\", ee_tif_filename)\n",
        "            description = ee_tif_filename[:-4]\n",
        "            raster_dictionary[description] = {\n",
        "                'ee_dataset_type': ee_dataset_type,\n",
        "                'ee_path': ee_path,\n",
        "                'ee_band': ee_band,\n",
        "                'image_path': ee_tif_dir,\n",
        "                'image_path_temp': ee_temp_dir,\n",
        "                'image_description': description,\n",
        "                'image_status': '',\n",
        "                'ee_object_id': ee_path\n",
        "            }\n",
        "\n",
        "# Control parallel processing\n",
        "max_concurrent_images = 10\n",
        "\n",
        "# Lock for updating progress\n",
        "progress_lock = threading.Lock()\n",
        "\n",
        "# Total raster count\n",
        "raster_number = len(raster_dictionary)\n",
        "\n",
        "# Global variable for tracking progress\n",
        "global_progress_index = 0\n",
        "\n",
        "# Function to display custom progress bar\n",
        "def display_progress():\n",
        "    percent = int((global_progress_index / raster_number) * 100) if raster_number > 0 else 0\n",
        "    bar_width = 80\n",
        "    filled_length = int(bar_width * global_progress_index // raster_number)\n",
        "    bar = '=' * filled_length + ' ' * (bar_width - filled_length)\n",
        "    progress_html = f\"\"\"\n",
        "    <div style=\"width:100%; margin-top:10px; margin-bottom:10px;\">\n",
        "        <div style=\"color:#CCCCCC; font-family:monospace;\">\n",
        "            Raster download progress: {percent}% [{bar}] {global_progress_index}/{raster_number}\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    clear_output(wait=True)\n",
        "    display(HTML(progress_html))\n",
        "\n",
        "# Display initial progress\n",
        "display_progress()\n",
        "\n",
        "def download_tile(raster_band, geometry, scale, output_path, max_retries=3):\n",
        "    \"\"\"Try to download a tile with the given geometry, handling EE-specific errors\"\"\"\n",
        "    for retry in range(max_retries):\n",
        "        try:\n",
        "            # Get the download URL\n",
        "            url = raster_band.getDownloadURL({\n",
        "                'scale': scale,\n",
        "                'region': geometry,\n",
        "                'format': 'GEO_TIFF',\n",
        "                'crs': 'EPSG:4326'\n",
        "            })\n",
        "            # Download the file\n",
        "            urlretrieve(url, output_path)\n",
        "            return True, None\n",
        "        except ee.EEException as e:\n",
        "            error_msg = str(e)\n",
        "            # Check for size-related errors specifically\n",
        "            if \"Total request size\" in error_msg and \"must be less than or equal to\" in error_msg:\n",
        "                return False, \"SIZE_LIMIT\"\n",
        "            else:\n",
        "                if retry < max_retries - 1:\n",
        "                    sleep(5)\n",
        "        except Exception as e:\n",
        "            if retry < max_retries - 1:\n",
        "                sleep(5)\n",
        "    return False, \"OTHER_ERROR\"\n",
        "\n",
        "def split_tile_vertically(geometry, n_parts=2):\n",
        "    \"\"\"Split a rectangular geometry into n_parts vertically\"\"\"\n",
        "    bounds = geometry.bounds().getInfo()['coordinates'][0]\n",
        "    min_x = min(coord[0] for coord in bounds)\n",
        "    min_y = min(coord[1] for coord in bounds)\n",
        "    max_x = max(coord[0] for coord in bounds)\n",
        "    max_y = max(coord[1] for coord in bounds)\n",
        "    height = max_y - min_y\n",
        "    part_height = height / n_parts\n",
        "    parts = []\n",
        "    for i in range(n_parts):\n",
        "        part_min_y = min_y + (i * part_height)\n",
        "        part_max_y = min_y + ((i + 1) * part_height)\n",
        "        parts.append(ee.Geometry.Rectangle([min_x, part_min_y, max_x, part_max_y]))\n",
        "    return parts\n",
        "\n",
        "def process_image(image_description, raster_info):\n",
        "    \"\"\"Process a single raster image - to be run in parallel\"\"\"\n",
        "    global global_progress_index\n",
        "    # Skip if already processed or failed\n",
        "    if exists(raster_info['image_path']) or raster_info.get('image_status') == 'failed':\n",
        "        with progress_lock:\n",
        "            global_progress_index += 1\n",
        "            # No direct widget update here - handled by update_progress_display thread\n",
        "        return True\n",
        "    # Get the band\n",
        "    ee_object = None\n",
        "    raster_band = None\n",
        "    tile_paths = []\n",
        "    try:\n",
        "        ee_path = raster_info['ee_path']\n",
        "        ee_band = raster_info['ee_band']\n",
        "        ee_dataset_type = raster_info['ee_dataset_type']\n",
        "        # Select the appropriate image and band\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image_collection = ee.ImageCollection(ee_path)\n",
        "            # Special handling for AlphaEarth to avoid memory limits\n",
        "            if 'SATELLITE_EMBEDDING' in ee_path:\n",
        "                # Extract year from image_description (e.g., \"alpha_earth_2024_ANNUAL_A00\" -> 2024)\n",
        "                year = image_description.split('_')[2]\n",
        "                # Filter to specific year but DON'T filter by bounds yet - let it mosaic first\n",
        "                ee_filtered = ee_image_collection.filterDate(f'{year}-01-01', f'{int(year)+1}-01-01')\n",
        "                ee_object = ee_filtered.mosaic()\n",
        "                # Now clip to your geometry AFTER mosaicking\n",
        "                # ee_object = ee_object.clip(ee_geometry)\n",
        "                projection = ee_filtered.first().projection()\n",
        "            else:\n",
        "                # Original logic for all other datasets - UNCHANGED\n",
        "                ee_object = ee_image_collection.qualityMosaic(ee_band)\n",
        "                projection = ee_image_collection.first().projection()\n",
        "            raster_band = ee_object.select([ee_band]).toFloat()\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_object = ee.Image(ee_path)\n",
        "            raster_band = ee_object.select([ee_band]).toFloat()\n",
        "            projection = ee_object.select(0).projection()\n",
        "        if clip_geometry:\n",
        "            # Get the footprint\n",
        "            footprint = ee_object.geometry()\n",
        "            # Calculate intersection with the footprint\n",
        "            download_geometry = ee_geometry.intersection(footprint)\n",
        "        else: download_geometry = ee_geometry\n",
        "        # Get image projection and scale\n",
        "        scale = projection.nominalScale().getInfo()\n",
        "        # Create a folder for this image's tiles\n",
        "        image_tiles_dir = join(temp_tiles_dir, image_description)\n",
        "        makedirs(image_tiles_dir, exist_ok=True)\n",
        "        # Track downloaded tiles\n",
        "        tile_paths = []\n",
        "        # First try to download the whole image at once\n",
        "        whole_image_path = join(image_tiles_dir, f\"{image_description}_whole.tif\")\n",
        "        success, error_type = download_tile(raster_band, download_geometry, scale, whole_image_path)\n",
        "        if success: tile_paths = [whole_image_path]\n",
        "        else:\n",
        "            # Log failure only if verbose\n",
        "            if verbose:\n",
        "                if error_type == \"SIZE_LIMIT\": print(f\"[{image_description}] Full image download failed due to size limit, starting adaptive tiling...\")\n",
        "                else: print(f\"[{image_description}] Full image download failed, starting adaptive tiling...\")\n",
        "            # Initial split factor depends on error type\n",
        "            initial_parts = 2 if error_type == \"SIZE_LIMIT\" else 2\n",
        "            # Start with initial split of the geometry\n",
        "            parts = split_tile_vertically(download_geometry, initial_parts)\n",
        "            tiles_to_process = []\n",
        "            for i in range(len(parts)):\n",
        "                part_id = str(uuid.uuid4()).replace('-', '')\n",
        "                tiles_to_process.append((parts[i], part_id, f\"{image_description}_part_{part_id}.tif\"))\n",
        "            successful_tile_height = None\n",
        "            # Process tiles until none are left\n",
        "            while tiles_to_process:\n",
        "                current_geometry, part_num, tile_filename = tiles_to_process.pop(0)\n",
        "                tile_path = join(image_tiles_dir, tile_filename)\n",
        "                # Try to download with current dimensions\n",
        "                success, error_type = download_tile(raster_band, current_geometry, scale, tile_path)\n",
        "                if success:\n",
        "                    tile_paths.append(tile_path)\n",
        "                    # If this is our first successful tile, remember its height\n",
        "                    if successful_tile_height is None:\n",
        "                        bounds = current_geometry.bounds().getInfo()['coordinates'][0]\n",
        "                        min_y = min(coord[1] for coord in bounds)\n",
        "                        max_y = max(coord[1] for coord in bounds)\n",
        "                        successful_tile_height = max_y - min_y\n",
        "                else: # If download failed, log it if verbose\n",
        "                    if verbose:\n",
        "                        print(f\"[{image_description}] Part {part_num} download failed: {error_type}\")\n",
        "                    # If size limit error, split more aggressively\n",
        "                    split_factor = 3 if error_type == \"SIZE_LIMIT\" else 2\n",
        "                    # If we have a successful tile height, try to use it\n",
        "                    if successful_tile_height is not None:\n",
        "                        bounds = current_geometry.bounds().getInfo()['coordinates'][0]\n",
        "                        min_y = min(coord[1] for coord in bounds)\n",
        "                        max_y = max(coord[1] for coord in bounds)\n",
        "                        current_height = max_y - min_y\n",
        "                        # Calculate how many parts we need to match the successful height\n",
        "                        needed_parts = max(split_factor, math.ceil(current_height / successful_tile_height))\n",
        "                        split_parts = split_tile_vertically(current_geometry, needed_parts)\n",
        "                    else: split_parts = split_tile_vertically(current_geometry, split_factor)\n",
        "                    # Add new parts to the processing queue\n",
        "                    for i, geom in enumerate(split_parts):\n",
        "                        new_part_num = str(uuid.uuid4()).replace('-', '')\n",
        "                        tiles_to_process.append((geom, new_part_num, f\"{image_description}_part_{new_part_num}.tif\"))\n",
        "                    # Remove the failed attempt file if it exists\n",
        "                    if os.path.exists(tile_path):\n",
        "                        remove(tile_path)\n",
        "        # Merge tiles using GDAL\n",
        "        if len(tile_paths) > 0:\n",
        "            if len(tile_paths) == 1:\n",
        "                # Just one tile, compress and copy directly\n",
        "                merged_temp_path = raster_info['image_path_temp']\n",
        "                gdal_translate_options = gdal.TranslateOptions(\n",
        "                    format=\"GTiff\",\n",
        "                    creationOptions=compression)\n",
        "                gdal.Translate(merged_temp_path, tile_paths[0], options=gdal_translate_options)\n",
        "            else:\n",
        "                # Multiple tiles need merging\n",
        "                vrt_path = join(image_tiles_dir, f\"{image_description}_mosaic.vrt\")\n",
        "                merged_temp_path = raster_info['image_path_temp']\n",
        "                # Create VRT from tiles\n",
        "                gdal.BuildVRT(vrt_path, tile_paths)\n",
        "                # Translate VRT to GeoTIFF with compression\n",
        "                gdal_translate_options = gdal.TranslateOptions(\n",
        "                    format=\"GTiff\",\n",
        "                    creationOptions=compression)\n",
        "                gdal.Translate(merged_temp_path, vrt_path, options=gdal_translate_options)\n",
        "            # Move to final location\n",
        "            move(merged_temp_path, raster_info['image_path'])\n",
        "            # Update progress tracking with thread safety\n",
        "            with progress_lock:\n",
        "                global_progress_index += 1\n",
        "                # No direct widget update here - handled by update_progress_display thread\n",
        "            # Clean up tile folder after successful merge\n",
        "            rmtree(image_tiles_dir)\n",
        "            return True\n",
        "        else:\n",
        "           if verbose: print(f\"[{image_description}] Failed: No tiles were successfully downloaded\")\n",
        "           raster_info['image_status'] = 'failed'\n",
        "           return False\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"Error processing {image_description}: {str(e)}\")\n",
        "        raster_info['image_status'] = 'failed'\n",
        "        return False\n",
        "    finally:\n",
        "        # Explicitly clear any large objects\n",
        "        ee_object = None\n",
        "        raster_band = None\n",
        "\n",
        "# Function to update progress display periodically\n",
        "def update_progress_display():\n",
        "    last_count = 0\n",
        "    while global_progress_index < raster_number:\n",
        "        sleep(0.5)  # Update every half second\n",
        "        current_count = 0\n",
        "        with progress_lock:\n",
        "            current_count = global_progress_index\n",
        "        if current_count != last_count:\n",
        "            display_progress()\n",
        "            last_count = current_count\n",
        "    # Final update to ensure 100% is shown\n",
        "    display_progress()\n",
        "\n",
        "# Count initially processed images\n",
        "global_progress_index = 0\n",
        "for v in raster_dictionary.values():\n",
        "    if v.get('image_status') == 'processed' or exists(v['image_path']):\n",
        "        global_progress_index += 1\n",
        "\n",
        "# Display initial progress\n",
        "display_progress()\n",
        "\n",
        "# Create a list of pending images to process\n",
        "pending_images = [(desc, img) for desc, img in raster_dictionary.items()\n",
        "                  if not exists(img['image_path']) and img.get('image_status') != 'failed']\n",
        "if verbose: print(f\"Starting processing of {len(pending_images)} rasters with {max_concurrent_images} parallel workers\")\n",
        "\n",
        "# Start the progress monitoring thread\n",
        "progress_thread = threading.Thread(target=update_progress_display)\n",
        "progress_thread.daemon = True\n",
        "progress_thread.start()\n",
        "\n",
        "try:\n",
        "    # Use ThreadPoolExecutor for parallel processing\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent_images) as executor:\n",
        "        futures = {executor.submit(process_image, desc, img): (desc, img) for desc, img in pending_images}\n",
        "        # Wait for completion and process results\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            desc, _ = futures[future]\n",
        "            try:\n",
        "                success = future.result()\n",
        "                # Only log failures if verbose\n",
        "                if not success and verbose: print(f\"Raster {desc} processing failed\")\n",
        "            except Exception as e:\n",
        "                if verbose: print(f\"Raster {desc} processing generated an exception: {e}\")\n",
        "                # Mark as failed\n",
        "                raster_dictionary[desc]['image_status'] = 'failed'\n",
        "except Exception as e:\n",
        "    print(f\"Error in thread pool execution: {e}\")\n",
        "finally:\n",
        "    # Make sure we wait for the progress thread to update one last time\n",
        "    if progress_thread.is_alive(): sleep(0.6)  # Give time for one last update\n",
        "print(f\"Processing complete\")\n",
        "\n",
        "# Count and display results\n",
        "processed_count = sum(1 for v in raster_dictionary.values()\n",
        "                      if exists(v['image_path']))\n",
        "failed_count = sum(1 for v in raster_dictionary.values()\n",
        "                   if v.get('image_status') == 'failed')\n",
        "\n",
        "print(f\"Final Status: {processed_count} rasters processed, {failed_count} rasters failed\")\n",
        "print(\"Check Earth Engine tasks here: https://code.earthengine.google.com/tasks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIBbo-gsS3nD"
      },
      "source": [
        "# GLAD LCLUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W90YInS-S5I5"
      },
      "outputs": [],
      "source": [
        "# GLAD data can be used in-place of TMF data for testing non-TMF areas.\n",
        "# LCLUC contains several land cover and land use types, each with continuous metrics.\n",
        "# This splits them into categories for better modelling, based on the legend:\n",
        "# https://glad.umd.edu/sites/default/files/legend_0.xlsx\n",
        "# This block should be run before resampling.\n",
        "\n",
        "lcluc_dict = {\n",
        "    'terra_vegetation_cover_percent': (0, 24),\n",
        "    'terra_stable_tree_m': (25, 48),\n",
        "    'wetland_vegetation_cover_percent': (100, 124),\n",
        "    'wetland_stable_tree_m': (125, 148),\n",
        "    'open_surface_water_percent_of_year': (200, 207),\n",
        "    'snow_ice': (241, 241),\n",
        "    'cropland': (244, 244),\n",
        "    'built_up': (250, 250),\n",
        "    'ocean': (254, 254),\n",
        "}\n",
        "\n",
        "lcluc_exists = False\n",
        "for lcluc_raster in os.listdir(ee_dir):\n",
        "  if 'LCLUC' in lcluc_raster:\n",
        "    lcluc_exists = True\n",
        "    lcluc_path = join(ee_dir, lcluc_raster)\n",
        "    lcluc_array = gdal.Open(lcluc_path).ReadAsArray()\n",
        "    for key, (lower, upper) in lcluc_dict.items():\n",
        "        split_lcluc_filename = f\"{lcluc_raster[:-4]}_{key}.tif\"\n",
        "        split_lcluc_filename_binary = f\"{lcluc_raster[:-4]}_{key}_binary.tif\"\n",
        "        split_lcluc_dir = join(glad_lcluc_dir, split_lcluc_filename)\n",
        "        split_lcluc_dir_binary = join(glad_lcluc_dir, split_lcluc_filename_binary)\n",
        "        if not exists(split_lcluc_dir) and not exists(split_lcluc_dir_binary):\n",
        "          split_lcluc_mask = np.logical_and(lcluc_array >= lower, lcluc_array <= upper)\n",
        "          split_lcluc_array = np.where(split_lcluc_mask, lcluc_array, 0) # outside the range set to 0\n",
        "          non_zero_percentage = np.count_nonzero(split_lcluc_array) / split_lcluc_array.size * 100\n",
        "          if non_zero_percentage >= 0.1:\n",
        "            # Check if there's only one unique non-zero value, and convert to a 1-0 binary raster if true\n",
        "            unique_non_zero_values = np.unique(split_lcluc_array[split_lcluc_array > 0])\n",
        "            if len(unique_non_zero_values) == 1:\n",
        "                split_lcluc_array = np.where(split_lcluc_array > 0, 1, 0)\n",
        "                split_lcluc_dir = split_lcluc_dir_binary\n",
        "            export_array_as_tif(split_lcluc_array, split_lcluc_dir, template=lcluc_path)\n",
        "            print(f\"{lcluc_raster} raster has been processed\")\n",
        "\n",
        "if not lcluc_exists: print(\"There are no GLAD LCLUC rasters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya6zMBMdbFPR"
      },
      "source": [
        "# Alpha Earth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts2FamVtbHER"
      },
      "outputs": [],
      "source": [
        "# Rename and move to Alpha Earth directory\n",
        "\n",
        "for filename in os.listdir(ee_dir):\n",
        "    if 'alpha_earth' in filename and filename.endswith('.tif'):\n",
        "        if match := re.search(r'_(\\d{4})_', filename):\n",
        "            year = match.group(1)\n",
        "            new_name = re.sub(r'_\\d{4}_', '_', filename).replace('ANNUAL_', '').replace('.tif', f'_{year}.tif')\n",
        "            move(join(ee_dir, filename), f\"{alpha_earth_dir}/{new_name}\")\n",
        "            print(f\"{filename} -> {new_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-UOVsvRuHNS"
      },
      "source": [
        "# Resample EE rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5yEcU40E8eD"
      },
      "outputs": [],
      "source": [
        "# Create dictionary of all tifs in Earth Engine and user upload directory\n",
        "resample_dict = {}\n",
        "for resample_raster in os.listdir(ee_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'categorical'\"})\n",
        "for resample_raster in os.listdir(user_upload_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'categorical'\"})\n",
        "for resample_raster in os.listdir(glad_lcluc_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'continuous'\"})\n",
        "resample_dict = {key: value for key, value in sorted(resample_dict.items())}\n",
        "\n",
        "# Select rasters for resampling and verify data type (categorical or continuous)\n",
        "print(\"selected_original_rasters = {\")\n",
        "for key, value in resample_dict.items():\n",
        "  if key.endswith('.tif'): print(f'\"{key}\": {value},')\n",
        "print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfxPFqlxE0Ok"
      },
      "outputs": [],
      "source": [
        "selected_original_rasters = {\n",
        "\"tmf_AnnualChanges_Dec1990.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1991.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1992.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1993.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1994.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1995.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1996.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1997.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1998.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1999.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2000.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2001.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2002.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2003.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2004.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2005.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2006.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2007.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2008.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2009.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2010.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2011.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2012.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2013.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2014.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2015.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2016.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2017.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2018.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2019.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2020.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2021.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2022.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2023.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2024.tif\": 'categorical',\n",
        "\"tmf_AnnualDisruptionObs2023_y2023.tif\": 'categorical',\n",
        "\"tmf_AnnualDisruptionObs2024_SumNonForest.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1982.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1983.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1984.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1985.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1986.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1987.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1988.tif\": 'categorical',\n",
        "# \"tmf_Ndisturb_C2_1982_2022_y1989.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1990.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1991.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1992.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1993.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1994.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1995.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1996.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1997.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1998.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1999.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2000.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2001.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2002.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2003.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2004.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2005.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2006.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2007.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2008.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2009.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2010.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2011.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2012.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2013.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2014.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2015.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2016.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2017.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2018.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2019.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2020.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2021.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2022.tif\": 'categorical',\n",
        "\"tmf_TransitionMap_MainClasses_TransitionMap_MainClasses.tif\": 'categorical',\n",
        "\"tmf_TransitionMap_Subtypes_TransitionMap_Subtypes.tif\": 'categorical',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEZwd6XYXqRk"
      },
      "outputs": [],
      "source": [
        "# Set resample algorithms for different raster types\n",
        "# See https://gdal.org/programs/gdalwarp.html\n",
        "categorical_alg = 'near'\n",
        "continuous_alg = 'bilinear'\n",
        "\n",
        "template = gdal.Open(template_tif_path)\n",
        "template_dimensions = template.GetGeoTransform()\n",
        "xres, yres = template_dimensions[1], -template_dimensions[5]\n",
        "xmin = template_dimensions[0]\n",
        "ymin = template_dimensions[3] - template.RasterYSize * yres\n",
        "xmax = xmin + template.RasterXSize * xres\n",
        "ymax = template_dimensions[3]\n",
        "\n",
        "# Resample progress\n",
        "resample_progress_index = 0\n",
        "resample_progress_label = widgets.Label(f\"Resample progress: {resample_progress_index}/{len(selected_original_rasters.items())}\")\n",
        "display(resample_progress_label)\n",
        "\n",
        "# Iterate over selected rasters\n",
        "for original_raster_name, data_type in selected_original_rasters.items():\n",
        "  resampled_raster_dir = join(resampled_dir, original_raster_name)\n",
        "  if not exists(resampled_raster_dir):\n",
        "    original_raster_dir = join(ee_dir, original_raster_name)\n",
        "    if not exists(original_raster_dir): original_raster_dir = join(user_upload_dir, original_raster_name)\n",
        "    if not exists(original_raster_dir): original_raster_dir = join(glad_lcluc_dir, original_raster_name)\n",
        "    # Set resample type\n",
        "    if data_type == 'categorical': resample_alg = categorical_alg\n",
        "    if data_type == 'continuous': resample_alg = continuous_alg\n",
        "    src = gdal.Warp(\n",
        "        resampled_raster_dir,\n",
        "        original_raster_dir,\n",
        "        xRes=xres, yRes=yres,\n",
        "        outputBounds=(xmin, ymin, xmax, ymax),\n",
        "        resampleAlg=resample_alg,\n",
        "        outputType=gdalconst.GDT_Float32)\n",
        "    # Compress and close\n",
        "    driver = gdal.GetDriverByName(\"GTiff\")\n",
        "    src = driver.CreateCopy(resampled_raster_dir, src, 0, options=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'])\n",
        "    src = None\n",
        "  # Update resample progress\n",
        "  resample_progress_index += 1\n",
        "  resample_progress_label.value = f\"Resample progress: {resample_progress_index}/{len(selected_original_rasters.items())}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhukvPP02gBm"
      },
      "source": [
        "# TMF binary features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdGYp7HAXkD4"
      },
      "outputs": [],
      "source": [
        "# Check TMF data users guide for classification. https://forobs.jrc.ec.europa.eu/static/tmf/TMF_DataUsersGuide.pdf\n",
        "\n",
        "cell_size_x = gdal.Open(join(areas_dir, 'cell_size_x.tif')).ReadAsArray()\n",
        "cell_size_y = gdal.Open(join(areas_dir, 'cell_size_y.tif')).ReadAsArray()\n",
        "cell_size_ha = np.mean(cell_size_x) * np.mean(cell_size_y) / 10_000\n",
        "sieve_size = int(np.ceil(0.5/cell_size_ha)) # Removes all forest patches smaller than 0.5 ha\n",
        "print(f\"Forest binary sieve size (>0.5 ha) is {sieve_size} pixels.\")\n",
        "\n",
        "# Generate list of valid TMF rasters to convert to binary\n",
        "binary_list = []\n",
        "for resampled_raster in os.listdir(resampled_dir):\n",
        "  # Verify these are in the filenames\n",
        "  if 'DisruptionObs' in resampled_raster or 'AnnualChanges' in resampled_raster or 'Ndisturb' in resampled_raster:\n",
        "    # Verify this is the position of the year in the filename\n",
        "    if '2024' in resampled_raster: year = 2024 # This one has a funny name\n",
        "    else: year = resampled_raster[-8:-4]\n",
        "    if int(year) >= 1990: binary_list.append(resampled_raster) # Data prior to 1990 is poor\n",
        "\n",
        "# Binary progress\n",
        "binary_progress_index = 0\n",
        "binary_progress_label = widgets.Label(f\"Binary progress: {binary_progress_index}/{len(binary_list)}\")\n",
        "display(binary_progress_label)\n",
        "\n",
        "for resampled_raster in binary_list:\n",
        "  if '2024' in resampled_raster: year = 2024 # This one has a funny name\n",
        "  else: year = resampled_raster[-8:-4]\n",
        "  # Forest binary\n",
        "  if 'AnnualChanges' in resampled_raster:\n",
        "    forest_binary_path = join(binary_dir, f\"forest_binary_{year}.tif\")\n",
        "    if not exists(forest_binary_path):\n",
        "      ac_raster_path = join(resampled_dir, resampled_raster)\n",
        "      ac_array = gdal.Open(ac_raster_path).ReadAsArray()\n",
        "      # Set classes 1 & 2 as 1, all else as 0\n",
        "      forest_binary_array = np.where((ac_array == 1) | (ac_array == 2), 1, 0)\n",
        "\n",
        "      # Sieve to 0.5 ha, using 8-connectedness (3, 3)\n",
        "      fb_array_labelled, fb_array_features = label(forest_binary_array, structure=np.ones((3, 3)))\n",
        "      # Determine the size of each patch\n",
        "      fb_array_sizes = ndi_sum(forest_binary_array, fb_array_labelled, range(fb_array_features + 1))\n",
        "      # Create a mask to remove patches smaller than the threshold\n",
        "      fb_array_mask_sizes = fb_array_sizes >= sieve_size\n",
        "      fb_array_mask_sizes[0] = 0 # Ensure non-forest (0) is excluded\n",
        "      fb_array_mask = fb_array_mask_sizes[fb_array_labelled]\n",
        "      # Apply the mask to the forest binary array and export\n",
        "      fb_array_sieved = forest_binary_array * fb_array_mask\n",
        "      export_array_as_tif(fb_array_sieved, forest_binary_path, dtype=gdal.GDT_Int16)\n",
        "\n",
        "  # Disturbance binary\n",
        "  if 'DisruptionObs' in resampled_raster or 'Ndisturb' in resampled_raster:\n",
        "    disturbance_binary_path = join(binary_dir, f\"disturbance_binary_{year}.tif\")\n",
        "    if not exists(disturbance_binary_path):\n",
        "      ac_raster_path = glob.glob(f\"{resampled_dir}/*AnnualChanges*{year}*\")\n",
        "      ac_array = gdal.Open(ac_raster_path[0]).ReadAsArray()\n",
        "      do_raster_path = join(resampled_dir, resampled_raster)\n",
        "      do_array = gdal.Open(do_raster_path).ReadAsArray()\n",
        "      # Set all disruption events to '1' if they're not classed as undisturbed forest or water in AnnualChanges\n",
        "      disturbance_binary_array = np.where((do_array >= 1) & ((ac_array != 1) & (ac_array != 5)), 1, 0)\n",
        "      export_array_as_tif(disturbance_binary_array, disturbance_binary_path, dtype=gdal.GDT_Int16)\n",
        "\n",
        "  # Update binary progress\n",
        "  binary_progress_index += 1\n",
        "  binary_progress_label.value = f\"Binary progress: {binary_progress_index}/{len(binary_list)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHmFlgGylGFL"
      },
      "outputs": [],
      "source": [
        "# Extract mangrove binary (optional)\n",
        "extract_mangrove_binary = True\n",
        "\n",
        "if extract_mangrove_binary:\n",
        "  mangrove_binary_path = join(binary_dir, \"mangrove_binary.tif\")\n",
        "  if not exists(mangrove_binary_path):\n",
        "    # Open subtypes array\n",
        "    for resampled_raster in os.listdir(resampled_dir):\n",
        "      if 'Subtypes' in resampled_raster:\n",
        "        subtypes_raster_path = join(resampled_dir, resampled_raster)\n",
        "        subtypes_raster_array = gdal.Open(subtypes_raster_path).ReadAsArray()\n",
        "\n",
        "    # Open oldest available forest binary raster for full mangrove extent (1990)\n",
        "    forest_binary_1990_path = join(binary_dir, \"forest_binary_1990.tif\")\n",
        "    forest_binary_1990_array = gdal.Open(forest_binary_1990_path).ReadAsArray()\n",
        "\n",
        "    mangrove_binary_array = np.logical_and(forest_binary_1990_array == 1,\n",
        "        np.logical_or(subtypes_raster_array == 12,(subtypes_raster_array >= 61) & (subtypes_raster_array <= 69))\n",
        "    )\n",
        "\n",
        "    # Calculate the percentage of forest pixels that are mangrove\n",
        "    forest_1990_pixels = np.sum(forest_binary_1990_array)\n",
        "    mangrove_pixels = np.sum(mangrove_binary_array)\n",
        "    if mangrove_pixels > 0:\n",
        "      mangrove_percent = (mangrove_pixels / forest_1990_pixels) * 100\n",
        "      export_array_as_tif(mangrove_binary_array, mangrove_binary_path, dtype=gdal.GDT_Int16)\n",
        "      print(f\"Number of mangrove pixels: {mangrove_pixels}\")\n",
        "      print(f\"Percentage of 1990 forest pixels that are mangrove: {mangrove_percent:.2f}%\")\n",
        "    else: print(\"There are no mangrove pixels in the template area.\")\n",
        "  else: print(\"A mangrove binary raster already exists. Delete it to generate a new one.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJnV0l75FpPN"
      },
      "source": [
        "# LU polygon binary features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vigY_9XqXhCQ"
      },
      "outputs": [],
      "source": [
        "# Selected 'land use' polygons.\n",
        "# Creating a 'complete recovery' or 'complete restoration' scenario requires ONE of these as a proxy.\n",
        "# This can be multiple combined PAs / polygons that have no or minimal history of human disturbance.\n",
        "\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area.gpkg', 'project_area_buffered_bbox.gpkg', 'gedi_area.gpkg', 'project_area_inverse.gpkg', 'gedi_area_inverse.gpkg']\n",
        "print(\"lu_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    print(f\"'{polygon}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w62_C-Vt0Emj"
      },
      "outputs": [],
      "source": [
        "lu_polygons = [\n",
        "# 'peninsular_malaysia.gpkg',\n",
        "'lu_yong.gpkg',\n",
        "'lu_yong_lipis.gpkg',\n",
        "'lu_berkelah_jerantut.gpkg',\n",
        "'lu_tekai_tembeling.gpkg',\n",
        "'lu_ais.gpkg',\n",
        "'lu_tekam.gpkg',\n",
        "'lu_berkelah_temerloh.gpkg',\n",
        "'lu_remen_chereh.gpkg',\n",
        "'lu_berkelah_kuantan.gpkg',\n",
        "# 'forest_reserves.gpkg',\n",
        "# 'forest_reserves_inverse.gpkg',\n",
        "'lu_old-growth_protected_areas.gpkg',\n",
        "]\n",
        "\n",
        "# Convert all template values to 'nodata' in preparation\n",
        "template_tif = gdal.Open(template_tif_path)\n",
        "template_mask_array = gdal.Open(template_tif_path).ReadAsArray()\n",
        "template_mask_array = np.zeros_like(template_mask_array, dtype=np.int16)\n",
        "\n",
        "for lu_polygon in lu_polygons:\n",
        "  lu_binary_name = f\"{lu_polygon[:-5]}_binary.tif\"\n",
        "  lu_binary_path = join(binary_dir, lu_binary_name)\n",
        "  if not exists(lu_binary_path):\n",
        "    lu_polygon_path = join(polygons_dir, lu_polygon)\n",
        "    export_array_as_tif(template_mask_array, lu_binary_path, dtype=gdal.GDT_Int16)\n",
        "    # Burn the value '1' where it overlaps with the project area polygon\n",
        "    burn_polygon_to_raster(lu_binary_path, lu_polygon_path, fixed=True, fixed_value=1, all_touched=False)\n",
        "    print(f\"{lu_binary_name} has been created.\")\n",
        "  else: print(f\"{lu_binary_name} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKSLJnR4SE5H"
      },
      "source": [
        "# Binary masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3rRcG0_ZOdd"
      },
      "outputs": [],
      "source": [
        "# Generate masks for later scenario predictions, e.g. so outputs only show forest.\n",
        "mask_type_list = []\n",
        "for binary in os.listdir(binary_dir):\n",
        "    mask_type = binary.split('_')[0]\n",
        "    if mask_type not in mask_type_list:\n",
        "        mask_type_list.append(mask_type)\n",
        "\n",
        "print(\"mask_types = [\")\n",
        "for mask_type in mask_type_list:\n",
        "    print(f\"'{mask_type}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F0klPciSEHh"
      },
      "outputs": [],
      "source": [
        "mask_types = [\n",
        "'forest',\n",
        "# 'lu',\n",
        "# 'disturbance',\n",
        "]\n",
        "\n",
        "# Create list of binary rasters to mask\n",
        "binary_mask_list = []\n",
        "for mask_type in mask_types:\n",
        "  for binary in os.listdir(binary_dir):\n",
        "    if mask_type in binary:\n",
        "      binary_mask_list.append(binary)\n",
        "\n",
        "# Binary progress\n",
        "mask_progress_index = 0\n",
        "mask_progress_label = widgets.Label(f\"Binary progress: {mask_progress_index}/{len(binary_mask_list)}\")\n",
        "display(mask_progress_label)\n",
        "\n",
        "# Create masks from the selected binary raster type\n",
        "for mask_type in mask_types:\n",
        "  for binary in binary_mask_list:\n",
        "    # Skip binaries that don't match current mask type\n",
        "    if not binary.startswith(mask_type): continue\n",
        "    binary_path = join(binary_dir, binary)\n",
        "    try: year = str(int(binary[-8:-4]))\n",
        "    except ValueError:\n",
        "        print(f\"Cannot parse year from {binary}, skipping mask creation.\")\n",
        "        mask_progress_index += 1\n",
        "        mask_progress_label.value = f\"Binary progress: {mask_progress_index}/{len(binary_mask_list)}\"\n",
        "        continue\n",
        "    mask_raster_path = join(scenario_mask_dir, f\"mask_{mask_type}_{year}.tif\")\n",
        "    if not exists(mask_raster_path):\n",
        "        binary_array = gdal.Open(binary_path).ReadAsArray()\n",
        "        mask_array = np.where(binary_array == 0, nodatavalue, 1)\n",
        "        export_array_as_tif(mask_array, mask_raster_path, dtype=gdal.GDT_Int16)\n",
        "        print(f\"A mask raster has been created: {mask_raster_path}\")\n",
        "    else: print(f\"A mask raster already exists at: {mask_raster_path}\")\n",
        "    # Update mask progress\n",
        "    mask_progress_index += 1\n",
        "    mask_progress_label.value = f\"Binary progress: {mask_progress_index}/{len(binary_mask_list)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCv0mNkb2pIY"
      },
      "source": [
        "# Binary feature edge effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqkninr4qy-g"
      },
      "outputs": [],
      "source": [
        "# Edge effect feature generation for binary rasters\n",
        "# Adds spatial awareness to tabular machine learning algorithms (e.g. XGBoost)\n",
        "# without performance and optimisation issues of deep learning models (e.g. CNN)\n",
        "# Outputs two features per binary raster:\n",
        "# - edge_distance: signed distance to class boundary in metres (positive = class 1, negative = class 0)\n",
        "# - local_density: Gaussian-weighted proportion of class 1 within threshold\n",
        "# Addresses satellite geolocation uncertainty and ecological edge effects\n",
        "\n",
        "binary_list = []\n",
        "for binary_raster in os.listdir(binary_dir) + os.listdir(resampled_dir):\n",
        "    if \"binary\" in binary_raster:\n",
        "        binary_list.append(binary_raster)\n",
        "\n",
        "edge_effect_progress_index = 0\n",
        "edge_effect_progress_label = widgets.Label(f\"Edge effect progress: {edge_effect_progress_index}/{len(binary_list)}\")\n",
        "display(edge_effect_progress_label)\n",
        "\n",
        "for binary_raster in binary_list:\n",
        "    if \"binary\" in binary_raster:\n",
        "        edge_distance_filename = binary_raster.replace('binary', 'edge_distance')\n",
        "        local_density_filename = binary_raster.replace('binary', 'local_density')\n",
        "        edge_distance_path = join(edge_effects_dir, edge_distance_filename)\n",
        "        local_density_path = join(edge_effects_dir, local_density_filename)\n",
        "\n",
        "        if not exists(edge_distance_path) or not exists(local_density_path):\n",
        "            binary_raster_path = join(binary_dir, binary_raster)\n",
        "            if not exists(binary_raster_path):\n",
        "                binary_raster_path = join(resampled_dir, binary_raster)\n",
        "            binary_array = gdal.Open(binary_raster_path).ReadAsArray()\n",
        "\n",
        "            # Generate edge distance and local density features\n",
        "            signed_distance, local_density = edge_effects(binary_array, 'binary', cell_size_x_path, cell_size_y_path, threshold_metres)\n",
        "\n",
        "            if not exists(edge_distance_path): export_array_as_tif(signed_distance, edge_distance_path, dtype=gdal.GDT_Int16)\n",
        "            if not exists(local_density_path): export_array_as_tif(local_density, local_density_path)\n",
        "\n",
        "    edge_effect_progress_index += 1\n",
        "    edge_effect_progress_label.value = f\"Edge effect progress: {edge_effect_progress_index}/{len(binary_list)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPSRn0UB-8TI"
      },
      "source": [
        "# Continuous feature precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D35niESIwR9"
      },
      "outputs": [],
      "source": [
        "# Creates a dictionary of optiomal precision based on number of desired unique values.\n",
        "# Limiting the number unique values avoids overfitting and reduces training time.\n",
        "\n",
        "continuous_features = False\n",
        "\n",
        "override_max_unique_values = False\n",
        "max_unique_values = 5000 # Should be >=10\n",
        "\n",
        "if continuous_features:\n",
        "  if override_max_unique_values == False:\n",
        "    dem_base_path = join(areas_dir, \"base_dem.tif\")\n",
        "    dem_base_array = gdal.Open(dem_base_path).ReadAsArray()\n",
        "    max_unique_values = int(np.ptp(dem_base_array)) # Precision based on elevation variance\n",
        "  resampled_precision_dict = {}\n",
        "\n",
        "  for resampled_feature, resample_type in selected_original_rasters.items():\n",
        "    if resample_type == 'continuous':\n",
        "      resampled_feature_path = join(resampled_dir, resampled_feature)\n",
        "      print(f\"Reading {resampled_feature}...\")\n",
        "      # Read raster as array\n",
        "      resampled_feature_array = gdal.Open(resampled_feature_path).ReadAsArray()\n",
        "      # Convert 'nodata' values to nan\n",
        "      resampled_feature_array[resampled_feature_array == nodatavalue] = np.nan\n",
        "      resampled_feature_array_masked = np.ma.array(resampled_feature_array, mask=np.isnan(resampled_feature_array))\n",
        "      # Count unique values in raster\n",
        "      unique_values = len(np.unique(resampled_feature_array_masked))\n",
        "      print(f\"There are {unique_values} unique values in {resampled_feature}\")\n",
        "      # Generate histogram from 100,000 random points\n",
        "      random_selection = np.random.choice(resampled_feature_array_masked.ravel(), size = 100_000, replace = False)\n",
        "      _ = plt.hist(random_selection, bins='auto')  # arguments are passed to np.histogram\n",
        "      plt.title(f\"{resampled_feature}\")\n",
        "      plt.show()\n",
        "      # Remove 0 values for log10\n",
        "      resampled_feature_array_masked[resampled_feature_array_masked == 0] = np.nan\n",
        "      resampled_feature_array_masked = np.ma.array(resampled_feature_array, mask=np.isnan(resampled_feature_array))\n",
        "      # Create log10 array for determining positions for rounding\n",
        "      array_log10 = np.log10(abs(resampled_feature_array_masked))\n",
        "      place_value_decimal = int(abs(np.min(array_log10)))\n",
        "      place_value_integer = int(0 - np.max(array_log10))\n",
        "      # Iterate down precision levels to determine optimal number of unique values\n",
        "      min_starting_precision = len(str(max_unique_values))\n",
        "      for precision in reversed(range(place_value_integer, max(min_starting_precision, place_value_decimal +1))):\n",
        "        rounded_array = np.round(resampled_feature_array, decimals=precision)\n",
        "        round_unique_values = len(np.unique(rounded_array))\n",
        "        optimal_precision = None\n",
        "        if round_unique_values <= max_unique_values:\n",
        "          optimal_precision = precision\n",
        "          print(f\"The optimal precison for {resampled_feature} is {optimal_precision}, with {round_unique_values} unique values.\")\n",
        "          resampled_precision_dict.update({f'{resampled_feature}':f'{optimal_precision}'})\n",
        "          break\n",
        "      if optimal_precision == None: print(\"There's a problem with setting precision.\")\n",
        "      print(\"___________________\\n\")\n",
        "\n",
        "  print(\"Dictionary for optimal rounding values:\")\n",
        "  resampled_precision_dict\n",
        "\n",
        "  precision_dict_csv_path = join(resampled_dir, 'rounding_dictionary.csv')\n",
        "  # Save rounding dictionary to CSV\n",
        "  with open(precision_dict_csv_path, 'w', newline='') as precision_dict_csv:\n",
        "      writer = csv.writer(precision_dict_csv)\n",
        "      writer.writerow(resampled_precision_dict.keys())\n",
        "      writer.writerow(resampled_precision_dict.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UjcjQ52NoT7"
      },
      "outputs": [],
      "source": [
        "if continuous_features:\n",
        "  # Open rounding dictionary and verify\n",
        "  with open(precision_dict_csv_path, 'r') as file:\n",
        "      keys, values = list(csv.reader(file))\n",
        "      topo_precision_dict = dict(zip(keys, values))\n",
        "\n",
        "  # Verify precision and correct if necessary\n",
        "  print(\"topo_precision_dict = {\")\n",
        "  for key, value in topo_precision_dict.items():\n",
        "      print(f'\"{key}\": {value},')\n",
        "  print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2kquWfYOeS-"
      },
      "outputs": [],
      "source": [
        "topo_precision_dict = {\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn-nfKEPOi9d"
      },
      "outputs": [],
      "source": [
        "# Creates both unsmoothed and smoothed feature versions\n",
        "# Adds spatial awareness to tabular machine learning algorithms (e.g. XGBoost)\n",
        "# Without performance and optimisation issues of deep learning models (e.g. CNN)\n",
        "# Smoothed version captures local patterns and gradients within ecological context\n",
        "# Also helps account for satellite geolocation inaccuracies\n",
        "\n",
        "# Continuous progress\n",
        "continuous_progress_index = 0\n",
        "continuous_progress_label = widgets.Label(f\"Continuous progress: {continuous_progress_index}/{len(topo_precision_dict.items())}\")\n",
        "display(continuous_progress_label)\n",
        "\n",
        "# Iterate over selected continuous rasters\n",
        "for continuous, precision in topo_precision_dict.items():\n",
        "    cont_raster_resampled_path = join(resampled_dir, continuous)\n",
        "    cont_raster_resampled_array = gdal.Open(cont_raster_resampled_path).ReadAsArray()\n",
        "    # Convert nodata values to 0\n",
        "    cont_raster_resampled_array[cont_raster_resampled_array == nodatavalue] = 0\n",
        "\n",
        "    # Set path and check if exists\n",
        "    cont_raster_unsmoothed_filename = f\"{continuous[:-4]}_unsmooth.tif\"\n",
        "    cont_raster_unsmoothed_path = join(continuous_final_dir, cont_raster_unsmoothed_filename)\n",
        "    if not exists(cont_raster_unsmoothed_path):\n",
        "        # Round and export unsmoothed continuous raster\n",
        "        # Preserves original values at pixel level for model comparison\n",
        "        cont_raster_unsmoothed_rounded = np.round(cont_raster_resampled_array, decimals=int(precision))\n",
        "        export_array_as_tif(cont_raster_unsmoothed_rounded, cont_raster_unsmoothed_path)\n",
        "\n",
        "    # Smooth using 2d spatial convolution\n",
        "    cont_raster_smoothed_filename = f\"{continuous[:-4]}_smooth.tif\"\n",
        "    cont_raster_smoothed_path = join(continuous_final_dir, cont_raster_smoothed_filename)\n",
        "    if not exists(cont_raster_smoothed_path):\n",
        "        # Apply gaussian smoothing via edge_effects function\n",
        "        # Creates spatially-aware version capturing local patterns and gradients\n",
        "        # Model receives both smoothed and unsmoothed versions for enhanced feature learning\n",
        "        cont_raster_smoothed = edge_effects(cont_raster_resampled_array, 'continuous', cell_size_x_path, cell_size_y_path, threshold_metres)\n",
        "        # Round and export smoothed continuous raster\n",
        "        cont_raster_smoothed_rounded = np.round(cont_raster_smoothed, decimals=int(precision))\n",
        "        export_array_as_tif(cont_raster_smoothed_rounded, cont_raster_smoothed_path)\n",
        "\n",
        "    # Update continuous progress\n",
        "    continuous_progress_index += 1\n",
        "    continuous_progress_label.value = f\"Continuous progress: {continuous_progress_index}/{len(topo_precision_dict.items())}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6r7JXbijM50"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU_AC6MjNfTN"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "m5Y3xYz-Va9p",
        "caWuFWxx22QT",
        "0zwwDicI25Nq",
        "Ya6zMBMdbFPR"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "V6E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}