{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/9_statistics_agbd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j0th2jhtnAR"
      },
      "source": [
        "# Imports & Subdirectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9drq4euvG0MS"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGWgfUqto0F"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Imports and upgrades\n",
        "!pip install geopandas\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MTERlyxuPd4"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from datetime import datetime\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "from os.path import exists, join\n",
        "from os import makedirs\n",
        "from osgeo import gdal, ogr, osr\n",
        "gdal.UseExceptions()\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import rasterio\n",
        "from rasterio import mask as msk\n",
        "import re\n",
        "import requests\n",
        "from shutil import copyfile\n",
        "from sklearn.metrics import root_mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwar9sxbts-3"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "masks_dir = join(areas_dir, \"masks\")\n",
        "targets_dir = join(base_dir, \"2_targets\")\n",
        "gedi_raster_final_dir = join(targets_dir, \"gedi_raster_final\")\n",
        "targets_pkl_final_dir = join(targets_dir, \"pkl_final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "differences_dir = join(base_dir, \"8_differences\")\n",
        "\n",
        "statistics_agbd_dir = join(base_dir, \"9_statistics_agbd\")\n",
        "sample_polygons_dir = join(statistics_agbd_dir, \"sample_polygons\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(statistics_agbd_dir, exist_ok=True)\n",
        "makedirs(sample_polygons_dir, exist_ok=True)\n",
        "\n",
        "# Cell area raster for accurate pixel-by-pixel area calculations\n",
        "cell_area_path = join(areas_dir, \"cell_area.tif\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atvGRSPYgwk9"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: Sample raster values\n",
        "def sample_raster_values(pd_dataframe, raster_path, geom_x, geom_y, feature=False, n_threads=1):\n",
        "    # Derive column name from filename\n",
        "    raster_name = raster_path.split('/')[-1][:-4]\n",
        "    if feature: raster_name = 'fea_' + raster_name\n",
        "    # Load raster and extract metadata\n",
        "    raster = gdal.Open(raster_path)\n",
        "    band = raster.GetRasterBand(1)\n",
        "    geotransform = raster.GetGeoTransform()\n",
        "    raster_array = band.ReadAsArray()\n",
        "    nodata = band.GetNoDataValue()\n",
        "    rows, cols = raster_array.shape\n",
        "    fill_value = nodata if nodata is not None else np.nan\n",
        "    # Initialise output array with nodata\n",
        "    sampled_values = np.full(len(geom_x), fill_value, dtype=raster_array.dtype)\n",
        "    # Worker function for threaded sampling\n",
        "    def sample_chunk(start, end):\n",
        "        x_idx = ((geom_x[start:end] - geotransform[0]) / geotransform[1]).astype(int)\n",
        "        y_idx = ((geom_y[start:end] - geotransform[3]) / geotransform[5]).astype(int)\n",
        "        valid = (x_idx >= 0) & (x_idx < cols) & (y_idx >= 0) & (y_idx < rows)\n",
        "        local_values = np.full(end - start, fill_value, dtype=raster_array.dtype)\n",
        "        local_values[valid] = raster_array[y_idx[valid], x_idx[valid]]\n",
        "        sampled_values[start:end] = local_values\n",
        "    # Split points into chunks and process in parallel\n",
        "    n_points = len(geom_x)\n",
        "    chunk_size = (n_points + n_threads - 1) // n_threads\n",
        "    chunk_ranges = [(i, min(i + chunk_size, n_points)) for i in range(0, n_points, chunk_size)]\n",
        "    with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
        "        executor.map(lambda r: sample_chunk(*r), chunk_ranges)\n",
        "    # Assign to dataframe and release resources\n",
        "    pd_dataframe[raster_name] = sampled_values\n",
        "    raster = band = None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model comparison"
      ],
      "metadata": {
        "id": "mFRSKZkpTiqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model comparison directories\n",
        "model_comparison_dir = join(statistics_agbd_dir, \"model_comparison\")\n",
        "products_dir = join(model_comparison_dir, \"products\")\n",
        "\n",
        "makedirs(model_comparison_dir, exist_ok=True)\n",
        "makedirs(products_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "DLypdPDvTkcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model comparison directories\n",
        "model_comparison_dir = join(statistics_agbd_dir, \"model_comparison\")\n",
        "products_dir = join(model_comparison_dir, \"products\")\n",
        "makedirs(model_comparison_dir, exist_ok=True)\n",
        "makedirs(products_dir, exist_ok=True)\n",
        "\n",
        "# Configurable raster patterns\n",
        "l4d_prefix, l4d_suffix = \"GEDI04_D_original_epsg32648_UTM\", \"_agbd.tif\"\n",
        "l4b_filename = \"GEDI04_B_original_epsg6933_EASE-Grid_MU.tif\"\n",
        "veg_grid_filename = \"GEDI_vegetation_grid_original_epsg6933_EASE-Grid_agbd-a0-qf_20190417_20230316.tif\"\n",
        "\n",
        "# L4D validation metrics URL\n",
        "l4d_gpkg_url = \"https://data.ornldaac.earthdata.nasa.gov/public/gedi/GEDI_L4D_Imputed_Waveforms/comp/GEDI_L4D_20190418_20230316_validation_metrics.gpkg\"\n",
        "\n",
        "# Template for clipping\n",
        "template_gdf = gpd.read_file(join(polygons_dir, \"template.gpkg\"))\n",
        "\n",
        "# Locate GEDI product rasters\n",
        "gedi_products = {'L4D': [], 'L4B': None, 'vegetation_grid': None}\n",
        "gedi04d_dir = join(gedi_raster_final_dir, \"GEDI04_D\")\n",
        "gedi04b_dir = join(gedi_raster_final_dir, \"GEDI04_B\")\n",
        "veg_dir = join(gedi_raster_final_dir, \"GEDI_vegetation_grid\")\n",
        "\n",
        "if exists(gedi04d_dir):\n",
        "    gedi_products['L4D'] = [join(gedi04d_dir, f) for f in os.listdir(gedi04d_dir) if f.startswith(l4d_prefix) and f.endswith(l4d_suffix)]\n",
        "print(f\"# L4D grid: {len(gedi_products['L4D'])} UTM tiles.\")\n",
        "\n",
        "if exists(gedi04b_dir) and exists(join(gedi04b_dir, l4b_filename)):\n",
        "    gedi_products['L4B'] = join(gedi04b_dir, l4b_filename)\n",
        "print(f\"# L4B grid: {'found' if gedi_products['L4B'] else 'not found'}\")\n",
        "\n",
        "if exists(veg_dir) and exists(join(veg_dir, veg_grid_filename)):\n",
        "    gedi_products['vegetation_grid'] = join(veg_dir, veg_grid_filename)\n",
        "print(f\"# Vegetation grid (AGBD): {'found' if gedi_products['vegetation_grid'] else 'not found'}\")\n",
        "\n",
        "# Discover model predictions\n",
        "model_predictions = {}\n",
        "for source_dir, subdir_name, category in [(scenarios_dir, 'scenario_predictions', 'scenarios'), (uncertainty_dir, 'uncertainty_predictions', 'uncertainty')]:\n",
        "    if not exists(source_dir): continue\n",
        "    for model_name in os.listdir(source_dir):\n",
        "        pred_dir = join(source_dir, model_name, subdir_name)\n",
        "        if not exists(pred_dir): continue\n",
        "        for f in os.listdir(pred_dir):\n",
        "            if not f.endswith('.tif'): continue\n",
        "            parts = f.split('__')\n",
        "            if category == 'scenarios' and len(parts) >= 2 and parts[0].isdigit() and len(parts[0]) == 4:\n",
        "                year = parts[0]\n",
        "            elif category == 'uncertainty' and len(parts) >= 3 and parts[0] == 'mean' and parts[1].isdigit() and len(parts[1]) == 4:\n",
        "                year = parts[1]\n",
        "            else: continue\n",
        "            if model_name not in model_predictions: model_predictions[model_name] = {}\n",
        "            if category not in model_predictions[model_name]: model_predictions[model_name][category] = []\n",
        "            model_predictions[model_name][category].append(year)\n",
        "for model in model_predictions:\n",
        "    for cat in model_predictions[model]:\n",
        "        model_predictions[model][cat] = sorted(set(model_predictions[model][cat]))\n",
        "\n",
        "# Print selectable dictionary\n",
        "print(\"\\n# Model predictions available (comment out to exclude):\")\n",
        "print(\"selected_predictions = {\")\n",
        "for model, categories in model_predictions.items():\n",
        "    print(f\"    '{model}': {{\")\n",
        "    for cat, years in categories.items():\n",
        "        print(f\"        '{cat}': [\")\n",
        "        for y in years:\n",
        "            print(f\"            '{y}',\")\n",
        "        print(f\"        ],\")\n",
        "    print(f\"    }},\")\n",
        "print(\"}\")"
      ],
      "metadata": {
        "id": "lulH4FkDPr4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L4D grid: 1 UTM tiles.\n",
        "# L4B grid: found\n",
        "# Vegetation grid: found\n",
        "\n",
        "# Model predictions available (comment out to exclude):\n",
        "selected_predictions = {\n",
        "    'agbd_251203_161707': {\n",
        "        'scenarios': [\n",
        "            '2018',\n",
        "            '2019',\n",
        "            '2020',\n",
        "            '2021',\n",
        "            '2022',\n",
        "            '2023',\n",
        "            '2024',\n",
        "        ],\n",
        "        'uncertainty': [\n",
        "            '2021',\n",
        "            '2024',\n",
        "        ],\n",
        "    },\n",
        "    'agbd_alpha_earth_251216_144719': {\n",
        "        'scenarios': [\n",
        "            '2023',\n",
        "            '2024',\n",
        "        ],\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "YoJzdkqfUwYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process existing GEDI data: L4D area-weighted polygon accuracy and mask all GEDI rasters\n",
        "# Forest mask year: 2023 used for all products as this is the final GEDI data collection year\n",
        "gedi_mask_year = 2023\n",
        "\n",
        "gpkg_raw = join(products_dir, \"GEDI_L4D_validation_metrics_raw.gpkg\")\n",
        "gpkg_clipped = join(products_dir, \"GEDI_L4D_validation_metrics_clipped.gpkg\")\n",
        "l4d_accuracy_path = join(model_comparison_dir, \"l4d_tile_accuracy.csv\")\n",
        "\n",
        "# Download validation gpkg\n",
        "if not exists(gpkg_raw):\n",
        "    print(\"Downloading L4D validation gpkg...\")\n",
        "    response = requests.get(l4d_gpkg_url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    with open(gpkg_raw, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192): f.write(chunk)\n",
        "    print(\"  Download complete\")\n",
        "else: print(\"L4D validation gpkg exists\")\n",
        "\n",
        "# Clip to template extent\n",
        "if not exists(gpkg_clipped):\n",
        "    print(\"Clipping gpkg to template extent...\")\n",
        "    tiles_gdf = gpd.read_file(gpkg_raw).to_crs(template_gdf.crs)\n",
        "    tiles_clipped = gpd.clip(tiles_gdf, template_gdf)\n",
        "    tiles_clipped.to_file(gpkg_clipped, driver=\"GPKG\")\n",
        "    print(f\"  Clipped to {len(tiles_clipped)} tiles\")\n",
        "else: print(\"Clipped gpkg exists\")\n",
        "\n",
        "# Calculate weighted accuracy\n",
        "if not exists(l4d_accuracy_path):\n",
        "    print(\"Calculating L4D area-weighted polygon accuracy...\")\n",
        "    tiles_gdf = gpd.read_file(gpkg_clipped)\n",
        "    rmse_col, mean_col = \"rmse_agbd\", \"agbd_mean_valid\"\n",
        "\n",
        "    # Load reference L4D raster\n",
        "    ref_raster = gedi_products['L4D'][0]\n",
        "    ref_ds = gdal.Open(ref_raster)\n",
        "    ref_proj, ref_gt = ref_ds.GetProjection(), ref_ds.GetGeoTransform()\n",
        "    ref_xsize, ref_ysize = ref_ds.RasterXSize, ref_ds.RasterYSize\n",
        "    ref_bounds = [ref_gt[0], ref_gt[3] + ref_gt[5] * ref_ysize, ref_gt[0] + ref_gt[1] * ref_xsize, ref_gt[3]]\n",
        "    ref_ds = None\n",
        "\n",
        "    # Resample forest mask to reference grid\n",
        "    gedi_mask_path = join(masks_dir, f\"mask_forest_{gedi_mask_year}.tif\")\n",
        "    mask_aligned = gdal.Warp('', gedi_mask_path, options=gdal.WarpOptions(format='MEM', dstSRS=ref_proj, outputBounds=ref_bounds, width=ref_xsize, height=ref_ysize, resampleAlg='near', dstNodata=nodatavalue))\n",
        "    mask_array = mask_aligned.GetRasterBand(1).ReadAsArray()\n",
        "    mask_aligned = None\n",
        "\n",
        "    # Reproject tiles to raster CRS\n",
        "    raster_srs = osr.SpatialReference()\n",
        "    raster_srs.ImportFromWkt(ref_proj)\n",
        "    raster_epsg = int(raster_srs.GetAuthorityCode(None))\n",
        "    tiles_reproj = tiles_gdf.to_crs(epsg=raster_epsg)\n",
        "\n",
        "    # Calculate forest pixel counts per tile\n",
        "    forest_counts = []\n",
        "    for idx, tile in tiles_reproj.iterrows():\n",
        "        minx, miny, maxx, maxy = tile.geometry.bounds\n",
        "        col_start, col_end = max(0, int((minx - ref_gt[0]) / ref_gt[1])), min(ref_xsize, int((maxx - ref_gt[0]) / ref_gt[1]))\n",
        "        row_start, row_end = max(0, int((ref_gt[3] - maxy) / abs(ref_gt[5]))), min(ref_ysize, int((ref_gt[3] - miny) / abs(ref_gt[5])))\n",
        "        if col_end > col_start and row_end > row_start:\n",
        "            tile_mask = mask_array[row_start:row_end, col_start:col_end]\n",
        "            forest_count = np.sum(tile_mask == 1)\n",
        "        else: forest_count = 0\n",
        "        forest_counts.append(forest_count)\n",
        "\n",
        "    tiles_gdf['forest_pixels'] = forest_counts\n",
        "    tiles_gdf['rrmse'] = (tiles_gdf[rmse_col] / tiles_gdf[mean_col]) * 100\n",
        "\n",
        "    # Filter and aggregate\n",
        "    tiles_valid = tiles_gdf[(tiles_gdf['forest_pixels'] > 0)].dropna(subset=[rmse_col, 'rrmse'])\n",
        "    weights = tiles_valid['forest_pixels'].values\n",
        "    weighted_rmse = np.sqrt(np.average(tiles_valid[rmse_col].values**2, weights=weights))\n",
        "    weighted_rrmse = np.average(tiles_valid['rrmse'].values, weights=weights)\n",
        "\n",
        "    # Save results\n",
        "    l4d_accuracy = {'source': 'L4D grid (area-weighted polygons)', 'rmse': weighted_rmse, 'rrmse': weighted_rrmse, 'n_tiles': len(tiles_valid), 'n_forest_pixels': int(weights.sum())}\n",
        "    pd.DataFrame([l4d_accuracy]).to_csv(l4d_accuracy_path, index=False)\n",
        "    print(f\"  Tiles: {len(tiles_valid)}, Forest pixels: {weights.sum():,}\")\n",
        "    print(f\"  RMSE={weighted_rmse:.4f}, rRMSE={weighted_rrmse:.2f}%\")\n",
        "    print(f\"  Saved: {l4d_accuracy_path}\")\n",
        "else:\n",
        "    l4d_accuracy = pd.read_csv(l4d_accuracy_path).iloc[0].to_dict()\n",
        "    print(f\"L4D tile accuracy loaded: RMSE={l4d_accuracy['rmse']:.4f}, rRMSE={l4d_accuracy['rrmse']:.2f}%\")\n",
        "\n",
        "# Copy and mask GEDI rasters to forest extent\n",
        "print(\"\\nCopying and masking GEDI rasters\")\n",
        "\n",
        "# Mask raster to forest extent, resampling mask if needed\n",
        "def apply_forest_mask(raster_path, mask_path, output_path):\n",
        "    if exists(output_path):\n",
        "        print(f\"  Exists: {os.path.basename(output_path)}\")\n",
        "        return\n",
        "    raster_ds = gdal.Open(raster_path)\n",
        "    raster_proj = raster_ds.GetProjection()\n",
        "    raster_gt = raster_ds.GetGeoTransform()\n",
        "    raster_xsize, raster_ysize = raster_ds.RasterXSize, raster_ds.RasterYSize\n",
        "    raster_bounds = [raster_gt[0], raster_gt[3] + raster_gt[5] * raster_ysize, raster_gt[0] + raster_gt[1] * raster_xsize, raster_gt[3]]\n",
        "    mask_resampled = gdal.Warp('', mask_path, options=gdal.WarpOptions(format='MEM', dstSRS=raster_proj, outputBounds=raster_bounds, width=raster_xsize, height=raster_ysize, resampleAlg='near', dstNodata=nodatavalue))\n",
        "    mask_array = mask_resampled.GetRasterBand(1).ReadAsArray()\n",
        "    raster_array = raster_ds.GetRasterBand(1).ReadAsArray()\n",
        "    raster_array = np.where(mask_array == 1, raster_array, nodatavalue)\n",
        "    driver = gdal.GetDriverByName('GTiff')\n",
        "    out_ds = driver.Create(output_path, raster_xsize, raster_ysize, 1, raster_ds.GetRasterBand(1).DataType, options=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'])\n",
        "    out_ds.SetGeoTransform(raster_gt)\n",
        "    out_ds.SetProjection(raster_proj)\n",
        "    out_band = out_ds.GetRasterBand(1)\n",
        "    out_band.WriteArray(raster_array)\n",
        "    out_band.SetNoDataValue(nodatavalue)\n",
        "    out_ds = raster_ds = mask_resampled = None\n",
        "    print(f\"  Created: {os.path.basename(output_path)}\")\n",
        "\n",
        "gedi_mask_path = join(masks_dir, f\"mask_forest_{gedi_mask_year}.tif\")\n",
        "\n",
        "# Mask L4D rasters\n",
        "l4d_masked = []\n",
        "if gedi_products['L4D']:\n",
        "    print(\"L4D rasters:\")\n",
        "    for raster_path in gedi_products['L4D']:\n",
        "        filename = os.path.basename(raster_path).replace('_original', '').replace('.tif', f'_masked_{gedi_mask_year}.tif')\n",
        "        output_path = join(products_dir, filename)\n",
        "        apply_forest_mask(raster_path, gedi_mask_path, output_path)\n",
        "        l4d_masked.append(output_path)\n",
        "\n",
        "# Mask L4B raster\n",
        "l4b_masked = None\n",
        "if gedi_products['L4B']:\n",
        "    print(\"L4B raster:\")\n",
        "    filename = os.path.basename(gedi_products['L4B']).replace('_original', '').replace('.tif', f'_masked_{gedi_mask_year}.tif')\n",
        "    output_path = join(products_dir, filename)\n",
        "    apply_forest_mask(gedi_products['L4B'], gedi_mask_path, output_path)\n",
        "    l4b_masked = output_path\n",
        "\n",
        "# Mask vegetation_grid raster\n",
        "veg_masked = None\n",
        "if gedi_products['vegetation_grid']:\n",
        "    print(\"Vegetation grid raster:\")\n",
        "    filename = os.path.basename(gedi_products['vegetation_grid']).replace('_original', '').replace('.tif', f'_masked_{gedi_mask_year}.tif')\n",
        "    output_path = join(products_dir, filename)\n",
        "    apply_forest_mask(gedi_products['vegetation_grid'], gedi_mask_path, output_path)\n",
        "    veg_masked = output_path\n",
        "\n",
        "print(f\"\\nMasked rasters: L4D={len(l4d_masked)}, L4B={'1' if l4b_masked else '0'}, Vegetation grid={'1' if veg_masked else '0'}\")"
      ],
      "metadata": {
        "id": "sI4day1vHela"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Point-based accuracy comparison\n",
        "# Select the final AGBD dataset used for modelling\n",
        "print(\"Available datasets\")\n",
        "for f in os.listdir(targets_pkl_final_dir):\n",
        "    if f.endswith(\".pkl\"):\n",
        "        print(f'original_gedi4a_dataset = \"{f}\"')"
      ],
      "metadata": {
        "id": "uxtXAuVwHnP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Point-based accuracy comparison\n",
        "# Sensitivity threshold for L4B only (beam sensitivity >= threshold)\n",
        "l4b_sensitivity_threshold = 0.98\n",
        "\n",
        "original_gedi4a_dataset = \"GEDI04_A.pkl\"\n",
        "\n",
        "# Load accuracy points from GEDI L4A dataset\n",
        "original_gedi4a_df = pd.read_pickle(join(targets_pkl_final_dir, original_gedi4a_dataset))\n",
        "accuracy_df = original_gedi4a_df[['geometry', 'timestamp', 'agbd', 'sensitivity']].copy()\n",
        "accuracy_df['year'] = pd.to_datetime(accuracy_df['timestamp']).dt.year\n",
        "print(f\"Loaded {len(accuracy_df)} accuracy points, years: {sorted(accuracy_df['year'].unique())}\")\n",
        "\n",
        "# Get EPSG code from raster\n",
        "def get_raster_epsg(raster_path):\n",
        "    raster_ds = gdal.Open(raster_path)\n",
        "    srs = osr.SpatialReference()\n",
        "    srs.ImportFromWkt(raster_ds.GetProjection())\n",
        "    epsg = int(srs.GetAuthorityCode(None))\n",
        "    raster_ds = None\n",
        "    return epsg\n",
        "\n",
        "# Sample raster at point locations, reprojecting to raster CRS\n",
        "def sample_raster_at_points(raster_path, points_df, geom_col='geometry'):\n",
        "    epsg = get_raster_epsg(raster_path)\n",
        "    gdf = gpd.GeoDataFrame(points_df, geometry=geom_col, crs='EPSG:4326').to_crs(epsg=epsg)\n",
        "    temp_df = pd.DataFrame(index=points_df.index)\n",
        "    sample_raster_values(temp_df, raster_path, gdf.geometry.x.values, gdf.geometry.y.values)\n",
        "    return temp_df.iloc[:, 0].values\n",
        "\n",
        "# Calculate accuracy metrics\n",
        "def calculate_metrics(observed, predicted):\n",
        "    obs, pred = observed.astype('float32'), predicted.astype('float32')\n",
        "    rmse = root_mean_squared_error(obs, pred)\n",
        "    return {'r2': r2_score(obs, pred), 'me': np.mean(obs - pred), 'rmse': rmse, 'rrmse': (rmse / np.mean(obs)) * 100, 'n_points': len(obs)}\n",
        "\n",
        "# All products use 2019-2023 GEDI points\n",
        "gedi_years = [2019, 2020, 2021, 2022, 2023]\n",
        "\n",
        "results = []\n",
        "\n",
        "# Add L4D area-weighted polygon accuracy\n",
        "if exists(l4d_accuracy_path):\n",
        "    l4d_tile = pd.read_csv(l4d_accuracy_path).iloc[0].to_dict()\n",
        "    results.append(l4d_tile)\n",
        "    print(f\"L4D grid area-weighted: RMSE={l4d_tile['rmse']:.4f}, rRMSE={l4d_tile['rrmse']:.2f}%\")\n",
        "\n",
        "# L4D point-based accuracy (no sensitivity threshold)\n",
        "print(\"\\nL4D grid point-based accuracy\")\n",
        "l4d_points = accuracy_df[accuracy_df['year'].isin(gedi_years)].copy()\n",
        "if len(l4d_points) > 0 and l4d_masked:\n",
        "    l4d_samples = np.full(len(l4d_points), nodatavalue, dtype=float)\n",
        "    for raster_path in l4d_masked:\n",
        "        values = sample_raster_at_points(raster_path, l4d_points)\n",
        "        mask = (l4d_samples == nodatavalue) & (values != nodatavalue)\n",
        "        l4d_samples[mask] = values[mask]\n",
        "    l4d_valid_mask = l4d_samples != nodatavalue\n",
        "    if l4d_valid_mask.sum() > 0:\n",
        "        metrics = calculate_metrics(l4d_points['agbd'].values[l4d_valid_mask], l4d_samples[l4d_valid_mask])\n",
        "        metrics['source'] = 'L4D grid (point)'\n",
        "        results.append(metrics)\n",
        "        print(f\"  R²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.4f}, rRMSE={metrics['rrmse']:.2f}%, n={metrics['n_points']}\")\n",
        "else: print(\"  No points or rasters available\")\n",
        "\n",
        "# L4B point-based accuracy (sensitivity threshold applied)\n",
        "print(f\"\\nL4B grid point-based accuracy (sensitivity >= {l4b_sensitivity_threshold})\")\n",
        "l4b_points = accuracy_df[(accuracy_df['year'].isin(gedi_years)) & (accuracy_df['sensitivity'] >= l4b_sensitivity_threshold)].copy()\n",
        "if len(l4b_points) > 0 and l4b_masked:\n",
        "    l4b_samples = sample_raster_at_points(l4b_masked, l4b_points)\n",
        "    l4b_valid_mask = l4b_samples != nodatavalue\n",
        "    if l4b_valid_mask.sum() > 0:\n",
        "        metrics = calculate_metrics(l4b_points['agbd'].values[l4b_valid_mask], l4b_samples[l4b_valid_mask])\n",
        "        metrics['source'] = 'L4B grid (point)'\n",
        "        results.append(metrics)\n",
        "        print(f\"  R²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.4f}, rRMSE={metrics['rrmse']:.2f}%, n={metrics['n_points']}\")\n",
        "else: print(\"  No points or raster available\")\n",
        "\n",
        "# Vegetation grid point-based accuracy (no sensitivity threshold)\n",
        "print(\"\\nVegetation grid point-based accuracy\")\n",
        "if veg_masked:\n",
        "    veg_points = accuracy_df[accuracy_df['year'].isin(gedi_years)].copy()\n",
        "    if len(veg_points) > 0:\n",
        "        veg_samples = sample_raster_at_points(veg_masked, veg_points)\n",
        "        veg_valid_mask = veg_samples != nodatavalue\n",
        "        if veg_valid_mask.sum() > 0:\n",
        "            metrics = calculate_metrics(veg_points['agbd'].values[veg_valid_mask], veg_samples[veg_valid_mask])\n",
        "            metrics['source'] = 'Veg grid (2019-2023)'\n",
        "            results.append(metrics)\n",
        "            print(f\"  R²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.4f}, rRMSE={metrics['rrmse']:.2f}%, n={metrics['n_points']}\")\n",
        "    else: print(\"  No points available\")\n",
        "else: print(\"  No raster available\")\n",
        "\n",
        "# Model predictions point-based accuracy (no sensitivity threshold, sample year+1)\n",
        "print(\"\\nModel predictions point-based accuracy\")\n",
        "category_labels = {'scenarios': 'single prediction', 'uncertainty': 'uncertainty mean'}\n",
        "for model_name, categories in selected_predictions.items():\n",
        "    for category, years in categories.items():\n",
        "        pred_dir = join(scenarios_dir if category == 'scenarios' else uncertainty_dir, model_name, 'scenario_predictions' if category == 'scenarios' else 'uncertainty_predictions')\n",
        "        for year in years:\n",
        "            raster_path = join(pred_dir, f\"{year}__{model_name}.tif\" if category == 'scenarios' else f\"mean__{year}__{model_name}.tif\")\n",
        "            if not exists(raster_path):\n",
        "                print(f\"  {model_name} {category} {year}: not found\")\n",
        "                continue\n",
        "            sample_year = int(year) + 1\n",
        "            pred_points = accuracy_df[accuracy_df['year'] == sample_year].copy()\n",
        "            if len(pred_points) == 0:\n",
        "                print(f\"  {model_name} {category} {year}: no points for {sample_year}\")\n",
        "                continue\n",
        "            pred_samples = sample_raster_at_points(raster_path, pred_points)\n",
        "            pred_valid_mask = pred_samples != nodatavalue\n",
        "            if pred_valid_mask.sum() > 0:\n",
        "                metrics = calculate_metrics(pred_points['agbd'].values[pred_valid_mask], pred_samples[pred_valid_mask])\n",
        "                metrics['source'] = f\"{model_name} ({category_labels[category]} {year})\"\n",
        "                results.append(metrics)\n",
        "                print(f\"  {metrics['source']}: R²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.4f}, rRMSE={metrics['rrmse']:.2f}%, n={metrics['n_points']}\")\n",
        "\n",
        "# Model cross-validation metrics from training\n",
        "print(\"\\nModel cross-validation metrics\")\n",
        "for model_name in selected_predictions.keys():\n",
        "    model_desc_path = join(models_dir, model_name, \"model_description.json\")\n",
        "    if not exists(model_desc_path):\n",
        "        print(f\"  {model_name}: not found\")\n",
        "        continue\n",
        "    with open(model_desc_path) as f:\n",
        "        model_desc = json.load(f)\n",
        "    results.append({'source': f'{model_name} (cross-validation)', 'r2': model_desc.get('score_validation (r2) mean', np.nan), 'me': model_desc.get('score_validation (me) mean', np.nan), 'rmse': model_desc.get('score_validation (rmse) mean', np.nan), 'rrmse': model_desc.get('score_validation (rrmse) mean', np.nan), 'n_points': np.nan})\n",
        "    print(f\"  {model_name}: R²={results[-1]['r2']:.4f}, RMSE={results[-1]['rmse']:.4f}, rRMSE={results[-1]['rrmse']:.2f}%\")\n",
        "\n",
        "# Build comparison table: products/models as rows, metrics as columns\n",
        "comparison_df = pd.DataFrame(results)\n",
        "comparison_df = comparison_df.drop(columns=['variate', 'n_tiles', 'n_forest_pixels'], errors='ignore')\n",
        "col_order = ['source', 'r2', 'me', 'rmse', 'rrmse', 'n_points']\n",
        "comparison_df = comparison_df[[c for c in col_order if c in comparison_df.columns]]\n",
        "comparison_path = join(model_comparison_dir, \"accuracy_comparison.csv\")\n",
        "comparison_df.to_csv(comparison_path, index=False)\n",
        "print(f\"\\nComparison saved: {comparison_path}\")\n",
        "print(comparison_df.to_string(index=False))"
      ],
      "metadata": {
        "id": "W9f8aHSuEj8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Point-based accuracy comparison\n",
        "\n",
        "# A sensitivity threshold of 0.98 was used in Tropical Evergreen Broadleaf Tree prediction strata for GEDI 4B\n",
        "# Dubayah et al (2023). GEDI L4B Gridded Aboveground Biomass Density, Version 2.1.\n",
        "# ORNL Distributed Active Archive Center. https://doi.org/10.3334/ORNLDAAC/2299\n",
        "# GEDI 4D did not have any threshold \"because the GEDI L4D k-NN algorithm did not have the same assumptions as the GEDI L4B algorithm\"\n",
        "# Seo et al (2025). GEDI L4D Imputed Waveforms, Version 2.\n",
        "# ORNL Distributed Active Archive Center. https://doi.org/10.3334/ORNLDAAC/2455\n",
        "# The documentation for GEDI gridded vegetation indices did not mention a sensitivity threhsold either.\n",
        "# Burns et al (2024). Gridded GEDI Vegetation Structure Metrics and Biomass Density at Multiple Resolutions (Version 1).\n",
        "# ORNL Distributed Active Archive Center. https://doi.org/10.3334/ORNLDAAC/2339\n",
        "sensitivity_threshold = 0.98\n",
        "\n",
        "original_gedi4a_dataset = \"GEDI04_A.pkl\"\n",
        "\n",
        "# Load accuracy points from GEDI L4A dataset\n",
        "original_gedi4a_df = pd.read_pickle(join(targets_pkl_final_dir, original_gedi4a_dataset))\n",
        "accuracy_df = original_gedi4a_df[['geometry', 'timestamp', 'agbd', 'sensitivity']].copy()\n",
        "accuracy_df['year'] = pd.to_datetime(accuracy_df['timestamp']).dt.year\n",
        "accuracy_df = accuracy_df[accuracy_df['sensitivity'] >= sensitivity_threshold].copy()\n",
        "print(f\"Loaded {len(accuracy_df)} accuracy points (sensitivity >= {sensitivity_threshold}), years: {sorted(accuracy_df['year'].unique())}\")\n",
        "\n",
        "# Get EPSG code from raster\n",
        "def get_raster_epsg(raster_path):\n",
        "    raster_ds = gdal.Open(raster_path)\n",
        "    srs = osr.SpatialReference()\n",
        "    srs.ImportFromWkt(raster_ds.GetProjection())\n",
        "    epsg = int(srs.GetAuthorityCode(None))\n",
        "    raster_ds = None\n",
        "    return epsg\n",
        "\n",
        "# Sample raster at point locations, reprojecting to raster CRS\n",
        "def sample_raster_at_points(raster_path, points_df, geom_col='geometry'):\n",
        "    epsg = get_raster_epsg(raster_path)\n",
        "    gdf = gpd.GeoDataFrame(points_df, geometry=geom_col, crs='EPSG:4326').to_crs(epsg=epsg)\n",
        "    temp_df = pd.DataFrame(index=points_df.index)\n",
        "    sample_raster_values(temp_df, raster_path, gdf.geometry.x.values, gdf.geometry.y.values)\n",
        "    return temp_df.iloc[:, 0].values\n",
        "\n",
        "# Calculate accuracy metrics\n",
        "def calculate_metrics(observed, predicted):\n",
        "    obs, pred = observed.astype('float32'), predicted.astype('float32')\n",
        "    rmse = root_mean_squared_error(obs, pred)\n",
        "    return {'r2': r2_score(obs, pred), 'me': np.mean(obs - pred), 'rmse': rmse, 'rrmse': (rmse / np.mean(obs)) * 100, 'n_points': len(obs)}\n",
        "\n",
        "# All products use 2019-2023 GEDI points\n",
        "gedi_years = [2019, 2020, 2021, 2022, 2023]\n",
        "\n",
        "results = []\n",
        "\n",
        "# Add L4D area-weighted polygon accuracy\n",
        "if exists(l4d_accuracy_path):\n",
        "    l4d_tile = pd.read_csv(l4d_accuracy_path).iloc[0].to_dict()\n",
        "    results.append(l4d_tile)\n",
        "    print(f\"L4D grid area-weighted: RMSE={l4d_tile['rmse']:.4f}, rRMSE={l4d_tile['rrmse']:.2f}%\")\n",
        "\n",
        "# L4D point-based accuracy\n",
        "print(\"\\nL4D grid point-based accuracy\")\n",
        "l4d_points = accuracy_df[accuracy_df['year'].isin(gedi_years)].copy()\n",
        "if len(l4d_points) > 0 and l4d_masked:\n",
        "    l4d_samples = np.full(len(l4d_points), nodatavalue, dtype=float)\n",
        "    for raster_path in l4d_masked:\n",
        "        values = sample_raster_at_points(raster_path, l4d_points)\n",
        "        mask = (l4d_samples == nodatavalue) & (values != nodatavalue)\n",
        "        l4d_samples[mask] = values[mask]\n",
        "    l4d_valid_mask = l4d_samples != nodatavalue\n",
        "    if l4d_valid_mask.sum() > 0:\n",
        "        metrics = calculate_metrics(l4d_points['agbd'].values[l4d_valid_mask], l4d_samples[l4d_valid_mask])\n",
        "        metrics['source'] = 'L4D grid (point)'\n",
        "        results.append(metrics)\n",
        "        print(f\"  R²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.4f}, rRMSE={metrics['rrmse']:.2f}%, n={metrics['n_points']}\")\n",
        "else: print(\"  No points or rasters available\")\n",
        "\n",
        "# L4B point-based accuracy\n",
        "print(\"\\nL4B grid point-based accuracy\")\n",
        "l4b_points = accuracy_df[accuracy_df['year'].isin(gedi_years)].copy()\n",
        "if len(l4b_points) > 0 and l4b_masked:\n",
        "    l4b_samples = sample_raster_at_points(l4b_masked, l4b_points)\n",
        "    l4b_valid_mask = l4b_samples != nodatavalue\n",
        "    if l4b_valid_mask.sum() > 0:\n",
        "        metrics = calculate_metrics(l4b_points['agbd'].values[l4b_valid_mask], l4b_samples[l4b_valid_mask])\n",
        "        metrics['source'] = 'L4B grid (point)'\n",
        "        results.append(metrics)\n",
        "        print(f\"  R²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.4f}, rRMSE={metrics['rrmse']:.2f}%, n={metrics['n_points']}\")\n",
        "else: print(\"  No points or raster available\")\n",
        "\n",
        "# Vegetation grid point-based accuracy\n",
        "print(\"\\nVegetation grid point-based accuracy\")\n",
        "if veg_masked:\n",
        "    veg_points = accuracy_df[accuracy_df['year'].isin(gedi_years)].copy()\n",
        "    if len(veg_points) > 0:\n",
        "        veg_samples = sample_raster_at_points(veg_masked, veg_points)\n",
        "        veg_valid_mask = veg_samples != nodatavalue\n",
        "        if veg_valid_mask.sum() > 0:\n",
        "            metrics = calculate_metrics(veg_points['agbd'].values[veg_valid_mask], veg_samples[veg_valid_mask])\n",
        "            metrics['source'] = 'Veg grid (2019-2023)'\n",
        "            results.append(metrics)\n",
        "            print(f\"  R²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.4f}, rRMSE={metrics['rrmse']:.2f}%, n={metrics['n_points']}\")\n",
        "    else: print(\"  No points available\")\n",
        "else: print(\"  No raster available\")\n",
        "\n",
        "# Model predictions point-based accuracy (sample year+1)\n",
        "print(\"\\nModel predictions point-based accuracy\")\n",
        "category_labels = {'scenarios': 'single prediction', 'uncertainty': 'uncertainty mean'}\n",
        "for model_name, categories in selected_predictions.items():\n",
        "    for category, years in categories.items():\n",
        "        pred_dir = join(scenarios_dir if category == 'scenarios' else uncertainty_dir, model_name, 'scenario_predictions' if category == 'scenarios' else 'uncertainty_predictions')\n",
        "        for year in years:\n",
        "            raster_path = join(pred_dir, f\"{year}__{model_name}.tif\" if category == 'scenarios' else f\"mean__{year}__{model_name}.tif\")\n",
        "            if not exists(raster_path):\n",
        "                print(f\"  {model_name} {category} {year}: not found\")\n",
        "                continue\n",
        "            sample_year = int(year) + 1\n",
        "            pred_points = accuracy_df[accuracy_df['year'] == sample_year].copy()\n",
        "            if len(pred_points) == 0:\n",
        "                print(f\"  {model_name} {category} {year}: no points for {sample_year}\")\n",
        "                continue\n",
        "            pred_samples = sample_raster_at_points(raster_path, pred_points)\n",
        "            pred_valid_mask = pred_samples != nodatavalue\n",
        "            if pred_valid_mask.sum() > 0:\n",
        "                metrics = calculate_metrics(pred_points['agbd'].values[pred_valid_mask], pred_samples[pred_valid_mask])\n",
        "                metrics['source'] = f\"{model_name} ({category_labels[category]} {year})\"\n",
        "                results.append(metrics)\n",
        "                print(f\"  {metrics['source']}: R²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.4f}, rRMSE={metrics['rrmse']:.2f}%, n={metrics['n_points']}\")\n",
        "\n",
        "# Model cross-validation metrics from training\n",
        "print(\"\\nModel cross-validation metrics\")\n",
        "for model_name in selected_predictions.keys():\n",
        "    model_desc_path = join(models_dir, model_name, \"model_description.json\")\n",
        "    if not exists(model_desc_path):\n",
        "        print(f\"  {model_name}: not found\")\n",
        "        continue\n",
        "    with open(model_desc_path) as f:\n",
        "        model_desc = json.load(f)\n",
        "    results.append({'source': f'{model_name} (cross-validation)', 'r2': model_desc.get('score_validation (r2) mean', np.nan), 'me': model_desc.get('score_validation (me) mean', np.nan), 'rmse': model_desc.get('score_validation (rmse) mean', np.nan), 'rrmse': model_desc.get('score_validation (rrmse) mean', np.nan), 'n_points': np.nan})\n",
        "    print(f\"  {model_name}: R²={results[-1]['r2']:.4f}, RMSE={results[-1]['rmse']:.4f}, rRMSE={results[-1]['rrmse']:.2f}%\")\n",
        "\n",
        "# Build comparison table: products/models as rows, metrics as columns\n",
        "comparison_df = pd.DataFrame(results)\n",
        "comparison_df = comparison_df.drop(columns=['variate', 'n_tiles', 'n_forest_pixels'], errors='ignore')\n",
        "col_order = ['source', 'r2', 'me', 'rmse', 'rrmse', 'n_points']\n",
        "comparison_df = comparison_df[[c for c in col_order if c in comparison_df.columns]]\n",
        "comparison_path = join(model_comparison_dir, \"accuracy_comparison.csv\")\n",
        "comparison_df.to_csv(comparison_path, index=False)\n",
        "print(f\"\\nComparison saved: {comparison_path}\")\n",
        "print(comparison_df.to_string(index=False))"
      ],
      "metadata": {
        "id": "LAHg3FT5rrIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjy-T1TqScbE"
      },
      "source": [
        "# Select model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj5RbVSuEp-B"
      },
      "outputs": [],
      "source": [
        "# Select if to source predictions from scenarios_dir or uncertainty_dir\n",
        "source_dir = uncertainty_dir\n",
        "# source_dir = scenarios_dir\n",
        "source_dir_name = f\"{source_dir.split('_')[-1]}_dir\"\n",
        "\n",
        "# Select the model\n",
        "for subdir in os.listdir(source_dir):\n",
        "  if 'scenario_masks' not in subdir:\n",
        "    print(f\"selected_model = '{subdir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txTXPYFLpH6v"
      },
      "outputs": [],
      "source": [
        "selected_model = 'agbd_251203_161707'\n",
        "\n",
        "# Define prediction, disturbance and intactness directories\n",
        "selected_model_prediction_dir = join(source_dir, selected_model)\n",
        "if source_dir == scenarios_dir: prediction_raster_dir = join(selected_model_prediction_dir, 'scenario_predictions')\n",
        "if source_dir == uncertainty_dir: prediction_raster_dir = join(selected_model_prediction_dir, 'uncertainty_predictions')\n",
        "model_differences_dir = join(differences_dir, f\"{selected_model}_{source_dir_name}\")\n",
        "disturbance_dir = join(model_differences_dir, 'disturbance')\n",
        "intactness_dir = join(model_differences_dir, 'intactness')\n",
        "restoration_dir = join(model_differences_dir, 'restoration')\n",
        "\n",
        "# Check prediction directory\n",
        "if not exists(prediction_raster_dir):\n",
        "  print(f\"Prediction directory doesn't exist yet: {prediction_raster_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(prediction_raster_dir))} rasters in {prediction_raster_dir}\")\n",
        "# Check disturbance directory\n",
        "if not exists(disturbance_dir):\n",
        "  print(f\"Disturbance directory doesn't exist yet: {disturbance_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(disturbance_dir))} rasters in {disturbance_dir}\")\n",
        "# Check intactness directory\n",
        "if not exists(intactness_dir):\n",
        "  print(f\"Intactness directory doesn't exist yet: {intactness_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(intactness_dir))} rasters in {intactness_dir}\")\n",
        "# Check restoration directory\n",
        "if not exists(restoration_dir):\n",
        "  print(f\"Intactness directory doesn't exist yet: {restoration_dir}\")\n",
        "  print(\"Try changing source directory, or re-run previous notebooks\")\n",
        "else: print(f\"There are {len(os.listdir(restoration_dir))} rasters in {restoration_dir}\")\n",
        "\n",
        "# Define model stats directory\n",
        "model_statistics_agbd_dir = join(statistics_agbd_dir, f\"{selected_model}_{source_dir_name}\")\n",
        "makedirs(model_statistics_agbd_dir, exist_ok=True)\n",
        "\n",
        "# Define and create converted AGB directories\n",
        "agb_total_raster_dir = join(model_statistics_agbd_dir, 'agb_total_rasters')\n",
        "agb_total_scenario_dir = join(agb_total_raster_dir, 'scenarios')\n",
        "agb_total_disturbance_dir = join(agb_total_raster_dir, 'disturbance')\n",
        "agb_total_restoration_dir = join(agb_total_raster_dir, 'restoration')\n",
        "\n",
        "makedirs(agb_total_raster_dir, exist_ok=True)\n",
        "makedirs(agb_total_scenario_dir, exist_ok=True)\n",
        "makedirs(agb_total_disturbance_dir, exist_ok=True)\n",
        "makedirs(agb_total_restoration_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPrMgTxDr3qE"
      },
      "source": [
        "# AGBD to AGB total rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yYVaDPyr3T9"
      },
      "outputs": [],
      "source": [
        "# Converts from measurements/ha to totals using cell area\n",
        "\n",
        "# This can be a higher precision than Mg/ha, as ~30 m pixels are ~0.9 ha,\n",
        "# with smaller values. Precision = 3 = 1 kg\n",
        "agb_mg_precision = 3\n",
        "agb_ci_mg_precision = 3\n",
        "\n",
        "# List all raster files in source directories\n",
        "scenario_mean_rasters = []\n",
        "scenario_ci_rasters = []\n",
        "disturbance_mean_rasters = []\n",
        "disturbance_ci_rasters = []\n",
        "restoration_mean_rasters = []\n",
        "restoration_ci_rasters = []\n",
        "\n",
        "# Collect scenario rasters\n",
        "if exists(prediction_raster_dir):\n",
        "    for f in os.listdir(prediction_raster_dir):\n",
        "        if f.endswith('.tif'):\n",
        "            full_path = join(prediction_raster_dir, f)\n",
        "            if source_dir == uncertainty_dir:\n",
        "                if 'mean__' in f: scenario_mean_rasters.append(full_path)\n",
        "                elif f.startswith('ci_'): scenario_ci_rasters.append(full_path)\n",
        "            else: scenario_mean_rasters.append(full_path)\n",
        "\n",
        "# Collect disturbance rasters\n",
        "if exists(disturbance_dir):\n",
        "    for f in os.listdir(disturbance_dir):\n",
        "        if f.endswith('.tif'):\n",
        "            full_path = join(disturbance_dir, f)\n",
        "            if source_dir == uncertainty_dir:\n",
        "                if 'mean__' in f: disturbance_mean_rasters.append(full_path)\n",
        "                elif f.startswith('ci_'): disturbance_ci_rasters.append(full_path)\n",
        "            else: disturbance_mean_rasters.append(full_path)\n",
        "\n",
        "# Collect restoration rasters\n",
        "if exists(restoration_dir):\n",
        "    for f in os.listdir(restoration_dir):\n",
        "        if f.endswith('.tif'):\n",
        "            full_path = join(restoration_dir, f)\n",
        "            if source_dir == uncertainty_dir:\n",
        "                if 'mean__' in f: restoration_mean_rasters.append(full_path)\n",
        "                elif f.startswith('ci_'): restoration_ci_rasters.append(full_path)\n",
        "            else: restoration_mean_rasters.append(full_path)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "scenario_mean_rasters = sorted(scenario_mean_rasters)\n",
        "scenario_ci_rasters = sorted(scenario_ci_rasters)\n",
        "disturbance_mean_rasters = sorted(disturbance_mean_rasters)\n",
        "disturbance_ci_rasters = sorted(disturbance_ci_rasters)\n",
        "restoration_mean_rasters = sorted(restoration_mean_rasters)\n",
        "restoration_ci_rasters = sorted(restoration_ci_rasters)\n",
        "\n",
        "# Create lookup dictionaries for CI matching (ci_{value}__ -> mean__)\n",
        "scenario_ci_lookup = {}\n",
        "for ci_raster in scenario_ci_rasters:\n",
        "    base_name = re.sub(r'ci_[^_]+__', 'mean__', os.path.basename(ci_raster))\n",
        "    scenario_ci_lookup[base_name] = ci_raster\n",
        "disturbance_ci_lookup = {}\n",
        "for ci_raster in disturbance_ci_rasters:\n",
        "    base_name = re.sub(r'ci_[^_]+__', 'mean__', os.path.basename(ci_raster))\n",
        "    disturbance_ci_lookup[base_name] = ci_raster\n",
        "restoration_ci_lookup = {}\n",
        "for ci_raster in restoration_ci_rasters:\n",
        "    base_name = re.sub(r'ci_[^_]+__', 'mean__', os.path.basename(ci_raster))\n",
        "    restoration_ci_lookup[base_name] = ci_raster\n",
        "\n",
        "# Load cell area raster\n",
        "cell_area = gdal.Open(cell_area_path)\n",
        "cell_area_array = cell_area.ReadAsArray().astype(np.float64)\n",
        "cell_area = None\n",
        "# Convert cell area from m2 to ha\n",
        "cell_area_ha = cell_area_array / 10000\n",
        "\n",
        "# Convert per-hectare rasters to total values using cell area.\n",
        "def process_rasters(raster_paths, ci_lookup, output_dir, is_disturbance=False, is_restoration=False):\n",
        "    progress_index = 0\n",
        "    progress_total = len(raster_paths)\n",
        "    raster_type = \"Restoration\" if is_restoration else (\"Disturbance\" if is_disturbance else \"Scenario\")\n",
        "    progress_label = widgets.Label(f\"{raster_type} rasters progress: {progress_index}/{progress_total}\")\n",
        "    display(progress_label)\n",
        "    print(f\"Processing {progress_total} {raster_type.lower()} rasters...\")\n",
        "    for raster_path in raster_paths:\n",
        "        base_filename = os.path.basename(raster_path)\n",
        "\n",
        "        # Extract scenario name\n",
        "        if source_dir == uncertainty_dir: raster_name = base_filename.split('__')[1].split('.')[0]\n",
        "        else: raster_name = base_filename.split('__')[0].split('.')[0]\n",
        "        output_agb_mg = join(output_dir, f\"agb_total_mg__{raster_name}.tif\")\n",
        "        agb_exists = exists(output_agb_mg)\n",
        "        agbd_array = nodata = valid_mask = None\n",
        "\n",
        "        # Create total AGB raster: AGB (Mg) = AGBD (Mg/ha) × area (ha)\n",
        "        if not agb_exists:\n",
        "            agbd = gdal.Open(raster_path)\n",
        "            agbd_array = agbd.ReadAsArray()\n",
        "            nodata = int(agbd.GetRasterBand(1).GetNoDataValue())\n",
        "            agbd = None\n",
        "            valid_mask = (agbd_array != nodata)\n",
        "            total_agb_mg = np.zeros_like(agbd_array, dtype='float64')\n",
        "            total_agb_mg[valid_mask] = agbd_array[valid_mask] * cell_area_ha[valid_mask]\n",
        "            total_agb_mg[~valid_mask] = nodata\n",
        "            if agb_mg_precision == 0:\n",
        "              total_agb_mg = np.round(total_agb_mg, agb_mg_precision).astype(np.int16)\n",
        "            else: total_agb_mg = np.round(total_agb_mg, agb_mg_precision)\n",
        "            export_array_as_tif(total_agb_mg, output_agb_mg, template=raster_path)\n",
        "\n",
        "        # Process CI rasters if available\n",
        "        if source_dir == uncertainty_dir:\n",
        "            base_name = os.path.basename(raster_path)\n",
        "            if base_name in ci_lookup:\n",
        "                ci_path = ci_lookup[base_name]\n",
        "                output_agb_ci_mg = join(output_dir, f\"agb_total_ci_95_mg__{raster_name}.tif\")\n",
        "\n",
        "                agb_ci_exists = exists(output_agb_ci_mg)\n",
        "                if not agb_ci_exists:\n",
        "                    ci_raster = gdal.Open(ci_path)\n",
        "                    ci_array = ci_raster.ReadAsArray().astype(np.float64)\n",
        "                    ci_raster = None\n",
        "                    # Load mean raster for nodata mask if needed\n",
        "                    if valid_mask is None:\n",
        "                        agbd = gdal.Open(raster_path)\n",
        "                        agbd_array = agbd.ReadAsArray()\n",
        "                        nodata = int(agbd.GetRasterBand(1).GetNoDataValue())\n",
        "                        agbd = None\n",
        "                        valid_mask = (agbd_array != nodata)\n",
        "                    # Total CI (Mg) = CI (Mg/ha) × area (ha)\n",
        "                    total_agb_ci_mg = np.zeros_like(ci_array, dtype='float64')\n",
        "                    total_agb_ci_mg[valid_mask] = ci_array[valid_mask] * cell_area_ha[valid_mask]\n",
        "                    total_agb_ci_mg[~valid_mask] = nodata\n",
        "                    if agb_ci_mg_precision == 0:\n",
        "                      total_agb_ci_mg = np.round(total_agb_ci_mg, agb_ci_mg_precision).astype(np.int16)\n",
        "                    else: total_agb_ci_mg = np.round(total_agb_ci_mg, agb_ci_mg_precision)\n",
        "                    export_array_as_tif(total_agb_ci_mg, output_agb_ci_mg, template=raster_path)\n",
        "\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"{raster_type} rasters progress: {progress_index}/{progress_total}\"\n",
        "    return progress_total\n",
        "\n",
        "scenario_count = process_rasters(scenario_mean_rasters, scenario_ci_lookup, agb_total_scenario_dir)\n",
        "disturbance_count = process_rasters(disturbance_mean_rasters, disturbance_ci_lookup, agb_total_disturbance_dir, is_disturbance=True)\n",
        "restoration_count = process_rasters(restoration_mean_rasters, restoration_ci_lookup, agb_total_restoration_dir, is_restoration=True)\n",
        "print(f\"Processed {scenario_count} scenario rasters, {disturbance_count} disturbance rasters and {restoration_count} restoration rasters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select sample polygons"
      ],
      "metadata": {
        "id": "TaKqkzjhRtDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select sample area polygons. This should be a single .gpkg with the field 'name' differentiating polygons.\n",
        "sample_polygons = []\n",
        "for geopackage in os.listdir(sample_polygons_dir):\n",
        "  sample_polygons.append(geopackage)\n",
        "if len(sample_polygons) == 0:\n",
        "  print(f\"No sample areas found. Upload .gpkg polygons to {sample_polygons_dir}\")\n",
        "else:\n",
        "  for sample_polygon in sample_polygons: print(f\"selected_sample_polygons = '{sample_polygon}'\")"
      ],
      "metadata": {
        "id": "y3qKKPXeSaqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_sample_polygons = 'tekai_sample_polygons.gpkg'\n",
        "\n",
        "# Define and create sample polygons directory\n",
        "selected_sample_polygons_dir = join(sample_polygons_dir, selected_sample_polygons)\n",
        "selected_sample_polygons_gpkg = gpd.read_file(selected_sample_polygons_dir)\n",
        "sample_polygons_statistics_agbd_dir = join(model_statistics_agbd_dir, selected_sample_polygons[:-5])\n",
        "makedirs(sample_polygons_statistics_agbd_dir, exist_ok=True)\n",
        "\n",
        "# Define and create statistic .csv directories\n",
        "land_and_forest_cover_by_area_dir = join(sample_polygons_statistics_agbd_dir, 'land_and_forest_cover_by_area')\n",
        "land_and_forest_cover_by_scenario_dir = join(sample_polygons_statistics_agbd_dir, 'land_and_forest_cover_by_scenario')\n",
        "scenario_stats_by_area_dir = join(sample_polygons_statistics_agbd_dir, 'scenario_stats_by_area')\n",
        "scenario_stats_by_scenario_dir = join(sample_polygons_statistics_agbd_dir, 'scenario_stats_by_scenario')\n",
        "disturbance_stats_by_area_dir = join(sample_polygons_statistics_agbd_dir, 'disturbance_stats_by_area')\n",
        "disturbance_stats_by_disturbance_dir = join(sample_polygons_statistics_agbd_dir, 'disturbance_stats_by_disturbance')\n",
        "restoration_stats_by_area_dir = join(sample_polygons_statistics_agbd_dir, 'restoration_stats_by_area')\n",
        "restoration_stats_by_restoration_dir = join(sample_polygons_statistics_agbd_dir, 'restoration_stats_by_restoration')\n",
        "intactness_stats_dir = join(sample_polygons_statistics_agbd_dir, 'intactness')\n",
        "report_statistics_agbd_dir = join(sample_polygons_statistics_agbd_dir, 'report_statistics')\n",
        "\n",
        "\n",
        "\n",
        "makedirs(land_and_forest_cover_by_area_dir, exist_ok=True)\n",
        "makedirs(land_and_forest_cover_by_scenario_dir, exist_ok=True)\n",
        "makedirs(scenario_stats_by_area_dir, exist_ok=True)\n",
        "makedirs(scenario_stats_by_scenario_dir, exist_ok=True)\n",
        "makedirs(disturbance_stats_by_area_dir, exist_ok=True)\n",
        "makedirs(disturbance_stats_by_disturbance_dir, exist_ok=True)\n",
        "makedirs(restoration_stats_by_area_dir, exist_ok=True)\n",
        "makedirs(restoration_stats_by_restoration_dir, exist_ok=True)\n",
        "makedirs(intactness_stats_dir, exist_ok=True)\n",
        "makedirs(report_statistics_agbd_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "Yokn9D4pSeaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Land area and forest cover"
      ],
      "metadata": {
        "id": "M8BULeeszQan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List available forest masks, which will be matched with land masks\n",
        "forest_masks = []\n",
        "for mask_file in os.listdir(masks_dir):\n",
        "    if mask_file.startswith('mask_forest_') and mask_file.endswith('.tif'):\n",
        "        scenario_name = mask_file.replace('mask_forest_', '').replace('.tif', '')\n",
        "        forest_masks.append(scenario_name)\n",
        "forest_masks = sorted(forest_masks)\n",
        "\n",
        "print('selected_forest_scenarios = [')\n",
        "for scenario in forest_masks:\n",
        "    print(f'  \"{scenario}\",')\n",
        "print(']')"
      ],
      "metadata": {
        "id": "pjaSLi2HzSCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_forest_scenarios = [\n",
        "  \"1990\",\n",
        "  \"1991\",\n",
        "  \"1992\",\n",
        "  \"1993\",\n",
        "  \"1994\",\n",
        "  \"1995\",\n",
        "  \"1996\",\n",
        "  \"1997\",\n",
        "  \"1998\",\n",
        "  \"1999\",\n",
        "  \"2000\",\n",
        "  \"2001\",\n",
        "  \"2002\",\n",
        "  \"2003\",\n",
        "  \"2004\",\n",
        "  \"2005\",\n",
        "  \"2006\",\n",
        "  \"2007\",\n",
        "  \"2008\",\n",
        "  \"2009\",\n",
        "  \"2010\",\n",
        "  \"2011\",\n",
        "  \"2012\",\n",
        "  \"2013\",\n",
        "  \"2014\",\n",
        "  \"2015\",\n",
        "  \"2016\",\n",
        "  \"2017\",\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_no_disturbance_since_oldgrowth\",\n",
        "  \"2021_oldgrowth_recovery\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_no_disturbance_since_oldgrowth\",\n",
        "  \"2024_oldgrowth_recovery\",\n",
        "  \"2024_road_mat_daling_deforestation\",\n",
        "]"
      ],
      "metadata": {
        "id": "1jV9ara96EVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forest cover statistics\n",
        "\n",
        "# Match forest masks to land masks by year\n",
        "forest_to_land_mask = {}\n",
        "missing_land_masks = []\n",
        "\n",
        "for scenario in selected_forest_scenarios:\n",
        "    year = scenario[:4]\n",
        "    land_mask_path = join(masks_dir, f\"mask_land_{year}.tif\")\n",
        "    if exists(land_mask_path):\n",
        "        forest_to_land_mask[scenario] = land_mask_path\n",
        "    else:\n",
        "        missing_land_masks.append((scenario, f\"mask_land_{year}.tif\"))\n",
        "\n",
        "# Validate all land masks exist\n",
        "if missing_land_masks:\n",
        "    print(\"Missing land masks. Return to 3_features_lcluc.ipynb to create:\")\n",
        "    for scenario, mask_name in missing_land_masks:\n",
        "        print(f\"  {mask_name} (required for {scenario})\")\n",
        "    raise FileNotFoundError(\"Land masks missing\")\n",
        "\n",
        "# Pre-allocate arrays\n",
        "polygon_names = [row[\"name\"] for _, row in selected_sample_polygons_gpkg.iterrows()]\n",
        "n_polygons = len(polygon_names)\n",
        "n_scenarios = len(selected_forest_scenarios)\n",
        "\n",
        "polygon_area_data = np.zeros(n_polygons)\n",
        "land_area_data = np.zeros((n_scenarios, n_polygons))\n",
        "forest_cover_data = np.zeros((n_scenarios, n_polygons))\n",
        "\n",
        "# Open cell area raster\n",
        "cell_area_dataset = rasterio.open(cell_area_path)\n",
        "\n",
        "# Progress tracking\n",
        "progress_total = n_polygons * n_scenarios\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Mask / polygon pair progress: {progress_index}/{progress_total}\")\n",
        "display(progress_label)\n",
        "\n",
        "try:\n",
        "    for poly_idx, (index, row) in enumerate(selected_sample_polygons_gpkg.iterrows()):\n",
        "        sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"]\n",
        "        polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "        # Mask cell area raster to polygon\n",
        "        cell_area_masked, _ = msk.mask(cell_area_dataset, polygons, crop=True, filled=False)\n",
        "        cell_area_masked = cell_area_masked.astype('float64')\n",
        "\n",
        "        # Calculate polygon area in km²\n",
        "        pixel_area_sum_m2 = np.ma.sum(cell_area_masked, dtype='float64')\n",
        "        polygon_area_data[poly_idx] = pixel_area_sum_m2 / 1e6\n",
        "\n",
        "        # Convert cell areas to hectares\n",
        "        cell_area_masked_ha = cell_area_masked / 10000\n",
        "\n",
        "        for scenario_idx, scenario in enumerate(selected_forest_scenarios):\n",
        "            # Land mask\n",
        "            land_mask_path = forest_to_land_mask[scenario]\n",
        "            with rasterio.open(land_mask_path) as land_src:\n",
        "                land_array, _ = msk.mask(land_src, polygons, crop=True, filled=False)\n",
        "                land_array = land_array.astype('float64')\n",
        "\n",
        "            land_mask = (~np.ma.getmaskarray(land_array)) & (land_array == 1)\n",
        "            land_cell_areas_ha = np.ma.array(cell_area_masked_ha.data, mask=~land_mask)\n",
        "            land_area_data[scenario_idx, poly_idx] = np.ma.sum(land_cell_areas_ha, dtype='float64')\n",
        "\n",
        "            # Forest mask\n",
        "            forest_mask_path = join(masks_dir, f\"mask_forest_{scenario}.tif\")\n",
        "            with rasterio.open(forest_mask_path) as forest_src:\n",
        "                forest_array, _ = msk.mask(forest_src, polygons, crop=True, filled=False)\n",
        "                forest_array = forest_array.astype('float64')\n",
        "\n",
        "            forest_mask = (~np.ma.getmaskarray(forest_array)) & (forest_array == 1)\n",
        "            forest_cell_areas_ha = np.ma.array(cell_area_masked_ha.data, mask=~forest_mask)\n",
        "            forest_cover_data[scenario_idx, poly_idx] = np.ma.sum(forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "            progress_index += 1\n",
        "            progress_label.value = f\"Mask / polygon pair progress: {progress_index}/{progress_total}\"\n",
        "\n",
        "finally: cell_area_dataset.close()\n",
        "\n",
        "# Calculate percentages\n",
        "pct_area_land = np.zeros((n_scenarios, n_polygons))\n",
        "pct_area_forest = np.zeros((n_scenarios, n_polygons))\n",
        "pct_land_forest = np.zeros((n_scenarios, n_polygons))\n",
        "for poly_idx in range(n_polygons):\n",
        "    area_ha = polygon_area_data[poly_idx] * 100\n",
        "    for scenario_idx in range(n_scenarios):\n",
        "        land_ha = land_area_data[scenario_idx, poly_idx]\n",
        "        forest_ha = forest_cover_data[scenario_idx, poly_idx]\n",
        "        pct_area_land[scenario_idx, poly_idx] = (land_ha / area_ha * 100) if area_ha > 0 else 0\n",
        "        pct_area_forest[scenario_idx, poly_idx] = (forest_ha / area_ha * 100) if area_ha > 0 else 0\n",
        "        pct_land_forest[scenario_idx, poly_idx] = (forest_ha / land_ha * 100) if land_ha > 0 else 0\n",
        "\n",
        "# Generate detailed stats by area\n",
        "for poly_idx, polygon_name in enumerate(polygon_names):\n",
        "    df = pd.DataFrame(index=selected_forest_scenarios)\n",
        "    df.index.name = 'scenario'\n",
        "    df['Area (km^2)'] = polygon_area_data[poly_idx]\n",
        "    df['Land area (ha)'] = land_area_data[:, poly_idx]\n",
        "    df['Forest cover (ha)'] = forest_cover_data[:, poly_idx]\n",
        "    df['% of area that is land'] = pct_area_land[:, poly_idx]\n",
        "    df['% of area that is forest'] = pct_area_forest[:, poly_idx]\n",
        "    df['% of land that is forest'] = pct_land_forest[:, poly_idx]\n",
        "    df.to_csv(join(land_and_forest_cover_by_area_dir, f'{polygon_name}.csv'))\n",
        "\n",
        "# Generate detailed stats by scenario\n",
        "for scenario_idx, scenario in enumerate(selected_forest_scenarios):\n",
        "    df = pd.DataFrame(index=polygon_names)\n",
        "    df.index.name = 'Name'\n",
        "    df['Area (km^2)'] = polygon_area_data\n",
        "    df['Land area (ha)'] = land_area_data[scenario_idx, :]\n",
        "    df['Forest cover (ha)'] = forest_cover_data[scenario_idx, :]\n",
        "    df['% of area that is land'] = pct_area_land[scenario_idx, :]\n",
        "    df['% of area that is forest'] = pct_area_forest[scenario_idx, :]\n",
        "    df['% of land that is forest'] = pct_land_forest[scenario_idx, :]\n",
        "    df.to_csv(join(land_and_forest_cover_by_scenario_dir, f'{scenario}.csv'))\n",
        "\n",
        "# Generate summary stats\n",
        "summary_land_and_forest_cover = pd.DataFrame(index=polygon_names)\n",
        "summary_land_and_forest_cover.index.name = 'Name'\n",
        "summary_land_and_forest_cover['Area (km^2)'] = polygon_area_data\n",
        "\n",
        "for scenario_idx, scenario in enumerate(selected_forest_scenarios):\n",
        "    summary_land_and_forest_cover[f'{scenario} land area (ha)'] = land_area_data[scenario_idx, :]\n",
        "    summary_land_and_forest_cover[f'{scenario} forest cover (ha)'] = forest_cover_data[scenario_idx, :]\n",
        "\n",
        "summary_land_and_forest_cover.to_csv(join(sample_polygons_statistics_agbd_dir, 'summary_land_and_forest_cover_stats.csv'))\n",
        "\n",
        "print(\"Forest cover statistics completed.\")"
      ],
      "metadata": {
        "id": "w1RxRR2HzSib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlQ2iLAiwQoB"
      },
      "source": [
        "# Scenario AGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgC9laZXPB9Y"
      },
      "outputs": [],
      "source": [
        "# Create list of available AGB total rasters and extract scenarios\n",
        "scenarios = set()\n",
        "for agb_total_raster in os.listdir(agb_total_scenario_dir):\n",
        "    if agb_total_raster.endswith('.tif') and 'agb_total_mg__' in agb_total_raster:\n",
        "        scenario_name = agb_total_raster.split(\"agb_total_mg__\")[1].split('.')[0]\n",
        "        scenarios.add(scenario_name)\n",
        "\n",
        "scenarios = sorted(list(scenarios))\n",
        "\n",
        "# Select scenario predictions to calculate statistics\n",
        "print('selected_scenarios = [')\n",
        "for scenario in scenarios:\n",
        "    print(f'  \"{scenario}\",')\n",
        "print(']\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_scenarios = [\n",
        "  \"2021\",\n",
        "  \"2021_no_disturbance_since_1993\",\n",
        "  \"2021_no_disturbance_since_oldgrowth\",\n",
        "  \"2021_oldgrowth_recovery\",\n",
        "  \"2024\",\n",
        "  \"2024_no_disturbance_since_1996\",\n",
        "  \"2024_no_disturbance_since_1997\",\n",
        "  \"2024_no_disturbance_since_1998\",\n",
        "  \"2024_no_disturbance_since_1999\",\n",
        "  \"2024_no_disturbance_since_2000\",\n",
        "  \"2024_no_disturbance_since_2001\",\n",
        "  \"2024_no_disturbance_since_2002\",\n",
        "  \"2024_no_disturbance_since_2003\",\n",
        "  \"2024_no_disturbance_since_2004\",\n",
        "  \"2024_no_disturbance_since_2005\",\n",
        "  \"2024_no_disturbance_since_2006\",\n",
        "  \"2024_no_disturbance_since_2007\",\n",
        "  \"2024_no_disturbance_since_2008\",\n",
        "  \"2024_no_disturbance_since_2009\",\n",
        "  \"2024_no_disturbance_since_2010\",\n",
        "  \"2024_no_disturbance_since_2011\",\n",
        "  \"2024_no_disturbance_since_2012\",\n",
        "  \"2024_no_disturbance_since_2013\",\n",
        "  \"2024_no_disturbance_since_2014\",\n",
        "  \"2024_no_disturbance_since_2015\",\n",
        "  \"2024_no_disturbance_since_2016\",\n",
        "  \"2024_no_disturbance_since_2017\",\n",
        "  \"2024_no_disturbance_since_2018\",\n",
        "  \"2024_no_disturbance_since_2019\",\n",
        "  \"2024_no_disturbance_since_2020\",\n",
        "  \"2024_no_disturbance_since_2021\",\n",
        "  \"2024_no_disturbance_since_2022\",\n",
        "  \"2024_no_disturbance_since_2023\",\n",
        "  \"2024_no_disturbance_since_2024\",\n",
        "  \"2024_no_disturbance_since_oldgrowth\",\n",
        "  \"2024_oldgrowth_recovery\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]"
      ],
      "metadata": {
        "id": "prHZVgO3v2ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utWU_0AEG0UX"
      },
      "outputs": [],
      "source": [
        "# Build lists of AGB total rasters for selected scenarios\n",
        "agb_total_rasters = []\n",
        "\n",
        "for scenario in selected_scenarios:\n",
        "    agb_total_path = join(agb_total_scenario_dir, f\"agb_total_mg__{scenario}.tif\")\n",
        "    if exists(agb_total_path):\n",
        "        agb_total_rasters.append(agb_total_path)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "agb_total_rasters = sorted(agb_total_rasters)\n",
        "\n",
        "# Toggle whether to generate uncertainty stats (only possible with uncertainty_dir)\n",
        "generate_uncertainty_stats = (source_dir == uncertainty_dir)\n",
        "\n",
        "# Pre-allocate arrays for statistics\n",
        "polygon_names = [row[\"name\"] for _, row in selected_sample_polygons_gpkg.iterrows()]\n",
        "n_polygons = len(polygon_names)\n",
        "n_scenarios = len(agb_total_rasters)\n",
        "\n",
        "agbd_mean_data = np.zeros((n_scenarios, n_polygons), dtype='float64')\n",
        "agbd_stdev_data = np.zeros((n_scenarios, n_polygons), dtype='float64')\n",
        "agb_total_data = np.zeros((n_scenarios, n_polygons), dtype='float64')\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    agbd_mean_ci95_data = np.zeros((n_scenarios, n_polygons), dtype='float64')\n",
        "    agbd_mean_uncertainty_data = np.zeros((n_scenarios, n_polygons), dtype='float64')\n",
        "    agb_total_ci95_data = np.zeros((n_scenarios, n_polygons), dtype='float64')\n",
        "\n",
        "# Open AGB total raster datasets\n",
        "agb_total_datasets = {path: rasterio.open(path) for path in agb_total_rasters}\n",
        "\n",
        "# Open AGB total CI95 datasets only if uncertainty stats are generated\n",
        "agb_total_ci95_datasets = {}\n",
        "if generate_uncertainty_stats:\n",
        "    for agb_total_raster in agb_total_rasters:\n",
        "        scenario_name = os.path.basename(agb_total_raster).split('agb_total_mg__')[1].split('.')[0]\n",
        "        agb_total_ci95_path = join(agb_total_scenario_dir, f\"agb_total_ci_95_mg__{scenario_name}.tif\")\n",
        "        if exists(agb_total_ci95_path):\n",
        "            agb_total_ci95_datasets[agb_total_raster] = rasterio.open(agb_total_ci95_path)\n",
        "\n",
        "# Load cell area raster once for all calculations\n",
        "cell_area_dataset = rasterio.open(cell_area_path)\n",
        "\n",
        "# Progress tracking\n",
        "progress_total = n_polygons * n_scenarios\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Raster / polygon pair progress: {progress_index}/{progress_total}\")\n",
        "display(progress_label)\n",
        "\n",
        "try:\n",
        "    for poly_idx, (index, row) in enumerate(selected_sample_polygons_gpkg.iterrows()):\n",
        "        sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"]\n",
        "        polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "        # Mask cell area raster to polygon\n",
        "        cell_area_masked, _ = msk.mask(cell_area_dataset, polygons, crop=True, filled=False)\n",
        "        cell_area_masked = cell_area_masked.astype('float64')\n",
        "        cell_area_masked_ha = cell_area_masked / 10000\n",
        "\n",
        "        for raster_idx, agb_total_raster in enumerate(agb_total_rasters):\n",
        "            # Mask AGB total raster to polygon\n",
        "            agb_total = agb_total_datasets[agb_total_raster]\n",
        "            agb_total_array_masked, _ = msk.mask(agb_total, polygons, crop=True, filled=False)\n",
        "            agb_total_array_masked = agb_total_array_masked.astype('float64')\n",
        "\n",
        "            # Valid pixels mask\n",
        "            valid_mask = ~np.ma.getmaskarray(agb_total_array_masked)\n",
        "\n",
        "            # Calculate area of valid pixels\n",
        "            valid_cell_areas_ha = np.ma.array(cell_area_masked_ha.data, mask=~valid_mask)\n",
        "            valid_area_ha = np.ma.sum(valid_cell_areas_ha, dtype='float64')\n",
        "\n",
        "            # Sum total AGB in Mg\n",
        "            agb_total_mg = np.ma.sum(agb_total_array_masked, dtype='float64')\n",
        "\n",
        "            # Calculate statistics\n",
        "            if np.ma.is_masked(agb_total_mg) or valid_area_ha <= 0:\n",
        "                agbd_mean_mg_ha = 0.0\n",
        "                agbd_stdev_mg_ha = 0.0\n",
        "                agb_total_tg = 0.0\n",
        "            else:\n",
        "                # Area-weighted mean AGBD\n",
        "                agbd_mean_mg_ha = agb_total_mg / valid_area_ha\n",
        "\n",
        "                # Back-calculate individual AGBD values for standard deviation\n",
        "                agbd_values = agb_total_array_masked / cell_area_masked_ha\n",
        "                valid_agbd = agbd_values[valid_mask]\n",
        "                valid_areas = cell_area_masked_ha[valid_mask]\n",
        "\n",
        "                # Area-weighted standard deviation\n",
        "                variance_weighted = np.sum(valid_areas * (valid_agbd - agbd_mean_mg_ha)**2) / valid_area_ha\n",
        "                agbd_stdev_mg_ha = np.sqrt(variance_weighted)\n",
        "\n",
        "                # Convert total AGB from Mg to Tg\n",
        "                agb_total_tg = agb_total_mg / 1000000\n",
        "\n",
        "            # Store results\n",
        "            agbd_mean_data[raster_idx, poly_idx] = agbd_mean_mg_ha\n",
        "            agbd_stdev_data[raster_idx, poly_idx] = agbd_stdev_mg_ha\n",
        "            agb_total_data[raster_idx, poly_idx] = agb_total_tg\n",
        "\n",
        "            if generate_uncertainty_stats and agb_total_raster in agb_total_ci95_datasets:\n",
        "                agb_total_ci95_raster = agb_total_ci95_datasets[agb_total_raster]\n",
        "                agb_total_ci95_array_masked, _ = msk.mask(agb_total_ci95_raster, polygons, crop=True, filled=False)\n",
        "                agb_total_ci95_array_masked = agb_total_ci95_array_masked.astype('float64')\n",
        "\n",
        "                agb_total_ci95_mg = abs(np.ma.sum(agb_total_ci95_array_masked, dtype='float64'))\n",
        "\n",
        "                if abs(agb_total_mg) > 0:\n",
        "                    agbd_mean_ci95_mg_ha = agb_total_ci95_mg / valid_area_ha\n",
        "                    agbd_uncertainty_pct = agb_total_ci95_mg / abs(agb_total_mg) * 100\n",
        "                else:\n",
        "                    agbd_mean_ci95_mg_ha = 0.0\n",
        "                    agbd_uncertainty_pct = 0.0\n",
        "\n",
        "                agb_total_ci95_tg = agb_total_ci95_mg / 1000000\n",
        "\n",
        "                agbd_mean_ci95_data[raster_idx, poly_idx] = agbd_mean_ci95_mg_ha\n",
        "                agbd_mean_uncertainty_data[raster_idx, poly_idx] = agbd_uncertainty_pct\n",
        "                agb_total_ci95_data[raster_idx, poly_idx] = agb_total_ci95_tg\n",
        "\n",
        "            progress_index += 1\n",
        "            progress_label.value = f\"Raster / polygon pair progress: {progress_index}/{progress_total}\"\n",
        "\n",
        "finally:\n",
        "    cell_area_dataset.close()\n",
        "    for dataset in agb_total_datasets.values():\n",
        "        dataset.close()\n",
        "    for dataset in agb_total_ci95_datasets.values():\n",
        "        dataset.close()\n",
        "\n",
        "# Create DataFrames from pre-allocated arrays\n",
        "df_agbd_mean_mg_ha = pd.DataFrame(agbd_mean_data, index=selected_scenarios, columns=polygon_names)\n",
        "df_agbd_mean_mg_ha.rename_axis('scenario', inplace=True)\n",
        "\n",
        "df_agbd_stdev_mg_ha = pd.DataFrame(agbd_stdev_data, index=selected_scenarios, columns=polygon_names)\n",
        "df_agbd_stdev_mg_ha.rename_axis('scenario', inplace=True)\n",
        "\n",
        "df_agb_total_tg = pd.DataFrame(agb_total_data, index=selected_scenarios, columns=polygon_names)\n",
        "df_agb_total_tg.rename_axis('scenario', inplace=True)\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    df_agbd_mean_ci95_mg_ha = pd.DataFrame(agbd_mean_ci95_data, index=selected_scenarios, columns=polygon_names)\n",
        "    df_agbd_mean_ci95_mg_ha.rename_axis('scenario', inplace=True)\n",
        "\n",
        "    df_agbd_uncertainty_pct = pd.DataFrame(agbd_mean_uncertainty_data, index=selected_scenarios, columns=polygon_names)\n",
        "    df_agbd_uncertainty_pct.rename_axis('scenario', inplace=True)\n",
        "\n",
        "    df_agb_total_ci95_tg = pd.DataFrame(agb_total_ci95_data, index=selected_scenarios, columns=polygon_names)\n",
        "    df_agb_total_ci95_tg.rename_axis('scenario', inplace=True)\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "    df_stats_list = [df_agbd_mean_mg_ha, df_agbd_mean_ci95_mg_ha, df_agbd_uncertainty_pct,\n",
        "                     df_agbd_stdev_mg_ha, df_agb_total_tg, df_agb_total_ci95_tg]\n",
        "else:\n",
        "    df_stats_list = [df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Generate summary stats\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" AGB (Tg)\")\n",
        "\n",
        "summary_components = [df_agb_total_tg_t]\n",
        "if generate_uncertainty_stats:\n",
        "    df_agb_total_ci95_tg_t = df_agb_total_ci95_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" AGB CI95 (Tg)\")\n",
        "    summary_components.append(df_agb_total_ci95_tg_t)\n",
        "\n",
        "summary_scenario_stats = pd.concat(summary_components, axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_scenario_stats.to_csv(join(sample_polygons_statistics_agbd_dir, 'summary_scenario_stats.csv'))\n",
        "\n",
        "# Generate detailed stats by area\n",
        "df_base = pd.DataFrame(index=selected_scenarios)\n",
        "df_base.rename_axis('scenario', inplace=True)\n",
        "\n",
        "for polygon_name in polygon_names:\n",
        "    df_detailed = df_base.copy()\n",
        "    for df_stats in df_stats_list:\n",
        "        if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"AGBD mean (Mg / ha)\"\n",
        "        elif df_stats.equals(df_agbd_stdev_mg_ha): stat_col = \"AGBD stdev (Mg / ha)\"\n",
        "        elif df_stats.equals(df_agb_total_tg): stat_col = \"AGB total (Tg)\"\n",
        "        elif generate_uncertainty_stats and df_stats.equals(df_agbd_mean_ci95_mg_ha): stat_col = \"AGBD CI95 (Mg / ha)\"\n",
        "        elif generate_uncertainty_stats and df_stats.equals(df_agbd_uncertainty_pct): stat_col = \"AGBD uncertainty (%)\"\n",
        "        elif generate_uncertainty_stats and df_stats.equals(df_agb_total_ci95_tg): stat_col = \"AGB total CI95 (Tg)\"\n",
        "        df_detailed[stat_col] = df_stats[polygon_name]\n",
        "    df_detailed.to_csv(join(scenario_stats_by_area_dir, f'{polygon_name}.csv'))\n",
        "\n",
        "# Generate detailed stats by scenario\n",
        "scenarios = {}\n",
        "for stats_csv in os.listdir(scenario_stats_by_area_dir):\n",
        "    polygon_name = stats_csv[:-4]\n",
        "    stats_csv_df = pd.read_csv(join(scenario_stats_by_area_dir, stats_csv))\n",
        "    for scenario in stats_csv_df['scenario'].unique():\n",
        "        scenario_df = stats_csv_df[stats_csv_df['scenario'] == scenario].copy()\n",
        "        scenario_df.drop('scenario', axis=1, inplace=True)\n",
        "        scenario_df.insert(0, 'Name', polygon_name)\n",
        "        if scenario in scenarios:\n",
        "            scenarios[scenario] = pd.concat([scenarios[scenario], scenario_df], ignore_index=True)\n",
        "        else:\n",
        "            scenarios[scenario] = scenario_df\n",
        "\n",
        "for scenario, scenario_df in scenarios.items():\n",
        "    scenario_df.to_csv(join(scenario_stats_by_scenario_dir, f'{scenario}.csv'), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHEWz3QcGh1b"
      },
      "source": [
        "# Disturbance AGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPm9wZwJBJnq"
      },
      "outputs": [],
      "source": [
        "# Create list of available AGB total disturbance rasters and extract disturbances\n",
        "disturbances = set()\n",
        "for agb_total_disturbance_raster in os.listdir(agb_total_disturbance_dir):\n",
        "    if agb_total_disturbance_raster.endswith('.tif') and 'agb_total_mg__' in agb_total_disturbance_raster:\n",
        "        disturbance_name = agb_total_disturbance_raster.split(\"agb_total_mg__\")[1].split('.')[0]\n",
        "        disturbances.add(disturbance_name)\n",
        "\n",
        "disturbances = sorted(list(disturbances))\n",
        "\n",
        "# Select disturbance rasters to calculate statistics\n",
        "print('selected_disturbances = [')\n",
        "for disturbancein disturbances:\n",
        "    print(f'  \"{dist}\",')\n",
        "print(']')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_disturbances = [\n",
        "  \"2021_deforestation_since_oldgrowth\",\n",
        "  \"2021_degradation_from_oldgrowth_to_1992\",\n",
        "  \"2021_degradation_since_1993\",\n",
        "  \"2021_degradation_since_oldgrowth\",\n",
        "  \"2024_deforestation_of_road_mat_daling_2023_deforestation\",\n",
        "  \"2024_deforestation_of_road_mat_daling_2023_degradation\",\n",
        "  \"2024_deforestation_since_oldgrowth\",\n",
        "  \"2024_degradation_from_oldgrowth_to_1995\",\n",
        "  \"2024_degradation_since_1996\",\n",
        "  \"2024_degradation_since_oldgrowth\",\n",
        "  \"2024_effect_of_degradation_in_1996\",\n",
        "  \"2024_effect_of_degradation_in_1997\",\n",
        "  \"2024_effect_of_degradation_in_1998\",\n",
        "  \"2024_effect_of_degradation_in_1999\",\n",
        "  \"2024_effect_of_degradation_in_2000\",\n",
        "  \"2024_effect_of_degradation_in_2001\",\n",
        "  \"2024_effect_of_degradation_in_2002\",\n",
        "  \"2024_effect_of_degradation_in_2003\",\n",
        "  \"2024_effect_of_degradation_in_2004\",\n",
        "  \"2024_effect_of_degradation_in_2005\",\n",
        "  \"2024_effect_of_degradation_in_2006\",\n",
        "  \"2024_effect_of_degradation_in_2007\",\n",
        "  \"2024_effect_of_degradation_in_2008\",\n",
        "  \"2024_effect_of_degradation_in_2009\",\n",
        "  \"2024_effect_of_degradation_in_2010\",\n",
        "  \"2024_effect_of_degradation_in_2011\",\n",
        "  \"2024_effect_of_degradation_in_2012\",\n",
        "  \"2024_effect_of_degradation_in_2013\",\n",
        "  \"2024_effect_of_degradation_in_2014\",\n",
        "  \"2024_effect_of_degradation_in_2015\",\n",
        "  \"2024_effect_of_degradation_in_2016\",\n",
        "  \"2024_effect_of_degradation_in_2017\",\n",
        "  \"2024_effect_of_degradation_in_2018\",\n",
        "  \"2024_effect_of_degradation_in_2019\",\n",
        "  \"2024_effect_of_degradation_in_2020\",\n",
        "  \"2024_effect_of_degradation_in_2021\",\n",
        "  \"2024_effect_of_degradation_in_2022\",\n",
        "  \"2024_effect_of_degradation_in_2023\",\n",
        "  \"2024_effect_of_degradation_in_2024\",\n",
        "]"
      ],
      "metadata": {
        "id": "y1hUcInxwsuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm_v5xr2G4sV"
      },
      "outputs": [],
      "source": [
        "# Build lists of AGB total disturbance rasters for selected disturbances\n",
        "agb_total_disturbance_rasters = []\n",
        "\n",
        "for disturbancein selected_disturbances:\n",
        "    agb_total_path = join(agb_total_disturbance_dir, f\"agb_total_mg__{dist}.tif\")\n",
        "    if exists(agb_total_path):\n",
        "        agb_total_disturbance_rasters.append(agb_total_path)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "agb_total_disturbance_rasters = sorted(agb_total_disturbance_rasters)\n",
        "\n",
        "# Toggle whether to generate uncertainty stats (only possible with uncertainty_dir)\n",
        "generate_uncertainty_stats = (source_dir == uncertainty_dir)\n",
        "\n",
        "# Pre-allocate arrays for statistics\n",
        "polygon_names = [row[\"name\"] for _, row in selected_sample_polygons_gpkg.iterrows()]\n",
        "n_polygons = len(polygon_names)\n",
        "n_disturbances = len(agb_total_disturbance_rasters)\n",
        "\n",
        "agbd_mean_data = np.zeros((n_disturbances, n_polygons), dtype='float64')\n",
        "agbd_stdev_data = np.zeros((n_disturbances, n_polygons), dtype='float64')\n",
        "agb_total_data = np.zeros((n_disturbances, n_polygons), dtype='float64')\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    agbd_mean_ci95_data = np.zeros((n_disturbances, n_polygons), dtype='float64')\n",
        "    agbd_mean_uncertainty_data = np.zeros((n_disturbances, n_polygons), dtype='float64')\n",
        "    agb_total_ci95_data = np.zeros((n_disturbances, n_polygons), dtype='float64')\n",
        "\n",
        "# Open AGB total disturbance raster datasets\n",
        "agb_total_disturbance_datasets = {path: rasterio.open(path) for path in agb_total_disturbance_rasters}\n",
        "\n",
        "# Open AGB total CI95 disturbance datasets only if uncertainty stats are generated\n",
        "agb_total_ci95_disturbance_datasets = {}\n",
        "if generate_uncertainty_stats:\n",
        "    for agb_total_disturbance_raster in agb_total_disturbance_rasters:\n",
        "        disturbance_name = os.path.basename(agb_total_disturbance_raster).split('agb_total_mg__')[1].split('.')[0]\n",
        "        agb_total_ci95_path = join(agb_total_disturbance_dir, f\"agb_total_ci_95_mg__{disturbance_name}.tif\")\n",
        "        if exists(agb_total_ci95_path):\n",
        "            agb_total_ci95_disturbance_datasets[agb_total_disturbance_raster] = rasterio.open(agb_total_ci95_path)\n",
        "\n",
        "# Load cell area raster once for all calculations\n",
        "cell_area_dataset = rasterio.open(cell_area_path)\n",
        "\n",
        "# Progress tracking\n",
        "progress_total = n_polygons * n_disturbances\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Raster / polygon pair progress: {progress_index}/{progress_total}\")\n",
        "display(progress_label)\n",
        "\n",
        "try:\n",
        "    for poly_idx, (index, row) in enumerate(selected_sample_polygons_gpkg.iterrows()):\n",
        "        sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"]\n",
        "        polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "        # Mask cell area raster to polygon\n",
        "        cell_area_masked, _ = msk.mask(cell_area_dataset, polygons, crop=True, filled=False)\n",
        "        cell_area_masked = cell_area_masked.astype('float64')\n",
        "        cell_area_masked_ha = cell_area_masked / 10000\n",
        "\n",
        "        for raster_idx, agb_total_disturbance_raster in enumerate(agb_total_disturbance_rasters):\n",
        "            # Mask AGB total disturbance raster to polygon\n",
        "            agb_total_disturbance= agb_total_disturbance_datasets[agb_total_disturbance_raster]\n",
        "            agb_total_array_masked, _ = msk.mask(agb_total_disturbance, polygons, crop=True, filled=False)\n",
        "            agb_total_array_masked = agb_total_array_masked.astype('float64')\n",
        "\n",
        "            # Valid pixels mask\n",
        "            valid_mask = ~np.ma.getmaskarray(agb_total_array_masked)\n",
        "\n",
        "            # Calculate area of valid pixels\n",
        "            valid_cell_areas_ha = np.ma.array(cell_area_masked_ha.data, mask=~valid_mask)\n",
        "            valid_area_ha = np.ma.sum(valid_cell_areas_ha, dtype='float64')\n",
        "\n",
        "            # Sum total AGB in Mg\n",
        "            agb_total_mg = np.ma.sum(agb_total_array_masked, dtype='float64')\n",
        "\n",
        "            # Calculate statistics\n",
        "            if np.ma.is_masked(agb_total_mg) or valid_area_ha <= 0:\n",
        "                agbd_mean_mg_ha = 0.0\n",
        "                agbd_stdev_mg_ha = 0.0\n",
        "                agb_total_tg = 0.0\n",
        "            else:\n",
        "                # Area-weighted mean AGBD\n",
        "                agbd_mean_mg_ha = agb_total_mg / valid_area_ha\n",
        "\n",
        "                # Back-calculate individual AGBD values for standard deviation\n",
        "                agbd_values = agb_total_array_masked / cell_area_masked_ha\n",
        "                valid_agbd = agbd_values[valid_mask]\n",
        "                valid_areas = cell_area_masked_ha[valid_mask]\n",
        "\n",
        "                # Area-weighted standard deviation\n",
        "                variance_weighted = np.sum(valid_areas * (valid_agbd - agbd_mean_mg_ha)**2) / valid_area_ha\n",
        "                agbd_stdev_mg_ha = np.sqrt(variance_weighted)\n",
        "\n",
        "                # Convert total AGB from Mg to Tg\n",
        "                agb_total_tg = agb_total_mg / 1000000\n",
        "\n",
        "            # Store results\n",
        "            agbd_mean_data[raster_idx, poly_idx] = agbd_mean_mg_ha\n",
        "            agbd_stdev_data[raster_idx, poly_idx] = agbd_stdev_mg_ha\n",
        "            agb_total_data[raster_idx, poly_idx] = agb_total_tg\n",
        "\n",
        "            if generate_uncertainty_stats and agb_total_disturbance_raster in agb_total_ci95_disturbance_datasets:\n",
        "                agb_total_ci95_raster = agb_total_ci95_disturbance_datasets[agb_total_disturbance_raster]\n",
        "                agb_total_ci95_array_masked, _ = msk.mask(agb_total_ci95_raster, polygons, crop=True, filled=False)\n",
        "                agb_total_ci95_array_masked = agb_total_ci95_array_masked.astype('float64')\n",
        "\n",
        "                agb_total_ci95_mg = abs(np.ma.sum(agb_total_ci95_array_masked, dtype='float64'))\n",
        "\n",
        "                if abs(agb_total_mg) > 0:\n",
        "                    agbd_mean_ci95_mg_ha = agb_total_ci95_mg / valid_area_ha\n",
        "                    agbd_uncertainty_pct = agb_total_ci95_mg / abs(agb_total_mg) * 100\n",
        "                else:\n",
        "                    agbd_mean_ci95_mg_ha = 0.0\n",
        "                    agbd_uncertainty_pct = 0.0\n",
        "\n",
        "                agb_total_ci95_tg = agb_total_ci95_mg / 1000000\n",
        "\n",
        "                agbd_mean_ci95_data[raster_idx, poly_idx] = agbd_mean_ci95_mg_ha\n",
        "                agbd_mean_uncertainty_data[raster_idx, poly_idx] = agbd_uncertainty_pct\n",
        "                agb_total_ci95_data[raster_idx, poly_idx] = agb_total_ci95_tg\n",
        "\n",
        "            progress_index += 1\n",
        "            progress_label.value = f\"Raster / polygon pair progress: {progress_index}/{progress_total}\"\n",
        "\n",
        "finally:\n",
        "    cell_area_dataset.close()\n",
        "    for dataset in agb_total_disturbance_datasets.values():\n",
        "        dataset.close()\n",
        "    for dataset in agb_total_ci95_disturbance_datasets.values():\n",
        "        dataset.close()\n",
        "\n",
        "# Create DataFrames from pre-allocated arrays\n",
        "df_agbd_mean_mg_ha = pd.DataFrame(agbd_mean_data, index=selected_disturbances, columns=polygon_names)\n",
        "df_agbd_mean_mg_ha.rename_axis('dist', inplace=True)\n",
        "\n",
        "df_agbd_stdev_mg_ha = pd.DataFrame(agbd_stdev_data, index=selected_disturbances, columns=polygon_names)\n",
        "df_agbd_stdev_mg_ha.rename_axis('dist', inplace=True)\n",
        "\n",
        "df_agb_total_tg = pd.DataFrame(agb_total_data, index=selected_disturbances, columns=polygon_names)\n",
        "df_agb_total_tg.rename_axis('dist', inplace=True)\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    df_agbd_mean_ci95_mg_ha = pd.DataFrame(agbd_mean_ci95_data, index=selected_disturbances, columns=polygon_names)\n",
        "    df_agbd_mean_ci95_mg_ha.rename_axis('dist', inplace=True)\n",
        "\n",
        "    df_agbd_uncertainty_pct = pd.DataFrame(agbd_mean_uncertainty_data, index=selected_disturbances, columns=polygon_names)\n",
        "    df_agbd_uncertainty_pct.rename_axis('dist', inplace=True)\n",
        "\n",
        "    df_agb_total_ci95_tg = pd.DataFrame(agb_total_ci95_data, index=selected_disturbances, columns=polygon_names)\n",
        "    df_agb_total_ci95_tg.rename_axis('dist', inplace=True)\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "    df_stats_list = [df_agbd_mean_mg_ha, df_agbd_mean_ci95_mg_ha, df_agbd_uncertainty_pct,\n",
        "                     df_agbd_stdev_mg_ha, df_agb_total_tg, df_agb_total_ci95_tg]\n",
        "else:\n",
        "    df_stats_list = [df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Generate summary stats\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" AGB (Tg)\")\n",
        "\n",
        "summary_components = [df_agb_total_tg_t]\n",
        "if generate_uncertainty_stats:\n",
        "    df_agb_total_ci95_tg_t = df_agb_total_ci95_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" AGB CI95 (Tg)\")\n",
        "    summary_components.append(df_agb_total_ci95_tg_t)\n",
        "\n",
        "summary_disturbance_stats = pd.concat(summary_components, axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_disturbance_stats.to_csv(join(sample_polygons_statistics_agbd_dir, 'summary_disturbance_stats.csv'))\n",
        "\n",
        "# Generate detailed stats by area\n",
        "df_base = pd.DataFrame(index=selected_disturbances)\n",
        "df_base.rename_axis('dist', inplace=True)\n",
        "\n",
        "for polygon_name in polygon_names:\n",
        "    df_detailed = df_base.copy()\n",
        "    for df_stats in df_stats_list:\n",
        "        if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"AGBD mean (Mg / ha)\"\n",
        "        elif df_stats.equals(df_agbd_stdev_mg_ha): stat_col = \"AGBD stdev (Mg / ha)\"\n",
        "        elif df_stats.equals(df_agb_total_tg): stat_col = \"AGB total (Tg)\"\n",
        "        elif generate_uncertainty_stats and df_stats.equals(df_agbd_mean_ci95_mg_ha): stat_col = \"AGBD CI95 (Mg / ha)\"\n",
        "        elif generate_uncertainty_stats and df_stats.equals(df_agbd_uncertainty_pct): stat_col = \"AGBD uncertainty (%)\"\n",
        "        elif generate_uncertainty_stats and df_stats.equals(df_agb_total_ci95_tg): stat_col = \"AGB total CI95 (Tg)\"\n",
        "        df_detailed[stat_col] = df_stats[polygon_name]\n",
        "    df_detailed.to_csv(join(disturbance_stats_by_area_dir, f'{polygon_name}.csv'))\n",
        "\n",
        "# Generate detailed stats by disturbance\n",
        "disturbances = {}\n",
        "for stats_csv in os.listdir(disturbance_stats_by_area_dir):\n",
        "    polygon_name = stats_csv[:-4]\n",
        "    stats_csv_df = pd.read_csv(join(disturbance_stats_by_area_dir, stats_csv))\n",
        "    for disturbancein stats_csv_df['dist'].unique():\n",
        "        disturbance_df = stats_csv_df[stats_csv_df['dist'] == dist].copy()\n",
        "        disturbance_df.drop('dist', axis=1, inplace=True)\n",
        "        disturbance_df.insert(0, 'Name', polygon_name)\n",
        "        if disturbancein disturbances:\n",
        "            disturbances[disturbance]= pd.concat([disturbances[dist], disturbance_df], ignore_index=True)\n",
        "        else:\n",
        "            disturbances[disturbance]= disturbance_df\n",
        "\n",
        "for disturbance, disturbance_df in disturbances.items():\n",
        "    disturbance_df.to_csv(join(disturbance_stats_by_disturbance_dir, f'{dist}.csv'), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restoration AGB"
      ],
      "metadata": {
        "id": "3uQBaJ0LJpAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of available AGB total restoration rasters and extract restorations\n",
        "restorations = set()\n",
        "for agb_total_restoration_raster in os.listdir(agb_total_restoration_dir):\n",
        "    if agb_total_restoration_raster.endswith('.tif') and 'agb_total_mg__' in agb_total_restoration_raster:\n",
        "        restoration_name = agb_total_restoration_raster.split(\"agb_total_mg__\")[1].split('.')[0]\n",
        "        restorations.add(restoration_name)\n",
        "\n",
        "restorations = sorted(list(restorations))\n",
        "\n",
        "# Select restoration rasters to calculate statistics\n",
        "print('selected_restorations = [')\n",
        "for rest in restorations:\n",
        "    print(f'  \"{rest}\",')\n",
        "print(']')"
      ],
      "metadata": {
        "id": "AuYzvzfjJsiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_restorations = [\n",
        "  \"2021_recovery_potential\",\n",
        "  \"2021_reforestation_and_recovery_potential\",\n",
        "  \"2024_recovery_potential\",\n",
        "  \"2024_reforestation_and_recovery_potential\",\n",
        "]"
      ],
      "metadata": {
        "id": "cVRg_G-mJxf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build lists of AGB total restoration rasters for selected restorations\n",
        "agb_total_restoration_rasters = []\n",
        "\n",
        "for rest in selected_restorations:\n",
        "    agb_total_path = join(agb_total_restoration_dir, f\"agb_total_mg__{rest}.tif\")\n",
        "    if exists(agb_total_path):\n",
        "        agb_total_restoration_rasters.append(agb_total_path)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "agb_total_restoration_rasters = sorted(agb_total_restoration_rasters)\n",
        "\n",
        "# Toggle whether to generate uncertainty stats (only possible with uncertainty_dir)\n",
        "generate_uncertainty_stats = (source_dir == uncertainty_dir)\n",
        "\n",
        "# Pre-allocate arrays for statistics\n",
        "polygon_names = [row[\"name\"] for _, row in selected_sample_polygons_gpkg.iterrows()]\n",
        "n_polygons = len(polygon_names)\n",
        "n_restorations = len(agb_total_restoration_rasters)\n",
        "\n",
        "agbd_mean_data = np.zeros((n_restorations, n_polygons), dtype='float64')\n",
        "agbd_stdev_data = np.zeros((n_restorations, n_polygons), dtype='float64')\n",
        "agb_total_data = np.zeros((n_restorations, n_polygons), dtype='float64')\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    agbd_mean_ci95_data = np.zeros((n_restorations, n_polygons), dtype='float64')\n",
        "    agbd_mean_uncertainty_data = np.zeros((n_restorations, n_polygons), dtype='float64')\n",
        "    agb_total_ci95_data = np.zeros((n_restorations, n_polygons), dtype='float64')\n",
        "\n",
        "# Open AGB total restoration raster datasets\n",
        "agb_total_restoration_datasets = {path: rasterio.open(path) for path in agb_total_restoration_rasters}\n",
        "\n",
        "# Open AGB total CI95 restoration datasets only if uncertainty stats are generated\n",
        "agb_total_ci95_restoration_datasets = {}\n",
        "if generate_uncertainty_stats:\n",
        "    for agb_total_restoration_raster in agb_total_restoration_rasters:\n",
        "        restoration_name = os.path.basename(agb_total_restoration_raster).split('agb_total_mg__')[1].split('.')[0]\n",
        "        agb_total_ci95_path = join(agb_total_restoration_dir, f\"agb_total_ci_95_mg__{restoration_name}.tif\")\n",
        "        if exists(agb_total_ci95_path):\n",
        "            agb_total_ci95_restoration_datasets[agb_total_restoration_raster] = rasterio.open(agb_total_ci95_path)\n",
        "\n",
        "# Load cell area raster once for all calculations\n",
        "cell_area_dataset = rasterio.open(cell_area_path)\n",
        "\n",
        "# Progress tracking\n",
        "progress_total = n_polygons * n_restorations\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Raster / polygon pair progress: {progress_index}/{progress_total}\")\n",
        "display(progress_label)\n",
        "\n",
        "try:\n",
        "    for poly_idx, (index, row) in enumerate(selected_sample_polygons_gpkg.iterrows()):\n",
        "        sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"]\n",
        "        polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "        # Mask cell area raster to polygon\n",
        "        cell_area_masked, _ = msk.mask(cell_area_dataset, polygons, crop=True, filled=False)\n",
        "        cell_area_masked = cell_area_masked.astype('float64')\n",
        "        cell_area_masked_ha = cell_area_masked / 10000\n",
        "\n",
        "        for raster_idx, agb_total_restoration_raster in enumerate(agb_total_restoration_rasters):\n",
        "            # Mask AGB total restoration raster to polygon\n",
        "            agb_total_restoration = agb_total_restoration_datasets[agb_total_restoration_raster]\n",
        "            agb_total_array_masked, _ = msk.mask(agb_total_restoration, polygons, crop=True, filled=False)\n",
        "            agb_total_array_masked = agb_total_array_masked.astype('float64')\n",
        "\n",
        "            # Valid pixels mask\n",
        "            valid_mask = ~np.ma.getmaskarray(agb_total_array_masked)\n",
        "\n",
        "            # Calculate area of valid pixels\n",
        "            valid_cell_areas_ha = np.ma.array(cell_area_masked_ha.data, mask=~valid_mask)\n",
        "            valid_area_ha = np.ma.sum(valid_cell_areas_ha, dtype='float64')\n",
        "\n",
        "            # Sum total AGB in Mg (positive values for restoration)\n",
        "            agb_total_mg = np.ma.sum(agb_total_array_masked, dtype='float64')\n",
        "\n",
        "            # Calculate statistics\n",
        "            if np.ma.is_masked(agb_total_mg) or valid_area_ha <= 0:\n",
        "                agbd_mean_mg_ha = 0.0\n",
        "                agbd_stdev_mg_ha = 0.0\n",
        "                agb_total_tg = 0.0\n",
        "            else:\n",
        "                # Area-weighted mean AGBD\n",
        "                agbd_mean_mg_ha = agb_total_mg / valid_area_ha\n",
        "\n",
        "                # Back-calculate individual AGBD values for standard deviation\n",
        "                agbd_values = agb_total_array_masked / cell_area_masked_ha\n",
        "                valid_agbd = agbd_values[valid_mask]\n",
        "                valid_areas = cell_area_masked_ha[valid_mask]\n",
        "\n",
        "                # Area-weighted standard deviation\n",
        "                variance_weighted = np.sum(valid_areas * (valid_agbd - agbd_mean_mg_ha)**2) / valid_area_ha\n",
        "                agbd_stdev_mg_ha = np.sqrt(variance_weighted)\n",
        "\n",
        "                # Convert total AGB from Mg to Tg\n",
        "                agb_total_tg = agb_total_mg / 1000000\n",
        "\n",
        "            # Store results\n",
        "            agbd_mean_data[raster_idx, poly_idx] = agbd_mean_mg_ha\n",
        "            agbd_stdev_data[raster_idx, poly_idx] = agbd_stdev_mg_ha\n",
        "            agb_total_data[raster_idx, poly_idx] = agb_total_tg\n",
        "\n",
        "            if generate_uncertainty_stats and agb_total_restoration_raster in agb_total_ci95_restoration_datasets:\n",
        "                agb_total_ci95_raster = agb_total_ci95_restoration_datasets[agb_total_restoration_raster]\n",
        "                agb_total_ci95_array_masked, _ = msk.mask(agb_total_ci95_raster, polygons, crop=True, filled=False)\n",
        "                agb_total_ci95_array_masked = agb_total_ci95_array_masked.astype('float64')\n",
        "\n",
        "                agb_total_ci95_mg = abs(np.ma.sum(agb_total_ci95_array_masked, dtype='float64'))\n",
        "\n",
        "                if abs(agb_total_mg) > 0:\n",
        "                    agbd_mean_ci95_mg_ha = agb_total_ci95_mg / valid_area_ha\n",
        "                    agbd_uncertainty_pct = agb_total_ci95_mg / abs(agb_total_mg) * 100\n",
        "                else:\n",
        "                    agbd_mean_ci95_mg_ha = 0.0\n",
        "                    agbd_uncertainty_pct = 0.0\n",
        "\n",
        "                agb_total_ci95_tg = agb_total_ci95_mg / 1000000\n",
        "\n",
        "                agbd_mean_ci95_data[raster_idx, poly_idx] = agbd_mean_ci95_mg_ha\n",
        "                agbd_mean_uncertainty_data[raster_idx, poly_idx] = agbd_uncertainty_pct\n",
        "                agb_total_ci95_data[raster_idx, poly_idx] = agb_total_ci95_tg\n",
        "\n",
        "            progress_index += 1\n",
        "            progress_label.value = f\"Raster / polygon pair progress: {progress_index}/{progress_total}\"\n",
        "\n",
        "finally:\n",
        "    cell_area_dataset.close()\n",
        "    for dataset in agb_total_restoration_datasets.values():\n",
        "        dataset.close()\n",
        "    for dataset in agb_total_ci95_restoration_datasets.values():\n",
        "        dataset.close()\n",
        "\n",
        "# Create DataFrames from pre-allocated arrays\n",
        "df_agbd_mean_mg_ha = pd.DataFrame(agbd_mean_data, index=selected_restorations, columns=polygon_names)\n",
        "df_agbd_mean_mg_ha.rename_axis('rest', inplace=True)\n",
        "\n",
        "df_agbd_stdev_mg_ha = pd.DataFrame(agbd_stdev_data, index=selected_restorations, columns=polygon_names)\n",
        "df_agbd_stdev_mg_ha.rename_axis('rest', inplace=True)\n",
        "\n",
        "df_agb_total_tg = pd.DataFrame(agb_total_data, index=selected_restorations, columns=polygon_names)\n",
        "df_agb_total_tg.rename_axis('rest', inplace=True)\n",
        "\n",
        "if generate_uncertainty_stats:\n",
        "    df_agbd_mean_ci95_mg_ha = pd.DataFrame(agbd_mean_ci95_data, index=selected_restorations, columns=polygon_names)\n",
        "    df_agbd_mean_ci95_mg_ha.rename_axis('rest', inplace=True)\n",
        "\n",
        "    df_agbd_uncertainty_pct = pd.DataFrame(agbd_mean_uncertainty_data, index=selected_restorations, columns=polygon_names)\n",
        "    df_agbd_uncertainty_pct.rename_axis('rest', inplace=True)\n",
        "\n",
        "    df_agb_total_ci95_tg = pd.DataFrame(agb_total_ci95_data, index=selected_restorations, columns=polygon_names)\n",
        "    df_agb_total_ci95_tg.rename_axis('rest', inplace=True)\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "    df_stats_list = [df_agbd_mean_mg_ha, df_agbd_mean_ci95_mg_ha, df_agbd_uncertainty_pct,\n",
        "                     df_agbd_stdev_mg_ha, df_agb_total_tg, df_agb_total_ci95_tg]\n",
        "else:\n",
        "    df_stats_list = [df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Generate summary stats\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" AGB (Tg)\")\n",
        "\n",
        "summary_components = [df_agb_total_tg_t]\n",
        "if generate_uncertainty_stats:\n",
        "    df_agb_total_ci95_tg_t = df_agb_total_ci95_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" AGB CI95 (Tg)\")\n",
        "    summary_components.append(df_agb_total_ci95_tg_t)\n",
        "\n",
        "summary_restoration_stats = pd.concat(summary_components, axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_restoration_stats.to_csv(join(sample_polygons_statistics_agbd_dir, 'summary_restoration_stats.csv'))\n",
        "\n",
        "# Generate detailed stats by area\n",
        "df_base = pd.DataFrame(index=selected_restorations)\n",
        "df_base.rename_axis('rest', inplace=True)\n",
        "\n",
        "for polygon_name in polygon_names:\n",
        "    df_detailed = df_base.copy()\n",
        "    for df_stats in df_stats_list:\n",
        "        if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"AGBD mean (Mg / ha)\"\n",
        "        elif df_stats.equals(df_agbd_stdev_mg_ha): stat_col = \"AGBD stdev (Mg / ha)\"\n",
        "        elif df_stats.equals(df_agb_total_tg): stat_col = \"AGB total (Tg)\"\n",
        "        elif generate_uncertainty_stats and df_stats.equals(df_agbd_mean_ci95_mg_ha): stat_col = \"AGBD CI95 (Mg / ha)\"\n",
        "        elif generate_uncertainty_stats and df_stats.equals(df_agbd_uncertainty_pct): stat_col = \"AGBD uncertainty (%)\"\n",
        "        elif generate_uncertainty_stats and df_stats.equals(df_agb_total_ci95_tg): stat_col = \"AGB total CI95 (Tg)\"\n",
        "        df_detailed[stat_col] = df_stats[polygon_name]\n",
        "    df_detailed.to_csv(join(restoration_stats_by_area_dir, f'{polygon_name}.csv'))\n",
        "\n",
        "# Generate detailed stats by restoration\n",
        "restorations = {}\n",
        "for stats_csv in os.listdir(restoration_stats_by_area_dir):\n",
        "    polygon_name = stats_csv[:-4]\n",
        "    stats_csv_df = pd.read_csv(join(restoration_stats_by_area_dir, stats_csv))\n",
        "    for restoration in stats_csv_df['rest'].unique():\n",
        "        restoration_df = stats_csv_df[stats_csv_df['rest'] == restoration].copy()\n",
        "        restoration_df.drop('rest', axis=1, inplace=True)\n",
        "        restoration_df.insert(0, 'Name', polygon_name)\n",
        "        if restoration in restorations:\n",
        "            restorations[restoration] = pd.concat([restorations[restoration], restoration_df], ignore_index=True)\n",
        "        else:\n",
        "            restorations[restoration] = restoration_df\n",
        "\n",
        "for restoration, restoration_df in restorations.items():\n",
        "    restoration_df.to_csv(join(restoration_stats_by_restoration_dir, f'{rest}.csv'), index=False)"
      ],
      "metadata": {
        "id": "-bJ5KAZmJrJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t45TVipaVma"
      },
      "source": [
        "# Intactness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6p_6d-0adF4"
      },
      "outputs": [],
      "source": [
        "# Create list of available intactness rasters\n",
        "intactness_rasters = []\n",
        "for root, dirs, files in os.walk(intactness_dir):\n",
        "    for file in files:\n",
        "        if \"intactness__\" in file and file.endswith('tif'):\n",
        "            relative_path = os.path.relpath(join(root, file), intactness_dir)\n",
        "            intactness_rasters.append(relative_path)\n",
        "\n",
        "# Select intactness rasters to calculate statistics\n",
        "print(\"# Select intactness raster to calculate statistics\")\n",
        "print(\"intactness_rasters = [\")\n",
        "for raster in intactness_rasters:\n",
        "    print(f\"'{raster}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8mA9ax8IjCj"
      },
      "outputs": [],
      "source": [
        "# Select intactness raster to calculate statistics\n",
        "intactness_rasters = [\n",
        "'2021_no_disturbance_since_1993__2021_degradation_since_1993/intactness__forest_reserves_10_quantiles__2021_no_disturbance_since_1993__2021_degradation_since_1993__agbd_251203_161707.tif',\n",
        "'2021_no_disturbance_since_1993__2021_degradation_since_1993/intactness__gedi_area_10_quantiles__2021_no_disturbance_since_1993__2021_degradation_since_1993__agbd_251203_161707.tif',\n",
        "'2021_no_disturbance_since_oldgrowth__2021_degradation_since_oldgrowth/intactness__forest_reserves_10_quantiles__2021_no_disturbance_since_oldgrowth__2021_degradation_since_oldgrowth__agbd_251203_161707.tif',\n",
        "'2021_no_disturbance_since_oldgrowth__2021_degradation_since_oldgrowth/intactness__gedi_area_10_quantiles__2021_no_disturbance_since_oldgrowth__2021_degradation_since_oldgrowth__agbd_251203_161707.tif',\n",
        "'2024_no_disturbance_since_1996__2024_degradation_since_1996/intactness__forest_reserves_10_quantiles__2024_no_disturbance_since_1996__2024_degradation_since_1996__agbd_251203_161707.tif',\n",
        "'2024_no_disturbance_since_1996__2024_degradation_since_1996/intactness__gedi_area_10_quantiles__2024_no_disturbance_since_1996__2024_degradation_since_1996__agbd_251203_161707.tif',\n",
        "'2024_no_disturbance_since_oldgrowth__2024_degradation_since_oldgrowth/intactness__forest_reserves_10_quantiles__2024_no_disturbance_since_oldgrowth__2024_degradation_since_oldgrowth__agbd_251203_161707.tif',\n",
        "'2024_no_disturbance_since_oldgrowth__2024_degradation_since_oldgrowth/intactness__gedi_area_10_quantiles__2024_no_disturbance_since_oldgrowth__2024_degradation_since_oldgrowth__agbd_251203_161707.tif',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6Q51AedNXu6"
      },
      "outputs": [],
      "source": [
        "# Intactness statistics\n",
        "# Calculates area-weighted intactness and percentage change statistics per polygon.\n",
        "# Two metrics computed:\n",
        "#   - Remaining forest: statistics for pixels with intactness > 0\n",
        "#   - Landscape: includes deforested pixels (intactness == 0) as score 0 or -100% change\n",
        "\n",
        "# Match intactness rasters to percentage change rasters\n",
        "intactness_percentage_paths = {}\n",
        "for intactness_raster in intactness_rasters:\n",
        "    intactness_raster_path = join(intactness_dir, intactness_raster)\n",
        "\n",
        "    # Percentage change raster located in same subdirectory as intactness raster\n",
        "    subdir = intactness_raster.split('/')[0]\n",
        "    percentage_change_filename = f\"percentage_change__{subdir}__{selected_model}.tif\"\n",
        "    percentage_change_path = join(intactness_dir, subdir, percentage_change_filename)\n",
        "    intactness_percentage_paths[intactness_raster_path] = percentage_change_path\n",
        "\n",
        "# Area-weighted mean and standard deviation\n",
        "def weighted_stats(values, weights):\n",
        "    if len(values) == 0:\n",
        "        return None, None\n",
        "    sum_of_weights = np.sum(weights)\n",
        "    if sum_of_weights <= 0:\n",
        "        return 0.0, 0.0\n",
        "    weighted_mean = np.sum(values * weights) / sum_of_weights\n",
        "    variance = np.sum(weights * (values - weighted_mean) ** 2) / sum_of_weights\n",
        "    weighted_std = np.sqrt(variance)\n",
        "    return weighted_mean, weighted_std\n",
        "\n",
        "# Area in hectares for each unique intactness score\n",
        "def calculate_score_areas(intactness_masked, cell_area_masked_ha):\n",
        "    score_areas = {}\n",
        "    if np.ma.count(intactness_masked) == 0:\n",
        "        return score_areas\n",
        "    unique_scores = np.unique(intactness_masked.compressed())\n",
        "    for score in unique_scores:\n",
        "        score_mask = (intactness_masked == score) & (~intactness_masked.mask)\n",
        "        if np.any(score_mask):\n",
        "            score_areas[int(score)] = np.sum(cell_area_masked_ha[score_mask], dtype='float64')\n",
        "        else:\n",
        "            score_areas[int(score)] = 0.0\n",
        "    return score_areas\n",
        "\n",
        "# Pre-open cell area dataset\n",
        "cell_area_dataset = rasterio.open(cell_area_path)\n",
        "\n",
        "# Progress tracking\n",
        "n_intactness_rasters = len(intactness_rasters)\n",
        "n_polygons = len(selected_sample_polygons_gpkg)\n",
        "progress_total = n_intactness_rasters * n_polygons\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Raster / polygon pair progress: {progress_index}/{progress_total}\")\n",
        "display(progress_label)\n",
        "\n",
        "try:\n",
        "    # Outer loop: one CSV output per intactness raster\n",
        "    for intactness_raster in intactness_rasters:\n",
        "        intactness_raster_path = join(intactness_dir, intactness_raster)\n",
        "        percentage_raster_path = intactness_percentage_paths[intactness_raster_path]\n",
        "\n",
        "        # Extract metadata from filename\n",
        "        # Format: intactness__{quantiles}__{baseline}__{disturbance}__{model}.tif\n",
        "        filename = intactness_raster.split('/')[-1]\n",
        "        parts = filename.split('__')\n",
        "        polygon_quantiles = parts[1]\n",
        "        baseline_name = parts[2]\n",
        "        disturbance = parts[3]\n",
        "\n",
        "        # Total score derived from quantiles (e.g. 'forest_reserves_10_quantiles' -> 10)\n",
        "        total_score = int(polygon_quantiles.split('_')[-2])\n",
        "        total_stdev = int(total_score / 2)\n",
        "\n",
        "        # Output CSV path\n",
        "        intactness_csv_name = f\"{polygon_quantiles}__{baseline_name}__{disturbance}.csv\"\n",
        "        intactness_csv_path = join(intactness_stats_dir, intactness_csv_name)\n",
        "\n",
        "        # Initialise output dataframe\n",
        "        df_intactness_stats = pd.DataFrame(columns=[\n",
        "            \"Name\",\n",
        "            \"Percentage change (remaining forest) mean\",\n",
        "            \"Percentage change (remaining forest) stdev\",\n",
        "            \"Percentage change (landscape) mean\",\n",
        "            \"Percentage change (landscape) stdev\",\n",
        "            f\"Intactness (remaining forest) mean / {total_score}\",\n",
        "            f\"Intactness (remaining forest) stdev / {total_stdev}\",\n",
        "            f\"Intactness (landscape) mean / {total_score}\",\n",
        "            f\"Intactness (landscape) stdev / {total_stdev}\"\n",
        "        ])\n",
        "\n",
        "        # Inner loop: one row per polygon\n",
        "        for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "            sample_polygon_geometry = row[\"geometry\"]\n",
        "            sample_polygon_name = row[\"name\"]\n",
        "            polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "            # Mask intactness raster to polygon\n",
        "            with rasterio.open(intactness_raster_path) as src:\n",
        "                intactness_masked, _ = msk.mask(src, polygons, crop=True, filled=False)\n",
        "                intactness_masked = intactness_masked.astype('float64')\n",
        "\n",
        "            # Skip polygon if no valid intactness data\n",
        "            if np.ma.count(intactness_masked) == 0:\n",
        "                new_row = pd.DataFrame([{\n",
        "                    'Name': sample_polygon_name,\n",
        "                    'Percentage change (remaining forest) mean': None,\n",
        "                    'Percentage change (remaining forest) stdev': None,\n",
        "                    'Percentage change (landscape) mean': None,\n",
        "                    'Percentage change (landscape) stdev': None,\n",
        "                    f'Intactness (remaining forest) mean / {total_score}': None,\n",
        "                    f'Intactness (remaining forest) stdev / {total_stdev}': None,\n",
        "                    f'Intactness (landscape) mean / {total_score}': None,\n",
        "                    f'Intactness (landscape) stdev / {total_stdev}': None,\n",
        "                }], dtype=object)\n",
        "                df_intactness_stats = pd.concat([df_intactness_stats, new_row], ignore_index=True)\n",
        "                progress_index += 1\n",
        "                progress_label.value = f\"Raster / polygon pair progress: {progress_index}/{progress_total}\"\n",
        "                continue\n",
        "\n",
        "            # Mask cell area raster to polygon, convert m2 to ha\n",
        "            cell_area_masked, _ = msk.mask(cell_area_dataset, polygons, crop=True, filled=False)\n",
        "            cell_area_masked = cell_area_masked.astype('float64')\n",
        "            cell_area_masked_ha = cell_area_masked / 10000\n",
        "\n",
        "            # Forest mask: valid pixels with intactness > 0 (remaining forest)\n",
        "            forest_mask = (~intactness_masked.mask) & (intactness_masked > 0)\n",
        "\n",
        "            # Deforested mask: valid pixels with intactness == 0 (lost since baseline)\n",
        "            deforested_mask = (~intactness_masked.mask) & (intactness_masked == 0)\n",
        "\n",
        "            # Calculate areas in hectares\n",
        "            forest_area_ha = np.sum(cell_area_masked_ha.data[forest_mask], dtype='float64')\n",
        "            deforested_area_ha = np.sum(cell_area_masked_ha.data[deforested_mask], dtype='float64')\n",
        "            landscape_area_ha = forest_area_ha + deforested_area_ha\n",
        "\n",
        "            # Mask percentage change raster to polygon\n",
        "            with rasterio.open(percentage_raster_path) as percent_src:\n",
        "                percent_change_masked, _ = msk.mask(percent_src, polygons, crop=True, filled=False)\n",
        "                percent_change_masked = percent_change_masked.astype('float64')\n",
        "\n",
        "            # Calculate percentage change statistics\n",
        "            if forest_area_ha > 0:\n",
        "                # Remaining forest: only pixels with intactness > 0\n",
        "                forest_percent_values = percent_change_masked.data[forest_mask]\n",
        "                forest_percent_weights = cell_area_masked_ha.data[forest_mask]\n",
        "                percent_forest_mean, percent_forest_std = weighted_stats(\n",
        "                    forest_percent_values, forest_percent_weights\n",
        "                )\n",
        "\n",
        "                # Landscape: include deforested pixels as -100% change\n",
        "                if deforested_area_ha > 0:\n",
        "                    landscape_mean_num = np.sum(forest_percent_values * forest_percent_weights) + deforested_area_ha * (-100.0)\n",
        "                    percent_landscape_mean = landscape_mean_num / landscape_area_ha\n",
        "\n",
        "                    forest_var_contrib = np.sum(forest_percent_weights * np.square(forest_percent_values - percent_landscape_mean))\n",
        "                    deforested_var_contrib = deforested_area_ha * np.square((-100.0) - percent_landscape_mean)\n",
        "                    percent_landscape_std = np.sqrt((forest_var_contrib + deforested_var_contrib) / landscape_area_ha)\n",
        "                else:\n",
        "                    # No deforestation: landscape stats equal remaining forest stats\n",
        "                    percent_landscape_mean = percent_forest_mean\n",
        "                    percent_landscape_std = percent_forest_std\n",
        "            else:\n",
        "                # No remaining forest: all deforested\n",
        "                percent_forest_mean = percent_forest_std = None\n",
        "                percent_landscape_mean = -100.0\n",
        "                percent_landscape_std = 0.0\n",
        "\n",
        "            # Calculate intactness statistics\n",
        "            if forest_area_ha > 0:\n",
        "                # Remaining forest: only pixels with intactness > 0\n",
        "                forest_intact_vals = intactness_masked.data[forest_mask]\n",
        "                forest_intact_weights = cell_area_masked_ha.data[forest_mask]\n",
        "                intactness_forest_mean, intactness_forest_std = weighted_stats(\n",
        "                    forest_intact_vals, forest_intact_weights\n",
        "                )\n",
        "\n",
        "                # Landscape: include deforested pixels as score 0\n",
        "                if deforested_area_ha > 0:\n",
        "                    intactness_landscape_mean = np.sum(forest_intact_vals * forest_intact_weights) / landscape_area_ha\n",
        "\n",
        "                    forest_var_contrib = np.sum(forest_intact_weights * np.square(forest_intact_vals - intactness_landscape_mean))\n",
        "                    deforested_var_contrib = deforested_area_ha * np.square(0 - intactness_landscape_mean)\n",
        "                    intactness_landscape_std = np.sqrt((forest_var_contrib + deforested_var_contrib) / landscape_area_ha)\n",
        "                else:\n",
        "                    # No deforestation: landscape stats equal remaining forest stats\n",
        "                    intactness_landscape_mean = intactness_forest_mean\n",
        "                    intactness_landscape_std = intactness_forest_std\n",
        "            else:\n",
        "                # No remaining forest: all deforested\n",
        "                intactness_forest_mean = intactness_forest_std = None\n",
        "                intactness_landscape_mean = 0.0\n",
        "                intactness_landscape_std = 0.0\n",
        "\n",
        "            # Calculate area per intactness score (includes score 0)\n",
        "            score_areas = calculate_score_areas(intactness_masked, cell_area_masked_ha)\n",
        "\n",
        "            # Build output row\n",
        "            new_row_dict = {\n",
        "                'Name': sample_polygon_name,\n",
        "                'Percentage change (remaining forest) mean': percent_forest_mean,\n",
        "                'Percentage change (remaining forest) stdev': percent_forest_std,\n",
        "                'Percentage change (landscape) mean': percent_landscape_mean,\n",
        "                'Percentage change (landscape) stdev': percent_landscape_std,\n",
        "                f'Intactness (remaining forest) mean / {total_score}': intactness_forest_mean,\n",
        "                f'Intactness (remaining forest) stdev / {total_stdev}': intactness_forest_std,\n",
        "                f'Intactness (landscape) mean / {total_score}': intactness_landscape_mean,\n",
        "                f'Intactness (landscape) stdev / {total_stdev}': intactness_landscape_std,\n",
        "            }\n",
        "\n",
        "            # Append score area columns dynamically\n",
        "            for score, area in score_areas.items(): new_row_dict[f'Intactness score {score} area (ha)'] = area\n",
        "            new_row = pd.DataFrame([new_row_dict], dtype=object)\n",
        "            df_intactness_stats = pd.concat([df_intactness_stats, new_row], ignore_index=True)\n",
        "\n",
        "            progress_index += 1\n",
        "            progress_label.value = f\"Raster / polygon pair progress: {progress_index}/{progress_total}\"\n",
        "\n",
        "        # Save CSV for this intactness raster\n",
        "        df_intactness_stats = df_intactness_stats.set_index('Name')\n",
        "        df_intactness_stats.to_csv(intactness_csv_path)\n",
        "        print(f\"Saved statistics to {intactness_csv_path}\")\n",
        "\n",
        "finally: cell_area_dataset.close()\n",
        "\n",
        "print(\"Intactness statistics completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC-rviHPi4q5"
      },
      "source": [
        "# Report statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHpAf6qg1dEf"
      },
      "outputs": [],
      "source": [
        "# Reduces statistics outputs to a more specific and intuitive format.\n",
        "\n",
        "# Load summary stats files\n",
        "summary_scenario_stats_df = pd.read_csv(join(sample_polygons_statistics_agbd_dir, 'summary_scenario_stats.csv'))\n",
        "summary_disturbance_stats_df = pd.read_csv(join(sample_polygons_statistics_agbd_dir, 'summary_disturbance_stats.csv'))\n",
        "summary_forest_cover_stats_df = pd.read_csv(join(sample_polygons_statistics_agbd_dir, 'summary_forest_cover_stats.csv'))\n",
        "\n",
        "# Extract forest cover scenarios (exclude percentage columns)\n",
        "forest_cover_cols = [col for col in summary_forest_cover_stats_df.columns\n",
        "                     if col.endswith('forest cover (ha)')]\n",
        "forest_cover_scenarios = [col.replace(' forest cover (ha)', '') for col in forest_cover_cols]\n",
        "\n",
        "print(\"# Forest cover scenarios\")\n",
        "print(\"forest_cover_list = [\")\n",
        "for scenario in forest_cover_scenarios:\n",
        "    print(f\"  '{scenario}',\")\n",
        "print(\"]\\n\")\n",
        "\n",
        "# Extract scenario list from summary stats\n",
        "print(\"# AGB scenarios (order matters)\")\n",
        "print(\"scenario_list = [\")\n",
        "for csv in os.listdir(scenario_stats_by_scenario_dir):\n",
        "    print(f\"  '{csv[:-4]}',\")\n",
        "print(\"]\\n\")\n",
        "\n",
        "# Extract and sort disturbance list\n",
        "disturbance_csv_files = [f[:-4] for f in os.listdir(disturbance_stats_by_disturbance_dir) if f.endswith('.csv')]\n",
        "\n",
        "def get_disturbance_type(filename):\n",
        "    if 'disturbance' in filename: return 3\n",
        "    elif 'deforestation' in filename: return 2\n",
        "    else: return 1  # degradation\n",
        "\n",
        "def get_ref_year_sort_key(ref_year):\n",
        "    # Numeric years sorted descending, non-numeric strings sorted alphabetically after\n",
        "    try:\n",
        "        return (0, -int(ref_year))\n",
        "    except ValueError:\n",
        "        return (1, ref_year)\n",
        "\n",
        "# Group by year and disturbance type\n",
        "files_by_category = {}\n",
        "for file in disturbance_csv_files:\n",
        "    year = file.split('_')[0]\n",
        "    disturbance_type = get_disturbance_type(file)\n",
        "    key = (year, disturbance_type)\n",
        "    if key not in files_by_category:\n",
        "        files_by_category[key] = []\n",
        "    files_by_category[key].append(file)\n",
        "\n",
        "print(\"# Disturbance scenarios\")\n",
        "print(\"disturbance_list = [\")\n",
        "for key in sorted(files_by_category.keys(), key=lambda k: (int(k[0]), k[1])):\n",
        "    files = files_by_category[key]\n",
        "    # Total files first\n",
        "    total_files = [f for f in files if '_total' in f]\n",
        "    for file in total_files:\n",
        "        print(f\"    '{file}',\")\n",
        "    # Group remaining by reference year\n",
        "    ref_year_files = {}\n",
        "    for file in files:\n",
        "        if '_total' in file: continue\n",
        "        ref_year = file.split('_')[-1]\n",
        "        if ref_year not in ref_year_files:\n",
        "            ref_year_files[ref_year] = []\n",
        "        ref_year_files[ref_year].append(file)\n",
        "    # Process each reference year\n",
        "    for ref_year in sorted(ref_year_files.keys(), key=get_ref_year_sort_key):\n",
        "        year_files = ref_year_files[ref_year]\n",
        "        since_files = [f for f in year_files if '_since_' in f]\n",
        "        before_files = [f for f in year_files if '_before_' in f]\n",
        "        for file in since_files:\n",
        "            print(f\"    '{file}',\")\n",
        "        for file in before_files:\n",
        "            print(f\"    # '{file}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forest cover scenarios\n",
        "forest_cover_list = [\n",
        "  '1993',\n",
        "  '2021',\n",
        "  '2024',\n",
        "  '2024_oldgrowth_all_land',\n",
        "]\n",
        "\n",
        "# AGB scenarios (order matters)\n",
        "scenario_list = [\n",
        "  '2021',\n",
        "  '2021_no_disturbance_since_1993',\n",
        "  '2021_oldgrowth_all_land',\n",
        "  '2024',\n",
        "]\n",
        "\n",
        "# Disturbance scenarios\n",
        "disturbance_list = [\n",
        "    '2021_degradation_since_1993',\n",
        "    '2021_deforestation_since_1993',\n",
        "]"
      ],
      "metadata": {
        "id": "1aPFC55DCnvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCMvXcIa2Q0D"
      },
      "outputs": [],
      "source": [
        "# Read summary stats\n",
        "summary_scenario_stats_df = pd.read_csv(join(sample_polygons_statistics_agbd_dir, 'summary_scenario_stats.csv'))\n",
        "summary_disturbance_stats_df = pd.read_csv(join(sample_polygons_statistics_agbd_dir, 'summary_disturbance_stats.csv'))\n",
        "summary_forest_cover_stats_df = pd.read_csv(join(sample_polygons_statistics_agbd_dir, 'summary_forest_cover_stats.csv'))\n",
        "\n",
        "# Create forest cover CSV (including total area km2)\n",
        "forest_cover = pd.DataFrame()\n",
        "forest_cover['Name'] = summary_forest_cover_stats_df['Name']\n",
        "forest_cover['Area (km^2)'] = summary_forest_cover_stats_df['Area (km^2)']\n",
        "for scenario in forest_cover_list:\n",
        "    col_name = f'{scenario} forest cover (ha)'\n",
        "    if col_name in summary_forest_cover_stats_df.columns:\n",
        "        forest_cover[col_name] = summary_forest_cover_stats_df[col_name]\n",
        "forest_cover.to_csv(join(report_statistics_agbd_dir, 'forest_cover.csv'), index=False)\n",
        "\n",
        "# Create scenarios total AGB CSV\n",
        "scenarios_total_agb = pd.DataFrame()\n",
        "scenarios_total_agb['Name'] = summary_scenario_stats_df['Unnamed: 0']\n",
        "for scenario in scenario_list:\n",
        "    scenarios_total_agb[f'{scenario} forest AGB (Tg)'] = summary_scenario_stats_df[f'{scenario} forest AGB (Tg)']\n",
        "if source_dir == uncertainty_dir:\n",
        "    for scenario in scenario_list:\n",
        "        scenarios_total_agb[f'{scenario} forest AGB CI95 (Tg)'] = summary_scenario_stats_df[f'{scenario} forest AGB CI95 (Tg)']\n",
        "scenarios_total_agb.to_csv(join(report_statistics_agbd_dir, 'scenarios_total_agb.csv'), index=False)\n",
        "\n",
        "# Create scenarios AGBD CSV\n",
        "scenarios_agbd = pd.DataFrame()\n",
        "scenarios_agbd['Name'] = summary_scenario_stats_df['Unnamed: 0']\n",
        "for scenario in scenario_list:\n",
        "    scenario_stats_df = pd.read_csv(join(scenario_stats_by_scenario_dir, f'{scenario}.csv'))\n",
        "    scenarios_agbd[f'{scenario} forest AGBD (Mg / ha)'] = scenario_stats_df['Forest AGBD mean (Mg / ha)']\n",
        "if source_dir == uncertainty_dir:\n",
        "    for scenario in scenario_list:\n",
        "        scenario_stats_df = pd.read_csv(join(scenario_stats_by_scenario_dir, f'{scenario}.csv'))\n",
        "        scenarios_agbd[f'{scenario} forest AGBD CI95 (Mg / ha)'] = scenario_stats_df['Forest AGBD CI95 (Mg / ha)']\n",
        "scenarios_agbd.to_csv(join(report_statistics_agbd_dir, 'scenarios_agbd.csv'), index=False)\n",
        "\n",
        "# Create disturbance total AGB CSV\n",
        "disturbance_total_agb = pd.DataFrame()\n",
        "disturbance_total_agb['Name'] = summary_disturbance_stats_df['Unnamed: 0']\n",
        "for disturbance in disturbance_list:\n",
        "    disturbance_total_agb[f'{disturbance} forest AGB (Tg)'] = summary_disturbance_stats_df[f'{disturbance} forest AGB (Tg)']\n",
        "if source_dir == uncertainty_dir:\n",
        "    for disturbance in disturbance_list:\n",
        "        disturbance_total_agb[f'{disturbance} forest AGB CI95 (Tg)'] = summary_disturbance_stats_df[f'{disturbance} forest AGB CI95 (Tg)']\n",
        "disturbance_total_agb.to_csv(join(report_statistics_agbd_dir, 'disturbance_total_agb.csv'), index=False)\n",
        "\n",
        "# Create disturbance AGBD CSV\n",
        "disturbance_agbd = pd.DataFrame()\n",
        "disturbance_agbd['Name'] = summary_scenario_stats_df['Unnamed: 0']\n",
        "for disturbance in disturbance_list:\n",
        "    disturbance_stats_df = pd.read_csv(join(disturbance_stats_by_disturbance_dir, f'{disturbance}.csv'))\n",
        "    disturbance_agbd[f'{disturbance} forest AGBD (Mg / ha)'] = disturbance_stats_df['Forest AGBD mean (Mg / ha)']\n",
        "if source_dir == uncertainty_dir:\n",
        "    for disturbance in disturbance_list:\n",
        "        disturbance_stats_df = pd.read_csv(join(disturbance_stats_by_disturbance_dir, f'{disturbance}.csv'))\n",
        "        disturbance_agbd[f'{disturbance} forest AGBD CI95 (Mg / ha)'] = disturbance_stats_df['Forest AGBD CI95 (Mg / ha)']\n",
        "disturbance_agbd.to_csv(join(report_statistics_agbd_dir, 'disturbance_agbd.csv'), index=False)\n",
        "\n",
        "print(\"Report statistics completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b70oSO_VNtkY"
      },
      "source": [
        "# Sankey plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHEy0mBX2yNj"
      },
      "outputs": [],
      "source": [
        "# Define and create directories\n",
        "sankey_labelled = join(sample_polygons_statistics_agbd_dir, 'sankey_labelled')\n",
        "sankey_unlabelled = join(sample_polygons_statistics_agbd_dir, 'sankey_unlabelled')\n",
        "sankey_labelled_svg = join(sample_polygons_statistics_agbd_dir, 'sankey_labelled_svg')\n",
        "sankey_unlabelled_svg = join(sample_polygons_statistics_agbd_dir, 'sankey_unlabelled_svg')\n",
        "\n",
        "for dir in [sankey_labelled, sankey_unlabelled, sankey_labelled_svg, sankey_unlabelled_svg]:\n",
        "    makedirs(dir, exist_ok=True)\n",
        "\n",
        "# Load the CSV files\n",
        "summary_scenario_stats = pd.read_csv(join(sample_polygons_statistics_agbd_dir,'summary_scenario_stats.csv'))\n",
        "summary_disturbance_stats = pd.read_csv(join(sample_polygons_statistics_agbd_dir,'summary_disturbance_stats.csv'))\n",
        "\n",
        "# Check that all rows in both .csv files have the same strings (polygon areas) in column A\n",
        "polygon_areas_stats = summary_scenario_stats.iloc[:, 0]\n",
        "polygon_areas_disturbance_stats = summary_disturbance_stats.iloc[:, 0]\n",
        "\n",
        "assert all(polygon_areas_stats == polygon_areas_disturbance_stats), \"Polygon areas do not match between the two CSV files.\"\n",
        "\n",
        "# Print columns relevant for sankey diagram configuration\n",
        "\n",
        "# Filter for AGB columns only (exclude forest cover and CI95 for initial selection)\n",
        "summary_agb_cols = [col for col in summary_scenario_stats.columns[1:] if 'forest AGB (Tg)' in col and 'CI95' not in col]\n",
        "disturbance_agb_cols = [col for col in summary_disturbance_stats.columns[1:] if 'forest AGB (Tg)' in col and 'CI95' not in col]\n",
        "\n",
        "print(\"=== summary_scenario_stats.csv AGB columns ===\")\n",
        "print(\"(for old_growth_agb_column and current_agb_column)\\n\")\n",
        "\n",
        "# Group by category\n",
        "current_year_cols = [col for col in summary_agb_cols if col.startswith('2024 ') or col.startswith('2023 ') or col.startswith('2022 ')]\n",
        "oldgrowth_cols = [col for col in summary_agb_cols if 'oldgrowth' in col and not col.endswith('_1 forest AGB (Tg)') and not col.endswith('_2 forest AGB (Tg)')]\n",
        "no_disturbance_cols = [col for col in summary_agb_cols if 'no_disturbance' in col]\n",
        "no_degradation_cols = [col for col in summary_agb_cols if 'no_degradation' in col]\n",
        "\n",
        "print(\"Current year scenarios:\")\n",
        "for i, col in enumerate(current_year_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\nOld-growth scenarios:\")\n",
        "for i, col in enumerate(oldgrowth_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\nNo disturbance scenarios:\")\n",
        "for i, col in enumerate(no_disturbance_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"=== summary_disturbance_stats.csv AGB columns ===\")\n",
        "print(\"(for degradation/deforestation since/total columns)\\n\")\n",
        "\n",
        "# Group disturbance columns\n",
        "degradation_cols = [col for col in disturbance_agb_cols if 'degradation' in col]\n",
        "deforestation_cols = [col for col in disturbance_agb_cols if 'deforestation' in col]\n",
        "disturbance_cols = [col for col in disturbance_agb_cols if 'disturbance' in col and 'effect' not in col]\n",
        "\n",
        "print(\"Degradation columns:\")\n",
        "for i, col in enumerate(degradation_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "print(\"\\nDeforestation columns:\")\n",
        "for i, col in enumerate(deforestation_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ykformx2-nl"
      },
      "outputs": [],
      "source": [
        "# Plot degradation and deforestation separately\n",
        "separate_disturbance = True\n",
        "# Plot degradation before and since a date separately\n",
        "separate_degradation = True\n",
        "\n",
        "# DPI (default is 96, output image will scale accordingly)\n",
        "dpi = 300\n",
        "# Relative width modifier (ratio, e.g. 0.5 or 2)\n",
        "width_modifier = 0.85\n",
        "\n",
        "# Title (polygon area), density and label variables (weight of 800 ~ bold, 400 ~ normal)\n",
        "show_title = True\n",
        "show_density = True\n",
        "show_labels = True\n",
        "left_axis_label = True\n",
        "svg_transparent_background = True\n",
        "title_font_size = 20\n",
        "title_font_weight = 600\n",
        "density_font_size = 17\n",
        "density_font_weight = 600\n",
        "label_font_size = 17\n",
        "label_font_weight = 600\n",
        "\n",
        "# Base columns and year (summary_scenario_stats)\n",
        "old_growth_agb_column = '2024_oldgrowth_all_land forest AGB (Tg)'\n",
        "current_agb_column = '2024 forest AGB (Tg)'\n",
        "current_year = current_agb_column.split(' ')[0]\n",
        "\n",
        "# Disturbance columns (summary_disturbance_stats)\n",
        "degradation_since_column = '2024_degradation_since_1996 forest AGB (Tg)'\n",
        "degradation_total_column = '2024_degradation_since_oldgrowth forest AGB (Tg)'\n",
        "deforestation_total_column = '2024_deforestation_since_oldgrowth forest AGB (Tg)'\n",
        "\n",
        "# Node labels and colours\n",
        "remaining_name = f'Remaining in {current_year}:'\n",
        "remaining_colour = '#007fff'\n",
        "degradation_before_name = 'Degradation < 1996'\n",
        "degradation_before_colour = '#8dc00d'\n",
        "degradation_since_name = 'Degradation > 1996'\n",
        "degradation_since_colour = '#ffff00'\n",
        "degradation_total_name = 'Degradation'\n",
        "degradation_total_colour = '#ffff00'\n",
        "deforestation_total_name = 'Deforestation'\n",
        "deforestation_total_colour = '#ffffff'\n",
        "disturbance_total_name = 'Disturbance'\n",
        "disturbance_total_colour = '#ffffff'\n",
        "\n",
        "# Validate separation settings\n",
        "assert not separate_degradation or separate_disturbance, \"separate_disturbance must be True if separate_degradation is True.\"\n",
        "\n",
        "# Function to get values from statistics\n",
        "def get_value(df, idx, column_name):\n",
        "    try:\n",
        "        value = df.loc[idx, column_name]\n",
        "        return 0.0 if pd.isnull(value) else float(value)\n",
        "    except KeyError:\n",
        "        print(f\"Column '{column_name}' not found in the dataframe.\")\n",
        "        return 0.0\n",
        "\n",
        "# Loop through each row (polygon area)\n",
        "for idx in summary_scenario_stats.index:\n",
        "    polygon_name = summary_scenario_stats.iloc[idx, 0]\n",
        "\n",
        "    # Get old-growth and current AGB values\n",
        "    old_growth_agb = get_value(summary_scenario_stats, idx, old_growth_agb_column)\n",
        "    current_agb = get_value(summary_scenario_stats, idx, current_agb_column)\n",
        "\n",
        "    # Get disturbance values and calculate before values\n",
        "    degradation_since = get_value(summary_disturbance_stats, idx, degradation_since_column)\n",
        "    degradation_total = get_value(summary_disturbance_stats, idx, degradation_total_column)\n",
        "    degradation_before = degradation_total - degradation_since\n",
        "    deforestation_total = get_value(summary_disturbance_stats, idx, deforestation_total_column)\n",
        "    disturbance_total = get_value(summary_disturbance_stats, idx, disturbance_total_column)\n",
        "\n",
        "    # Statistical assertions\n",
        "    if separate_degradation:\n",
        "        discrepancy = abs(degradation_before + degradation_since - degradation_total)\n",
        "        if discrepancy >= 1e-6:\n",
        "            print(f\"{polygon_name}: degradation_before + degradation_since != degradation_total (discrepancy: {discrepancy:.6e})\")\n",
        "    if separate_disturbance:\n",
        "        discrepancy = abs(degradation_total + deforestation_total - disturbance_total)\n",
        "        if discrepancy >= 1e-6:\n",
        "            print(f\"{polygon_name}: degradation_total + deforestation_total != disturbance_total (discrepancy: {discrepancy:.6e})\")\n",
        "    discrepancy = abs(current_agb - disturbance_total - old_growth_agb)\n",
        "    if discrepancy >= 1e-6:\n",
        "        print(f\"{polygon_name}: current_agb - disturbance_total != old_growth_agb (discrepancy: {discrepancy:.6e})\")\n",
        "        print(\"Note: Constraining degradation floor to disturbance or capping disturbances to 0 can break equality when amalgamating across areas\")\n",
        "\n",
        "    # Load detailed stats for AGBD and CI95 values\n",
        "    stats_df = pd.read_csv(join(scenario_stats_by_area_dir, f\"{polygon_name}.csv\"))\n",
        "    old_growth_index = stats_df.index[stats_df['scenario'] == f\"{old_growth_agb_column.split(' ')[0]}\"].item()\n",
        "    current_index = stats_df.index[stats_df['scenario'] == f\"{current_agb_column.split(' ')[0]}\"].item()\n",
        "\n",
        "    old_growth_mean_agbd = get_value(stats_df, old_growth_index, \"Forest AGBD mean (Mg / ha)\")\n",
        "    current_mean_agbd = get_value(stats_df, current_index, \"Forest AGBD mean (Mg / ha)\")\n",
        "\n",
        "    uncertainty = 'Forest AGB total CI95 (Tg)' in stats_df.columns\n",
        "    if uncertainty:\n",
        "        old_growth_agb_ci95 = get_value(stats_df, old_growth_index, \"Forest AGB total CI95 (Tg)\")\n",
        "        old_growth_mean_agbd_ci95 = get_value(stats_df, old_growth_index, \"Forest AGBD CI95 (Mg / ha)\")\n",
        "        current_agb_ci95 = get_value(stats_df, current_index, \"Forest AGB total CI95 (Tg)\")\n",
        "        current_mean_agbd_ci95 = get_value(stats_df, current_index, \"Forest AGBD CI95 (Mg / ha)\")\n",
        "\n",
        "    # Build title and subtitle text\n",
        "    title_name = f\"{polygon_name}\"\n",
        "\n",
        "    if uncertainty:\n",
        "        subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} ± {old_growth_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "        subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} ± {current_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "        left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} ± {old_growth_agb_ci95:.2f} Tg\" if left_axis_label else ''\n",
        "        remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} ± {current_agb_ci95:.2f} Tg\"\n",
        "    else:\n",
        "        subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} Mg / ha\"\n",
        "        subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} Mg / ha\"\n",
        "        left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} Tg\" if left_axis_label else ''\n",
        "        remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} Tg\"\n",
        "\n",
        "    if separate_disturbance and separate_degradation:\n",
        "        nodes = [left_axis, degradation_before_name, degradation_since_name, deforestation_total_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0, 0], [1, 2, 3, 4]\n",
        "        values = [-degradation_before, -degradation_since, -deforestation_total, current_agb]\n",
        "        colors = [degradation_before_colour, degradation_since_colour, deforestation_total_colour, remaining_colour]\n",
        "\n",
        "    elif separate_disturbance and not separate_degradation:\n",
        "        nodes = [left_axis, degradation_total_name, deforestation_total_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0, 0], [1, 2, 3]\n",
        "        values = [-degradation_total, -deforestation_total, current_agb]\n",
        "        colors = [degradation_total_colour, deforestation_total_colour, remaining_colour]\n",
        "\n",
        "    else:\n",
        "        nodes = [left_axis, disturbance_total_name, remaining_name_agb]\n",
        "        sources, targets = [0, 0], [1, 2]\n",
        "        values = [-(degradation_total + deforestation_total), current_agb]\n",
        "        colors = [disturbance_total_colour, remaining_colour]\n",
        "\n",
        "    node_colors = [remaining_colour] + colors\n",
        "\n",
        "    # Add percentages to node labels\n",
        "    percentages = [(abs(val) / old_growth_agb * 100) for val in values]\n",
        "    for i in range(1, len(nodes)):\n",
        "        if i - 1 < len(percentages):\n",
        "            nodes[i] += f\" ({percentages[i-1]:.0f}%)\"\n",
        "\n",
        "    # Configure title and density annotations\n",
        "    title_and_density = [\n",
        "        dict(x=0, y=1.28, xref='paper', yref='paper', text=title_name, showarrow=False, xanchor='left', align='left',\n",
        "             font=dict(family=\"arial, sans serif\", size=title_font_size, color=\"black\", weight=title_font_weight)),\n",
        "        dict(x=0, y=1.19, xref='paper', yref='paper', text=subtitle_1_name, showarrow=False, xanchor='left', align='left',\n",
        "             font=dict(family=\"arial, sans serif\", size=density_font_size, color=\"black\", weight=density_font_weight)),\n",
        "        dict(x=0, y=1.11, xref='paper', yref='paper', text=subtitle_2_name, showarrow=False, xanchor='left', align='left',\n",
        "             font=dict(family=\"arial, sans serif\", size=density_font_size, color=\"black\", weight=density_font_weight))\n",
        "    ]\n",
        "\n",
        "    if show_title and not show_density:\n",
        "        title_and_density = title_and_density[0:1]\n",
        "    elif not show_title and show_density:\n",
        "        title_and_density = title_and_density[1:3]\n",
        "    elif not show_title and not show_density:\n",
        "        title_and_density = []\n",
        "\n",
        "    # Remove labels if toggled off\n",
        "    if not show_labels:\n",
        "        nodes = [''] * len(nodes)\n",
        "\n",
        "    # Create sankey diagram\n",
        "    fig = go.Figure(data=[go.Sankey(\n",
        "        arrangement=\"freeform\",\n",
        "        node=dict(label=nodes, color=node_colors, pad=15, thickness=20, line=dict(color=\"black\", width=1)),\n",
        "        link=dict(source=sources, target=targets, value=values, color=colors, line=dict(color=\"black\", width=1))\n",
        "    )])\n",
        "\n",
        "    fig.update_layout(\n",
        "        width=700 * width_modifier, height=500,\n",
        "        font=dict(family=\"arial, sans serif\", size=label_font_size, color=\"black\", weight=label_font_weight),\n",
        "        margin=dict(l=25, r=25, t=115, b=25),\n",
        "        annotations=title_and_density\n",
        "    )\n",
        "\n",
        "    # Save labelled versions\n",
        "    fig.write_image(join(sankey_labelled, f'sankey_diagram_{polygon_name}.png'), scale=dpi / 96)\n",
        "    if svg_transparent_background:\n",
        "        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "    fig.write_image(join(sankey_labelled_svg, f'sankey_diagram_vector_{polygon_name}.svg'), scale=dpi / 96)\n",
        "\n",
        "    # Create and save unlabelled versions\n",
        "    fig_unlabelled = go.Figure(data=[go.Sankey(\n",
        "        arrangement=\"freeform\",\n",
        "        node=dict(label=[''] * len(nodes), color=node_colors, pad=15, thickness=20, line=dict(color=\"black\", width=1)),\n",
        "        link=dict(source=sources, target=targets, value=values, color=colors, line=dict(color=\"black\", width=1))\n",
        "    )])\n",
        "\n",
        "    fig_unlabelled.update_layout(\n",
        "        width=700 * width_modifier, height=500,\n",
        "        font=dict(family=\"arial, sans serif\", size=label_font_size, color=\"black\", weight=label_font_weight),\n",
        "        margin=dict(l=25, r=25, t=115, b=25)\n",
        "    )\n",
        "\n",
        "    fig_unlabelled.write_image(join(sankey_unlabelled, f'sankey_diagram_{polygon_name}.png'), scale=dpi / 96)\n",
        "    if svg_transparent_background:\n",
        "        fig_unlabelled.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "    fig_unlabelled.write_image(join(sankey_unlabelled_svg, f'sankey_diagram_vector_{polygon_name}.svg'), scale=dpi / 96)\n",
        "\n",
        "    # Display figure with white background\n",
        "    fig.update_layout(plot_bgcolor='white', paper_bgcolor='white')\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXAGIvpk_KWS"
      },
      "source": [
        "# Disconnected runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgzXDe-Fnm3T"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "zjy-T1TqScbE",
        "wPrMgTxDr3qE",
        "TaKqkzjhRtDp",
        "M8BULeeszQan",
        "hlQ2iLAiwQoB",
        "XHEWz3QcGh1b"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}