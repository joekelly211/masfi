{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/6_scenarios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8jLRKRSfgb"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkvV803eSUQY"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEpdIjOwSiB1"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install astropy\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!pip install xgboost\n",
        "!apt-get install -y gdal-bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQDSsvGeSi4u"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "from contextlib import contextmanager\n",
        "try: import cupy # Only works on GPU runtime\n",
        "except: None\n",
        "import gc\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import json\n",
        "import math\n",
        "from os import makedirs\n",
        "from os.path import join, exists, basename\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import psutil\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "import re\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "import shutil\n",
        "from shutil import copyfile, rmtree\n",
        "import subprocess\n",
        "import time\n",
        "import xgboost as xgb\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxgT3K1Sjum"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "masks_dir = join(areas_dir, \"masks\")\n",
        "\n",
        "features_dir = join(base_dir, \"3_features\")\n",
        "feature_alpha_earth_dir = join(features_dir, \"alpha_earth\")\n",
        "feature_continuous_final_dir = join(features_dir, 'continuous_final')\n",
        "feature_binary_dir = join(features_dir, \"binary\")\n",
        "feature_edge_effects_dir = join(features_dir, \"binary_edge_effects\")\n",
        "feature_alternate_dir = join(features_dir, \"alternate\")\n",
        "feature_topo_dsm_final_dir = join(features_dir, 'topo_dsm_final')\n",
        "feature_topo_dtm_final_dir = join(features_dir, 'topo_dtm_final')\n",
        "feature_geographic_final_dir = join(features_dir, 'geographic_final')\n",
        "\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(scenarios_dir, exist_ok=True)\n",
        "\n",
        "# Define final feature directories for scenario creation\n",
        "final_features_dir_list = [\n",
        "    feature_alpha_earth_dir,\n",
        "    feature_continuous_final_dir,\n",
        "    feature_edge_effects_dir,\n",
        "    feature_alternate_dir,\n",
        "    feature_topo_dsm_final_dir,\n",
        "    feature_topo_dtm_final_dir,\n",
        "    feature_geographic_final_dir\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96FAfeyooMLz"
      },
      "outputs": [],
      "source": [
        "# Global function: Search final feature directories\n",
        "def locate_feature(feature_list, covariates_renamed):\n",
        "    result = []\n",
        "    for feature in feature_list:\n",
        "      if f\"fea_{feature}\" not in covariates_renamed:\n",
        "        for dir in final_features_dir_list:\n",
        "          if exists(join(dir, f\"{feature}.tif\")):\n",
        "            result.append(f\"{basename(dir)}/{feature}\")\n",
        "            break\n",
        "        else: raise FileNotFoundError(f\"{feature}.tif not found\")\n",
        "    return result\n",
        "\n",
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = vector = None\n",
        "    try:\n",
        "        raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "        vector = ogr.Open(polygon_path)\n",
        "        if not raster or not vector:\n",
        "            raise ValueError(\"Cannot open input files\")\n",
        "        layer = vector.GetLayer()\n",
        "        options = [\"ALL_TOUCHED=TRUE\"] if all_touched else []\n",
        "        if fixed:\n",
        "            gdal.RasterizeLayer(raster, [1], layer, burn_values=[fixed_value], options=options)\n",
        "        else:\n",
        "            attr_name = column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()\n",
        "            options.append(f\"ATTRIBUTE={attr_name}\")\n",
        "            gdal.RasterizeLayer(raster, [1], layer, options=options)\n",
        "    finally:\n",
        "        if raster: raster.FlushCache()\n",
        "        raster = vector = None\n",
        "\n",
        "# Global function: edge effects\n",
        "# Provides spatial awareness analogous to CNN receptive fields for tabular models\n",
        "# Data_type: 'binary' or 'continuous'.\n",
        "cell_size_y_path = join(areas_dir, 'cell_size_y.tif')\n",
        "cell_size_x_path = join(areas_dir, 'cell_size_x.tif')\n",
        "def edge_effects(array, data_type, cell_size_x_path, cell_size_y_path, threshold_metres):\n",
        "    # Determine pixel size from cell size rasters.\n",
        "    cell_size_x = gdal.Open(cell_size_x_path)\n",
        "    cell_size_x_array = cell_size_x.ReadAsArray()\n",
        "    cell_size_x = None\n",
        "    cell_size_y = gdal.Open(cell_size_y_path)\n",
        "    cell_size_y_array = cell_size_y.ReadAsArray()\n",
        "    cell_size_y = None\n",
        "    mean_cell_resolution = np.mean([np.mean(cell_size_x_array), np.mean(cell_size_y_array)])\n",
        "    # Maximum pixel distance for kernel extent.\n",
        "    max_pixel_distance = threshold_metres / mean_cell_resolution\n",
        "    # 2D Gaussian weight distribution follows chi-squared with df=2.\n",
        "    # Cumulative probability within radius r: P = 1 - exp(-r² / 2σ²).\n",
        "    # Solving for r at P=0.95: r = σ * sqrt(-2 * ln(0.05)) ≈ 2.45σ.\n",
        "    # Setting r = max_pixel_distance ensures 95% of kernel weight falls within threshold.\n",
        "    gaussian_stdev = max_pixel_distance / 2.45\n",
        "    kernel_radius = int(np.ceil(max_pixel_distance))\n",
        "    kernel_size = 2 * kernel_radius + 1\n",
        "    # Gaussian kernel for spatial weighting.\n",
        "    kernel = Gaussian2DKernel(x_stddev=gaussian_stdev, y_stddev=gaussian_stdev,\n",
        "                              x_size=kernel_size, y_size=kernel_size)\n",
        "    # Circular mask enforces ecological threshold as hard boundary.\n",
        "    # Square kernels would include pixels beyond threshold at corners.\n",
        "    y, x = np.ogrid[:kernel_size, :kernel_size]\n",
        "    centre = kernel_radius\n",
        "    distance_from_centre = np.sqrt((x - centre)**2 + (y - centre)**2)\n",
        "    circular_mask = distance_from_centre <= max_pixel_distance\n",
        "    # Apply mask and renormalise to sum to 1.\n",
        "    # Renormalisation ensures consistent weighting after truncation.\n",
        "    kernel_array = kernel.array.copy()\n",
        "    kernel_array[~circular_mask] = 0\n",
        "    kernel_array /= kernel_array.sum()\n",
        "    # Gaussian smoothing captures local spatial context.\n",
        "    # For binary: represents local class density within threshold.\n",
        "    # For continuous: represents local weighted mean within threshold.\n",
        "    # boundary='extend' extrapolates edge values beyond raster extent.\n",
        "    smoothed = convolve(array.astype(float), kernel_array, boundary='extend')\n",
        "    if data_type == 'continuous': return smoothed # Without rounding\n",
        "    if data_type == 'binary': smoothed = np.round(smoothed, 2) # Round\n",
        "    # Binary data: compute signed distance to class boundary.\n",
        "    # Euclidean distance transform gives centre-to-centre pixel distance.\n",
        "    dist_from_ones = distance_transform_edt(array == 0)\n",
        "    dist_from_zeros = distance_transform_edt(array == 1)\n",
        "    # Convert to distance from pixel centre to class boundary.\n",
        "    # Class boundary lies between adjacent pixels of different classes.\n",
        "    # Subtracting 0.5 pixels approximates centre-to-boundary distance.\n",
        "    # Sign encodes class membership: positive = class 1, negative = class 0.\n",
        "    # Magnitude encodes proximity to boundary (edge effects zone).\n",
        "    signed_distance = np.where(\n",
        "        array == 1,\n",
        "        np.maximum(dist_from_zeros - 0.5, 0) * mean_cell_resolution,\n",
        "        -np.maximum(dist_from_ones - 0.5, 0) * mean_cell_resolution\n",
        "    )\n",
        "    # Cap at threshold: pixels beyond are interior, not edge-influenced.\n",
        "    # Round to integer metres for cleaner feature representation.\n",
        "    signed_distance = np.round(np.clip(signed_distance, -threshold_metres, threshold_metres)).astype(np.int16)\n",
        "    return signed_distance, smoothed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_OkHaUkTCyB"
      },
      "source": [
        "# Select model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mB0XrPNYU_I"
      },
      "outputs": [],
      "source": [
        "# Select a model\n",
        "model_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file == 'model.json':\n",
        "      print(f'selected_model = \"{subdir.split(f\"{models_dir}/\",1)[1]}\"')\n",
        "      model_exists = True\n",
        "if not model_exists:\n",
        "  print(\"No model exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbXY6iP_qjYc"
      },
      "outputs": [],
      "source": [
        "selected_model = \"agbd_251203_161707\"\n",
        "\n",
        "# This must be True when using AlphaEarth features.\n",
        "# Alternate scenarios cannot be created with AlphaEarth's embeddings.\n",
        "# Mixing Alpha Earth with features of lower resolution has not been tested.\n",
        "alpha_earth = False\n",
        "\n",
        "# Set this to True for anything (e.g. elevation) with only a single prediction\n",
        "single_prediction = False\n",
        "\n",
        "# Define model directories\n",
        "selected_model_dir = join(models_dir,selected_model)\n",
        "selected_model_shap_dir = join(selected_model_dir, \"shap\")\n",
        "selected_model_json = join(selected_model_dir, \"model.json\")\n",
        "selected_model_descr_dir = join(selected_model_dir, \"model_description.json\")\n",
        "selected_model_dataset_path = join(selected_model_dir, f\"{selected_model}.pkl\")\n",
        "selected_model_dataset = pd.read_pickle(selected_model_dataset_path)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(selected_model_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_target = model_dataset_description[\"selected_target\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "selected_features = model_dataset_description[\"selected_features\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_features_mappings = model_dataset_description[\"categorical_features_mappings\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "filter_parameter = model_dataset_description[\"filter_parameter\"]\n",
        "filter_values_to_include = model_dataset_description[\"filter_values_to_include\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "covariates = [covariate[4:] for covariate in covariates_renamed]\n",
        "\n",
        "# Create scenarios model directory\n",
        "scenarios_model_dir = join(scenarios_dir, selected_model)\n",
        "makedirs(scenarios_model_dir, exist_ok=True)\n",
        "\n",
        "# Copy model_dataset_description.json\n",
        "with open(join(scenarios_model_dir, \"model_dataset_description.json\"), \"w\") as file:\n",
        "  file.write(json.dumps(model_dataset_description))\n",
        "\n",
        "# Create subdirectories\n",
        "scenario_masks_dir = join(scenarios_model_dir, \"scenario_masks\")\n",
        "tile_templates_dir = join(scenarios_model_dir, 'tile_templates')\n",
        "tile_features_dir = join(scenarios_model_dir, \"tile_features\")\n",
        "tile_scenario_masks_dir = join(scenarios_model_dir, \"tile_scenario_masks\")\n",
        "tile_feature_stacks_dir = join(scenarios_model_dir, \"tile_feature_stacks\")\n",
        "tile_prediction_cache_dir = join(scenarios_model_dir,\"tile_prediction_cache\")\n",
        "scenario_predictions_dir = join(scenarios_model_dir, \"scenario_predictions\")\n",
        "\n",
        "makedirs(scenario_masks_dir, exist_ok=True)\n",
        "makedirs(tile_templates_dir, exist_ok=True)\n",
        "makedirs(tile_features_dir, exist_ok=True)\n",
        "makedirs(tile_scenario_masks_dir, exist_ok=True)\n",
        "makedirs(tile_feature_stacks_dir, exist_ok=True)\n",
        "makedirs(tile_prediction_cache_dir, exist_ok=True)\n",
        "makedirs(scenario_predictions_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmlqEP9rFapf"
      },
      "source": [
        "# Define yearly scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJZoEWycn8-x"
      },
      "outputs": [],
      "source": [
        "# Yearly scenarios may include years after the model scenario if feature data is available later than GEDI data.\n",
        "# Date of the prediction is ~December 31st, e.g. '2024' is 31/12/2024, requiring features up to 2024.\n",
        "\n",
        "model_scenario_override = None # set if cannot be automatically determined from model features\n",
        "\n",
        "yearly_features = [\"forest\", \"disturbance\", \"alpha_earth\"]\n",
        "\n",
        "# Remove the 'fea_' prefix from each feature\n",
        "model_features = sorted([feature[4:] for feature in selected_features])\n",
        "\n",
        "# Create a list of feature years from the model's features\n",
        "model_feature_years = []\n",
        "for feature in model_features:\n",
        "  for yearly_feature in yearly_features:\n",
        "    if yearly_feature in feature:\n",
        "      model_feature_years.append(int(feature[-4:]))\n",
        "# Locate features in final feature directories\n",
        "model_features = locate_feature(model_features, covariates_renamed)\n",
        "\n",
        "# Determine the model scenario from the maximum year\n",
        "if model_scenario_override != None: model_scenario = model_scenario_override\n",
        "else: model_scenario = max(model_feature_years)\n",
        "model_scenario_filename = f\"{model_scenario}.csv\"\n",
        "model_scenario_path = join(scenarios_model_dir, model_scenario_filename)\n",
        "pd.DataFrame(model_features).to_csv(model_scenario_path, index=False)\n",
        "print(f\"The maximum year used in the model is {model_scenario}, which has been created as the first scenario.\\n\")\n",
        "print(f\"The {model_scenario} scenario feature list has been saved to:\\n {model_scenario_path}\\n\")\n",
        "\n",
        "# Available feature years\n",
        "final_feature_years_set = set()\n",
        "if alpha_earth:\n",
        "  for final_feature in os.listdir(feature_alpha_earth_dir):\n",
        "    try: final_feature_years_set.add(int(final_feature[-8:-4]))\n",
        "    except: continue\n",
        "else:\n",
        "  for final_feature in os.listdir(feature_edge_effects_dir):\n",
        "    try: final_feature_years_set.add(int(final_feature[-8:-4]))\n",
        "    except: continue\n",
        "final_feature_years = list(final_feature_years_set)\n",
        "\n",
        "# Find the first and last feature years\n",
        "first_feature_year = min(final_feature_years)\n",
        "last_feature_year = max(final_feature_years)\n",
        "additional_feature_years = last_feature_year - model_scenario\n",
        "print(f\"The first available feature year is {first_feature_year} and the last is {last_feature_year}.\\n\")\n",
        "\n",
        "# Calculate the range of scenario years and minimum scenario year\n",
        "model_scenario_year_range = max(model_feature_years) - (min(model_feature_years))\n",
        "minimum_yearly_scenario = first_feature_year + model_scenario_year_range\n",
        "print(f\"The earliest scenario year that can be predicted is {minimum_yearly_scenario}.\")\n",
        "print(f\"The latest scenario year that can be predicted is {last_feature_year}.\")\n",
        "print(f\"This is based on the number of yearly features used to train the model and the total availability of features.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPd_nrFYeGJ3"
      },
      "outputs": [],
      "source": [
        "# Select static features which are the same in every scenario, e.g. topography\n",
        "print(\"static_features = [\")\n",
        "for feature in model_features:\n",
        "  if feature not in covariates_renamed:\n",
        "    print(f'  \"{feature}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARVJed3vTgBv"
      },
      "outputs": [],
      "source": [
        "# # GEDI elevation\n",
        "\n",
        "# static_features = [\n",
        "#   \"coast_proximity_km\",\n",
        "#   \"latitude\",\n",
        "#   \"longitude\",\n",
        "#   \"topo_dsm_smooth_aspect_cosine\",\n",
        "#   \"topo_dsm_smooth_aspect_sine\",\n",
        "#   \"topo_dsm_smooth_circular_variance_aspect_03\",\n",
        "#   \"topo_dsm_smooth_circular_variance_aspect_07\",\n",
        "#   \"topo_dsm_smooth_circular_variance_aspect_11\",\n",
        "#   \"topo_dsm_smooth_deviation_mean_elevation_03\",\n",
        "#   \"topo_dsm_smooth_deviation_mean_elevation_07\",\n",
        "#   \"topo_dsm_smooth_deviation_mean_elevation_11\",\n",
        "#   \"topo_dsm_smooth_eastness\",\n",
        "#   \"topo_dsm_smooth_elevation\",\n",
        "#   \"topo_dsm_smooth_northness\",\n",
        "#   \"topo_dsm_smooth_profile_curvature\",\n",
        "#   \"topo_dsm_smooth_roughness_03\",\n",
        "#   \"topo_dsm_smooth_roughness_07\",\n",
        "#   \"topo_dsm_smooth_roughness_11\",\n",
        "#   \"topo_dsm_smooth_slope\",\n",
        "#   \"topo_dsm_smooth_stream_power_index_log10\",\n",
        "#   \"topo_dsm_smooth_surface_area_ratio\",\n",
        "#   \"topo_dsm_smooth_tangential_curvature\",\n",
        "#   \"topo_dsm_smooth_topographic_position_index_03\",\n",
        "#   \"topo_dsm_smooth_topographic_position_index_07\",\n",
        "#   \"topo_dsm_smooth_topographic_position_index_11\",\n",
        "#   \"topo_dsm_smooth_topographic_ruggedness_index\",\n",
        "#   \"topo_dsm_smooth_topographic_wetness_index\",\n",
        "#   \"topo_dsm_unsmooth_aspect_cosine\",\n",
        "#   \"topo_dsm_unsmooth_aspect_sine\",\n",
        "#   \"topo_dsm_unsmooth_circular_variance_aspect_03\",\n",
        "#   \"topo_dsm_unsmooth_circular_variance_aspect_07\",\n",
        "#   \"topo_dsm_unsmooth_circular_variance_aspect_11\",\n",
        "#   \"topo_dsm_unsmooth_deviation_mean_elevation_03\",\n",
        "#   \"topo_dsm_unsmooth_deviation_mean_elevation_07\",\n",
        "#   \"topo_dsm_unsmooth_deviation_mean_elevation_11\",\n",
        "#   \"topo_dsm_unsmooth_eastness\",\n",
        "#   \"topo_dsm_unsmooth_elevation\",\n",
        "#   \"topo_dsm_unsmooth_northness\",\n",
        "#   \"topo_dsm_unsmooth_profile_curvature\",\n",
        "#   \"topo_dsm_unsmooth_roughness_03\",\n",
        "#   \"topo_dsm_unsmooth_roughness_07\",\n",
        "#   \"topo_dsm_unsmooth_roughness_11\",\n",
        "#   \"topo_dsm_unsmooth_slope\",\n",
        "#   \"topo_dsm_unsmooth_stream_power_index_log10\",\n",
        "#   \"topo_dsm_unsmooth_surface_area_ratio\",\n",
        "#   \"topo_dsm_unsmooth_tangential_curvature\",\n",
        "#   \"topo_dsm_unsmooth_topographic_position_index_03\",\n",
        "#   \"topo_dsm_unsmooth_topographic_position_index_07\",\n",
        "#   \"topo_dsm_unsmooth_topographic_position_index_11\",\n",
        "#   \"topo_dsm_unsmooth_topographic_ruggedness_index\",\n",
        "#   \"topo_dsm_unsmooth_topographic_wetness_index\",\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djq-5Tvpaapn"
      },
      "outputs": [],
      "source": [
        "static_features = [\n",
        "  \"coast_proximity_km\",\n",
        "  \"latitude\",\n",
        "  \"longitude\",\n",
        "  \"topo_dtm_smooth_aspect_cosine\",\n",
        "  \"topo_dtm_smooth_aspect_sine\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_03\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_07\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_11\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_03\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_07\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_11\",\n",
        "  \"topo_dtm_smooth_eastness\",\n",
        "  \"topo_dtm_smooth_elevation\",\n",
        "  \"topo_dtm_smooth_northness\",\n",
        "  \"topo_dtm_smooth_profile_curvature\",\n",
        "  \"topo_dtm_smooth_roughness_03\",\n",
        "  \"topo_dtm_smooth_roughness_07\",\n",
        "  \"topo_dtm_smooth_roughness_11\",\n",
        "  \"topo_dtm_smooth_slope\",\n",
        "  \"topo_dtm_smooth_stream_power_index_log10\",\n",
        "  \"topo_dtm_smooth_surface_area_ratio\",\n",
        "  \"topo_dtm_smooth_tangential_curvature\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_03\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_07\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_11\",\n",
        "  \"topo_dtm_smooth_topographic_ruggedness_index\",\n",
        "  \"topo_dtm_smooth_topographic_wetness_index\",\n",
        "  \"topo_dtm_unsmooth_aspect_cosine\",\n",
        "  \"topo_dtm_unsmooth_aspect_sine\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_03\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_07\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_11\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_03\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_07\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_11\",\n",
        "  \"topo_dtm_unsmooth_eastness\",\n",
        "  \"topo_dtm_unsmooth_elevation\",\n",
        "  \"topo_dtm_unsmooth_northness\",\n",
        "  \"topo_dtm_unsmooth_profile_curvature\",\n",
        "  \"topo_dtm_unsmooth_roughness_03\",\n",
        "  \"topo_dtm_unsmooth_roughness_07\",\n",
        "  \"topo_dtm_unsmooth_roughness_11\",\n",
        "  \"topo_dtm_unsmooth_slope\",\n",
        "  \"topo_dtm_unsmooth_stream_power_index_log10\",\n",
        "  \"topo_dtm_unsmooth_surface_area_ratio\",\n",
        "  \"topo_dtm_unsmooth_tangential_curvature\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_03\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_07\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_11\",\n",
        "  \"topo_dtm_unsmooth_topographic_ruggedness_index\",\n",
        "  \"topo_dtm_unsmooth_topographic_wetness_index\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg4XvxT5foVR"
      },
      "outputs": [],
      "source": [
        "# Note that areas that were forested in a historic yearly scenario but were reservoirs\n",
        "# or flooded at the time topographic feature data was collected (~2014 for GLO-30 DEM)\n",
        "# will be predicted based on a flat topography at the elevation of the water's surface.\n",
        "\n",
        "# Set scenario features as all non-static features\n",
        "scenario_features = sorted(list(set([f.split('/')[-1] for f in model_features]) - set(static_features)))\n",
        "\n",
        "# Create feature lists for all possible yearly scenarios\n",
        "for yearly_scenario in range(minimum_yearly_scenario, last_feature_year +1):\n",
        "  year_difference = model_scenario - yearly_scenario\n",
        "  yearly_scenario_features = []\n",
        "  for scenario_feature in scenario_features:\n",
        "    try:\n",
        "      year_change = int(scenario_feature[-4:]) - year_difference\n",
        "      yearly_scenario_feature = scenario_feature[:-4] + str(year_change)\n",
        "      yearly_scenario_features.append(yearly_scenario_feature)\n",
        "    except: yearly_scenario_features.append(scenario_feature)\n",
        "  # Compile yearly features\n",
        "  yearly_scenario_features = sorted(yearly_scenario_features + static_features)\n",
        "  # Locate features in final feature directories\n",
        "  yearly_scenario_features = locate_feature(yearly_scenario_features, covariates_renamed)\n",
        "  yearly_scenario_filename = f\"{yearly_scenario}.csv\"\n",
        "  yearly_scenario_dir = join(scenarios_model_dir,yearly_scenario_filename)\n",
        "  pd.DataFrame(yearly_scenario_features).to_csv(yearly_scenario_dir, index=False)\n",
        "\n",
        "# Open the most recent yearly scenario feature list\n",
        "most_recent_scenario_csv = join(scenarios_model_dir,f\"{last_feature_year}.csv\")\n",
        "most_recent_scenario_features = pd.Series.tolist(pd.read_csv(most_recent_scenario_csv).iloc[:,0])\n",
        "\n",
        "print(f\"Lists of features for all possible yearly scenarios have been exported to {scenarios_model_dir}/.\")\n",
        "print(f\"Ensure all features in these lists have been copied to:\\n{features_dir}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3w7svaUvs07"
      },
      "source": [
        "# Define alternate scenarios (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmcpUriAFfZ6"
      },
      "source": [
        "## Disturbance requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWA1QsHGQw_P"
      },
      "outputs": [],
      "source": [
        "# Tool to identify required scenarios for forest AGBD disturbance mapping\n",
        "# Outputs: (deforestation, degradation) for oldgrowth/area-based, degradation only otherwise\n",
        "# Decomposition uses nodata masking\n",
        "\n",
        "use_tool = False\n",
        "\n",
        "def select_disturbance_scenarios():\n",
        "    # Header\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"\\nDISTURBANCE SCENARIO SELECTOR\\n\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Methodology explainer\n",
        "    print(\"AGBD loss from forest disturbance calculated by comparing scenarios.\")\n",
        "    print(\"Degradation: AGBD loss in pixels that remain forest.\")\n",
        "    print(\"Deforestation: AGBD loss from forest-to-nonforest transitions.\")\n",
        "    print(\"Total disturbance: deforestation + degradation.\\n\")\n",
        "    print(\"Deforestation and total disturbance only available for:\")\n",
        "    print(\"  - Oldgrowth baseline (cumulative loss since intact forest)\")\n",
        "    print(\"  - Area-based scenarios (forecast loss from planned land use change)\")\n",
        "    print(\"Year-based counterfactuals output the effects of degradation only.\\n\")\n",
        "\n",
        "    # Year of interest\n",
        "    year = input(\"Enter year of interest: \")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Calculation type selection\n",
        "    calc_prompt = (\n",
        "        \"Select the effect of disturbance to calculate:\\n\"\n",
        "        \"1. Since an oldgrowth state\\n\"\n",
        "        \"2. Since a baseline year\\n\"\n",
        "        \"3. In a multi-year interval\\n\"\n",
        "        \"4. In a single year\\n\"\n",
        "        \"5. In an area (polygon)\\n\\n\"\n",
        "        \"Enter your choice (1-5): \")\n",
        "    calc_type = input(calc_prompt)\n",
        "\n",
        "    # Initialise outputs\n",
        "    scenario_pair = None\n",
        "    scenarios_required = []\n",
        "    output_names = None\n",
        "    output_description = None\n",
        "\n",
        "    # Option 1: Cumulative since oldgrowth\n",
        "    # Uses disturbance_since_dictionary\n",
        "    # Compares actual AGBD against oldgrowth counterfactual\n",
        "    if calc_type == \"1\":\n",
        "        actual = year\n",
        "        counterfactual = f\"{year}_no_disturbance_since_oldgrowth\"\n",
        "        scenario_pair = (actual, counterfactual)\n",
        "        scenarios_required = [actual, counterfactual]\n",
        "        output_names = (\n",
        "            f\"{year}_deforestation_since_oldgrowth\",\n",
        "            f\"{year}_degradation_since_oldgrowth\")\n",
        "        output_description = (\n",
        "            f\"AGBD loss in {year} from cumulative disturbance since intact forest.\\n\"\n",
        "            f\"Comparison: actual {year} vs modelled {year} if no disturbance ever occurred.\")\n",
        "\n",
        "    # Option 2: Cumulative since baseline year\n",
        "    # Uses disturbance_since_dictionary\n",
        "    # Compares actual AGBD against no_disturbance counterfactual\n",
        "    # Degradation only\n",
        "    elif calc_type == \"2\":\n",
        "        print(\"\\n\")\n",
        "        baseline_year = input(\"Enter baseline year (disturbance occurring after this year will be measured): \")\n",
        "        if int(baseline_year) >= int(year):\n",
        "            print(\"\\nError: Baseline year must be before year of interest\")\n",
        "            return None\n",
        "        since_year = str(int(baseline_year) + 1)\n",
        "        actual = year\n",
        "        counterfactual = f\"{year}_no_disturbance_since_{since_year}\"\n",
        "        scenario_pair = (actual, counterfactual)\n",
        "        scenarios_required = [actual, counterfactual]\n",
        "        output_names = f\"{year}_degradation_since_{since_year}\"\n",
        "        output_description = (\n",
        "            f\"AGBD loss in {year} from degradation events since {baseline_year}.\\n\"\n",
        "            f\"Comparison: actual {year} vs modelled {year} if no disturbance since {since_year}.\\n\"\n",
        "            f\"Degradation only: deforestation requires oldgrowth baseline.\")\n",
        "\n",
        "    # Option 3: Multi-year interval\n",
        "    # Uses degradation_interval_dictionary\n",
        "    # Compares two counterfactuals to isolate interval effect\n",
        "    # Degradation only\n",
        "    elif calc_type == \"3\":\n",
        "        print(\"\\n\")\n",
        "        interval_prompt = (\n",
        "            \"Select interval baseline:\\n\"\n",
        "            \"1. From oldgrowth\\n\"\n",
        "            \"2. From a specific year\\n\\n\"\n",
        "            \"Enter your choice (1-2): \")\n",
        "        interval_type = input(interval_prompt)\n",
        "        print(\"\\n\")\n",
        "        end_year = input(\"Enter interval end year: \")\n",
        "        if int(end_year) >= int(year):\n",
        "            print(\"\\nError: Interval end year must be before year of interest\")\n",
        "            return None\n",
        "        end_since = str(int(end_year) + 1)\n",
        "        cf_recent = f\"{year}_no_disturbance_since_{end_since}\"\n",
        "        if interval_type == \"1\":\n",
        "            cf_baseline = f\"{year}_no_disturbance_since_oldgrowth\"\n",
        "            scenario_pair = (cf_recent, cf_baseline)\n",
        "            scenarios_required = [cf_recent, cf_baseline]\n",
        "            output_names = f\"{year}_degradation_from_oldgrowth_to_{end_year}\"\n",
        "            output_description = (\n",
        "                f\"AGBD loss in {year} from degradation events between oldgrowth and {end_year}.\\n\"\n",
        "                f\"Comparison: modelled {year} (no disturbance since {end_since}) vs modelled {year} (oldgrowth).\\n\"\n",
        "                f\"Isolates historical degradation; excludes events after {end_year}.\")\n",
        "        else:\n",
        "            start_year = input(\"Enter interval start year: \")\n",
        "            if int(start_year) >= int(end_year):\n",
        "                print(\"\\nError: Start year must be before end year\")\n",
        "                return None\n",
        "            cf_baseline = f\"{year}_no_disturbance_since_{start_year}\"\n",
        "            scenario_pair = (cf_recent, cf_baseline)\n",
        "            scenarios_required = [cf_recent, cf_baseline]\n",
        "            output_names = f\"{year}_degradation_from_{start_year}_to_{end_year}\"\n",
        "            output_description = (\n",
        "                f\"AGBD loss in {year} from degradation events between {start_year} and {end_year}.\\n\"\n",
        "                f\"Comparison: modelled {year} (no disturbance since {end_since}) vs modelled {year} (no disturbance since {start_year}).\\n\"\n",
        "                f\"Isolates interval effect; excludes events before {start_year} and after {end_year}.\")\n",
        "\n",
        "    # Option 4: Single year effect\n",
        "    # Uses degradation_single_year_dictionary\n",
        "    # Consecutive counterfactuals isolate effect of one year\n",
        "    # Same-year case compares actual against no_disturbance_since_year\n",
        "    # Degradation only\n",
        "    elif calc_type == \"4\":\n",
        "        print(\"\\n\")\n",
        "        effect_year = input(\"Enter year whose effect to measure: \")\n",
        "        if int(effect_year) > int(year):\n",
        "            print(\"\\nError: Effect year cannot be after year of interest\")\n",
        "            return None\n",
        "        if effect_year == year:\n",
        "            actual = year\n",
        "            counterfactual = f\"{year}_no_disturbance_since_{year}\"\n",
        "            scenario_pair = (actual, counterfactual)\n",
        "            scenarios_required = [actual, counterfactual]\n",
        "            output_description = (\n",
        "                f\"AGBD loss in {year} from degradation events in {effect_year}.\\n\"\n",
        "                f\"Comparison: actual {year} vs modelled {year} (no disturbance since {year}).\\n\"\n",
        "                f\"Isolates same-year degradation effect.\")\n",
        "        else:\n",
        "            effect_year_plus1 = str(int(effect_year) + 1)\n",
        "            cf_after = f\"{year}_no_disturbance_since_{effect_year_plus1}\"\n",
        "            cf_before = f\"{year}_no_disturbance_since_{effect_year}\"\n",
        "            scenario_pair = (cf_after, cf_before)\n",
        "            scenarios_required = [cf_after, cf_before]\n",
        "            output_description = (\n",
        "                f\"AGBD loss in {year} from degradation events in {effect_year}.\\n\"\n",
        "                f\"Comparison: modelled {year} (no disturbance since {effect_year_plus1}) vs modelled {year} (no disturbance since {effect_year}).\\n\"\n",
        "                f\"Isolates single-year effect using consecutive counterfactuals.\")\n",
        "        output_names = f\"{year}_effect_of_degradation_in_{effect_year}\"\n",
        "\n",
        "    # Option 5: Area-based (polygon)\n",
        "    # Uses disturbance_area_dictionary\n",
        "    # Compares polygon-based alternate scenario against actual\n",
        "    elif calc_type == \"5\":\n",
        "        print(\"\\n\")\n",
        "        polygon_name = input(\"Enter polygon name (from .gpkg filename, without extension): \")\n",
        "        print(\"\\n\")\n",
        "        area_type_prompt = (\n",
        "            \"Select polygon disturbance type:\\n\"\n",
        "            \"1. Deforestation (with degradation buffer)\\n\"\n",
        "            \"2. Degradation\\n\\n\"\n",
        "            \"Enter your choice (1-2): \")\n",
        "        area_type = input(area_type_prompt)\n",
        "        print(\"\\n\")\n",
        "        year_affix = input(\"Enter year affix for polygon scenario: \")\n",
        "        if area_type == \"1\":\n",
        "            print(\"\\n\")\n",
        "            buffer_size = input(\"Enter buffer size in metres: \")\n",
        "            scenario_name = f\"{year}_{polygon_name}_deforestation_{year_affix}_{buffer_size}m_degradation_buffer\"\n",
        "            output_base = f\"{year}_deforestation_of_{polygon_name}_{year_affix}\"\n",
        "            output_description = (\n",
        "                f\"Forecast AGBD loss in {year} from deforestation of {polygon_name} ({year_affix}).\\n\"\n",
        "                f\"Comparison: modelled {year} (with deforestation + {buffer_size}m degradation buffer) vs actual {year}.\\n\"\n",
        "                f\"Deforestation: loss in cleared area. Degradation: loss in buffer zone.\")\n",
        "        else:\n",
        "            scenario_name = f\"{year}_{polygon_name}_degradation_{year_affix}\"\n",
        "            output_base = f\"{year}_degradation_of_{polygon_name}_{year_affix}\"\n",
        "            output_description = (\n",
        "                f\"Forecast AGBD loss in {year} from degradation of {polygon_name} ({year_affix}).\\n\"\n",
        "                f\"Comparison: modelled {year} (with degradation scenario) vs actual {year}.\")\n",
        "        scenario_pair = (scenario_name, year)\n",
        "        scenarios_required = [scenario_name, year]\n",
        "        output_names = (\n",
        "            f\"{output_base}_deforestation\",\n",
        "            f\"{output_base}_degradation\")\n",
        "    else:\n",
        "        print(\"\\nError: Invalid selection\")\n",
        "        return None\n",
        "\n",
        "    # Display results\n",
        "    result_text = []\n",
        "    result_text.append(\"\\n\" + \"=\"*50)\n",
        "    result_text.append(\"\\n\" + output_description)\n",
        "    result_text.append(\"\\nOutput rasters:\")\n",
        "    if isinstance(output_names, tuple):\n",
        "        for name in output_names: result_text.append(f\"  {name}\")\n",
        "    else:\n",
        "        result_text.append(f\"  {output_names}\")\n",
        "    result_text.append(f\"\\nScenarios required:\")\n",
        "    for s in scenarios_required: result_text.append(f\"  {s}\")\n",
        "    result_text.append(\"\\n\" + \"=\"*50)\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"\\n\".join(result_text))\n",
        "    return output_names, scenario_pair, scenarios_required\n",
        "\n",
        "\n",
        "# Run tool\n",
        "if use_tool:\n",
        "    if alpha_earth: print(\"Alternate scenarios cannot be created with Alpha Earth features.\")\n",
        "    else: select_disturbance_scenarios()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN8AwXXAbxAn"
      },
      "source": [
        "## No disturbance since..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQmk9oahbwd-"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios remove both degradation and deforestation for specific time ranges.\n",
        "# They are used to calculate the effect of degradation and deforestation on forest AGBD.\n",
        "# It does not attempt to model loss of forest to water (e.g. reservoirs), as these often pre-date\n",
        "# available satellite data (especially topographic), and there is no management application,\n",
        "# i.e. it's unlikely the reservoir will ever be restored to forest.\n",
        "\n",
        "# Define ranges for 'no disturbance' scenarios\n",
        "define_no_disturbance_scenarios = True\n",
        "\n",
        "# No disturbance ranges as tuples of (start_year, end_year)\n",
        "no_disturbance_ranges = [\n",
        "    (1993, 2021),\n",
        "    (1996, 2024),\n",
        "    (1997, 2024),\n",
        "    (1998, 2024),\n",
        "    (1999, 2024),\n",
        "    (2000, 2024),\n",
        "    (2001, 2024),\n",
        "    (2002, 2024),\n",
        "    (2003, 2024),\n",
        "    (2004, 2024),\n",
        "    (2005, 2024),\n",
        "    (2006, 2024),\n",
        "    (2007, 2024),\n",
        "    (2008, 2024),\n",
        "    (2009, 2024),\n",
        "    (2010, 2024),\n",
        "    (2011, 2024),\n",
        "    (2012, 2024),\n",
        "    (2013, 2024),\n",
        "    (2014, 2024),\n",
        "    (2015, 2024),\n",
        "    (2016, 2024),\n",
        "    (2017, 2024),\n",
        "    (2018, 2024),\n",
        "    (2019, 2024),\n",
        "    (2020, 2024),\n",
        "    (2021, 2024),\n",
        "    (2022, 2024),\n",
        "    (2023, 2024),\n",
        "    (2024, 2024),\n",
        "]\n",
        "\n",
        "# Create a feature list for 'no disturbance' scenarios\n",
        "if define_no_disturbance_scenarios:\n",
        "  if alpha_earth: print(\"Alternate scenarios cannot be created with Alpha Earth features.\")\n",
        "  else:\n",
        "    for start_year, end_year in no_disturbance_ranges:\n",
        "      assert end_year <= last_feature_year, \"End years must be at or before the last feature year.\"\n",
        "      assert end_year >= minimum_yearly_scenario, \"End years must be at or after the minimum yearly scenario.\"\n",
        "      assert start_year >= first_feature_year, \"Start years must be at or after the first feature year.\"\n",
        "      assert start_year >= end_year - model_scenario_year_range, \"Start years must be within the model scenario range of the end year.\"\n",
        "      assert start_year <= end_year, \"The start year must less than or equal to the end year.\"\n",
        "\n",
        "      # Determine base features based on the end year of the range\n",
        "      scenario_features_csv = join(scenarios_model_dir, f\"{end_year}.csv\")\n",
        "      base_features = [f.split('/')[-1] for f in pd.read_csv(scenario_features_csv).iloc[:,0]]\n",
        "\n",
        "      no_disturbance_features = []\n",
        "      for scenario_feature in base_features:\n",
        "        if \"disturbance_edge_distance\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          # Replace disturbance with the minimum (negative) edge distance\n",
        "          if scenario_feature_year >= start_year: no_disturbance_features.append(\"minimum_edge_distance\")\n",
        "          else: no_disturbance_features.append(scenario_feature)\n",
        "        elif \"disturbance_local_density\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          # Replace disturbance with the minimum (negative) edge distance\n",
        "          if scenario_feature_year >= start_year: no_disturbance_features.append(\"minimum_local_density\")\n",
        "          else: no_disturbance_features.append(scenario_feature)\n",
        "        elif \"forest_edge_distance\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          forest_year = start_year - 1\n",
        "          alternate_forest = f\"forest_edge_distance_{forest_year}\"\n",
        "          if scenario_feature_year > forest_year: no_disturbance_features.append(alternate_forest)\n",
        "          else: no_disturbance_features.append(scenario_feature)\n",
        "        elif \"forest_local_density\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          forest_year = start_year - 1\n",
        "          alternate_forest = f\"forest_local_density_{forest_year}\"\n",
        "          if scenario_feature_year > forest_year: no_disturbance_features.append(alternate_forest)\n",
        "          else: no_disturbance_features.append(scenario_feature)\n",
        "        else: no_disturbance_features.append(scenario_feature)\n",
        "      # Locate features in final feature directories\n",
        "      no_disturbance_features = locate_feature(no_disturbance_features, covariates_renamed)\n",
        "\n",
        "      no_disturbance_scenario_filename = f\"{end_year}_no_disturbance_since_{start_year}.csv\"\n",
        "      no_disturbance_scenario_path = join(scenarios_model_dir, no_disturbance_scenario_filename)\n",
        "      pd.DataFrame(no_disturbance_features).to_csv(no_disturbance_scenario_path, index=False)\n",
        "      print(f\"Feature list for a scenario without disturbance between {start_year} and {end_year} exported to {no_disturbance_scenario_filename}.\")\n",
        "else:\n",
        "  print(\"The 'no disturbance' scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcZt7tm97Pkx"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios simulate old-growth forest using a proxy area specified by the user.\n",
        "# Forest extent (i.e. 'no deforestation') can be set from yearly feature, or all historic / potential forest area.\n",
        "# A second version of the scenario without the oldgrowth proxy will be created for comparison.\n",
        "# In rare cases, areas with unexpectedly high AGBD will have a lower AGBD estimate with the oldgrowth proxy,\n",
        "# and the highest estimate will be used for that pixel.\n",
        "\n",
        "define_no_disturbance_since_oldgrowth = True\n",
        "\n",
        "# List of land-use base feature names. All but one are 'redundant'.\n",
        "# One should be selected as 'oldgrowth_feature'.\n",
        "# Both _edge_distance and _local_density variants will be modified.\n",
        "print(\"oldgrowth_redundant_features = [\")\n",
        "seen_bases = set()\n",
        "for feature in model_features:\n",
        "    if \"lu_\" in feature:\n",
        "        if \"_edge_distance\" in feature: base_name = feature.replace(\"_edge_distance\", \"\")\n",
        "        elif \"_local_density\" in feature: base_name = feature.replace(\"_local_density\", \"\")\n",
        "        else: continue\n",
        "        if base_name not in seen_bases:\n",
        "            seen_bases.add(base_name)\n",
        "            print(f'  \"{base_name}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tkzW8mKBKxH"
      },
      "outputs": [],
      "source": [
        "most_recent_scenario_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3XroFkc7Niz"
      },
      "outputs": [],
      "source": [
        "# Base name of the feature that best indicates oldgrowth to the model.\n",
        "# Both _edge_distance and _local_density variants will be modified.\n",
        "oldgrowth_feature = 'lu_old-growth_protected_areas'\n",
        "\n",
        "# Base names of features that may confound the old-growth proxy.\n",
        "# Both _edge_distance and _local_density variants will be removed for the old-growth scenarios.\n",
        "oldgrowth_redundant_features = [\n",
        "    \"lu_ais\",\n",
        "    \"lu_berkelah_jerantut\",\n",
        "    \"lu_berkelah_kuantan\",\n",
        "    \"lu_berkelah_temerloh\",\n",
        "    \"lu_old-growth_protected_areas\",\n",
        "    \"lu_remen_chereh\",\n",
        "    \"lu_tekai_tembeling\",\n",
        "    \"lu_tekam\",\n",
        "    \"lu_yong_lipis\",\n",
        "    \"lu_yong\",\n",
        "]\n",
        "\n",
        "# Should be set to the year(s) of interest\n",
        "# Given fluctuations in rivers or lake extents etc.\n",
        "oldgrowth_land_years = [\n",
        "    2021,\n",
        "    last_feature_year\n",
        "]\n",
        "\n",
        "for year in oldgrowth_land_years:\n",
        "  assert year in final_feature_years, \"Years in 'oldgrowth_land_years' must be available in the final yearly features.\"\n",
        "\n",
        "# This is the prefix in the binary edge effects directory for the maximum forest extent\n",
        "# By default it is 'land'(according to TMF data), assuming all land in the study area\n",
        "# was once forest. Upload a different extent to binary rasters if this was not the case,\n",
        "# then run the edge effects section and change this prefix.\n",
        "oldgrowth_forest_extent = \"land\"\n",
        "\n",
        "\n",
        "most_recent_scenario_features = [f.split('/')[-1] for f in most_recent_scenario_features]\n",
        "if define_no_disturbance_since_oldgrowth:\n",
        "  if alpha_earth: print(\"Alternate scenarios cannot be created with Alpha Earth features.\")\n",
        "  else:\n",
        "      for year in oldgrowth_land_years:\n",
        "        no_disturbance_since_oldgrowth_name = f\"{year}_no_disturbance_since_oldgrowth\"\n",
        "        oldgrowth_all_features_1 = []\n",
        "        oldgrowth_all_features_2 = []\n",
        "        for scenario_feature in most_recent_scenario_features:\n",
        "          if \"disturbance_edge_distance\" in scenario_feature:\n",
        "            feature_1 = feature_2 = \"minimum_edge_distance\"\n",
        "          elif \"disturbance_local_density\" in scenario_feature:\n",
        "            feature_1 = feature_2 = \"minimum_local_density\"\n",
        "          elif \"forest_edge_distance\" in scenario_feature:\n",
        "            feature_1 = feature_2 = f\"{oldgrowth_forest_extent}_edge_distance_{year}\"\n",
        "          elif \"forest_local_density\" in scenario_feature:\n",
        "            feature_1 = feature_2 = f\"{oldgrowth_forest_extent}_local_density_{year}\"\n",
        "          elif scenario_feature.startswith(oldgrowth_feature):\n",
        "            if 'edge_distance' in scenario_feature: feature_1 = \"maximum_edge_distance\" # LU interior max\n",
        "            if 'local_density' in scenario_feature: feature_1 = \"maximum_local_density\"\n",
        "            feature_2 = scenario_feature\n",
        "          elif any(scenario_feature.startswith(rf) for rf in oldgrowth_redundant_features):\n",
        "            if 'edge_distance' in scenario_feature: feature_1 = \"minimum_edge_distance\" # LU exterior min (neg)\n",
        "            if 'local_density' in scenario_feature: feature_1 = \"minimum_local_density\"\n",
        "            feature_2 = scenario_feature\n",
        "          else: feature_1 = feature_2 = scenario_feature\n",
        "          oldgrowth_all_features_1.append(feature_1)\n",
        "          oldgrowth_all_features_2.append(feature_2)\n",
        "        # Locate features in final feature directories\n",
        "        oldgrowth_all_features_1 = locate_feature(oldgrowth_all_features_1, covariates_renamed)\n",
        "        oldgrowth_all_features_2 = locate_feature(oldgrowth_all_features_2, covariates_renamed)\n",
        "\n",
        "        # Compare feature lists and save appropriate CSVs.\n",
        "        if oldgrowth_all_features_1 == oldgrowth_all_features_2:\n",
        "          filename = f\"{no_disturbance_since_oldgrowth_name}_1.csv\"\n",
        "          pd.DataFrame(oldgrowth_all_features_1).to_csv(join(scenarios_model_dir, filename), index=False)\n",
        "          print(f\"Feature lists were identical, only saved {filename}\")\n",
        "        else:\n",
        "          for suffix, features in [(\"1\", oldgrowth_all_features_1), (\"2\", oldgrowth_all_features_2)]:\n",
        "            filename = f\"{no_disturbance_since_oldgrowth_name}_{suffix}.csv\"\n",
        "            pd.DataFrame(features).to_csv(join(scenarios_model_dir, filename), index=False)\n",
        "          print(f\"Feature lists for {no_disturbance_since_oldgrowth_name} have been exported to {no_disturbance_since_oldgrowth_name}_1.csv and {no_disturbance_since_oldgrowth_name}_2.csv.\\n\")\n",
        "\n",
        "else: print(\"Old-growth scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9w2IzcgVNvx"
      },
      "source": [
        "## Area-based disturbance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQRgPdbiVNVF"
      },
      "outputs": [],
      "source": [
        "# Use polygons to select areas for alternate scenarios of area-based disturbance\n",
        "define_area_based_disturbance = True\n",
        "\n",
        "# Set the edge effect distance for the alternate scenario features\n",
        "# This should match the distance used in '3_features_lcluc.ipynb'.\n",
        "edge_effect_distance = 120\n",
        "\n",
        "if define_area_based_disturbance:\n",
        "  if alpha_earth: print(\"Alternate scenarios cannot be created with Alpha Earth features.\")\n",
        "  else:\n",
        "    # Exclude existing polygons from search\n",
        "    polygons_to_exclude = ['project_area.gpkg', 'project_area_buffered_bbox.gpkg', 'gedi_area.gpkg', 'template.gpkg']\n",
        "    print(\"# Modify this dictionary by:\")\n",
        "    print(\"# 1) Commenting out any polygons not being used for disturbance.\")\n",
        "    print(\"# 2) Changing the type from 'deforestation' to 'degradation' if necessary.\")\n",
        "    print(\"# 3) Changing the tuple years from 'range' to 'discrete' to specify individual years.\")\n",
        "    print(\"# 4) If years are discrete, add one or more. If a range, add the start and end year.\")\n",
        "    print(\"# 5) Changing the alternate scenario year for each area if needed.\")\n",
        "    print(\"# 6) Copy and paste lines for multiple scenarios with the same area (different disturbance types, different years).\")\n",
        "    print(\"# Remember to ensure all keys are unique - if you copy and paste, manually change the keys.\\n\")\n",
        "\n",
        "    # Exclude existing polygons from search\n",
        "    polygons_to_exclude = ['project_area.gpkg', 'gedi_area.gpkg', 'template.gpkg']\n",
        "    exclude_lu_polygons = False\n",
        "\n",
        "    print(\"disturbance_polygons = {\")\n",
        "    index = 1\n",
        "    first_disturbance_year = last_feature_year - model_scenario_year_range\n",
        "    for polygon in sorted(os.listdir(polygons_dir)):\n",
        "      if polygon not in polygons_to_exclude and 'inverse' not in polygon and 'buffered' not in polygon:\n",
        "        if not exclude_lu_polygons:\n",
        "          print(f\"    {index}: ['{polygon[:-5]}', 'deforestation','range', ({first_disturbance_year}, {last_feature_year}), {last_feature_year}],\")\n",
        "          index += 1\n",
        "        if exclude_lu_polygons and 'lu_' not in polygon:\n",
        "          print(f\"    {index}: ['{polygon[:-5]}', 'deforestation','range', ({first_disturbance_year}, {last_feature_year}), {last_feature_year}],\")\n",
        "          index += 1\n",
        "    print(\"}\\n\")\n",
        "\n",
        "else: print(\"Area-based disturbance scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI_Ck3HQVWLo"
      },
      "outputs": [],
      "source": [
        "disturbance_polygons = {\n",
        "    3: ['road_mat_daling', 'deforestation','range', (2023, 2024), 2024],\n",
        "}\n",
        "\n",
        "# The alternate year is set to 2024.\n",
        "# The years for alternate area-based disturbance can be between 1996 and 2024\n",
        "\n",
        "# Modify this dictionary by:\n",
        "# 1) Commenting out any polygons not being used for disturbance.\n",
        "# 2) Changing the type from 'deforestation' to 'degradation' if necessary.\n",
        "# 3) Changing the tuple years from 'range' to 'discrete' to specify individual years.\n",
        "# 4) If years are discrete, add one or more. If a range, add the start and end year.\n",
        "# 5) Copy and paste lines for multiple scenarios with the same area (different disturbance types, different years).\n",
        "# Remember to ensure all keys are unique - if you copy and paste, manually change the keys.\n",
        "\n",
        "if define_area_based_disturbance:\n",
        "\n",
        "    if alpha_earth: print(\"Alternate scenarios cannot be created with Alpha Earth features.\")\n",
        "    else:\n",
        "      # Validate disturbance types, year types and available years.\n",
        "      for area_index, value in disturbance_polygons.items():\n",
        "          polygon_name = value[0]\n",
        "          disturbance_type = value[1]\n",
        "          year_type, years_data = value[2], value[3]\n",
        "          alternate_scenario_year = value[4]\n",
        "\n",
        "          # Calculate first available disturbance year for this area's alternate scenario year\n",
        "          first_disturbance_year = alternate_scenario_year - model_scenario_year_range\n",
        "\n",
        "          # Validate alternate scenario year\n",
        "          assert alternate_scenario_year >= minimum_yearly_scenario, f\"Alternate scenario year for {polygon_name} must be between {minimum_yearly_scenario} and {last_feature_year}.\"\n",
        "          assert alternate_scenario_year <= last_feature_year, f\"Alternate scenario year for {polygon_name} must be between {minimum_yearly_scenario} and {last_feature_year}.\"\n",
        "          # Validate disturbance types and year types\n",
        "          assert disturbance_type in ['deforestation', 'degradation'], f\"Disturbance type for {polygon_name} must be 'deforestation' or 'degradation'.\"\n",
        "          if year_type == 'range':\n",
        "            start_year, end_year = years_data\n",
        "            assert start_year <= end_year, f\"The start year for {polygon_name} {disturbance_type} must be before the end year.\"\n",
        "\n",
        "          # Validate deforestation constraints\n",
        "          if disturbance_type == 'deforestation':\n",
        "            assert year_type == 'range', f\"Year type for {polygon_name} deforestation must be 'range'.\"\n",
        "            assert end_year == alternate_scenario_year, f\"Deforestation in {polygon_name} must end in the alternate scenario year {alternate_scenario_year}. Deforestation is considered permanent land-cover change.\"\n",
        "            assert start_year >= first_disturbance_year, f\"The start year for deforestation in {polygon_name} must be >= the first available disturbance year {first_disturbance_year}.\"\n",
        "            all_years = list(range(start_year, end_year + 1))\n",
        "\n",
        "          # Validate degradation constraints\n",
        "          if disturbance_type == 'degradation':\n",
        "            assert year_type in ['range', 'discrete'], f\"Year type for {polygon_name} degradation must be 'range' or 'discrete'.\"\n",
        "            if year_type == 'range':\n",
        "                all_years = list(range(start_year, end_year + 1))\n",
        "            else: all_years = list(years_data)\n",
        "            for year in all_years:\n",
        "                assert year <= alternate_scenario_year, f\"Years for {polygon_name} degradation (check {year}) must be <= the alternate scenario year {alternate_scenario_year}.\"\n",
        "                assert year >= first_disturbance_year, f\"Years for {polygon_name} degradation (check {year}) must >= the first available disturbance year {first_disturbance_year}.\"\n",
        "\n",
        "          # Simplify dictionary\n",
        "          disturbance_polygons[area_index] = [polygon_name, disturbance_type, all_years, alternate_scenario_year]\n",
        "\n",
        "      print(\"The 'disturbance_polygons' dictionary is valid.\")\n",
        "else: print(\"Area-based disturbance scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enzNAFf17h6P"
      },
      "outputs": [],
      "source": [
        "if define_area_based_disturbance:\n",
        "    if alpha_earth: print(\"Alternate scenarios cannot be created with Alpha Earth features.\")\n",
        "    else:\n",
        "      # Define the projects CRS to check the area polygon matches\n",
        "      crs_epsg = 4326\n",
        "      # This setting buffers any deforestation area to add degradation around it.\n",
        "      # At least 1 pixel distance (e.g. 30 m) might be realistic. Otherwise set to None.\n",
        "      buffer_distance_metres = 30\n",
        "\n",
        "      # Define a temporary directory for copying binary rasters and burning the area's polygon\n",
        "      binary_temp_dir = join(scenarios_model_dir, 'binary_temp')\n",
        "      makedirs(binary_temp_dir, exist_ok=True)\n",
        "\n",
        "      # Calculate progress totals before processing\n",
        "      total_areas = len(disturbance_polygons)\n",
        "      total_rasters = 0\n",
        "\n",
        "      for area_index, parameters in disturbance_polygons.items():\n",
        "          polygon_name = parameters[0]\n",
        "          disturbance_type = parameters[1]\n",
        "          disturbance_years = parameters[2]\n",
        "\n",
        "          # Raster counting: degradation = years x 2, deforestation = years x 5\n",
        "          # (edge_distance + local_density for disturbance, plus same for forest, plus mask)\n",
        "          if disturbance_type == 'degradation': area_raster_count = len(disturbance_years) * 2\n",
        "          elif disturbance_type == 'deforestation': area_raster_count = len(disturbance_years) * 4 + 1\n",
        "          total_rasters += area_raster_count\n",
        "\n",
        "      # Progress indicators\n",
        "      area_progress_index, area_progress_label = 0, widgets.Label(value=f\"Area progress: 0 / {total_areas}\")\n",
        "      display(area_progress_label)\n",
        "      raster_progress_index, raster_progress_label = 0, widgets.Label(value=f\"Raster progress: 0 / {total_rasters}\")\n",
        "      display(raster_progress_label)\n",
        "\n",
        "      for area_index, parameters in disturbance_polygons.items():\n",
        "\n",
        "          # Extract alternate area-based disturbance parameters\n",
        "          area_disturbance_features = []\n",
        "          polygon_name = parameters[0]\n",
        "          disturbance_type = parameters[1]\n",
        "          disturbance_years = parameters[2]\n",
        "          alternate_scenario_year = parameters[3]\n",
        "\n",
        "          # Determine base features by the alternate scenario's year for this area\n",
        "          alternate_year_scenario_csv = join(scenarios_model_dir, f\"{alternate_scenario_year}.csv\")\n",
        "          base_features = [f.split('/')[-1] for f in pd.read_csv(alternate_year_scenario_csv).iloc[:,0]]\n",
        "\n",
        "          # Define area polygon\n",
        "          area_polygon_path = join(polygons_dir, f\"{polygon_name}.gpkg\")\n",
        "          if disturbance_type == 'deforestation' and buffer_distance_metres:\n",
        "            area_buffered_path = join(polygons_dir, f\"{polygon_name}_buffered_{buffer_distance_metres}.gpkg\")\n",
        "            if not exists(area_buffered_path):\n",
        "              area_polygon = gpd.read_file(join(polygons_dir, f\"{polygon_name}.gpkg\"))\n",
        "              if area_polygon.crs.to_epsg() == crs_epsg:\n",
        "                # Suppress warning about not being a geographic CRS, as we account for this.\n",
        "                # However larger buffers or project areas near the poles might still need to be converted.\n",
        "                warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "                # Get the centroid of the project polygon\n",
        "                area_polygon_centroid = area_polygon.centroid.values[0]\n",
        "                # Convert the buffer distance from meters to decimal degrees based on the location at the centroid\n",
        "                buffer_distance_degrees = buffer_distance_metres / (111320 * abs(math.cos(math.radians(area_polygon_centroid.y))))\n",
        "                # Buffer the polygon and save\n",
        "                area_polygon_buffered = area_polygon.buffer(buffer_distance_degrees)\n",
        "                gdf = gpd.GeoDataFrame(geometry=area_polygon_buffered, crs=f\"EPSG:{crs_epsg}\")\n",
        "                gdf.to_file(area_buffered_path, driver='GPKG')\n",
        "                print(f\"Buffered the project area to {buffer_distance_metres} and exported to the polygons directory.\")\n",
        "              else: print(f\"Reproject {polygon_name}.gpkg to EPSG:4326.\")\n",
        "          else: area_buffered_path = None\n",
        "\n",
        "          # Track which binary rasters have been processed to avoid duplicate edge_effects calls\n",
        "          processed_disturbance_years = set()\n",
        "          processed_forest_years = set()\n",
        "\n",
        "          for scenario_feature in base_features:\n",
        "              # Handle disturbance edge_distance features\n",
        "              if \"disturbance_edge_distance\" in scenario_feature:\n",
        "                  scenario_feature_year = int(scenario_feature[-4:])\n",
        "                  if scenario_feature_year in disturbance_years:\n",
        "                    # Define feature names for both edge_distance and local_density\n",
        "                    if disturbance_type == 'deforestation':\n",
        "                      distance_name = f\"disturbance_edge_distance_{scenario_feature_year}_{polygon_name}_deforestation_{buffer_distance_metres}m_buffer\"\n",
        "                      density_name = f\"disturbance_local_density_{scenario_feature_year}_{polygon_name}_deforestation_{buffer_distance_metres}m_buffer\"\n",
        "                    else:\n",
        "                      distance_name = f\"disturbance_edge_distance_{scenario_feature_year}_{polygon_name}_degradation\"\n",
        "                      density_name = f\"disturbance_local_density_{scenario_feature_year}_{polygon_name}_degradation\"\n",
        "                    distance_path = join(feature_alternate_dir, f\"{distance_name}.tif\")\n",
        "                    density_path = join(feature_alternate_dir, f\"{density_name}.tif\")\n",
        "\n",
        "                    if not exists(distance_path) or not exists(density_path):\n",
        "                      # Copy the disturbance binary raster for burning '1' to the polygon area\n",
        "                      binary_raster_name = f\"disturbance_binary_{scenario_feature_year}.tif\"\n",
        "                      binary_raster_path = join(feature_binary_dir, binary_raster_name)\n",
        "                      binary_raster_temp_path = join(binary_temp_dir, binary_raster_name)\n",
        "                      copyfile(binary_raster_path, binary_raster_temp_path)\n",
        "                      if area_buffered_path: burn_polygon_to_raster(binary_raster_temp_path, area_buffered_path, fixed_value=1, all_touched=True)\n",
        "                      else: burn_polygon_to_raster(binary_raster_temp_path, area_polygon_path, fixed_value=1, all_touched=True)\n",
        "                      # Apply edge effects and export both arrays\n",
        "                      binary_burned = gdal.Open(binary_raster_temp_path)\n",
        "                      binary_burned_array = binary_burned.ReadAsArray()\n",
        "                      binary_burned = None\n",
        "                      distance_array, density_array = edge_effects(binary_burned_array, 'binary', cell_size_x_path, cell_size_y_path, edge_effect_distance)\n",
        "                      if not exists(distance_path): export_array_as_tif(distance_array, distance_path)\n",
        "                      if not exists(density_path): export_array_as_tif(density_array, density_path)\n",
        "\n",
        "                    area_disturbance_features.append(distance_name)\n",
        "                    processed_disturbance_years.add(scenario_feature_year)\n",
        "                    raster_progress_index += 2\n",
        "                    raster_progress_label.value = f\"Raster progress: {raster_progress_index} / {total_rasters}\"\n",
        "                  else: area_disturbance_features.append(scenario_feature)\n",
        "\n",
        "              # Handle disturbance local_density features\n",
        "              elif \"disturbance_local_density\" in scenario_feature:\n",
        "                  scenario_feature_year = int(scenario_feature[-4:])\n",
        "                  if scenario_feature_year in disturbance_years:\n",
        "                    # Feature was created when processing edge_distance\n",
        "                    if disturbance_type == 'deforestation':\n",
        "                      density_name = f\"disturbance_local_density_{scenario_feature_year}_{polygon_name}_deforestation_{buffer_distance_metres}m_buffer\"\n",
        "                    else:\n",
        "                      density_name = f\"disturbance_local_density_{scenario_feature_year}_{polygon_name}_degradation\"\n",
        "                    area_disturbance_features.append(density_name)\n",
        "                  else: area_disturbance_features.append(scenario_feature)\n",
        "\n",
        "              # Handle forest edge_distance features (deforestation only)\n",
        "              elif \"forest_edge_distance\" in scenario_feature:\n",
        "                  if disturbance_type == 'deforestation':\n",
        "                      scenario_feature_year = int(scenario_feature[-4:])\n",
        "                      if scenario_feature_year in disturbance_years:\n",
        "                        # Define feature names for both edge_distance and local_density\n",
        "                        distance_name = f\"forest_edge_distance_{scenario_feature_year}_{polygon_name}_deforestation\"\n",
        "                        density_name = f\"forest_local_density_{scenario_feature_year}_{polygon_name}_deforestation\"\n",
        "                        distance_path = join(feature_alternate_dir, f\"{distance_name}.tif\")\n",
        "                        density_path = join(feature_alternate_dir, f\"{density_name}.tif\")\n",
        "\n",
        "                        if not exists(distance_path) or not exists(density_path):\n",
        "                          # Copy the forest binary raster for burning '0' to the polygon area\n",
        "                          binary_raster_name = f\"forest_binary_{scenario_feature_year}.tif\"\n",
        "                          binary_raster_path = join(feature_binary_dir, binary_raster_name)\n",
        "                          binary_raster_temp_path = join(binary_temp_dir, binary_raster_name)\n",
        "                          copyfile(binary_raster_path, binary_raster_temp_path)\n",
        "                          burn_polygon_to_raster(binary_raster_temp_path, area_polygon_path, fixed_value=0, all_touched=True)\n",
        "                          # Apply edge effects and export both arrays\n",
        "                          binary_burned = gdal.Open(binary_raster_temp_path)\n",
        "                          binary_burned_array = binary_burned.ReadAsArray()\n",
        "                          binary_burned = None\n",
        "                          distance_array, density_array = edge_effects(binary_burned_array, 'binary', cell_size_x_path, cell_size_y_path, edge_effect_distance)\n",
        "                          if not exists(distance_path): export_array_as_tif(distance_array, distance_path)\n",
        "                          if not exists(density_path): export_array_as_tif(density_array, density_path)\n",
        "\n",
        "                        area_disturbance_features.append(distance_name)\n",
        "                        processed_forest_years.add(scenario_feature_year)\n",
        "                        raster_progress_index += 2\n",
        "                        raster_progress_label.value = f\"Raster progress: {raster_progress_index} / {total_rasters}\"\n",
        "                      else: area_disturbance_features.append(scenario_feature)\n",
        "                  else: area_disturbance_features.append(scenario_feature)\n",
        "\n",
        "              # Handle forest local_density features (deforestation only)\n",
        "              elif \"forest_local_density\" in scenario_feature:\n",
        "                  if disturbance_type == 'deforestation':\n",
        "                      scenario_feature_year = int(scenario_feature[-4:])\n",
        "                      if scenario_feature_year in disturbance_years:\n",
        "                        # Feature was created when processing edge_distance\n",
        "                        density_name = f\"forest_local_density_{scenario_feature_year}_{polygon_name}_deforestation\"\n",
        "                        area_disturbance_features.append(density_name)\n",
        "                      else: area_disturbance_features.append(scenario_feature)\n",
        "                  else: area_disturbance_features.append(scenario_feature)\n",
        "              else: area_disturbance_features.append(scenario_feature)\n",
        "\n",
        "          # Locate features in final feature directories\n",
        "          area_disturbance_features = locate_feature(area_disturbance_features, covariates_renamed)\n",
        "\n",
        "          # Add name affix based on whether years are a range or discrete\n",
        "          if disturbance_type == \"deforestation\": year_affix = f'_{min(disturbance_years)}'\n",
        "          else:\n",
        "            if len(disturbance_years) != (max(disturbance_years) - min(disturbance_years) + 1):\n",
        "                sorted_years = sorted(disturbance_years)\n",
        "                parts, start = [], sorted_years[0]\n",
        "                for i, year in enumerate(sorted_years[1:] + [None], 1):\n",
        "                    if year != sorted_years[i-1] + 1:\n",
        "                        end = sorted_years[i-1]\n",
        "                        parts.append(f\"{start}-{end}\" if start != end else str(start))\n",
        "                        start = year\n",
        "                year_affix = \"_\" + \"_\".join(parts)\n",
        "            else: year_affix = f'_{min(disturbance_years)}-{max(disturbance_years)}'\n",
        "\n",
        "          if disturbance_type == 'deforestation':\n",
        "            if buffer_distance_metres:\n",
        "              area_disturbance_scenario_name = f\"{alternate_scenario_year}_{polygon_name}_deforestation{year_affix}_{buffer_distance_metres}m_degradation_buffer\"\n",
        "            else: area_disturbance_scenario_name = f\"{alternate_scenario_year}_{polygon_name}_deforestation{year_affix}_0m_degradation_buffer\"\n",
        "            # Create a new forest mask for the area-based disturbance scenario\n",
        "            mask_raster_path = join(masks_dir, f\"mask_forest_{alternate_scenario_year}_{polygon_name}_deforestation.tif\")\n",
        "            if not exists(mask_raster_path):\n",
        "              # Ensure original forest binary is copied to temp and burned with polygon\n",
        "              scenario_year_forest_binary_path = join(binary_temp_dir, f\"forest_binary_{alternate_scenario_year}.tif\")\n",
        "              forest_binary_source = join(feature_binary_dir, f\"forest_binary_{alternate_scenario_year}.tif\")\n",
        "              copyfile(forest_binary_source, scenario_year_forest_binary_path)\n",
        "              burn_polygon_to_raster(scenario_year_forest_binary_path, area_polygon_path, fixed_value=0, all_touched=True)\n",
        "              # Create mask from burned forest data\n",
        "              scenario_year_forest_binary = gdal.Open(scenario_year_forest_binary_path)\n",
        "              scenario_year_forest_binary_array = scenario_year_forest_binary.ReadAsArray()\n",
        "              scenario_year_forest_binary = None\n",
        "              mask_array = np.where(scenario_year_forest_binary_array == 0, nodatavalue, 1)\n",
        "              export_array_as_tif(mask_array, mask_raster_path)\n",
        "              print(f\"A mask raster has been created for {area_disturbance_scenario_name}.\")\n",
        "            raster_progress_index += 1\n",
        "            raster_progress_label.value = f\"Raster progress: {raster_progress_index} / {total_rasters}\"\n",
        "\n",
        "          else: area_disturbance_scenario_name = f\"{alternate_scenario_year}_{polygon_name}_degradation{year_affix}\"\n",
        "\n",
        "          # Clear temporary binary raster folder\n",
        "          for temp_file in os.listdir(binary_temp_dir): os.remove(join(binary_temp_dir, temp_file))\n",
        "\n",
        "          # Export the alternate area-based disturbance scenario\n",
        "          no_degradation_scenario_path = join(scenarios_model_dir, f\"{area_disturbance_scenario_name}.csv\")\n",
        "          pd.DataFrame(area_disturbance_features).to_csv(no_degradation_scenario_path, index=False)\n",
        "          print(f\"Feature list for {area_disturbance_scenario_name} has been exported.\")\n",
        "\n",
        "          # Update area progress\n",
        "          area_progress_index += 1\n",
        "          area_progress_label.value = f\"Area progress: {area_progress_index} / {total_areas}\"\n",
        "\n",
        "      print(\"\\nAlternate area-based disturbance scenarios complete.\")\n",
        "      Path.rmdir(binary_temp_dir) # Delete temporary directory\n",
        "\n",
        "else: print(\"Area-based disturbance scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTBj-edFgbKY"
      },
      "source": [
        "## Recovered to oldgrowth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzJJ2P1rysph"
      },
      "outputs": [],
      "source": [
        "# This simulates edge effects on oldgrowth forest within an actual forest extent.\n",
        "# Assumes no forest regrowth (from non-forest land-cover), only recovery.\n",
        "define_oldgrowth_recovery = True\n",
        "\n",
        "# List of land-use base feature names. All but one are 'redundant'.\n",
        "# One should be selected as 'oldgrowth_feature'.\n",
        "# Both _edge_distance and _local_density variants will be modified.\n",
        "print(\"oldgrowth_redundant_features = [\")\n",
        "seen_bases = set()\n",
        "for feature in model_features:\n",
        "    if \"lu_\" in feature:\n",
        "        if \"_edge_distance\" in feature: base_name = feature.replace(\"_edge_distance\", \"\")\n",
        "        elif \"_local_density\" in feature: base_name = feature.replace(\"_local_density\", \"\")\n",
        "        else: continue\n",
        "        if base_name not in seen_bases:\n",
        "            seen_bases.add(base_name)\n",
        "            print(f'  \"{base_name}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3xxfOzPyCoc"
      },
      "outputs": [],
      "source": [
        "# Base name of the feature that best indicates oldgrowth to the model.\n",
        "# Both _edge_distance and _local_density variants will be modified.\n",
        "oldgrowth_feature = 'lu_old-growth_protected_areas'\n",
        "\n",
        "# Base names of features that may confound the old-growth proxy.\n",
        "# Both _edge_distance and _local_density variants will be removed for the old-growth scenarios.\n",
        "oldgrowth_redundant_features = [\n",
        "    \"lu_ais\",\n",
        "    \"lu_berkelah_jerantut\",\n",
        "    \"lu_berkelah_kuantan\",\n",
        "    \"lu_berkelah_temerloh\",\n",
        "    \"lu_old-growth_protected_areas\",\n",
        "    \"lu_remen_chereh\",\n",
        "    \"lu_tekai_tembeling\",\n",
        "    \"lu_tekam\",\n",
        "    \"lu_yong_lipis\",\n",
        "    \"lu_yong\",\n",
        "]\n",
        "\n",
        "# Set to the year(s) of interest to use that forest extent\n",
        "oldgrowth_recovery_years = [\n",
        "    2021,\n",
        "    last_feature_year\n",
        "]\n",
        "\n",
        "for year in oldgrowth_recovery_years:\n",
        "  assert year in final_feature_years, \"Years in 'oldgrowth_recovery_years' must be available in the final yearly features.\"\n",
        "\n",
        "# Generate 'oldgrowth' scenarios for each specified year (i.e. forest extent in that year).\n",
        "if define_oldgrowth_recovery:\n",
        "    for year in oldgrowth_recovery_years:\n",
        "      oldgrowth_recovery_features_1 = []\n",
        "      oldgrowth_recovery_features_2 = []\n",
        "      old_growth_scenario_year_diff = last_feature_year - year\n",
        "      for scenario_feature in most_recent_scenario_features:\n",
        "        if \"disturbance_edge_distance\" in scenario_feature:\n",
        "          feature_1 = feature_2 = \"minimum_edge_distance\"\n",
        "        elif \"disturbance_local_density\" in scenario_feature:\n",
        "          feature_1 = feature_2 = \"minimum_local_density\"\n",
        "        elif \"forest_edge_distance\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          new_year = (scenario_feature_year - old_growth_scenario_year_diff > first_feature_year) and \\\n",
        "                    scenario_feature_year - old_growth_scenario_year_diff or first_feature_year\n",
        "          feature_1 = feature_2 = f\"forest_edge_distance_{new_year}\"\n",
        "        elif \"forest_local_density\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          new_year = (scenario_feature_year - old_growth_scenario_year_diff > first_feature_year) and \\\n",
        "                    scenario_feature_year - old_growth_scenario_year_diff or first_feature_year\n",
        "          feature_1 = feature_2 = f\"forest_local_density_{new_year}\"\n",
        "        elif scenario_feature.startswith(oldgrowth_feature):\n",
        "          if 'edge_distance' in scenario_feature: feature_1 = \"maximum_edge_distance\" # LU interior max\n",
        "          if 'local_density' in scenario_feature: feature_1 = \"maximum_local_density\"\n",
        "          feature_2 = scenario_feature\n",
        "        elif any(scenario_feature.startswith(rf) for rf in oldgrowth_redundant_features):\n",
        "          if 'edge_distance' in scenario_feature: feature_1 = \"minimum_edge_distance\" # LU exterior min (neg)\n",
        "          if 'local_density' in scenario_feature: feature_1 = \"minimum_local_density\"\n",
        "          feature_2 = scenario_feature\n",
        "        else: feature_1 = feature_2 = scenario_feature\n",
        "        oldgrowth_recovery_features_1.append(feature_1)\n",
        "        oldgrowth_recovery_features_2.append(feature_2)\n",
        "      # Locate features in final feature directories\n",
        "      oldgrowth_recovery_features_1 = locate_feature(oldgrowth_recovery_features_1, covariates_renamed)\n",
        "      oldgrowth_recovery_features_2 = locate_feature(oldgrowth_recovery_features_2, covariates_renamed)\n",
        "\n",
        "      # Compare feature lists and save appropriate CSVs.\n",
        "      if oldgrowth_recovery_features_1 == oldgrowth_recovery_features_2:\n",
        "        filename = f\"{year}_oldgrowth_recovery_1.csv\"\n",
        "        pd.DataFrame(oldgrowth_recovery_features_1).to_csv(join(scenarios_model_dir, filename), index=False)\n",
        "        print(f\"Feature lists were identical, only saved {filename}\")\n",
        "      else:\n",
        "        for suffix, features in [(\"1\", oldgrowth_recovery_features_1), (\"2\", oldgrowth_recovery_features_2)]:\n",
        "          filename = f\"{year}_oldgrowth_recovery_{suffix}.csv\"\n",
        "          pd.DataFrame(features).to_csv(join(scenarios_model_dir, filename), index=False)\n",
        "        print(f\"Feature lists for scenarios where all forest in {year} becomes old-growth\")\n",
        "        print(f\"have been exported to {year}_oldgrowth_recovery_1.csv and {year}_oldgrowth_recovery_2.csv.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy2nfHshozVQ"
      },
      "source": [
        "# Feature verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chCSZTV4XB8k"
      },
      "outputs": [],
      "source": [
        "# Check all features in scenario .csvs exist\n",
        "scenario_csv_list = []\n",
        "all_features_exist = True # Changes to false if feature missing\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    csv_dir = join(scenarios_model_dir, csv)\n",
        "    csv_feature_list = pd.Series.tolist(pd.read_csv(csv_dir).iloc[:,0])\n",
        "    csv_feature_dir_list = []\n",
        "    for csv_feature in csv_feature_list:\n",
        "      if csv_feature not in covariates: csv_feature_dir_list.append(f\"{features_dir}/{csv_feature}.tif\")\n",
        "    for feature in csv_feature_dir_list:\n",
        "      if not exists(feature):\n",
        "        all_features_exist = False\n",
        "        print(f\"The following feature is missing:\\n{feature}\\n and is required for the scenario '{csv[:-4]}'\")\n",
        "\n",
        "if all_features_exist: print(\"All required features are present.\")\n",
        "print(\"Covariate features e.g. 'beam' and 'sensitivity' will be added at the prediction stage.\")\n",
        "\n",
        "# Check all features against template dimensions\n",
        "# Find all features in all scenarios for tiling\n",
        "feature_paths = set()\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "    if csv.endswith('.csv'):\n",
        "        features = pd.read_csv(join(scenarios_model_dir, csv)).iloc[:,0]\n",
        "        for feature in features:\n",
        "            feature_path = join(features_dir, f\"{feature}.tif\")\n",
        "            feature_paths.add(feature_path)\n",
        "feature_paths = list(feature_paths)\n",
        "\n",
        "if not alpha_earth: # Higher resolution than template\n",
        "  scenario_template = gdal.Open(template_tif_path)\n",
        "  scenario_template_dimensions, scenario_template_projection = scenario_template.GetGeoTransform(), scenario_template.GetProjection()\n",
        "  scenario_template = None\n",
        "  feature_issue = False\n",
        "  for feature_path in feature_paths:\n",
        "    feature = gdal.Open(feature_path)\n",
        "    feature_dimensions, feature_projection = feature.GetGeoTransform(), feature.GetProjection()\n",
        "    feature = None\n",
        "    if feature_dimensions != scenario_template_dimensions:\n",
        "      print(f\"{feature} dimensions:\\n{feature_dimensions}\\ndo not match the scenario template dimensions:\\n{scenario_template_dimensions}\\n\")\n",
        "      feature_issue = True\n",
        "    if feature_projection != scenario_template_projection:\n",
        "      print(f\"{feature} projection:\\n{feature_projection}\\ndoes not match the scenario template projection:\\n{scenario_template_projection}\\n\\n\")\n",
        "      feature_issue = True\n",
        "  if not feature_issue: print(f\"All features listed in the scenario .csv files have the correct dimensions and projection.\")\n",
        "  else: print(\"Correct and / or resample the feature(s).\")\n",
        "else: print(\"AlphaEarth features cannot be mixed with other features unless resampled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUajnuo7Jxz_"
      },
      "source": [
        "# Scenario masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3POcM4jSJ2Ud"
      },
      "outputs": [],
      "source": [
        "# Scenario masks\n",
        "\n",
        "# Use polygons for masking, only areas inside the polygons will be included.\n",
        "\n",
        "# Exclude existing polygons from search\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "\n",
        "print(\"scenario_mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"  '{polygon[:-5]}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VNTQWqfK__e"
      },
      "outputs": [],
      "source": [
        "scenario_mask_polygons = [\n",
        "  # 'project_area',\n",
        "  'gedi_area',\n",
        "]\n",
        "\n",
        "# Optional forest mask override for 'no_disturbance_since_oldgrowth' scenarios.\n",
        "# If None, uses the yearly land mask (mask_land_{scenario_year}).\n",
        "# Set to a mask filename (without .tif) to use a different forest extent,\n",
        "# e.g. 'mask_land_2020' or a custom mask representing maximum potential forest cover.\n",
        "oldgrowth_scenario_mask_override = None\n",
        "\n",
        "# Create inverse polygons for masking\n",
        "template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "for polygon in scenario_mask_polygons:\n",
        "  inverse_polygon_path = join(polygons_dir, f\"{polygon}_inverse.gpkg\")\n",
        "  if not exists(inverse_polygon_path):\n",
        "    polygon_path = join(polygons_dir, f\"{polygon}.gpkg\")\n",
        "    template_polygon = gpd.read_file(template_polygon_path)\n",
        "    polygon_read = gpd.read_file(polygon_path)\n",
        "    polygon_crs = polygon_read.crs.to_epsg()\n",
        "    inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "    inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "    inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "    print(f\"An inverse masking polygon for {polygon} has been created in {polygons_dir}.\")\n",
        "  else: print(f\"An inverse masking polygon for {polygon} already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB9NP0W4LLxF"
      },
      "outputs": [],
      "source": [
        "# Collect all scenarios with .csv feature lists\n",
        "all_scenario_csvs = [csv[:-4] for csv in os.listdir(scenarios_model_dir) if csv.endswith('.csv')]\n",
        "\n",
        "# Determine last feature year for future scenario handling\n",
        "final_feature_years_list = []\n",
        "for final_feature in os.listdir(feature_edge_effects_dir):\n",
        "  try: final_feature_years_list.append(int(final_feature[-8:-4]))\n",
        "  except: continue\n",
        "last_feature_year_masks = max(final_feature_years_list)\n",
        "\n",
        "# Progress\n",
        "mask_progress_index = 0\n",
        "mask_progress_label = widgets.Label(f\"Scenario mask progress: 0 / {len(all_scenario_csvs)}\")\n",
        "display(mask_progress_label)\n",
        "\n",
        "for scenario in sorted(all_scenario_csvs):\n",
        "  scenario_mask_path = join(scenario_masks_dir, f\"{scenario}.tif\")\n",
        "  if exists(scenario_mask_path):\n",
        "    mask_progress_index += 1\n",
        "    mask_progress_label.value = f\"Scenario mask progress: {mask_progress_index} / {len(all_scenario_csvs)}\"\n",
        "    continue\n",
        "  scenario_year = int(scenario[:4])\n",
        "  forest_mask_path = None\n",
        "\n",
        "  # Match 'no_disturbance_since_oldgrowth' scenarios\n",
        "  if 'no_disturbance_since_oldgrowth' in scenario:\n",
        "    if oldgrowth_scenario_mask_override: forest_mask_path = join(masks_dir, f\"{oldgrowth_scenario_mask_override}.tif\")\n",
        "    else: forest_mask_path = join(masks_dir, f\"mask_land_{scenario_year}.tif\")\n",
        "  # Match area-based deforestation scenarios\n",
        "  elif 'deforestation' in scenario:\n",
        "    for mask in sorted(os.listdir(masks_dir)):\n",
        "      if 'deforestation' in mask:\n",
        "        mask_middle = mask[12:-4]  # Remove 'mask_forest_' prefix and '.tif' suffix\n",
        "        if scenario.startswith(mask_middle):\n",
        "          forest_mask_path = join(masks_dir, mask)\n",
        "          break\n",
        "  # Match 'no_disturbance_since' scenarios\n",
        "  # Use the mask from the year before disturbance removal begins\n",
        "  elif 'no_disturbance_since' in scenario:\n",
        "    disturbance_since_year = int(scenario.split('_since_')[1][:4])\n",
        "    forest_mask_path = join(masks_dir, f\"mask_forest_{disturbance_since_year - 1}.tif\")\n",
        "  # Match future scenarios with most recent forest mask\n",
        "  elif scenario_year > last_feature_year_masks:\n",
        "    forest_mask_path = join(masks_dir, f\"mask_forest_{last_feature_year_masks}.tif\")\n",
        "  # Match all other historic and degradation scenarios\n",
        "  # Degradation uses same mask as historic, no additional deforestation\n",
        "  else: forest_mask_path = join(masks_dir, f\"mask_forest_{scenario_year}.tif\")\n",
        "\n",
        "  if forest_mask_path is None or not exists(forest_mask_path):\n",
        "    print(f\"Warning: No forest mask found for {scenario} at {forest_mask_path}, skipping.\")\n",
        "    mask_progress_index += 1\n",
        "    mask_progress_label.value = f\"Scenario mask progress: {mask_progress_index} / {len(all_scenario_csvs)}\"\n",
        "    continue\n",
        "\n",
        "  # Load forest mask\n",
        "  forest_mask = gdal.Open(forest_mask_path)\n",
        "  combined_mask = forest_mask.ReadAsArray()\n",
        "  forest_mask = None\n",
        "  # Intersect with land mask for scenario year\n",
        "  land_mask_path = join(masks_dir, f\"mask_land_{scenario_year}.tif\")\n",
        "  if exists(land_mask_path):\n",
        "    land_mask = gdal.Open(land_mask_path)\n",
        "    land_mask_array = land_mask.ReadAsArray()\n",
        "    land_mask = None\n",
        "    combined_mask = np.where((combined_mask == 1) & (land_mask_array == 1), 1, 0)\n",
        "  else:\n",
        "    print(f\"Warning: No land mask for year {scenario_year}, using forest mask only.\")\n",
        "    combined_mask = np.where(combined_mask == 1, 1, 0)\n",
        "  # Apply polygon masks via burn\n",
        "  if scenario_mask_polygons:\n",
        "    temp_mask_path = join(scenario_masks_dir, f\"temp_{scenario}.tif\")\n",
        "    export_array_as_tif(combined_mask.astype(np.float32), temp_mask_path, compress=False)\n",
        "    for polygon_name in scenario_mask_polygons:\n",
        "      inverse_polygon_path = join(polygons_dir, f\"{polygon_name}_inverse.gpkg\")\n",
        "      if exists(inverse_polygon_path):\n",
        "        burn_polygon_to_raster(temp_mask_path, inverse_polygon_path, fixed_value=0, all_touched=False)\n",
        "    temp_mask = gdal.Open(temp_mask_path)\n",
        "    combined_mask = temp_mask.ReadAsArray()\n",
        "    temp_mask = None\n",
        "    os.remove(temp_mask_path)\n",
        "  # Convert to 1/nodata Int16\n",
        "  final_mask = np.where(combined_mask == 1, 1, nodatavalue).astype(np.int16)\n",
        "  export_array_as_tif(final_mask, scenario_mask_path, dtype=gdal.GDT_Int16)\n",
        "  masked_fraction = np.sum(final_mask != 1) / final_mask.size\n",
        "  print(f\"{scenario}: {masked_fraction:.1%} masked\")\n",
        "  print(f\"  Forest mask: {basename(forest_mask_path)}\")\n",
        "  mask_progress_index += 1\n",
        "  mask_progress_label.value = f\"Scenario mask progress: {mask_progress_index} / {len(all_scenario_csvs)}\"\n",
        "\n",
        "print(\"\\nScenario masks complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T9UqJrzWECr"
      },
      "source": [
        "# Tiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zizOofP5dgW"
      },
      "outputs": [],
      "source": [
        "# Load the model scenario features for template tile creation\n",
        "model_scenario_features = pd.Series.tolist(pd.read_csv(model_scenario_path).iloc[:,0])\n",
        "model_scenario_features_dirs = [features_dir + '/' + feature + '.tif' for feature in model_scenario_features]\n",
        "# Create a template feature array from the first feature that isn't a covariate (these are created later)\n",
        "template_base_path = next(r for r in model_scenario_features_dirs if all(c not in r for c in covariates))\n",
        "template_base = gdal.Open(template_base_path)\n",
        "template_base_array = template_base.ReadAsArray()\n",
        "template_base_xsize, template_base_ysize = template_base.GetRasterBand(1).XSize, template_base.GetRasterBand(1).YSize\n",
        "template_base = None\n",
        "print(f\"The template feature is {template_base_xsize} x {template_base_ysize} pixels.\")\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles_exist = len(template_tile_list)\n",
        "\n",
        "if n_tiles_exist < 1: print(\"There are currently no template tiles. Run the next section.\")\n",
        "if n_tiles_exist >= 1:\n",
        "  tile_size_y_rounded_exist_ds = gdal.Open(join(tile_templates_dir,'template_tile_1.tif'))\n",
        "  tile_size_y_rounded_exist = tile_size_y_rounded_exist_ds.GetRasterBand(1).YSize\n",
        "  tile_size_y_rounded_exist_ds = None\n",
        "  tile_size_y_remainder_exist_ds = gdal.Open(join(tile_templates_dir,f'template_tile_{n_tiles_exist}.tif'))\n",
        "  tile_size_y_remainder_exist = tile_size_y_remainder_exist_ds.GetRasterBand(1).YSize\n",
        "  tile_size_y_remainder_exist_ds = None\n",
        "  if n_tiles_exist == 1:\n",
        "    print(f\"There is a single 'tile' with a height of {tile_size_y_rounded_exist} pixels.\")\n",
        "    tile_size_y_remainder_exist = 0\n",
        "  else:  print(f\"There are {n_tiles_exist} template tiles, the first {n_tiles_exist-1} having a height of {tile_size_y_rounded_exist} pixels, the last {tile_size_y_remainder_exist} pixels.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njhqSH8e5cPu"
      },
      "outputs": [],
      "source": [
        "# Large template areas and / or numbers of features may be too much for the available memory.\n",
        "# This section defines how to split predictions into tiles that can then be merged.\n",
        "override_n_tiles = False  # Useful if the tile number has already been tested.\n",
        "n_tiles_override = 1\n",
        "\n",
        "memory_utilisation = 0.8 # Set to 0.8 to ensure crashes are avoided\n",
        "\n",
        "assert memory_utilisation > 0 and memory_utilisation <= 1, \"Set memory_utilisation to a value between 0 and 1\"\n",
        "\n",
        "# Calculate total size of feature stack\n",
        "feature_stack_size = template_base_array.size * len(model_scenario_features_dirs)\n",
        "\n",
        "# Calculate memory and the number of tiles required (assuming highest precision raster is Float32)\n",
        "total_memory_needed = 32 / 8 * feature_stack_size *2 # 8 bits per byte, *2 for transposing feature stack\n",
        "print(f'RAM required for each prediction: ~{total_memory_needed/(1024**3):.3f} GB')\n",
        "print(f'RAM currently available: {psutil.virtual_memory().free / (1024**3):.3f} GB')\n",
        "n_tiles_temp = int(np.ceil(total_memory_needed / (psutil.virtual_memory().free * memory_utilisation)))\n",
        "\n",
        "# Calculate template tile size (split on the y axis only)\n",
        "tile_size_y_rounded = int(np.ceil(template_base_ysize/n_tiles_temp)) # Round the number of y pixels in each tile\n",
        "tile_size_y_remainder = template_base_ysize%tile_size_y_rounded # Calculate the remainder for the last tile\n",
        "n_tiles = max(1, len(range(0, template_base_ysize, tile_size_y_rounded))) # Update the number of tiles to include the remainder\n",
        "\n",
        "if override_n_tiles:\n",
        "  tile_size_y_rounded = int(np.ceil(template_base_ysize / n_tiles_override))\n",
        "  tile_size_y_remainder = template_base_ysize % tile_size_y_rounded\n",
        "  n_tiles = n_tiles_override\n",
        "  print(\"n_tiles has been overridden.\")\n",
        "\n",
        "print(f'The prediction template should be divided into {n_tiles} tiles to avoid crashing.')\n",
        "\n",
        "# Check if tiles need to be changed\n",
        "change_tiles = True\n",
        "if override_n_tiles:\n",
        "  if n_tiles == n_tiles_exist: change_tiles = False\n",
        "if n_tiles == n_tiles_exist and tile_size_y_rounded == tile_size_y_rounded_exist and tile_size_y_remainder == tile_size_y_remainder_exist:\n",
        "  change_tiles = False\n",
        "\n",
        "if change_tiles:\n",
        "  # Clear all tile directories\n",
        "  for tile in Path(tile_templates_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for tile in Path(tile_features_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for scenario_stack_dir in Path(tile_feature_stacks_dir).glob(\"**/*\"):\n",
        "    shutil.rmtree(scenario_stack_dir)\n",
        "  for tile in Path(tile_prediction_cache_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "\n",
        "  # Generate new tile templates based on available memory\n",
        "  tile_number = 1\n",
        "  for y_start in range(0, template_base_ysize, tile_size_y_rounded):\n",
        "    if tile_size_y_remainder != 0 and tile_number == n_tiles: tile_size_y = tile_size_y_remainder\n",
        "    else: tile_size_y = tile_size_y_rounded\n",
        "    tiling_string = \"gdal_translate -of GTIFF -srcwin \" + str(0)+ \", \" + str(y_start) + \", \" + str(template_base_xsize) + \", \" + str(tile_size_y) + \" \" + str(template_base_path) + \" \" + str(tile_templates_dir) + \"/template_tile_\" + str(tile_number) + \".tif\"\n",
        "    os.system(tiling_string)\n",
        "    tile_number += 1\n",
        "  print(\"Template tile creation complete.\")\n",
        "\n",
        "else: print(\"No changes to existing tiles are required.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P_nom3fcuJu"
      },
      "outputs": [],
      "source": [
        "# Create feature tiles.\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "template_tile_1 = gdal.Open(join(tile_templates_dir, 'template_tile_1.tif'))\n",
        "tile_size_y_rounded = template_tile_1.GetRasterBand(1).YSize\n",
        "template_tile_1 = None\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "if n_tiles == 1: print(\"Feature tile creation skipped. Feature stack creation will use the original features.\")\n",
        "else:\n",
        "  # Find all features in all scenarios for tiling\n",
        "  feature_paths = set()\n",
        "  for csv in os.listdir(scenarios_model_dir):\n",
        "      if csv.endswith('.csv'):\n",
        "          features = pd.read_csv(join(scenarios_model_dir, csv)).iloc[:,0]\n",
        "          for feature in features:\n",
        "              feature_path = join(features_dir, f\"{feature}.tif\")\n",
        "              feature_paths.add(feature_path)\n",
        "  feature_paths = list(feature_paths)\n",
        "  # Progress\n",
        "  n_features = len(feature_paths)\n",
        "  feature_progress_index, feature_progress_label = 0, widgets.Label(value=f\"Feature progress: 0 / {n_features}\")\n",
        "  display(feature_progress_label)\n",
        "  tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "  display(tile_progress_label)\n",
        "\n",
        "  # Loop through each feature in the scenarios features directory\n",
        "  for feature_path in feature_paths:\n",
        "      feature = gdal.Open(feature_path)\n",
        "      feature_array = feature.ReadAsArray()\n",
        "      feature = None\n",
        "      # Split feature array into tiles matching template dimensions\n",
        "      y_start = 0\n",
        "      for tile_count in range(1, n_tiles + 1):\n",
        "          feature_tile_filename = f\"{basename(feature_path)[:-4]}_{tile_count}.tif\"\n",
        "          feature_tile_path = join(tile_features_dir, feature_tile_filename)\n",
        "          # Get tile dimensions from template\n",
        "          template_tile_path = join(tile_templates_dir, f\"template_tile_{tile_count}.tif\")\n",
        "          template_tile = gdal.Open(template_tile_path)\n",
        "          tile_ysize = template_tile.GetRasterBand(1).YSize\n",
        "          template_tile = None\n",
        "          # Export feature chunk if tile does not exist\n",
        "          if not exists(feature_tile_path):\n",
        "              feature_chunk = feature_array[y_start:y_start + tile_ysize, :]\n",
        "              export_array_as_tif(feature_chunk, feature_tile_path, template_tile_path)\n",
        "          # Update y offset and tile progress\n",
        "          y_start += tile_ysize\n",
        "          tile_progress_index += 1\n",
        "          tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_tiles}\"\n",
        "\n",
        "      # Reset tile progress and update feature progress\n",
        "      tile_progress_index = 0\n",
        "      feature_progress_index += 1\n",
        "      feature_progress_label.value = f\"Feature progress: {feature_progress_index} / {n_features}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycMGrs9uQ_0p"
      },
      "outputs": [],
      "source": [
        "# Create scenario mask tiles.\n",
        "if n_tiles == 1: print(\"Scenario mask tile creation skipped. Feature stack creation will use the original masks.\")\n",
        "else:\n",
        "  # Find all scenario masks\n",
        "  mask_paths = [join(scenario_masks_dir, f) for f in os.listdir(scenario_masks_dir) if f.endswith('.tif')]\n",
        "  # Progress\n",
        "  n_masks = len(mask_paths)\n",
        "  mask_progress_index, mask_progress_label = 0, widgets.Label(value=f\"Mask progress: 0 / {n_masks}\")\n",
        "  display(mask_progress_label)\n",
        "  mask_tile_progress_index, mask_tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "  display(mask_tile_progress_label)\n",
        "\n",
        "  # Loop through each scenario mask\n",
        "  for mask_path in mask_paths:\n",
        "      mask = gdal.Open(mask_path)\n",
        "      mask_array = mask.ReadAsArray()\n",
        "      mask = None\n",
        "      # Split mask array into tiles matching template dimensions\n",
        "      y_start = 0\n",
        "      for tile_count in range(1, n_tiles + 1):\n",
        "          mask_tile_filename = f\"{basename(mask_path)[:-4]}_{tile_count}.tif\"\n",
        "          mask_tile_path = join(tile_scenario_masks_dir, mask_tile_filename)\n",
        "          # Get tile dimensions from template\n",
        "          template_tile_path = join(tile_templates_dir, f\"template_tile_{tile_count}.tif\")\n",
        "          template_tile = gdal.Open(template_tile_path)\n",
        "          tile_ysize = template_tile.GetRasterBand(1).YSize\n",
        "          template_tile = None\n",
        "          # Export mask chunk if tile does not exist\n",
        "          if not exists(mask_tile_path):\n",
        "              mask_chunk = mask_array[y_start:y_start + tile_ysize, :]\n",
        "              export_array_as_tif(mask_chunk, mask_tile_path, template_tile_path, dtype=gdal.GDT_Int16)\n",
        "          # Update y offset and tile progress\n",
        "          y_start += tile_ysize\n",
        "          mask_tile_progress_index += 1\n",
        "          mask_tile_progress_label.value = f\"Tile progress: {mask_tile_progress_index} / {n_tiles}\"\n",
        "      # Reset tile progress and update mask progress\n",
        "      mask_tile_progress_index = 0\n",
        "      mask_progress_index += 1\n",
        "      mask_progress_label.value = f\"Mask progress: {mask_progress_index} / {n_masks}\"\n",
        "\n",
        "  print(\"Scenario mask tile creation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0r89JLYW_xU"
      },
      "source": [
        "# Feature stacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhSrhXg47Yfk"
      },
      "outputs": [],
      "source": [
        "# Load neutral effect values for covariates from SHAP analysis.\n",
        "# Used to minimise covariate bias when making spatial predictions.\n",
        "# Otherwise set a 'manual override' at a suitable value.\n",
        "\n",
        "# Manual overrides (applied after SHAP values loaded)\n",
        "covariate_overrides = {\n",
        "    # 'fea_beam': 5,\n",
        "    # 'fea_sensitivity': 0.99,\n",
        "}\n",
        "\n",
        "covariate_values = {}\n",
        "feature_analysis_path = join(selected_model_shap_dir, 'shap_feature_analysis.csv')\n",
        "if covariates_renamed:\n",
        "    print(f\"Covariates defined in model: {covariates_renamed}\\n\")\n",
        "    if exists(feature_analysis_path):\n",
        "        feature_analysis = pd.read_csv(feature_analysis_path)\n",
        "        print(\"Neutral effect values from SHAP analysis:\")\n",
        "        for cov in covariates_renamed:\n",
        "            row = feature_analysis[feature_analysis['Dataset name'] == cov]\n",
        "            if not row.empty:\n",
        "                neutral_val = row['Neutral_Effect_Value'].values[0]\n",
        "                shap_at_neutral = row['SHAP_at_Neutral'].values[0]\n",
        "                covariate_values[cov] = neutral_val\n",
        "                print(f\"  {cov}: {neutral_val:.4f} (SHAP at neutral: {shap_at_neutral:.4f})\")\n",
        "            else: print(f\"  {cov}: not found in SHAP analysis\")\n",
        "    else: print(\"SHAP feature analysis not found. Set values in covariate_overrides.\")\n",
        "\n",
        "    # Apply overrides and verify completeness\n",
        "    covariate_values.update(covariate_overrides)\n",
        "    if covariate_overrides: print(f\"\\nManual overrides applied: {covariate_overrides}\")\n",
        "    missing = [c for c in covariates_renamed if c not in covariate_values]\n",
        "    assert not missing, f\"Missing covariate values: {missing}. Set in covariate_overrides.\"\n",
        "\n",
        "    # Cast to float32 for consistency with feature stack dtype\n",
        "    covariate_values = {k: np.float32(v) for k, v in covariate_values.items()}\n",
        "    print(f\"Final covariate values: {covariate_values}\")\n",
        "else: print(\"No covariates defined in model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaHFv1nuW5CG"
      },
      "outputs": [],
      "source": [
        "# Create feature stack arrays for each scenario\n",
        "# Collect scenarios with .csv feature lists\n",
        "scenarios_list = []\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    scenarios_list.append(csv[:-4])\n",
        "# Select scenarios to generate tiled feature stacks\n",
        "print(\"scenarios_to_stack = [\")\n",
        "for scenario in sorted(scenarios_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnqlQjQiCXWh"
      },
      "outputs": [],
      "source": [
        "scenarios_to_stack = [\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_no_disturbance_since_1993\",\n",
        "  \"2021_no_disturbance_since_oldgrowth_1\",\n",
        "  \"2021_no_disturbance_since_oldgrowth_2\",\n",
        "  \"2021_oldgrowth_recovery_1\",\n",
        "  \"2021_oldgrowth_recovery_2\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_no_disturbance_since_1996\",\n",
        "  \"2024_no_disturbance_since_1997\",\n",
        "  \"2024_no_disturbance_since_1998\",\n",
        "  \"2024_no_disturbance_since_1999\",\n",
        "  \"2024_no_disturbance_since_2000\",\n",
        "  \"2024_no_disturbance_since_2001\",\n",
        "  \"2024_no_disturbance_since_2002\",\n",
        "  \"2024_no_disturbance_since_2003\",\n",
        "  \"2024_no_disturbance_since_2004\",\n",
        "  \"2024_no_disturbance_since_2005\",\n",
        "  \"2024_no_disturbance_since_2006\",\n",
        "  \"2024_no_disturbance_since_2007\",\n",
        "  \"2024_no_disturbance_since_2008\",\n",
        "  \"2024_no_disturbance_since_2009\",\n",
        "  \"2024_no_disturbance_since_2010\",\n",
        "  \"2024_no_disturbance_since_2011\",\n",
        "  \"2024_no_disturbance_since_2012\",\n",
        "  \"2024_no_disturbance_since_2013\",\n",
        "  \"2024_no_disturbance_since_2014\",\n",
        "  \"2024_no_disturbance_since_2015\",\n",
        "  \"2024_no_disturbance_since_2016\",\n",
        "  \"2024_no_disturbance_since_2017\",\n",
        "  \"2024_no_disturbance_since_2018\",\n",
        "  \"2024_no_disturbance_since_2019\",\n",
        "  \"2024_no_disturbance_since_2020\",\n",
        "  \"2024_no_disturbance_since_2021\",\n",
        "  \"2024_no_disturbance_since_2022\",\n",
        "  \"2024_no_disturbance_since_2023\",\n",
        "  \"2024_no_disturbance_since_2024\",\n",
        "  \"2024_no_disturbance_since_oldgrowth_1\",\n",
        "  \"2024_no_disturbance_since_oldgrowth_2\",\n",
        "  \"2024_oldgrowth_recovery_1\",\n",
        "  \"2024_oldgrowth_recovery_2\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f-uJ-EkoxIym"
      },
      "outputs": [],
      "source": [
        "# Check all scenarios exist\n",
        "assert set(scenarios_to_stack).issubset(scenarios_list), \"Not all selected scenarios exist.\"\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = [f for f in os.listdir(tile_templates_dir) if f.endswith('.tif') and f.startswith('template_tile')]\n",
        "n_tiles = len(template_tile_list)\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "covariate_set = set(covariates)\n",
        "\n",
        "# Progress\n",
        "scenario_progress_index, scenario_progress_label = 0, widgets.Label(value=f\"Scenario progress: 0 / {len(scenarios_to_stack)}\")\n",
        "display(scenario_progress_label)\n",
        "stack_progress_index, stack_progress_label = 0, widgets.Label(value=f\"Tiled feature stack progress: 0 / {n_tiles}\")\n",
        "display(stack_progress_label)\n",
        "\n",
        "for scenario in scenarios_to_stack:\n",
        "    scenario_feature_stacks_dir = join(tile_feature_stacks_dir, scenario)\n",
        "    makedirs(scenario_feature_stacks_dir, exist_ok=True)\n",
        "    scenario_features_csv = join(scenarios_model_dir, f\"{scenario}.csv\")\n",
        "    scenario_features = pd.Series.tolist(pd.read_csv(scenario_features_csv).iloc[:,0])\n",
        "\n",
        "    for tile_count in range(1, n_tiles + 1):\n",
        "      stack_path = join(scenario_feature_stacks_dir, f\"feature_stack_{scenario}_{tile_count}.npy\")\n",
        "      indices_path = join(scenario_feature_stacks_dir, f\"valid_indices_{scenario}_{tile_count}.npy\")\n",
        "      if exists(stack_path) and exists(indices_path):\n",
        "        stack_progress_index += 1\n",
        "        stack_progress_label.value = f\"Tiled feature stack progress: {stack_progress_index} / {n_tiles}\"\n",
        "        continue\n",
        "\n",
        "      # Load scenario mask tile and extract valid indices (C-order for consistent reconstruction)\n",
        "      if n_tiles == 1: mask_path = join(scenario_masks_dir, f\"{scenario}.tif\")\n",
        "      else: mask_path = join(tile_scenario_masks_dir, f\"{scenario}_{tile_count}.tif\")\n",
        "      if not exists(mask_path):\n",
        "        raise FileNotFoundError(f\"Missing mask for scenario {scenario}, tile {tile_count}: {mask_path}\")\n",
        "      mask = gdal.Open(mask_path)\n",
        "      mask_array = mask.ReadAsArray()\n",
        "      mask = None\n",
        "      valid_indices = np.where(mask_array.ravel(order='C') == 1)[0].astype(np.int32)\n",
        "      n_valid = len(valid_indices)\n",
        "      mask_array = None\n",
        "\n",
        "      # Build feature tile paths, filtering out covariates\n",
        "      if n_tiles == 1:\n",
        "        feature_tile_paths = [join(features_dir, f\"{f}.tif\") for f in scenario_features]\n",
        "      else:\n",
        "        feature_tile_paths = [join(tile_features_dir, f\"{f.split('/')[-1]}_{tile_count}.tif\") for f in scenario_features]\n",
        "      covariate_tile_set = {f\"{cov}_{tile_count}\" for cov in covariate_set}\n",
        "      valid_feature_paths = [p for p in feature_tile_paths\n",
        "                             if basename(p).replace('.tif', '') not in covariate_set\n",
        "                             and basename(p).replace('.tif', '') not in covariate_tile_set]\n",
        "\n",
        "      # Handle tiles with no valid pixels\n",
        "      if n_valid == 0:\n",
        "        n_features_total = len(valid_feature_paths)\n",
        "        if covariates_renamed: n_features_total += len(covariates_renamed)\n",
        "        np.save(stack_path, np.empty((0, n_features_total), dtype=np.float32))\n",
        "        np.save(indices_path, valid_indices)\n",
        "        stack_progress_index += 1\n",
        "        stack_progress_label.value = f\"Tiled feature stack progress: {stack_progress_index} / {n_tiles}\"\n",
        "        continue\n",
        "\n",
        "      # Verify all feature tiles exist\n",
        "      missing_features = [f for f in valid_feature_paths if not exists(f)]\n",
        "      if missing_features:\n",
        "        raise FileNotFoundError(f\"Missing feature tiles for scenario {scenario}, tile {tile_count}:\\n\" + \"\\n\".join(missing_features))\n",
        "      if not valid_feature_paths:\n",
        "        raise ValueError(f\"No non-covariate features found for scenario {scenario}, tile {tile_count}\")\n",
        "\n",
        "      # Read all features via VRT\n",
        "      vrt_options = gdal.BuildVRTOptions(separate=True)\n",
        "      vrt = gdal.BuildVRT('', valid_feature_paths, options=vrt_options)\n",
        "      if vrt is None:\n",
        "        raise RuntimeError(f\"Failed to build VRT for scenario {scenario}, tile {tile_count}\")\n",
        "      feature_cube = vrt.ReadAsArray()  # (n_features, height, width)\n",
        "      vrt = None\n",
        "\n",
        "      # Reshape to (n_pixels, n_features) C-order, extract valid pixels only\n",
        "      n_features, height, width = feature_cube.shape\n",
        "      feature_flat = feature_cube.transpose(1, 2, 0).reshape(-1, n_features, order='C')\n",
        "      feature_cube = None\n",
        "      feature_stack_valid = feature_flat[valid_indices, :].astype(np.float32)\n",
        "      feature_flat = None\n",
        "\n",
        "      # Append covariate columns\n",
        "      if covariates_renamed:\n",
        "        covariate_block = np.column_stack([np.full(n_valid, covariate_values[cov], dtype=np.float32)\n",
        "                                           for cov in covariates_renamed])\n",
        "        feature_stack_valid = np.hstack([feature_stack_valid, covariate_block])\n",
        "        covariate_block = None\n",
        "\n",
        "      # Save feature stack and valid indices\n",
        "      np.save(stack_path, feature_stack_valid)\n",
        "      np.save(indices_path, valid_indices)\n",
        "      feature_stack_valid = valid_indices = None\n",
        "\n",
        "      # Force Drive sync and verify\n",
        "      subprocess.run(['sync'], check=True)\n",
        "      last_size_stack, last_size_indices = -1, -1\n",
        "      for attempt in range(10):\n",
        "        time.sleep(5)\n",
        "        if exists(stack_path) and exists(indices_path):\n",
        "          size_stack, size_indices = os.path.getsize(stack_path), os.path.getsize(indices_path)\n",
        "          if size_stack == last_size_stack and size_indices == last_size_indices and size_stack > 0:\n",
        "            try:\n",
        "              np.load(stack_path)\n",
        "              np.load(indices_path)\n",
        "              break\n",
        "            except: pass\n",
        "          last_size_stack, last_size_indices = size_stack, size_indices\n",
        "      else: raise RuntimeError(f\"Drive sync failed: {stack_path}\")\n",
        "\n",
        "      stack_progress_index += 1\n",
        "      stack_progress_label.value = f\"Tiled feature stack progress: {stack_progress_index} / {n_tiles}\"\n",
        "\n",
        "    stack_progress_index = 0\n",
        "    scenario_progress_index += 1\n",
        "    scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index} / {len(scenarios_to_stack)}\"\n",
        "\n",
        "print(\"\\nFeature stacks complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oysAS9DI7yRg"
      },
      "source": [
        "# Predict scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y92V4nKGcyE"
      },
      "outputs": [],
      "source": [
        "# This is for testing models and scenarios, or making predictions where no\n",
        "# uncertainty metric for the variate (e.g. standard error or stdev) is available.\n",
        "# If these are available, proceed to 7_uncertainty.ipynb.\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = [f for f in os.listdir(tile_templates_dir) if f.endswith('.tif') and f.startswith('template_tile')]\n",
        "n_tiles = len(template_tile_list)\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "if n_tiles == 1: print(f\"# There is 1 template tile.\\n\")\n",
        "if n_tiles > 1: print(f\"# There are {n_tiles} template tiles.\\n\")\n",
        "\n",
        "# Collect available scenarios from the feature stack tiles directory\n",
        "scenario_stacks_list = []\n",
        "for scenario in os.listdir(tile_feature_stacks_dir):\n",
        "    stack_files = [f for f in os.listdir(join(tile_feature_stacks_dir, scenario)) if f.startswith('feature_stack_')]\n",
        "    if len(stack_files) == n_tiles:\n",
        "        scenario_stacks_list.append(scenario)\n",
        "\n",
        "print(\"# Note: If you end a runtime after the creation of many large feature stacks,\")\n",
        "print(\"# it will take time for the notebook to recognise their existence again due to\")\n",
        "print(\"# Google Drive latency issues. If the stacks do not appear here after some time,\")\n",
        "print(\"# run the feature stack section again until they do.\\n\")\n",
        "\n",
        "print(\"scenarios_to_predict = [\")\n",
        "for scenario in sorted(scenario_stacks_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRlFYaIGNXbI"
      },
      "outputs": [],
      "source": [
        "# There is 1 template tile.\n",
        "\n",
        "# Note: If you end a runtime after the creation of many large feature stacks,\n",
        "# it will time for the notebook to recognise their existence again due to\n",
        "# Google Drive latency issues. If the stacks do not appear here after some time,\n",
        "# run the feature stack section again until they do.\n",
        "\n",
        "scenarios_to_predict = [\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_no_disturbance_since_1993\",\n",
        "  \"2021_no_disturbance_since_oldgrowth_1\",\n",
        "  \"2021_no_disturbance_since_oldgrowth_2\",\n",
        "  \"2021_oldgrowth_recovery_1\",\n",
        "  \"2021_oldgrowth_recovery_2\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_no_disturbance_since_1996\",\n",
        "  \"2024_no_disturbance_since_1997\",\n",
        "  \"2024_no_disturbance_since_1998\",\n",
        "  \"2024_no_disturbance_since_1999\",\n",
        "  \"2024_no_disturbance_since_2000\",\n",
        "  \"2024_no_disturbance_since_2001\",\n",
        "  \"2024_no_disturbance_since_2002\",\n",
        "  \"2024_no_disturbance_since_2003\",\n",
        "  \"2024_no_disturbance_since_2004\",\n",
        "  \"2024_no_disturbance_since_2005\",\n",
        "  \"2024_no_disturbance_since_2006\",\n",
        "  \"2024_no_disturbance_since_2007\",\n",
        "  \"2024_no_disturbance_since_2008\",\n",
        "  \"2024_no_disturbance_since_2009\",\n",
        "  \"2024_no_disturbance_since_2010\",\n",
        "  \"2024_no_disturbance_since_2011\",\n",
        "  \"2024_no_disturbance_since_2012\",\n",
        "  \"2024_no_disturbance_since_2013\",\n",
        "  \"2024_no_disturbance_since_2014\",\n",
        "  \"2024_no_disturbance_since_2015\",\n",
        "  \"2024_no_disturbance_since_2016\",\n",
        "  \"2024_no_disturbance_since_2017\",\n",
        "  \"2024_no_disturbance_since_2018\",\n",
        "  \"2024_no_disturbance_since_2019\",\n",
        "  \"2024_no_disturbance_since_2020\",\n",
        "  \"2024_no_disturbance_since_2021\",\n",
        "  \"2024_no_disturbance_since_2022\",\n",
        "  \"2024_no_disturbance_since_2023\",\n",
        "  \"2024_no_disturbance_since_2024\",\n",
        "  \"2024_no_disturbance_since_oldgrowth_1\",\n",
        "  \"2024_no_disturbance_since_oldgrowth_2\",\n",
        "  \"2024_oldgrowth_recovery_1\",\n",
        "  \"2024_oldgrowth_recovery_2\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TibU8xTZfcuL"
      },
      "outputs": [],
      "source": [
        "# Prediction raster precision. GEDI AGBD can be set to 0, any higher is\n",
        "# spurious due to the wide prediction intervals of the source data.\n",
        "raster_precision = 0\n",
        "\n",
        "# Probabilities instead of classes IF binary classification\n",
        "predict_probabilities = False\n",
        "\n",
        "# Classification threshold IF binary classification\n",
        "classification_threshold = 0.5\n",
        "\n",
        "# Detect GPU availability and set predictor type. Note that XGBoost inference is\n",
        "# not much faster on GPU than CPU, and transferring large feature stacks from\n",
        "# CPU to GPU memory can actually make it much slower. TPU is not used, but the\n",
        "# TPU Colab runtime can provide more memory and more CPU workers.\n",
        "try:\n",
        "    test_array = cupy.array([1, 2, 3])\n",
        "    del test_array\n",
        "    predictor_type = 'gpu_predictor'\n",
        "    gpu_id, use_gpu = 0, True\n",
        "    print(\"GPU detected and accessible - using GPU to load the feature stack and to predict.\")\n",
        "except:\n",
        "    predictor_type = 'cpu_predictor'\n",
        "    gpu_id, use_gpu = -1, False\n",
        "    print(\"GPU not accessible - using CPU prediction\")\n",
        "xgb.set_config(verbosity=0, use_rmm=use_gpu)\n",
        "\n",
        "# Load model and detect type\n",
        "booster = xgb.Booster()\n",
        "booster.load_model(selected_model_json)\n",
        "model_config = json.loads(booster.save_config())\n",
        "\n",
        "objective_name = model_config['learner']['objective']['name']\n",
        "num_class = int(model_config['learner']['learner_model_param'].get('num_class', '0'))\n",
        "classification = any(obj_type in objective_name for obj_type in ['logistic', 'softprob', 'softmax'])\n",
        "multiclass = classification and num_class > 2\n",
        "if classification and multiclass: print(f\"Model type: Multiclass classification ({num_class} classes)\")\n",
        "elif classification: print(\"Model type: Binary classification\")\n",
        "else: print(\"Model type: Regression\")\n",
        "\n",
        "# Build feature_types list matching selected_features order\n",
        "feature_types = []\n",
        "for feat in selected_features:\n",
        "    if feat in categorical_columns:\n",
        "        feature_types.append('c')\n",
        "    else:\n",
        "        feature_types.append('q')\n",
        "\n",
        "# Select appropriate predictor type and set feature_types\n",
        "if classification:\n",
        "    XGBPredictor = xgb.XGBClassifier()\n",
        "    XGBPredictor.load_model(selected_model_json)\n",
        "    XGBPredictor.set_params(predictor=predictor_type, feature_types=feature_types)\n",
        "    if use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "else:\n",
        "    XGBPredictor = xgb.XGBRegressor()\n",
        "    XGBPredictor.load_model(selected_model_json)\n",
        "    XGBPredictor.set_params(predictor=predictor_type, feature_types=feature_types)\n",
        "    if use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = [file for file in os.listdir(tile_templates_dir)\n",
        "                     if file.endswith('.tif') and file[:13] == 'template_tile']\n",
        "n_tiles = len(template_tile_list)\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "template_tile = gdal.Open(join(tile_templates_dir,'template_tile_1.tif'))\n",
        "template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "template_tile = None\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "if alpha_earth: template_base_path = next(r for r in model_scenario_features_dirs if all(c not in r for c in covariates))\n",
        "else: template_base_path = template_tif_path\n",
        "\n",
        "# Progress tracking\n",
        "scenario_progress_index = 0\n",
        "scenario_progress_label = widgets.Label(f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\")\n",
        "display(scenario_progress_label)\n",
        "tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "display(tile_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_predict:\n",
        "  scenario_prediction_filename = f\"{scenario}__{selected_model}.tif\"\n",
        "  scenario_prediction_path = join(scenario_predictions_dir, scenario_prediction_filename)\n",
        "  # Skip oldgrowth _1 and _2 scenarios if merged file already exists\n",
        "  if '_oldgrowth_1' in scenario or '_oldgrowth_2' in scenario or '_oldgrowth_recovery_1' in scenario or '_oldgrowth_recovery_2' in scenario:\n",
        "    merged_scenario = scenario.replace('_1', '').replace('_2', '')\n",
        "    merged_prediction_path = join(scenario_predictions_dir, f\"{merged_scenario}__{selected_model}.tif\")\n",
        "    if exists(merged_prediction_path):\n",
        "      print(f\"Merged file exists, skipping: {scenario}\")\n",
        "      scenario_progress_index += 1\n",
        "      scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\"\n",
        "      continue\n",
        "  if not exists(scenario_prediction_path):\n",
        "    scenario_feature_stack_dir = join(tile_feature_stacks_dir, scenario)\n",
        "    n_stacks = len([f for f in os.listdir(scenario_feature_stack_dir) if f.startswith('feature_stack_')])\n",
        "    tile_cache_scenario_dir = join(tile_prediction_cache_dir, f\"{scenario}__{selected_model}\")\n",
        "    makedirs(tile_cache_scenario_dir, exist_ok=True)\n",
        "    for tile_count in range(1, n_stacks + 1):\n",
        "      scenario_tile_filename = f\"scenario_tile_{tile_count}.tif\"\n",
        "      scenario_tile_exists = scenario_tile_filename in os.listdir(tile_cache_scenario_dir)\n",
        "      if not scenario_tile_exists:\n",
        "        # Load template tile parameters\n",
        "        template_tile_path = join(tile_templates_dir, f\"template_tile_{tile_count}.tif\")\n",
        "        template_tile = gdal.Open(template_tile_path)\n",
        "        template_tile_y = template_tile.GetRasterBand(1).YSize\n",
        "        template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "        template_tile = None\n",
        "        n_pixels = template_tile_y * template_tile_x\n",
        "        # Load feature stack and valid indices\n",
        "        stack_path = join(scenario_feature_stack_dir, f\"feature_stack_{scenario}_{tile_count}.npy\")\n",
        "        indices_path = join(scenario_feature_stack_dir, f\"valid_indices_{scenario}_{tile_count}.npy\")\n",
        "        feature_stack = np.load(stack_path)\n",
        "        valid_indices = np.load(indices_path)\n",
        "        n_valid = len(valid_indices)\n",
        "        # Handle empty tiles (no valid pixels)\n",
        "        if n_valid == 0:\n",
        "          if raster_precision == 0:\n",
        "            prediction_tile = np.full((template_tile_y, template_tile_x), nodatavalue, dtype=np.int16)\n",
        "          else:\n",
        "            prediction_tile = np.full((template_tile_y, template_tile_x), nodatavalue, dtype=np.float32)\n",
        "          export_array_as_tif(prediction_tile, join(tile_cache_scenario_dir, scenario_tile_filename),\n",
        "                              template=template_tile_path, compress=False)\n",
        "          prediction_tile = None\n",
        "          tile_progress_index += 1\n",
        "          tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_stacks}\"\n",
        "          continue\n",
        "        # Load stack to GPU if available\n",
        "        if use_gpu:\n",
        "          try: feature_stack = cupy.asarray(feature_stack)\n",
        "          except Exception as e:\n",
        "              if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "                  print(\"GPU memory insufficient, switching to CPU for this tile.\")\n",
        "                  cupy.get_default_memory_pool().free_all_blocks()\n",
        "                  gc.collect()\n",
        "                  XGBPredictor.set_params(device='cpu', predictor='cpu_predictor')\n",
        "              else: raise\n",
        "        # Predict - terminate runtime if GPU prediction fails\n",
        "        try:\n",
        "            if classification and predict_probabilities and not multiclass:\n",
        "                # Get probability of class 1 for binary classification\n",
        "                prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                prediction = prediction_proba[:, 1]  # Probability of class 1\n",
        "            else:\n",
        "                if classification and not multiclass:\n",
        "                    # Use predict_proba for better accuracy in binary classification\n",
        "                    prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                    prediction = (prediction_proba[:, 1] > classification_threshold).astype(int)\n",
        "                else:\n",
        "                    prediction = XGBPredictor.predict(feature_stack)\n",
        "                    if classification:\n",
        "                        # Check if prediction is 2D (probabilities) and convert to class labels\n",
        "                        if prediction.ndim > 1 and prediction.shape[1] > 1: prediction = np.argmax(prediction, axis=1)\n",
        "                        # Ensure prediction is integer type for classification\n",
        "                        prediction = prediction.astype(int)\n",
        "        except Exception as e:\n",
        "            if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "                print(\"GPU memory insufficient for prediction. Terminating runtime to save compute units, restart with TPU.\")\n",
        "                runtime.unassign()\n",
        "            else: raise\n",
        "        feature_stack = None\n",
        "        # Reconstruct full tile from valid indices (C-order to match stacking)\n",
        "        if raster_precision == 0:\n",
        "          prediction_flat = np.full(n_pixels, nodatavalue, dtype=np.int16)\n",
        "          prediction_flat[valid_indices] = np.round(prediction).astype(np.int16)\n",
        "        else:\n",
        "          prediction_flat = np.full(n_pixels, nodatavalue, dtype=np.float32)\n",
        "          prediction_flat[valid_indices] = np.round(prediction, raster_precision)\n",
        "        prediction = valid_indices = None\n",
        "        prediction_tile = prediction_flat.reshape((template_tile_y, template_tile_x), order='C')\n",
        "        prediction_flat = None\n",
        "        # Export prediction array as .tif\n",
        "        export_array_as_tif(prediction_tile, join(tile_cache_scenario_dir, scenario_tile_filename),\n",
        "                          template=template_tile_path, compress=False)\n",
        "        prediction_tile = None\n",
        "      tile_progress_index += 1\n",
        "      tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_stacks}\"\n",
        "    # Prepare empty array for merging tiles\n",
        "    prediction_array = np.empty((0, template_tile_x))\n",
        "    # Ensure tiles are in the correct order\n",
        "    tile_files = sorted([f for f in os.listdir(tile_cache_scenario_dir) if f.endswith('.tif')],\n",
        "                        key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "    # Read each tile .tif as an array, stack, then export as a .tif\n",
        "    for fname in tile_files:\n",
        "        tile = gdal.Open(join(tile_cache_scenario_dir, fname))\n",
        "        tile_array = tile.ReadAsArray()\n",
        "        prediction_array = np.vstack((prediction_array, tile_array))\n",
        "        tile = None\n",
        "    if raster_precision == 0: prediction_array = np.round(prediction_array, raster_precision).astype(np.int16)\n",
        "    else: prediction_array = np.round(prediction_array, raster_precision)\n",
        "    export_array_as_tif(prediction_array, scenario_prediction_path, template=template_base_path)\n",
        "    # Delete scenario tile cache directory\n",
        "    shutil.rmtree(tile_cache_scenario_dir)\n",
        "  # Reset tile progress\n",
        "  tile_progress_index = 0\n",
        "  # Update scenario progress\n",
        "  scenario_progress_index += 1\n",
        "  scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\"\n",
        "print(\"\\nScenario predictions complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge oldgrowth versions by taking maximum values.\n",
        "# Version 1 uses land-use proxy for pre-Landsat undisturbed forest.\n",
        "# Version 2 removes all disturbance without the proxy.\n",
        "# Taking maximum avoids underestimation where proxy may not capture all oldgrowth characteristics.\n",
        "\n",
        "oldgrowth_v1_files = [f for f in os.listdir(scenario_predictions_dir)\n",
        "                      if ('_oldgrowth_recovery_1__' in f or '_no_disturbance_since_oldgrowth_1__' in f)\n",
        "                      and f.endswith('.tif')]\n",
        "\n",
        "if not oldgrowth_v1_files: print(\"No oldgrowth version 1 predictions found.\")\n",
        "else:\n",
        "  for v1_file in oldgrowth_v1_files:\n",
        "    # Construct version 2 and merged filenames\n",
        "    base_name = v1_file.split('__')[0][:-1] + '2'  # Replace '1' with '2'\n",
        "    rest_of_name = '__' + v1_file.split('__')[1]\n",
        "    v2_file = f\"{base_name}{rest_of_name}\"\n",
        "    merged_file = v1_file.replace('_1__', '__')\n",
        "    merged_path = join(scenario_predictions_dir, merged_file)\n",
        "    # Skip if merged file already exists\n",
        "    if exists(merged_path):\n",
        "      print(f\"Merged file already exists: {merged_file}\")\n",
        "      continue\n",
        "    v1_path = join(scenario_predictions_dir, v1_file)\n",
        "    v2_path = join(scenario_predictions_dir, v2_file)\n",
        "    # Merge or copy depending on version 2 existence\n",
        "    if exists(v2_path):\n",
        "      print(f\"Merging oldgrowth versions for {v1_file.split('__')[0]}...\")\n",
        "      oldgrowth_1 = gdal.Open(v1_path)\n",
        "      oldgrowth_1_array = oldgrowth_1.ReadAsArray()\n",
        "      oldgrowth_1 = None\n",
        "      oldgrowth_2 = gdal.Open(v2_path)\n",
        "      oldgrowth_2_array = oldgrowth_2.ReadAsArray()\n",
        "      oldgrowth_2 = None\n",
        "      merged_array = np.maximum(oldgrowth_1_array, oldgrowth_2_array)\n",
        "      export_array_as_tif(merged_array, merged_path, template=template_base_path)\n",
        "      merged_array = oldgrowth_1_array = oldgrowth_2_array = None\n",
        "      print(f\"Merged version exported: {merged_file}\")\n",
        "      # Delete originals\n",
        "      os.remove(v1_path)\n",
        "      os.remove(v2_path)\n",
        "    else:\n",
        "      shutil.copy2(v1_path, merged_path)\n",
        "      print(f\"Version 2 not found, copied version 1: {merged_file}\")\n",
        "      os.remove(v1_path)\n",
        "\n",
        "  print(\"\\nOldgrowth merging complete.\")"
      ],
      "metadata": {
        "id": "-AQSm9YKKWXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-7ctfgdoF2"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcuArY00N9Mc"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "QJ8jLRKRSfgb",
        "y_OkHaUkTCyB",
        "FmlqEP9rFapf",
        "X3w7svaUvs07",
        "xy2nfHshozVQ",
        "hUajnuo7Jxz_",
        "1T9UqJrzWECr",
        "Y0r89JLYW_xU"
      ],
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}