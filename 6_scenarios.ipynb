{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/6_scenarios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8jLRKRSfgb"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkvV803eSUQY"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEpdIjOwSiB1"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install astropy\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!apt-get install -y gdal-bin\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQDSsvGeSi4u"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "from contextlib import contextmanager\n",
        "try: import cupy # Only works on GPU runtime\n",
        "except: None\n",
        "import gc\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import json\n",
        "import math\n",
        "from os import makedirs\n",
        "from os.path import join, exists\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import psutil\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "import re\n",
        "from scipy import ndimage\n",
        "import shutil\n",
        "from shutil import copyfile, rmtree\n",
        "import subprocess\n",
        "import time\n",
        "import xgboost as xgb\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxgT3K1Sjum"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "\n",
        "feature_dir = join(base_dir, \"3_features\")\n",
        "feature_resampled_dir = join(feature_dir, \"resampled\")\n",
        "feature_binary_dir = join(feature_dir, \"binary\")\n",
        "feature_final_dir = join(feature_dir, \"final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "masks_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(scenarios_dir, exist_ok=True)\n",
        "makedirs(masks_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96FAfeyooMLz"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = [\"COMPRESS=DEFLATE\", \"PREDICTOR=3\", \"ZLEVEL=9\"]\n",
        "    else: options = []\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = vector = None\n",
        "    try:\n",
        "        raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "        vector = ogr.Open(polygon_path)\n",
        "        if not raster or not vector:\n",
        "            raise ValueError(\"Cannot open input files\")\n",
        "        layer = vector.GetLayer()\n",
        "        options = [\"ALL_TOUCHED=TRUE\"] if all_touched else []\n",
        "        if fixed:\n",
        "            gdal.RasterizeLayer(raster, [1], layer, burn_values=[fixed_value], options=options)\n",
        "        else:\n",
        "            attr_name = column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()\n",
        "            options.append(f\"ATTRIBUTE={attr_name}\")\n",
        "            gdal.RasterizeLayer(raster, [1], layer, options=options)\n",
        "    finally:\n",
        "        if raster: raster.FlushCache()\n",
        "        raster = vector = None\n",
        "\n",
        "def edge_effects(binary_array):\n",
        "  # Set smoothing kernel and precision\n",
        "  kernel, precision = Gaussian2DKernel(x_stddev=3, y_stddev=3), 2\n",
        "  # Reclassify for binary differentiation after proximity conversion\n",
        "  differentiator_array = binary_array.copy()\n",
        "  differentiator_array[differentiator_array == 1] = 10\n",
        "  # Positive proximity\n",
        "  positive_distances = ndimage.distance_transform_edt(binary_array == 0) # target pixels\n",
        "  positive_proximity_array = np.where(positive_distances > 2, 0, positive_distances) # max distance 2\n",
        "  # Negative proximity\n",
        "  negative_distances = ndimage.distance_transform_edt(binary_array == 1) # target pixels\n",
        "  negative_proximity_array = np.where(negative_distances > 2, 0, negative_distances) # max distance 2\n",
        "  # Sum proximities and differentiator\n",
        "  pixel_prox_summed =  differentiator_array + positive_proximity_array + negative_proximity_array\n",
        "  # Reclassify for better semantic understanding of pixel proximity\n",
        "  pixel_prox_reclassed = pixel_prox_summed.copy()\n",
        "  pixel_prox_reclass_table = [(0, 0, -4), (1, 1, -1), (1.4, 1.5, -2), (2, 2, -3), (10, 10, 3), (11, 11, 0), (11.4, 11.5, 1), (12, 12, 2)]\n",
        "  for min_value, max_value, new_value in pixel_prox_reclass_table:\n",
        "    pixel_prox_reclassed[(pixel_prox_reclassed >= min_value) & (pixel_prox_reclassed <= max_value)] = new_value\n",
        "  # Smooth binary array using 2D convolution\n",
        "  binary_smoothed = convolve(binary_array, kernel, boundary='extend')\n",
        "  # Sum pixel proximity and smoothed binary array\n",
        "  edge_effects_array = np.round(pixel_prox_reclassed + binary_smoothed, precision)\n",
        "  return edge_effects_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_OkHaUkTCyB"
      },
      "source": [
        "# Select model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mB0XrPNYU_I"
      },
      "outputs": [],
      "source": [
        "# Select a model\n",
        "model_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file == 'model.json':\n",
        "      print(f'selected_model = \"{subdir.split(f\"{models_dir}/\",1)[1]}\"')\n",
        "      model_exists = True\n",
        "if not model_exists:\n",
        "  print(\"No model exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbXY6iP_qjYc"
      },
      "outputs": [],
      "source": [
        "selected_model = \"agbd_tekai_250625_003858\"\n",
        "\n",
        "# Define model directories\n",
        "selected_model_dir = join(models_dir,selected_model)\n",
        "selected_model_json = join(selected_model_dir, \"model.json\")\n",
        "selected_model_descr_dir = join(selected_model_dir, \"model_description.json\")\n",
        "selected_model_dataset_path = join(selected_model_dir, f\"{selected_model}.pkl\")\n",
        "selected_model_dataset = pd.read_pickle(selected_model_dataset_path)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(selected_model_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_target = model_dataset_description[\"selected_target\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "covariates_categorised = model_dataset_description[\"covariates_categorised\"]\n",
        "selected_features = model_dataset_description[\"selected_features\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "filter_parameter = model_dataset_description[\"filter_parameter\"]\n",
        "filter_values_to_include = model_dataset_description[\"filter_values_to_include\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "covariates = [covariate[4:] for covariate in covariates_renamed]\n",
        "\n",
        "# Create scenarios model directory\n",
        "scenarios_model_dir = join(scenarios_dir, selected_model)\n",
        "makedirs(scenarios_model_dir, exist_ok=True)\n",
        "\n",
        "# Copy model_dataset_description.json\n",
        "with open(join(scenarios_model_dir, \"model_dataset_description.json\"), \"w\") as file:\n",
        "  file.write(json.dumps(model_dataset_description))\n",
        "\n",
        "# Create subdirectories\n",
        "features_dir = join(scenarios_model_dir, \"features\")\n",
        "tile_templates_dir = join(scenarios_model_dir, 'tile_templates')\n",
        "tile_features_dir = join(scenarios_model_dir, \"tile_features\")\n",
        "tile_feature_stacks_dir = join(scenarios_model_dir, \"tile_feature_stacks\")\n",
        "tile_prediction_cache_dir = join(scenarios_model_dir,\"tile_prediction_cache\")\n",
        "scenario_predictions_unmasked_dir = join(scenarios_model_dir,\"scenario_predictions_unmasked\")\n",
        "scenario_predictions_dir = join(scenarios_model_dir, \"scenario_predictions\")\n",
        "\n",
        "makedirs(features_dir, exist_ok=True)\n",
        "makedirs(tile_templates_dir, exist_ok=True)\n",
        "makedirs(tile_features_dir, exist_ok=True)\n",
        "makedirs(tile_feature_stacks_dir, exist_ok=True)\n",
        "makedirs(tile_prediction_cache_dir, exist_ok=True)\n",
        "makedirs(scenario_predictions_unmasked_dir, exist_ok=True)\n",
        "makedirs(scenario_predictions_dir, exist_ok=True)\n",
        "\n",
        "# Copy features from the final features directory\n",
        "for feature in os.listdir(feature_final_dir):\n",
        "  if feature not in os.listdir(features_dir):\n",
        "    feature_original_path = join(feature_final_dir, feature)\n",
        "    feature_copy_path = join(features_dir, feature)\n",
        "    copyfile(feature_original_path, feature_copy_path)\n",
        "print(f\"All features present in the following directory have already been copied over: {feature_final_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmlqEP9rFapf"
      },
      "source": [
        "# Define yearly scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJZoEWycn8-x"
      },
      "outputs": [],
      "source": [
        "# Yearly scenarios may include years after the model scenario if feature data is available later than GEDI data.\n",
        "# Date of the prediction is ~December 31st, e.g. '2024' is 31/12/2024, requiring features up to 2024.\n",
        "\n",
        "model_scenario_override = None # set if cannot be automatically determined from model features\n",
        "\n",
        "yearly_features = [\"forest_with_edge_effects\", \"disturbance_with_edge_effects\"]\n",
        "\n",
        "# Remove the 'fea_' prefix from each feature\n",
        "model_features = sorted([feature[4:] for feature in selected_features])\n",
        "\n",
        "# Create a list of feature years from the model's features\n",
        "model_feature_years = []\n",
        "for feature in model_features:\n",
        "  for yearly_feature in yearly_features:\n",
        "    if yearly_feature in feature:\n",
        "      model_feature_years.append(int(feature[-4:]))\n",
        "\n",
        "# Determine the model scenario from the maximum year\n",
        "if model_scenario_override != None: model_scenario = model_scenario_override\n",
        "else: model_scenario = max(model_feature_years)\n",
        "model_scenario_filename = f\"{model_scenario}.csv\"\n",
        "model_scenario_dir = join(scenarios_model_dir, model_scenario_filename)\n",
        "print(f\"The maximum year used in the model is {model_scenario}, which has been created as the first scenario.\\n\")\n",
        "print(f\"The {model_scenario} scenario feature list has been saved to:\\n {model_scenario_dir}\\n\")\n",
        "print(f\"Ensure all features in this list have been copied to:\\n{features_dir}\\n\")\n",
        "\n",
        "# Save the model scenario features as a .csv\n",
        "pd.DataFrame(model_features).to_csv(model_scenario_dir, index=False)\n",
        "\n",
        "# Determine available feature years\n",
        "final_feature_years = []\n",
        "for final_feature in os.listdir(feature_final_dir):\n",
        "  if final_feature.endswith('.tif') and final_feature[-9] == '_':\n",
        "    try: final_feature_years.append(int(final_feature[-8:-4]))\n",
        "    except: continue\n",
        "\n",
        "# Find the first and last feature years\n",
        "first_feature_year = min(final_feature_years)\n",
        "last_feature_year = max(final_feature_years)\n",
        "additional_feature_years = last_feature_year - model_scenario\n",
        "print(f\"The first available feature year is {first_feature_year} and the last is {last_feature_year}.\\n\")\n",
        "\n",
        "# Calculate the range of scenario years and minimum scenario year\n",
        "model_scenario_year_range = max(model_feature_years) - (min(model_feature_years))\n",
        "minimum_yearly_scenario = first_feature_year + model_scenario_year_range\n",
        "print(f\"The earliest scenario year that can be predicted is {minimum_yearly_scenario}.\")\n",
        "print(f\"The latest scenario year that can be predicted is {last_feature_year}.\")\n",
        "print(f\"This is based on the number of yearly features used to train the model and the total availability of features.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPd_nrFYeGJ3"
      },
      "outputs": [],
      "source": [
        "# Select constant features which are the same in every scenario, e.g. topography\n",
        "print(\"constant_features = [\")\n",
        "for feature in model_features:\n",
        "  if \"beam\" not in feature and \"sensitivity\" not in feature:\n",
        "    print(f'  \"{feature}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARVJed3vTgBv"
      },
      "outputs": [],
      "source": [
        "constant_features = [\n",
        "  \"coast_proximity_km\",\n",
        "  \"latitude\",\n",
        "  \"longitude\",\n",
        "  \"topo_dtm_smooth_aspect_cosine\",\n",
        "  \"topo_dtm_smooth_aspect_sine\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_03\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_07\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_11\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_03\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_07\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_11\",\n",
        "  \"topo_dtm_smooth_eastness\",\n",
        "  \"topo_dtm_smooth_elevation\",\n",
        "  \"topo_dtm_smooth_northness\",\n",
        "  \"topo_dtm_smooth_profile_curvature\",\n",
        "  \"topo_dtm_smooth_roughness_03\",\n",
        "  \"topo_dtm_smooth_roughness_07\",\n",
        "  \"topo_dtm_smooth_roughness_11\",\n",
        "  \"topo_dtm_smooth_slope\",\n",
        "  \"topo_dtm_smooth_stream_power_index_log10\",\n",
        "  \"topo_dtm_smooth_surface_area_ratio\",\n",
        "  \"topo_dtm_smooth_tangential_curvature\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_03\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_07\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_11\",\n",
        "  \"topo_dtm_smooth_topographic_ruggedness_index\",\n",
        "  \"topo_dtm_smooth_topographic_wetness_index\",\n",
        "  \"topo_dtm_unsmooth_aspect_cosine\",\n",
        "  \"topo_dtm_unsmooth_aspect_sine\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_03\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_07\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_11\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_03\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_07\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_11\",\n",
        "  \"topo_dtm_unsmooth_eastness\",\n",
        "  \"topo_dtm_unsmooth_elevation\",\n",
        "  \"topo_dtm_unsmooth_northness\",\n",
        "  \"topo_dtm_unsmooth_profile_curvature\",\n",
        "  \"topo_dtm_unsmooth_roughness_03\",\n",
        "  \"topo_dtm_unsmooth_roughness_07\",\n",
        "  \"topo_dtm_unsmooth_roughness_11\",\n",
        "  \"topo_dtm_unsmooth_slope\",\n",
        "  \"topo_dtm_unsmooth_stream_power_index_log10\",\n",
        "  \"topo_dtm_unsmooth_surface_area_ratio\",\n",
        "  \"topo_dtm_unsmooth_tangential_curvature\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_03\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_07\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_11\",\n",
        "  \"topo_dtm_unsmooth_topographic_ruggedness_index\",\n",
        "  \"topo_dtm_unsmooth_topographic_wetness_index\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg4XvxT5foVR"
      },
      "outputs": [],
      "source": [
        "# Note that areas that were forested in a historic yearly scenario but were reservoirs\n",
        "# or flooded at the time topographic feature data was collected (~2014 for GLO-30 DEM)\n",
        "# will be predicted based on a flat topography at the elevation of the water's surface.\n",
        "\n",
        "# Set scenario features as all non-constant features\n",
        "scenario_features = sorted(list(set(model_features) - set(constant_features)))\n",
        "\n",
        "# Create feature lists for all possible yearly scenarios\n",
        "for yearly_scenario in range(minimum_yearly_scenario, last_feature_year +1):\n",
        "  year_difference = model_scenario - yearly_scenario\n",
        "  yearly_scenario_features = []\n",
        "  for scenario_feature in scenario_features:\n",
        "    try:\n",
        "      year_change = int(scenario_feature[-4:]) - year_difference\n",
        "      yearly_scenario_feature = scenario_feature[:-4] + str(year_change)\n",
        "      yearly_scenario_features.append(yearly_scenario_feature)\n",
        "    except: yearly_scenario_features.append(scenario_feature)\n",
        "  # Compile yearly features and save as a .csv\n",
        "  yearly_scenario_features = sorted(yearly_scenario_features + constant_features)\n",
        "  yearly_scenario_filename = f\"{yearly_scenario}.csv\"\n",
        "  yearly_scenario_dir = join(scenarios_model_dir,yearly_scenario_filename)\n",
        "  pd.DataFrame(yearly_scenario_features).to_csv(yearly_scenario_dir, index=False)\n",
        "\n",
        "# Open the most recent yearly scenario feature list\n",
        "most_recent_scenario_csv = join(scenarios_model_dir,f\"{last_feature_year}.csv\")\n",
        "most_recent_scenario_features = pd.Series.tolist(pd.read_csv(most_recent_scenario_csv).iloc[:,0])\n",
        "\n",
        "print(f\"Lists of features for all possible yearly scenarios have been exported to {scenarios_model_dir}/.\")\n",
        "print(f\"Ensure all features in these lists have been copied to:\\n{features_dir}\\n\")\n",
        "\n",
        "# Create a 'no disturbance' feature for alternate scenarios.\n",
        "# Assumes the minimum possible value is present in the first scenario year.\n",
        "minimum_disturbance_name = f\"disturbance_with_edge_effects_0000\"\n",
        "minimum_disturbance_path = join(features_dir, f\"{minimum_disturbance_name}.tif\")\n",
        "if not exists(minimum_disturbance_path):\n",
        "  example_disturbance = join(features_dir, f\"disturbance_with_edge_effects_{first_feature_year}.tif\")\n",
        "  example_disturbance_array = gdal.Open(example_disturbance).ReadAsArray()\n",
        "  minimum_disturbance_value = example_disturbance_array.min()\n",
        "  minimum_disturbance_array = np.where(example_disturbance_array, minimum_disturbance_value, minimum_disturbance_value)\n",
        "  export_array_as_tif(minimum_disturbance_array, minimum_disturbance_path, template = example_disturbance)\n",
        "  print(f\"The minimum disturbance value is {minimum_disturbance_value}\\n,\")\n",
        "  print(f\"which has been used to create the 'minimum disturbance' feature {minimum_disturbance_name}.\")\n",
        "else: print(f\"The minimum disturbance feature {minimum_disturbance_name} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3w7svaUvs07"
      },
      "source": [
        "# Define alternate scenarios (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmcpUriAFfZ6"
      },
      "source": [
        "## Disturbance requirement tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWA1QsHGQw_P"
      },
      "outputs": [],
      "source": [
        "# Tool to identify required scenarios for forest AGBD disturbance mapping\n",
        "use_tool = True\n",
        "\n",
        "def select_forest_scenarios():\n",
        "    # Initialize variables\n",
        "    calculation_note = None\n",
        "    is_specific_effects_calculation = False\n",
        "\n",
        "    # Print header\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"\\nFOREST DISTURBANCE SELECTOR\\n\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Step 1: Collect disturbance type\n",
        "    disturbance_prompt = (\n",
        "        \"Select disturbance type:\\n\"\n",
        "        \"1. Degradation\\n\"\n",
        "        \"2. Deforestation\\n\"\n",
        "        \"3. Disturbance (degradation + deforestation)\\n\\n\"\n",
        "        \"Enter your choice (1-3): \"\n",
        "    )\n",
        "    disturbance_type = input(disturbance_prompt)\n",
        "\n",
        "    # Step 2: Year of interest\n",
        "    print(\"\\n\")\n",
        "    year_of_interest = input(\"Enter year of interest: \")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Step 3: Collect baseline type\n",
        "    baseline_prompt = (\n",
        "        \"Select baseline type:\\n\"\n",
        "        \"1. Since oldgrowth state\\n\"\n",
        "        \"2. Since a baseline year\\n\"\n",
        "        \"3. Effect before first available disturbance year\\n\"\n",
        "        \"4. Effect of a specific year\\n\"\n",
        "        \"5. Effect in the same year\\n\\n\"\n",
        "        \"Enter your choice (1-5): \"\n",
        "    )\n",
        "    baseline_type = input(baseline_prompt)\n",
        "\n",
        "    # Initialize result variables\n",
        "    selected_difference = None\n",
        "    scenario_pair = None\n",
        "    other_requirements = []\n",
        "    is_specific_effects_calculation = (baseline_type in [\"3\", \"4\", \"5\"])\n",
        "\n",
        "    # Process based on baseline type\n",
        "    if baseline_type == \"1\":  # Since oldgrowth state\n",
        "        if disturbance_type == \"1\":  # Degradation\n",
        "            selected_difference = f\"{year_of_interest}_degradation_since_oldgrowth\"\n",
        "            scenario_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth\")\n",
        "\n",
        "        elif disturbance_type == \"2\":  # Deforestation\n",
        "            selected_difference = f\"{year_of_interest}_deforestation_since_oldgrowth\"\n",
        "            # This requires both degradation_since_oldgrowth and disturbance_since_oldgrowth\n",
        "            deg_oldgrowth_diff = f\"{year_of_interest}_degradation_since_oldgrowth\"\n",
        "            deg_oldgrowth_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth\")\n",
        "            dist_oldgrowth_diff = f\"{year_of_interest}_disturbance_since_oldgrowth\"\n",
        "            dist_oldgrowth_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth_all_land\")\n",
        "\n",
        "            other_requirements.append((deg_oldgrowth_diff, deg_oldgrowth_pair))\n",
        "            other_requirements.append((dist_oldgrowth_diff, dist_oldgrowth_pair))\n",
        "            calculation_note = f\"Calculated as {dist_oldgrowth_diff} - {deg_oldgrowth_diff}\"\n",
        "\n",
        "        elif disturbance_type == \"3\":  # Total disturbance (degradation + deforestation)\n",
        "            selected_difference = f\"{year_of_interest}_disturbance_since_oldgrowth\"\n",
        "            scenario_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth_all_land\")\n",
        "\n",
        "    elif baseline_type == \"2\":  # Since a baseline year\n",
        "        # Get and validate baseline year\n",
        "        print(\"\\n\")\n",
        "        baseline_year = input(\"Enter baseline year (must be before year of interest): \")\n",
        "        if int(baseline_year) >= int(year_of_interest):\n",
        "            print(\"\\nError: Baseline year must be before year of interest\")\n",
        "            return None\n",
        "\n",
        "        baseline_year_plus1 = str(int(baseline_year) + 1)\n",
        "\n",
        "        if disturbance_type == \"1\":  # Degradation\n",
        "            selected_difference = f\"{year_of_interest}_degradation_since_{baseline_year_plus1}\"\n",
        "            scenario_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{baseline_year_plus1}\")\n",
        "\n",
        "        elif disturbance_type == \"2\":  # Deforestation\n",
        "            selected_difference = f\"{year_of_interest}_deforestation_since_{baseline_year_plus1}\"\n",
        "            # This requires both degradation_since and disturbance_since for the same period\n",
        "            deg_since_diff = f\"{year_of_interest}_degradation_since_{baseline_year_plus1}\"\n",
        "            deg_since_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{baseline_year_plus1}\")\n",
        "            dist_since_diff = f\"{year_of_interest}_disturbance_since_{baseline_year_plus1}\"\n",
        "            dist_since_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{baseline_year_plus1}\")\n",
        "\n",
        "            other_requirements.append((deg_since_diff, deg_since_pair))\n",
        "            other_requirements.append((dist_since_diff, dist_since_pair))\n",
        "            calculation_note = f\"Calculated as {dist_since_diff} - {deg_since_diff}\"\n",
        "\n",
        "        elif disturbance_type == \"3\":  # Total disturbance since baseline\n",
        "            selected_difference = f\"{year_of_interest}_disturbance_since_{baseline_year_plus1}\"\n",
        "            scenario_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{baseline_year_plus1}\")\n",
        "\n",
        "    elif baseline_type == \"3\":  # Effect before first available disturbance year\n",
        "        # Get and validate first available disturbance year\n",
        "        print(\"\\n\")\n",
        "        first_year = input(\"Enter first available disturbance year in your data: \")\n",
        "        if int(first_year) >= int(year_of_interest):\n",
        "            print(\"\\nError: first available disturbance year must be before year of interest\")\n",
        "            return None\n",
        "\n",
        "        if disturbance_type == \"1\":  # Degradation\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_degradation_before_{first_year}\"\n",
        "\n",
        "            # Define required difference components\n",
        "            since_first = f\"{year_of_interest}_degradation_since_{first_year}\"\n",
        "            since_first_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{first_year}\")\n",
        "\n",
        "            oldgrowth_difference = f\"{year_of_interest}_degradation_since_oldgrowth\"\n",
        "            oldgrowth_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth\")\n",
        "\n",
        "            other_requirements.append((since_first, since_first_pair))\n",
        "            other_requirements.append((oldgrowth_difference, oldgrowth_pair))\n",
        "            calculation_note = f\"Calculated as {oldgrowth_difference} - {since_first}\"\n",
        "\n",
        "        elif disturbance_type == \"2\":  # Deforestation\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_deforestation_before_{first_year}\"\n",
        "\n",
        "            # Define required difference components for deforestation_since_first\n",
        "            deg_since_first = f\"{year_of_interest}_degradation_since_{first_year}\"\n",
        "            deg_since_first_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{first_year}\")\n",
        "            dist_since_first = f\"{year_of_interest}_disturbance_since_{first_year}\"\n",
        "            dist_since_first_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{first_year}\")\n",
        "\n",
        "            # Define required difference components for deforestation_since_oldgrowth\n",
        "            deg_oldgrowth_diff = f\"{year_of_interest}_degradation_since_oldgrowth\"\n",
        "            deg_oldgrowth_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth\")\n",
        "            dist_oldgrowth_diff = f\"{year_of_interest}_disturbance_since_oldgrowth\"\n",
        "            dist_oldgrowth_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth_all_land\")\n",
        "\n",
        "            other_requirements.append((deg_since_first, deg_since_first_pair))\n",
        "            other_requirements.append((dist_since_first, dist_since_first_pair))\n",
        "            other_requirements.append((deg_oldgrowth_diff, deg_oldgrowth_pair))\n",
        "            other_requirements.append((dist_oldgrowth_diff, dist_oldgrowth_pair))\n",
        "\n",
        "            defor_since_first = f\"{year_of_interest}_deforestation_since_{first_year}\"\n",
        "            defor_since_oldgrowth = f\"{year_of_interest}_deforestation_since_oldgrowth\"\n",
        "            calculation_note = f\"Calculated as {defor_since_oldgrowth} - {defor_since_first}\"\n",
        "\n",
        "        elif disturbance_type == \"3\":  # Total disturbance before first year\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_disturbance_before_{first_year}\"\n",
        "\n",
        "            # Define required difference components\n",
        "            since_first = f\"{year_of_interest}_disturbance_since_{first_year}\"\n",
        "            since_first_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{first_year}\")\n",
        "\n",
        "            oldgrowth_difference = f\"{year_of_interest}_disturbance_since_oldgrowth\"\n",
        "            oldgrowth_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth_all_land\")\n",
        "\n",
        "            other_requirements.append((since_first, since_first_pair))\n",
        "            other_requirements.append((oldgrowth_difference, oldgrowth_pair))\n",
        "            calculation_note = f\"Calculated as {oldgrowth_difference} - {since_first}\"\n",
        "\n",
        "    elif baseline_type == \"4\":  # Effect of a specific year\n",
        "        # Get and validate specific year\n",
        "        print(\"\\n\")\n",
        "        specific_year = input(\"Enter the specific year whose effect you want to measure: \")\n",
        "        if int(specific_year) >= int(year_of_interest):\n",
        "            print(\"\\nError: Specific year must be before year of interest\")\n",
        "            return None\n",
        "\n",
        "        specific_year_plus1 = str(int(specific_year) + 1)\n",
        "\n",
        "        if disturbance_type == \"1\":  # Degradation\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_degradation_in_{specific_year}\"\n",
        "\n",
        "            # Define required difference components\n",
        "            since_specific = f\"{year_of_interest}_degradation_since_{specific_year}\"\n",
        "            since_specific_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{specific_year}\")\n",
        "\n",
        "            since_after = f\"{year_of_interest}_degradation_since_{specific_year_plus1}\"\n",
        "            since_after_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{specific_year_plus1}\")\n",
        "\n",
        "            other_requirements.append((since_specific, since_specific_pair))\n",
        "            other_requirements.append((since_after, since_after_pair))\n",
        "            calculation_note = f\"Calculated as {since_specific} - {since_after}\"\n",
        "\n",
        "        elif disturbance_type == \"2\":  # Deforestation\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_deforestation_in_{specific_year}\"\n",
        "\n",
        "            # Define components for since_specific\n",
        "            deg_since_specific = f\"{year_of_interest}_degradation_since_{specific_year}\"\n",
        "            deg_since_specific_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{specific_year}\")\n",
        "            dist_since_specific = f\"{year_of_interest}_disturbance_since_{specific_year}\"\n",
        "            dist_since_specific_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{specific_year}\")\n",
        "\n",
        "            # Define components for since_after\n",
        "            deg_since_after = f\"{year_of_interest}_degradation_since_{specific_year_plus1}\"\n",
        "            deg_since_after_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{specific_year_plus1}\")\n",
        "            dist_since_after = f\"{year_of_interest}_disturbance_since_{specific_year_plus1}\"\n",
        "            dist_since_after_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{specific_year_plus1}\")\n",
        "\n",
        "            other_requirements.append((deg_since_specific, deg_since_specific_pair))\n",
        "            other_requirements.append((dist_since_specific, dist_since_specific_pair))\n",
        "            other_requirements.append((deg_since_after, deg_since_after_pair))\n",
        "            other_requirements.append((dist_since_after, dist_since_after_pair))\n",
        "\n",
        "            defor_since_specific = f\"{year_of_interest}_deforestation_since_{specific_year}\"\n",
        "            defor_since_after = f\"{year_of_interest}_deforestation_since_{specific_year_plus1}\"\n",
        "            calculation_note = f\"Calculated as {defor_since_specific} - {defor_since_after}\"\n",
        "\n",
        "        elif disturbance_type == \"3\":  # Total disturbance effect\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_disturbance_in_{specific_year}\"\n",
        "\n",
        "            # Define required difference components\n",
        "            since_specific = f\"{year_of_interest}_disturbance_since_{specific_year}\"\n",
        "            since_specific_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{specific_year}\")\n",
        "\n",
        "            since_after = f\"{year_of_interest}_disturbance_since_{specific_year_plus1}\"\n",
        "            since_after_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{specific_year_plus1}\")\n",
        "\n",
        "            other_requirements.append((since_specific, since_specific_pair))\n",
        "            other_requirements.append((since_after, since_after_pair))\n",
        "            calculation_note = f\"Calculated as {since_specific} - {since_after}\"\n",
        "\n",
        "    elif baseline_type == \"5\":  # Effect in the same year\n",
        "        if disturbance_type == \"1\":  # Degradation\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_degradation_in_{year_of_interest}\"\n",
        "\n",
        "            # This is a copy operation\n",
        "            source_difference = f\"{year_of_interest}_degradation_since_{year_of_interest}\"\n",
        "            source_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{year_of_interest}\")\n",
        "\n",
        "            other_requirements.append((source_difference, source_pair))\n",
        "            calculation_note = f\"Copy and rename {source_difference}\"\n",
        "\n",
        "        elif disturbance_type == \"2\":  # Deforestation\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_deforestation_in_{year_of_interest}\"\n",
        "\n",
        "            # Define components for same-year deforestation\n",
        "            deg_same_year = f\"{year_of_interest}_degradation_since_{year_of_interest}\"\n",
        "            deg_same_year_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{year_of_interest}\")\n",
        "            dist_same_year = f\"{year_of_interest}_disturbance_since_{year_of_interest}\"\n",
        "            dist_same_year_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{year_of_interest}\")\n",
        "\n",
        "            other_requirements.append((deg_same_year, deg_same_year_pair))\n",
        "            other_requirements.append((dist_same_year, dist_same_year_pair))\n",
        "\n",
        "            defor_same_year = f\"{year_of_interest}_deforestation_since_{year_of_interest}\"\n",
        "            calculation_note = f\"Copy and rename {defor_same_year}\"\n",
        "\n",
        "        elif disturbance_type == \"3\":  # Total disturbance in same year\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_disturbance_in_{year_of_interest}\"\n",
        "\n",
        "            # This is a copy operation\n",
        "            source_difference = f\"{year_of_interest}_disturbance_since_{year_of_interest}\"\n",
        "            source_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{year_of_interest}\")\n",
        "\n",
        "            other_requirements.append((source_difference, source_pair))\n",
        "            calculation_note = f\"Copy and rename {source_difference}\"\n",
        "\n",
        "    else:\n",
        "        print(\"\\nError: Invalid baseline type selection.\")\n",
        "        return None\n",
        "\n",
        "    # Build result display\n",
        "    result_text = []\n",
        "    result_text.append(\"\\n\" + \"=\"*50)\n",
        "    result_text.append(f\"\\nSELECTED DIFFERENCE: {selected_difference}\")\n",
        "\n",
        "    # Add required differences and their scenario pairs\n",
        "    if other_requirements:\n",
        "        result_text.append(f\"\\nRequired difference and scenario pairs:\")\n",
        "        for diff, pair in other_requirements:\n",
        "            result_text.append(f\"'{diff}' {pair}\")\n",
        "\n",
        "        if calculation_note:\n",
        "            result_text.append(f\"\\n{calculation_note}\")\n",
        "    # Only include scenario pair when no other requirements\n",
        "    elif scenario_pair:\n",
        "        result_text.append(f\"\\nScenario pair required: {scenario_pair}\")\n",
        "\n",
        "    result_text.append(\"\\n\" + \"=\"*50)\n",
        "\n",
        "    # Print results with fewer new lines\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"\\n\".join(result_text))\n",
        "\n",
        "    # Return appropriate values\n",
        "    if is_specific_effects_calculation:\n",
        "        return selected_difference, other_requirements\n",
        "    else:\n",
        "        return selected_difference, scenario_pair, other_requirements\n",
        "\n",
        "# Run the function\n",
        "if use_tool:\n",
        "  if __name__ == \"__main__\":\n",
        "      select_forest_scenarios()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKfhj6mVgRvp"
      },
      "source": [
        "## No degradation scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rK4JtDSAtn6"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios remove degradation for specific time ranges\n",
        "\n",
        "# Define ranges for 'no degradation' scenarios\n",
        "define_no_degradation_scenarios = True\n",
        "\n",
        "# No degradation ranges as tuples of (start_year, end_year)\n",
        "no_degradation_ranges = [\n",
        "    (1993, 2021),\n",
        "    (1996, 2024),\n",
        "    # (1997, 2024),\n",
        "    # (1998, 2024),\n",
        "    # (1999, 2024),\n",
        "    # (2000, 2024),\n",
        "    # (2001, 2024),\n",
        "    # (2002, 2024),\n",
        "    # (2003, 2024),\n",
        "    # (2004, 2024),\n",
        "    # (2005, 2024),\n",
        "    # (2006, 2024),\n",
        "    # (2007, 2024),\n",
        "    # (2008, 2024),\n",
        "    # (2009, 2024),\n",
        "    # (2010, 2024),\n",
        "    # (2011, 2024),\n",
        "    # (2012, 2024),\n",
        "    # (2013, 2024),\n",
        "    # (2014, 2024),\n",
        "    # (2015, 2024),\n",
        "    # (2016, 2024),\n",
        "    # (2017, 2024),\n",
        "    # (2018, 2024),\n",
        "    # (2019, 2024),\n",
        "    # (2020, 2024),\n",
        "    # (2021, 2024),\n",
        "    # (2022, 2024),\n",
        "    # (2023, 2024),\n",
        "    # (2024, 2024),\n",
        "]\n",
        "\n",
        "# Create a feature list for 'no degradation' scenarios\n",
        "if define_no_degradation_scenarios:\n",
        "  for start_year, end_year in no_degradation_ranges:\n",
        "    assert end_year <= last_feature_year, \"End years must be at or before the last feature year.\"\n",
        "    assert end_year >= minimum_yearly_scenario, \"End years must be at or after the minimum yearly scenario.\"\n",
        "    assert start_year >= first_feature_year, \"Start years must be at or after the first feature year.\"\n",
        "    assert start_year >= end_year - model_scenario_year_range, \"Start years must be within the model scenario range of the end year.\"\n",
        "    assert start_year <= end_year, \"The start year must less than or equal to the end year.\"\n",
        "\n",
        "    # Determine base features based on the end year of the range\n",
        "    scenario_features_csv = join(scenarios_model_dir, f\"{end_year}.csv\")\n",
        "    base_features = pd.Series.tolist(pd.read_csv(scenario_features_csv).iloc[:,0])\n",
        "\n",
        "    no_degradation_features = []\n",
        "    for scenario_feature in base_features:\n",
        "      if \"disturbance_with_edge_effects\" in scenario_feature:\n",
        "        scenario_feature_year = int(scenario_feature[-4:])\n",
        "        # Replace disturbance feature if it falls within the specified range\n",
        "        if scenario_feature_year >= start_year:\n",
        "          no_degradation_features.append(minimum_disturbance_name)\n",
        "        else:\n",
        "          no_degradation_features.append(scenario_feature)\n",
        "      else:\n",
        "        no_degradation_features.append(scenario_feature)\n",
        "\n",
        "    no_degradation_scenario_filename = f\"{end_year}_no_degradation_since_{start_year}.csv\"\n",
        "    no_degradation_scenario_path = join(scenarios_model_dir, no_degradation_scenario_filename)\n",
        "    pd.DataFrame(no_degradation_features).to_csv(no_degradation_scenario_path, index=False)\n",
        "    print(f\"Feature list for a scenario without degradation between {start_year} and {end_year} exported to {no_degradation_scenario_filename}.\")\n",
        "else:\n",
        "  print(\"The 'no degradation' scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN8AwXXAbxAn"
      },
      "source": [
        "## No disturbance scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQmk9oahbwd-"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios remove both disturbance and deforestation for specific time ranges\n",
        "\n",
        "# Define ranges for 'no disturbance' scenarios\n",
        "define_no_disturbance_scenarios = True\n",
        "\n",
        "# No disturbance ranges as tuples of (start_year, end_year)\n",
        "no_disturbance_ranges = [\n",
        "    (1993, 2021),\n",
        "    (1996, 2024),\n",
        "    (1997, 2024),\n",
        "    (1998, 2024),\n",
        "    (1999, 2024),\n",
        "    (2000, 2024),\n",
        "    (2001, 2024),\n",
        "    (2002, 2024),\n",
        "    (2003, 2024),\n",
        "    (2004, 2024),\n",
        "    (2005, 2024),\n",
        "    (2006, 2024),\n",
        "    (2007, 2024),\n",
        "    (2008, 2024),\n",
        "    (2009, 2024),\n",
        "    (2010, 2024),\n",
        "    (2011, 2024),\n",
        "    (2012, 2024),\n",
        "    (2013, 2024),\n",
        "    (2014, 2024),\n",
        "    (2015, 2024),\n",
        "    (2016, 2024),\n",
        "    (2017, 2024),\n",
        "    (2018, 2024),\n",
        "    (2019, 2024),\n",
        "    (2020, 2024),\n",
        "    (2021, 2024),\n",
        "    (2022, 2024),\n",
        "    (2023, 2024),\n",
        "    (2024, 2024),\n",
        "]\n",
        "\n",
        "# Create a feature list for 'no disturbance' scenarios\n",
        "if define_no_disturbance_scenarios:\n",
        "  for start_year, end_year in no_disturbance_ranges:\n",
        "    assert end_year <= last_feature_year, \"End years must be at or before the last feature year.\"\n",
        "    assert end_year >= minimum_yearly_scenario, \"End years must be at or after the minimum yearly scenario.\"\n",
        "    assert start_year >= first_feature_year, \"Start years must be at or after the first feature year.\"\n",
        "    assert start_year >= end_year - model_scenario_year_range, \"Start years must be within the model scenario range of the end year.\"\n",
        "    assert start_year <= end_year, \"The start year must less than or equal to the end year.\"\n",
        "\n",
        "    # Determine base features based on the end year of the range\n",
        "    scenario_features_csv = join(scenarios_model_dir, f\"{end_year}.csv\")\n",
        "    base_features = pd.Series.tolist(pd.read_csv(scenario_features_csv).iloc[:,0])\n",
        "\n",
        "    no_disturbance_features = []\n",
        "    for scenario_feature in base_features:\n",
        "      if \"disturbance_with_edge_effects\" in scenario_feature:\n",
        "        scenario_feature_year = int(scenario_feature[-4:])\n",
        "        # Replace disturbance feature if it falls within the specified range\n",
        "        if scenario_feature_year >= start_year: no_disturbance_features.append(minimum_disturbance_name)\n",
        "        else: no_disturbance_features.append(scenario_feature)\n",
        "      elif \"forest_with_edge_effects\" in scenario_feature:\n",
        "        scenario_feature_year = int(scenario_feature[-4:])\n",
        "        forest_edge_effects_year = start_year - 1\n",
        "        alternate_forest_edge_effects = scenario_feature[:-4] + str(forest_edge_effects_year)\n",
        "        # Replace forest feature if it falls within the specified range\n",
        "        if scenario_feature_year > forest_edge_effects_year: no_disturbance_features.append(alternate_forest_edge_effects)\n",
        "        else: no_disturbance_features.append(scenario_feature)\n",
        "      else: no_disturbance_features.append(scenario_feature)\n",
        "\n",
        "    no_disturbance_scenario_filename = f\"{end_year}_no_disturbance_since_{start_year}.csv\"\n",
        "    no_disturbance_scenario_path = join(scenarios_model_dir, no_disturbance_scenario_filename)\n",
        "    pd.DataFrame(no_disturbance_features).to_csv(no_disturbance_scenario_path, index=False)\n",
        "    print(f\"Feature list for a scenario without disturbance between {start_year} and {end_year} exported to {no_disturbance_scenario_filename}.\")\n",
        "else:\n",
        "  print(\"The 'no disturbance' scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTBj-edFgbKY"
      },
      "source": [
        "## Oldgrowth scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5355gl2awb3"
      },
      "outputs": [],
      "source": [
        "# List of land-use features. All but one are 'redundant',\n",
        "# One should be selected as 'oldgrowth_feature'.\n",
        "print(\"oldgrowth_redundant_features = [\")\n",
        "for feature in model_features:\n",
        "  if \"lu_\" in feature:\n",
        "    print(f'  \"{feature}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6u03s8VHfN8"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios simulate old-growth forest using a proxy area specified by the user.\n",
        "# Forest extent (i.e. 'no deforestation') can be set from yearly feature, or all historic / potential forest area.\n",
        "# A second version of the scenario without the oldgrowth proxy will be created for comparison.\n",
        "# In rare cases, areas with unexpectedly high AGBD will have a lower AGBD estimate with the oldgrowth proxy,\n",
        "# and the highest estimate will be used for that pixel.\n",
        "\n",
        "define_oldgrowth_scenarios = True\n",
        "oldgrowth_yearly_scenarios = [\n",
        "    2021,\n",
        "    last_feature_year\n",
        "]\n",
        "\n",
        "for year in oldgrowth_yearly_scenarios:\n",
        "  assert year in final_feature_years, \"Years in 'oldgrowth_yearly_scenarios' must be available in the final yearly features.\"\n",
        "\n",
        "# oldgrowth_all_land will be created for each year in oldgrowth_yearly_scenarios\n",
        "simulate_oldgrowth_all_land = True\n",
        "\n",
        "# The feature that best indicates oldgrowth to the model, e.g. certain protected areas\n",
        "# This will be modified to cover the entire template area for the old-growth scenarios\n",
        "oldgrowth_feature = 'lu_old-growth_protected_areas_with_edge_effects'\n",
        "\n",
        "# Some features may confound the old-growth proxy, e.g. protected areas that are not known to be old-growth\n",
        "# These will be removed for the old-growth scenarios\n",
        "oldgrowth_redundant_features = [\n",
        "  \"lu_ais_with_edge_effects\",\n",
        "  \"lu_berkelah_jerantut_with_edge_effects\",\n",
        "  \"lu_berkelah_kuantan_with_edge_effects\",\n",
        "  \"lu_berkelah_temerloh_with_edge_effects\",\n",
        "  \"lu_old-growth_protected_areas_with_edge_effects\",\n",
        "  \"lu_remen_chereh_with_edge_effects\",\n",
        "  \"lu_tekai_tembeling_with_edge_effects\",\n",
        "  \"lu_tekam_with_edge_effects\",\n",
        "  \"lu_yong_lipis_with_edge_effects\",\n",
        "  \"lu_yong_with_edge_effects\",\n",
        "]\n",
        "\n",
        "if define_oldgrowth_scenarios:\n",
        "  # Expand the oldgrowth feature to the entire template area\n",
        "  oldgrowth_feature_all_dir = join(features_dir, f\"{oldgrowth_feature}_all.tif\")\n",
        "  if not exists(oldgrowth_feature_all_dir):\n",
        "    oldgrowth_feature_dir = join(features_dir, f\"{oldgrowth_feature}.tif\")\n",
        "    oldgrowth_feature_array = gdal.Open(oldgrowth_feature_dir).ReadAsArray()\n",
        "    oldgrowth_feature_max_value = oldgrowth_feature_array.max()\n",
        "    print(f\"The maximum value for the oldgrowth feature '{oldgrowth_feature}' is {oldgrowth_feature_max_value}.\")\n",
        "    oldgrowth_feature_all_array = np.where(oldgrowth_feature_array, oldgrowth_feature_max_value, oldgrowth_feature_max_value)\n",
        "    oldgrowth_feature_all_dir = join(features_dir, f\"{oldgrowth_feature}_all.tif\")\n",
        "    export_array_as_tif(oldgrowth_feature_all_array, oldgrowth_feature_all_dir, template = oldgrowth_feature_dir)\n",
        "    print(f\"The oldgrowth proxy {oldgrowth_feature} has been expanded to the entire scenario area\")\n",
        "    print(f\"And exported to {oldgrowth_feature_all_dir}\")\n",
        "  else: print(f\"The oldgrowth feature '{oldgrowth_feature}_all.tif' already exists.\\n\")\n",
        "\n",
        "  # Remove the redundant features from the oldgrowth template area\n",
        "  for redundant_feature in oldgrowth_redundant_features:\n",
        "    redundant_feature_none_dir = join(features_dir, f\"{redundant_feature}_none.tif\")\n",
        "    if not exists(redundant_feature_none_dir):\n",
        "      redundant_feature_dir = join(features_dir, f\"{redundant_feature}.tif\")\n",
        "      redundant_feature_array = gdal.Open(redundant_feature_dir).ReadAsArray()\n",
        "      redundant_feature_min_value = redundant_feature_array.min()\n",
        "      print(f\"The minimum value for the redundant feature {redundant_feature} is {redundant_feature_min_value}.\")\n",
        "      redundant_feature_none_array = np.where(redundant_feature_array, redundant_feature_min_value, redundant_feature_min_value)\n",
        "      export_array_as_tif(redundant_feature_none_array, redundant_feature_none_dir, redundant_feature_dir)\n",
        "      print(f\"The oldgrowth redundant feature {oldgrowth_feature} has been removed from the entire template area\")\n",
        "      print(f\"And exported to {oldgrowth_feature_all_dir}.\")\n",
        "    else: print(f\"The oldgrowth redundant feature '{redundant_feature}_none.tif' already exists.\\n\")\n",
        "\n",
        "  # Generate 'oldgrowth' scenarios for each specified year (i.e. forest extent in that year)\n",
        "  for year in oldgrowth_yearly_scenarios:\n",
        "    # Two oldgrowth feature sets created\n",
        "    oldgrowth_features_1 = []\n",
        "    oldgrowth_features_2 = []\n",
        "    old_growth_scenario_year_diff = last_feature_year - year\n",
        "    for scenario_feature in most_recent_scenario_features:\n",
        "      if \"disturbance_with_edge_effects\" in scenario_feature:\n",
        "        feature_1 = feature_2 = minimum_disturbance_name\n",
        "      elif \"forest_with_edge_effects\" in scenario_feature:\n",
        "        scenario_feature_year = int(scenario_feature[-4:])\n",
        "        new_year = (scenario_feature_year-old_growth_scenario_year_diff > first_feature_year) and \\\n",
        "                  f\"forest_with_edge_effects_{scenario_feature_year-old_growth_scenario_year_diff}\" or \\\n",
        "                  f\"forest_with_edge_effects_{first_feature_year}\"\n",
        "        feature_1 = feature_2 = new_year\n",
        "      # Only change oldgrowth feature and redundant feature in one version\n",
        "      elif scenario_feature == oldgrowth_feature:\n",
        "        feature_1 = f\"{scenario_feature}_all\"\n",
        "        feature_2 = scenario_feature\n",
        "      elif scenario_feature in oldgrowth_redundant_features:\n",
        "        feature_1 = f\"{scenario_feature}_none\"\n",
        "        feature_2 = scenario_feature\n",
        "      else: feature_1 = feature_2 = scenario_feature\n",
        "      oldgrowth_features_1.append(feature_1)\n",
        "      oldgrowth_features_2.append(feature_2)\n",
        "\n",
        "    # Compare feature lists and save appropriate CSVs\n",
        "    if oldgrowth_features_1 == oldgrowth_features_2:\n",
        "      # Lists are identical, only save version 1\n",
        "      filename = f\"{year}_oldgrowth_1.csv\"\n",
        "      pd.DataFrame(oldgrowth_features_1).to_csv(join(scenarios_model_dir, filename), index=False)\n",
        "      print(f\"Feature lists were identical, only saved {filename}\")\n",
        "    else:\n",
        "      # Lists are different, save both versions\n",
        "      for suffix, features in [(\"1\", oldgrowth_features_1), (\"2\", oldgrowth_features_2)]:\n",
        "        filename = f\"{year}_oldgrowth_{suffix}.csv\"\n",
        "        pd.DataFrame(features).to_csv(join(scenarios_model_dir, filename), index=False)\n",
        "      print(f\"Feature lists for scenarios where all forest in {year} was old-growth\")\n",
        "      print(f\"have been exported to {year}_oldgrowth_1.csv and {year}_oldgrowth_2.csv.\\n\")\n",
        "\n",
        "  # Generate 'oldgrowth_all_land' features and scenarios\n",
        "  if simulate_oldgrowth_all_land:\n",
        "    for year in oldgrowth_yearly_scenarios:\n",
        "      oldgrowth_all_land_name = f\"{year}_oldgrowth_all_land\"\n",
        "      # Create a forest feature for all land that exists in the first feature year (e.g. 1990)\n",
        "      forest_oldgrowth_all_land_name = f\"forest_with_edge_effects_{oldgrowth_all_land_name}\"\n",
        "      forest_oldgrowth_all_land_path = join(features_dir, f\"{forest_oldgrowth_all_land_name}.tif\")\n",
        "      annual_changes_filename = f\"tmf_AnnualChanges_Dec{year}.tif\"\n",
        "      annual_changes_path = join(feature_resampled_dir, annual_changes_filename)\n",
        "      if not exists(forest_oldgrowth_all_land_path):\n",
        "        if exists(annual_changes_path):\n",
        "          first_annual_changes_array = gdal.Open(annual_changes_path).ReadAsArray()\n",
        "          # Convert all water values to '0' and non-water values to '1'\n",
        "          forest_oldgrowth_all_land_array = np.where(first_annual_changes_array == 5, 0, 1)\n",
        "          # Set smoothing kernel and precision\n",
        "          export_array_as_tif(edge_effects(forest_oldgrowth_all_land_array), forest_oldgrowth_all_land_path)\n",
        "          print(f\"{forest_oldgrowth_all_land_name} has been created and saved to\\n{features_dir}\\n\")\n",
        "        else: print(f\"The TMF annual changes {year} raster needed for '{year}_oldgrowth_all_land' is not in the indicated directory:\\n{annual_changes_path}\\n\")\n",
        "      print(f\"{forest_oldgrowth_all_land_name} already exists in\\n{features_dir}\\n\")\n",
        "\n",
        "      if exists(forest_oldgrowth_all_land_path):\n",
        "        # Two oldgrowth feature sets created, both removing disturbance from all years\n",
        "        # oldgrowth_all_land_1 also adds the land-use oldgrowth proxy\n",
        "        # During masking, the version with highest AGBD value is selected.\n",
        "        oldgrowth_all_features_1 = []\n",
        "        oldgrowth_all_features_2 = []\n",
        "        for scenario_feature in most_recent_scenario_features:\n",
        "          if \"disturbance_with_edge_effects\" in scenario_feature:\n",
        "            feature_1 = feature_2 = minimum_disturbance_name\n",
        "          elif \"forest_with_edge_effects\" in scenario_feature:\n",
        "            feature_1 = feature_2 = forest_oldgrowth_all_land_name\n",
        "          # Only change oldgrowth feature and redundant feature in one version\n",
        "          elif scenario_feature == oldgrowth_feature:\n",
        "            feature_1 = f\"{scenario_feature}_all\"\n",
        "            feature_2 = scenario_feature\n",
        "          elif scenario_feature in oldgrowth_redundant_features:\n",
        "            feature_1 = f\"{scenario_feature}_none\"\n",
        "            feature_2 = scenario_feature\n",
        "          else: feature_1 = feature_2 = scenario_feature\n",
        "          oldgrowth_all_features_1.append(feature_1)\n",
        "          oldgrowth_all_features_2.append(feature_2)\n",
        "\n",
        "        # Compare feature lists and save appropriate CSVs\n",
        "        if oldgrowth_all_features_1 == oldgrowth_all_features_2:\n",
        "          # Lists are identical, only save version 1\n",
        "          filename = f\"{oldgrowth_all_land_name}_1.csv\"\n",
        "          pd.DataFrame(oldgrowth_all_features_1).to_csv(join(scenarios_model_dir, filename), index=False)\n",
        "          print(f\"Feature lists were identical, only saved {filename}\")\n",
        "        else:\n",
        "          # Lists are different, save both versions\n",
        "          for suffix, features in [(\"1\", oldgrowth_all_features_1), (\"2\", oldgrowth_all_features_2)]:\n",
        "            filename = f\"{oldgrowth_all_land_name}_{suffix}.csv\"\n",
        "            pd.DataFrame(features).to_csv(join(scenarios_model_dir, filename), index=False)\n",
        "          print(f\"Feature lists for {oldgrowth_all_land_name} have been exported to {oldgrowth_all_land_name}_1.csv and {oldgrowth_all_land_name}_2.csv.\\n\")\n",
        "\n",
        "      # Create an all-land forest mask for 'all land' oldgrowth\n",
        "      oldgrowth_all_mask_path = join(masks_dir, f\"mask_forest_{oldgrowth_all_land_name}.tif\")\n",
        "      if not exists(oldgrowth_all_mask_path):\n",
        "        first_annual_changes_array = gdal.Open(annual_changes_path).ReadAsArray()\n",
        "        # Convert all water values to 'nodata' and non-water values to '1'\n",
        "        oldgrowth_all_mask_array = np.where(first_annual_changes_array == 5, nodatavalue, 1)\n",
        "        export_array_as_tif(oldgrowth_all_mask_array, oldgrowth_all_mask_path)\n",
        "        print(f\"A mask for {oldgrowth_all_land_name} has been created at\\n{oldgrowth_all_mask_path}\\n\")\n",
        "      else: print(f\"A mask for {oldgrowth_all_land_name} already exists at\\n{oldgrowth_all_mask_path}\\n\")\n",
        "\n",
        "else: print(\"Old-growth scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9w2IzcgVNvx"
      },
      "source": [
        "## Area-based disturbance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQRgPdbiVNVF"
      },
      "outputs": [],
      "source": [
        "# Use polygons to select areas for alternate scenarios of area-based disturbance\n",
        "define_area_based_disturbance = True\n",
        "\n",
        "if define_area_based_disturbance:\n",
        "  # Exclude existing polygons from search\n",
        "  polygons_to_exclude = ['project_area.gpkg', 'project_area_buffered_bbox.gpkg', 'gedi_area.gpkg', 'template.gpkg']\n",
        "  print(\"# Modify this dictionary by:\")\n",
        "  print(\"# 1) Commenting out any polygons not being used for disturbance.\")\n",
        "  print(\"# 2) Changing the type from 'deforestation' to 'degradation' if necessary.\")\n",
        "  print(\"# 3) Changing the tuple years from 'range' to 'discrete' to specify individual years.\")\n",
        "  print(\"# 4) If years are discrete, add one or more. If a range, add the start and end year.\")\n",
        "  print(\"# 5) Changing the alternate scenario year for each area if needed.\")\n",
        "  print(\"# 6) Copy and paste lines for multiple scenarios with the same area (different disturbance types, different years).\")\n",
        "  print(\"# Remember to ensure all keys are unique - if you copy and paste, manually change the keys.\\n\")\n",
        "\n",
        "  # Exclude existing polygons from search\n",
        "  polygons_to_exclude = ['project_area.gpkg', 'gedi_area.gpkg', 'template.gpkg']\n",
        "  exclude_lu_polygons = False\n",
        "\n",
        "  print(\"disturbance_polygons = {\")\n",
        "  index = 1\n",
        "  first_disturbance_year = last_feature_year - model_scenario_year_range\n",
        "  for polygon in sorted(os.listdir(polygons_dir)):\n",
        "    if polygon not in polygons_to_exclude and 'inverse' not in polygon and 'buffered' not in polygon:\n",
        "      if not exclude_lu_polygons:\n",
        "        print(f\"    {index}: ['{polygon[:-5]}', 'deforestation','range', ({first_disturbance_year}, {last_feature_year}), {last_feature_year}],\")\n",
        "        index += 1\n",
        "      if exclude_lu_polygons and 'lu_' not in polygon:\n",
        "        print(f\"    {index}: ['{polygon[:-5]}', 'deforestation','range', ({first_disturbance_year}, {last_feature_year}), {last_feature_year}],\")\n",
        "        index += 1\n",
        "  print(\"}\\n\")\n",
        "\n",
        "else: print(\"Area-based disturbance scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI_Ck3HQVWLo"
      },
      "outputs": [],
      "source": [
        "# The alternate year is set to 2024.\n",
        "# The years for alternate area-based disturbance can be between 1996 and 2024\n",
        "\n",
        "# Modify this dictionary by:\n",
        "# 1) Commenting out any polygons not being used for disturbance.\n",
        "# 2) Changing the type from 'deforestation' to 'degradation' if necessary.\n",
        "# 3) Changing the tuple years from 'range' to 'discrete' to specify individual years.\n",
        "# 4) If years are discrete, add one or more. If a range, add the start and end year.\n",
        "# 5) Copy and paste lines for multiple scenarios with the same area (different disturbance types, different years).\n",
        "# Remember to ensure all keys are unique - if you copy and paste, manually change the keys.\n",
        "\n",
        "if define_area_based_disturbance:\n",
        "  disturbance_polygons = {\n",
        "      3: ['road_mat_daling', 'deforestation','range', (2023, 2024), 2024],\n",
        "  }\n",
        "\n",
        "  # Validate disturbance types, year types and available years.\n",
        "  for area_index, value in disturbance_polygons.items():\n",
        "      polygon_name = value[0]\n",
        "      disturbance_type = value[1]\n",
        "      year_type, years_data = value[2], value[3]\n",
        "      alternate_scenario_year = value[4]\n",
        "\n",
        "      # Calculate first available disturbance year for this area's alternate scenario year\n",
        "      first_disturbance_year = alternate_scenario_year - model_scenario_year_range\n",
        "\n",
        "      # Validate alternate scenario year\n",
        "      assert alternate_scenario_year >= minimum_yearly_scenario, f\"Alternate scenario year for {polygon_name} must be between {minimum_yearly_scenario} and {last_feature_year}.\"\n",
        "      assert alternate_scenario_year <= last_feature_year, f\"Alternate scenario year for {polygon_name} must be between {minimum_yearly_scenario} and {last_feature_year}.\"\n",
        "      # Validate disturbance types and year types\n",
        "      assert disturbance_type in ['deforestation', 'degradation'], f\"Disturbance type for {polygon_name} must be 'deforestation' or 'degradation'.\"\n",
        "      if year_type == 'range':\n",
        "        start_year, end_year = years_data\n",
        "        assert start_year <= end_year, f\"The start year for {polygon_name} {disturbance_type} must be before the end year.\"\n",
        "\n",
        "      # Validate deforestation constraints\n",
        "      if disturbance_type == 'deforestation':\n",
        "        assert year_type == 'range', f\"Year type for {polygon_name} deforestation must be 'range'.\"\n",
        "        assert end_year == alternate_scenario_year, f\"Deforestation in {polygon_name} must end in the alternate scenario year {alternate_scenario_year}. Deforestation is considered permanent land-cover change.\"\n",
        "        assert start_year >= first_disturbance_year, f\"The start year for deforestation in {polygon_name} must be >= the first available disturbance year {first_disturbance_year}.\"\n",
        "        all_years = list(range(start_year, end_year + 1))\n",
        "\n",
        "      # Validate degradation constraints\n",
        "      if disturbance_type == 'degradation':\n",
        "        assert year_type in ['range', 'discrete'], f\"Year type for {polygon_name} degradation must be 'range' or 'discrete'.\"\n",
        "        if year_type == 'range':\n",
        "            all_years = list(range(start_year, end_year + 1))\n",
        "        else: all_years = list(years_data)\n",
        "        for year in all_years:\n",
        "            assert year <= alternate_scenario_year, f\"Years for {polygon_name} degradation (check {year}) must be <= the alternate scenario year {alternate_scenario_year}.\"\n",
        "            assert year >= first_disturbance_year, f\"Years for {polygon_name} degradation (check {year}) must >= the first available disturbance year {first_disturbance_year}.\"\n",
        "\n",
        "      # Simplify dictionary\n",
        "      disturbance_polygons[area_index] = [polygon_name, disturbance_type, all_years, alternate_scenario_year]\n",
        "\n",
        "  print(\"The 'disturbance_polygons' dictionary is valid.\")\n",
        "else: print(\"Area-based disturbance scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enzNAFf17h6P"
      },
      "outputs": [],
      "source": [
        "if define_area_based_disturbance:\n",
        "  # Define the projects CRS to check the area polygon matches\n",
        "  crs_epsg = 4326\n",
        "  # This setting buffers any deforestation area to add degradation around it.\n",
        "  # At least 1 pixel distance (e.g. 30 m) might be realistic. Otherwise set to None.\n",
        "  buffer_distance_metres = 30\n",
        "\n",
        "  # Define a temporary directory for copying binary rasters and burning the area's polygon\n",
        "  binary_temp_dir = join(scenarios_model_dir, 'binary_temp')\n",
        "  makedirs(binary_temp_dir, exist_ok=True)\n",
        "\n",
        "  # Calculate progress totals before processing - simplified counting\n",
        "  total_areas = len(disturbance_polygons)\n",
        "  total_rasters = 0\n",
        "\n",
        "  for area_index, parameters in disturbance_polygons.items():\n",
        "      polygon_name = parameters[0]\n",
        "      disturbance_type = parameters[1]\n",
        "      disturbance_years = parameters[2]\n",
        "\n",
        "      # Simplified raster counting: degradation = years x 1, deforestation = years x 3\n",
        "      if disturbance_type == 'degradation':\n",
        "          area_raster_count = len(disturbance_years) * 1\n",
        "      elif disturbance_type == 'deforestation':\n",
        "          area_raster_count = len(disturbance_years) * 3  # disturbance + forest + mask\n",
        "\n",
        "      total_rasters += area_raster_count\n",
        "\n",
        "  # Progress indicators\n",
        "  area_progress_index, area_progress_label = 0, widgets.Label(value=f\"Area progress: 0 / {total_areas}\")\n",
        "  display(area_progress_label)\n",
        "  raster_progress_index, raster_progress_label = 0, widgets.Label(value=f\"Raster progress: 0 / {total_rasters}\")\n",
        "  display(raster_progress_label)\n",
        "\n",
        "  for area_index, parameters in disturbance_polygons.items():\n",
        "\n",
        "      # Extract alternate area-based disturbance parameters\n",
        "      area_disturbance_features = []\n",
        "      polygon_name = parameters[0]\n",
        "      disturbance_type = parameters[1]\n",
        "      disturbance_years = parameters[2]\n",
        "      alternate_scenario_year = parameters[3]\n",
        "\n",
        "      # Determine base features by the alternate scenario's year for this area\n",
        "      alternate_year_scenario_csv = join(scenarios_model_dir, f\"{alternate_scenario_year}.csv\")\n",
        "      base_features = pd.Series.tolist(pd.read_csv(alternate_year_scenario_csv).iloc[:,0])\n",
        "\n",
        "      # Define area polygon\n",
        "      area_polygon_path = join(polygons_dir, f\"{polygon_name}.gpkg\")\n",
        "      if disturbance_type == 'deforestation' and buffer_distance_metres:\n",
        "        area_buffered_path = join(polygons_dir, f\"{polygon_name}_buffered_{buffer_distance_metres}.gpkg\")\n",
        "        if not exists(area_buffered_path):\n",
        "          area_polygon = gpd.read_file(join(polygons_dir, f\"{polygon_name}.gpkg\"))\n",
        "          if area_polygon.crs.to_epsg() == crs_epsg:\n",
        "            # Suppress warning about not being a geographic CRS, as we account for this.\n",
        "            # However larger buffers or project areas near the poles might still need to be converted.\n",
        "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "            # Get the centroid of the project polygon\n",
        "            area_polygon_centroid = area_polygon.centroid.values[0]\n",
        "            # Convert the buffer distance from meters to decimal degrees based on the location at the centroid\n",
        "            buffer_distance_degrees = buffer_distance_metres / (111320 * abs(math.cos(math.radians(area_polygon_centroid.y))))\n",
        "            # Buffer the polygon and save\n",
        "            area_polygon_buffered = area_polygon.buffer(buffer_distance_degrees)\n",
        "            gdf = gpd.GeoDataFrame(geometry=area_polygon_buffered, crs=f\"EPSG:{crs_epsg}\")\n",
        "            gdf.to_file(area_buffered_path, driver='GPKG')\n",
        "            print(f\"Buffered the project area to {buffer_distance_metres} and exported to the polygons directory.\")\n",
        "          else: print(f\"Reproject {polygon_name}.gpkg to EPSG:4326.\")\n",
        "      else: area_buffered_path = None\n",
        "\n",
        "      for scenario_feature in base_features:\n",
        "          # Handle disturbance features if years match\n",
        "          if \"disturbance\" in scenario_feature:\n",
        "              scenario_feature_year = int(scenario_feature[-4:])\n",
        "              if scenario_feature_year in disturbance_years:\n",
        "                # Define the new feature name and path\n",
        "                if disturbance_type == 'deforestation': area_disturbance_name = f\"{scenario_feature}_{polygon_name}_deforestation_{buffer_distance_metres}m_buffer\"\n",
        "                else: area_disturbance_name = f\"{scenario_feature}_{polygon_name}_degradation\"\n",
        "                area_based_disturbance_path = join(features_dir,  f\"{area_disturbance_name}.tif\")\n",
        "\n",
        "                if not exists(area_based_disturbance_path):\n",
        "                  # Copy the disturbance binary raster for burning '1' to the the polygon area\n",
        "                  binary_raster_name = f\"disturbance_binary_{scenario_feature_year}.tif\"\n",
        "                  binary_raster_path = join(feature_binary_dir, binary_raster_name)\n",
        "                  binary_raster_temp_path = join(binary_temp_dir, binary_raster_name)\n",
        "                  copyfile(binary_raster_path, binary_raster_temp_path)\n",
        "                  if area_buffered_path: burn_polygon_to_raster(binary_raster_temp_path, area_buffered_path, fixed_value=1, all_touched=True)\n",
        "                  else: burn_polygon_to_raster(binary_raster_temp_path, area_polygon_path, fixed_value=1, all_touched=True)\n",
        "                  # Open and apply edge effects to the burned array, then export the raster\n",
        "                  binary_burned_array = gdal.Open(binary_raster_temp_path).ReadAsArray()\n",
        "                  export_array_as_tif(edge_effects(binary_burned_array), area_based_disturbance_path)\n",
        "\n",
        "                area_disturbance_features.append(area_disturbance_name)\n",
        "                raster_progress_index += 1\n",
        "                raster_progress_label.value = f\"Raster progress: {raster_progress_index} / {total_rasters}\"\n",
        "              else: area_disturbance_features.append(scenario_feature)\n",
        "\n",
        "          # Handle forest features if disturbance type is deforestation and years match\n",
        "          elif \"forest_with_edge_effects\" in scenario_feature:\n",
        "              if disturbance_type == 'deforestation':\n",
        "                  scenario_feature_year = int(scenario_feature[-4:])\n",
        "                  if scenario_feature_year in disturbance_years:\n",
        "\n",
        "                    # Define the new feature name and path\n",
        "                    area_disturbance_name = f\"{scenario_feature}_{polygon_name}_deforestation\"\n",
        "                    area_based_disturbance_path = join(features_dir, f\"{area_disturbance_name}.tif\")\n",
        "\n",
        "                    if not exists(area_based_disturbance_path):\n",
        "                      # Copy the forest binary raster for burning '0' to the the polygon area\n",
        "                      binary_raster_name = f\"forest_binary_{scenario_feature_year}.tif\"\n",
        "                      binary_raster_path = join(feature_binary_dir, binary_raster_name)\n",
        "                      binary_raster_temp_path = join(binary_temp_dir, binary_raster_name)\n",
        "                      copyfile(binary_raster_path, binary_raster_temp_path)\n",
        "                      burn_polygon_to_raster(binary_raster_temp_path, area_polygon_path, fixed_value=0, all_touched=True)\n",
        "                      # Open and apply edge effects to the burned array, then export the raster\n",
        "                      binary_burned_array = gdal.Open(binary_raster_temp_path).ReadAsArray()\n",
        "                      export_array_as_tif(edge_effects(binary_burned_array), area_based_disturbance_path)\n",
        "\n",
        "                    area_disturbance_features.append(area_disturbance_name)\n",
        "                    raster_progress_index += 1\n",
        "                    raster_progress_label.value = f\"Raster progress: {raster_progress_index} / {total_rasters}\"\n",
        "                  else: area_disturbance_features.append(scenario_feature)\n",
        "              else: area_disturbance_features.append(scenario_feature)\n",
        "          else: area_disturbance_features.append(scenario_feature)\n",
        "\n",
        "      # Add name affix based on whether years are a range or discrete\n",
        "      if disturbance_type == \"deforestation\": year_affix = f'_{min(disturbance_years)}'\n",
        "      else:\n",
        "        if len(disturbance_years) != (max(disturbance_years) - min(disturbance_years) + 1):\n",
        "            sorted_years = sorted(disturbance_years)\n",
        "            parts, start = [], sorted_years[0]\n",
        "            for i, year in enumerate(sorted_years[1:] + [None], 1):\n",
        "                if year != sorted_years[i-1] + 1:\n",
        "                    end = sorted_years[i-1]\n",
        "                    parts.append(f\"{start}-{end}\" if start != end else str(start))\n",
        "                    start = year\n",
        "            year_affix = \"_\" + \"_\".join(parts)\n",
        "        else: year_affix = f'_{min(disturbance_years)}-{max(disturbance_years)}'\n",
        "\n",
        "      if disturbance_type == 'deforestation':\n",
        "        area_disturbance_scenario_name = f\"{alternate_scenario_year}_{polygon_name}_deforestation{year_affix}_{buffer_distance_metres}m_degradation_buffer\"\n",
        "        # Create a new forest mask for the area-based disturbance scenario\n",
        "        mask_raster_path = join(masks_dir, f\"mask_forest_{alternate_scenario_year}_{polygon_name}_deforestation.tif\")\n",
        "        if not exists(mask_raster_path):\n",
        "          # Ensure original forest binary is copied to temp and burned with polygon\n",
        "          scenario_year_forest_binary_path = join(binary_temp_dir, f\"forest_binary_{alternate_scenario_year}.tif\")\n",
        "          forest_binary_source = join(feature_binary_dir, f\"forest_binary_{alternate_scenario_year}.tif\")\n",
        "          copyfile(forest_binary_source, scenario_year_forest_binary_path)\n",
        "          burn_polygon_to_raster(scenario_year_forest_binary_path, area_polygon_path, fixed_value=0, all_touched=True)\n",
        "          # Create mask from burned forest data\n",
        "          scenario_year_forest_binary_array = gdal.Open(scenario_year_forest_binary_path).ReadAsArray()\n",
        "          mask_array = np.where(scenario_year_forest_binary_array == 0, nodatavalue, 1)\n",
        "          export_array_as_tif(mask_array, mask_raster_path)\n",
        "          print(f\"A mask raster has been created for {area_disturbance_scenario_name}.\")\n",
        "        raster_progress_index += 1\n",
        "        raster_progress_label.value = f\"Raster progress: {raster_progress_index} / {total_rasters}\"\n",
        "\n",
        "      else: area_disturbance_scenario_name = f\"{alternate_scenario_year}_{polygon_name}_degradation{year_affix}\"\n",
        "\n",
        "      # Clear temporary binary raster folder\n",
        "      for temp_file in os.listdir(binary_temp_dir): os.remove(join(binary_temp_dir, temp_file))\n",
        "\n",
        "      # Export the alternate area-based disturbance scenario\n",
        "      no_degradation_scenario_path = join(scenarios_model_dir, f\"{area_disturbance_scenario_name}.csv\")\n",
        "      pd.DataFrame(area_disturbance_features).to_csv(no_degradation_scenario_path, index=False)\n",
        "      print(f\"Feature list for {area_disturbance_scenario_name} has been exported.\")\n",
        "\n",
        "      # Update area progress\n",
        "      area_progress_index += 1\n",
        "      area_progress_label.value = f\"Area progress: {area_progress_index} / {total_areas}\"\n",
        "\n",
        "  print(\"\\nAlternate area-based disturbance scenarios complete.\")\n",
        "else: print(\"Area-based disturbance scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy2nfHshozVQ"
      },
      "source": [
        "# Feature verification (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chCSZTV4XB8k"
      },
      "outputs": [],
      "source": [
        "# Check all features in scenario .csvs exist\n",
        "scenario_csv_list = []\n",
        "all_features_exist = True # Changes to false if feature missing\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    csv_dir = join(scenarios_model_dir, csv)\n",
        "    csv_feature_list = pd.Series.tolist(pd.read_csv(csv_dir).iloc[:,0])\n",
        "    csv_feature_dir_list = []\n",
        "    for csv_feature in csv_feature_list:\n",
        "      if csv_feature not in covariates: csv_feature_dir_list.append(f\"{features_dir}/{csv_feature}.tif\")\n",
        "    for feature in csv_feature_dir_list:\n",
        "      if not exists(feature):\n",
        "        all_features_exist = False\n",
        "        print(f\"The following feature is missing:\\n{feature}\\n and is required for the scenario '{csv[:-4]}'\")\n",
        "\n",
        "if all_features_exist: print(\"All required features are present.\")\n",
        "print(\"Covariate features e.g. 'beam' and 'sensitivity' will be added at the prediction stage.\")\n",
        "\n",
        "# Check all features against template dimensions\n",
        "scenario_template = gdal.Open(template_tif_path)\n",
        "scenario_template_dimensions, scenario_template_projection = scenario_template.GetGeoTransform(), scenario_template.GetProjection()\n",
        "feature_issue = False\n",
        "for feature in os.listdir(features_dir):\n",
        "  if feature.endswith('.tif'):\n",
        "    feature_dir = join(features_dir, feature)\n",
        "    feature_open = gdal.Open(feature_dir)\n",
        "    feature_dimensions, feature_projection = feature_open.GetGeoTransform(), feature_open.GetProjection()\n",
        "    if feature_dimensions != scenario_template_dimensions:\n",
        "      print(f\"{feature} dimensions:\\n{feature_dimensions}\\ndo not match the scenario template dimensions:\\n{scenario_template_dimensions}\\n\")\n",
        "      feature_issue = True\n",
        "    if feature_projection != scenario_template_projection:\n",
        "      print(f\"{feature} projection:\\n{feature_projection}\\ndoes not match the scenario template projection:\\n{scenario_template_projection}\\n\\n\")\n",
        "      feature_issue = True\n",
        "\n",
        "if not feature_issue: print(f\"All features in the following directory have the correct dimensions and projection:\\n{features_dir}\")\n",
        "else: print(\"Correct and / or resample the feature(s).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T9UqJrzWECr"
      },
      "source": [
        "# Tiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zizOofP5dgW"
      },
      "outputs": [],
      "source": [
        "# Load the model scenario features for template tile creation\n",
        "model_scenario_features = pd.Series.tolist(pd.read_csv(model_scenario_dir).iloc[:,0])\n",
        "model_scenario_features_dirs = [features_dir + '/' + feature + '.tif' for feature in model_features]\n",
        "# Create a template feature array from the first feature that isn't a covariate (these are created later)\n",
        "template_base_path = next(r for r in model_scenario_features_dirs if all(c not in r for c in covariates))\n",
        "template_base = gdal.Open(template_base_path)\n",
        "template_base_array = template_base.ReadAsArray()\n",
        "template_base_xsize, template_base_ysize = template_base.GetRasterBand(1).XSize, template_base.GetRasterBand(1).YSize\n",
        "print(f\"The template feature is {template_base_xsize} x {template_base_ysize} pixels.\")\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles_exist = len(template_tile_list)\n",
        "\n",
        "if n_tiles_exist < 1: print(\"There are currently no template tiles. Run the next section.\")\n",
        "if n_tiles_exist >= 1:\n",
        "  tile_size_y_rounded_exist = gdal.Open(join(tile_templates_dir,'template_tile_1.tif'))\n",
        "  tile_size_y_rounded_exist_y = tile_size_y_rounded_exist.GetRasterBand(1).YSize\n",
        "  tile_size_y_remainder_exist = gdal.Open(join(tile_templates_dir,f'template_tile_{n_tiles_exist}.tif'))\n",
        "  tile_size_y_remainder_exist_y = tile_size_y_remainder_exist.GetRasterBand(1).YSize\n",
        "  if n_tiles_exist == 1: print(f\"There is a single 'tile' with a height of {tile_size_y_rounded_exist}.\"); tile_size_y_remainder_exist = 0\n",
        "  else: print(f\"There are {n_tiles_exist} template tiles, the first {n_tiles_exist-1} having a height of {tile_size_y_rounded_exist} pixels, the last {tile_size_y_remainder_exist} pixels.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njhqSH8e5cPu"
      },
      "outputs": [],
      "source": [
        "# Large template areas and / or numbers of features may be too much for the available memory.\n",
        "# This section defines how to split predictions into tiles that can then be merged.\n",
        "override_n_tiles = True  # Useful if the tile number has already been tested.\n",
        "n_tiles_override = 1\n",
        "tile_size_y_rounded_override = int(14910/n_tiles_override)\n",
        "\n",
        "memory_utilisation = 0.8 # Set to 0.8 to ensure crashes are avoided\n",
        "\n",
        "assert memory_utilisation > 0 and memory_utilisation <= 1, \"Set memory_utilisation to a value between 0 and 1\"\n",
        "\n",
        "# Calculate total size of feature stack\n",
        "feature_stack_size = template_base_array.size * len(model_scenario_features_dirs)\n",
        "\n",
        "# Calculate memory and the number of tiles required\n",
        "total_memory_needed = 64 / 8 * feature_stack_size\n",
        "print(f'RAM required for each prediction: ~{total_memory_needed/(1024**3):.3f} GB')\n",
        "print(f'RAM currently available: {psutil.virtual_memory().free / (1024**3):.3f} GB')\n",
        "n_tiles_temp = int(np.ceil(total_memory_needed / (psutil.virtual_memory().free * memory_utilisation)))\n",
        "\n",
        "# Calculate template tile size (split on the y axis only)\n",
        "tile_size_y_rounded = int(np.ceil(template_base_ysize/n_tiles_temp)) # Round the number of y pixels in each tile\n",
        "tile_size_y_remainder = template_base_ysize%tile_size_y_rounded # Calculate the remainder for the last tile\n",
        "n_tiles = max(1, len(range(0, template_base_ysize, tile_size_y_rounded))) # Update the number of tiles to include the remainder\n",
        "\n",
        "if override_n_tiles:\n",
        "  tile_size_y_rounded = tile_size_y_rounded_override\n",
        "  tile_size_y_remainder = template_base_ysize%tile_size_y_rounded\n",
        "  n_tiles = n_tiles_override\n",
        "  print(\"n_tiles has been overridden.\")\n",
        "\n",
        "print(f'The prediction template should be divided into {n_tiles} tiles to avoid crashing.')\n",
        "\n",
        "# Check if tiles need to be changed\n",
        "change_tiles = True\n",
        "if override_n_tiles:\n",
        "  if n_tiles == n_tiles_exist: change_tiles = False\n",
        "if n_tiles == n_tiles_exist and tile_size_y_rounded == tile_size_y_rounded_exist and tile_size_y_remainder == tile_size_y_remainder_exist:\n",
        "  change_tiles = False\n",
        "\n",
        "if change_tiles:\n",
        "  # Clear all tile directories\n",
        "  for tile in Path(tile_templates_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for tile in Path(tile_features_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for scenario_stack_dir in Path(tile_feature_stacks_dir).glob(\"**/*\"):\n",
        "    shutil.rmtree(scenario_stack_dir)\n",
        "  for tile in Path(tile_prediction_cache_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "\n",
        "  # Generate new tile templates based on available memory\n",
        "  tile_number = 1\n",
        "  for y_start in range(0, template_base_ysize, tile_size_y_rounded):\n",
        "    if tile_size_y_remainder != 0 and tile_number == n_tiles: tile_size_y = tile_size_y_remainder\n",
        "    else: tile_size_y = tile_size_y_rounded\n",
        "    tiling_string = \"gdal_translate -of GTIFF -srcwin \" + str(0)+ \", \" + str(y_start) + \", \" + str(template_base_xsize) + \", \" + str(tile_size_y) + \" \" + str(template_base_path) + \" \" + str(tile_templates_dir) + \"/template_tile_\" + str(tile_number) + \".tif\"\n",
        "    os.system(tiling_string)\n",
        "    tile_number += 1\n",
        "  print(\"Template tile creation complete.\")\n",
        "\n",
        "else: print(\"No changes to existing tiles are required.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P_nom3fcuJu"
      },
      "outputs": [],
      "source": [
        "# Create feature tiles.\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "template_tile_1 = gdal.Open(join(tile_templates_dir, 'template_tile_1.tif'))\n",
        "tile_size_y_rounded = template_tile_1.GetRasterBand(1).YSize\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "if n_tiles == 1: print(\"Feature tile creation skipped. Feature stack creation will use the original features.\")\n",
        "else:\n",
        "  # Progress\n",
        "  n_features = len(os.listdir(features_dir))\n",
        "  feature_progress_index, feature_progress_label = 0, widgets.Label(value=f\"Feature progress: 0 / {n_features}\")\n",
        "  display(feature_progress_label)\n",
        "  tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "  display(tile_progress_label)\n",
        "\n",
        "  # Loop through each feature in the 6_scenarios features directory\n",
        "  for feature in os.listdir(features_dir):\n",
        "    # Create list of tile directories\n",
        "    feature_dir = join(features_dir, feature)\n",
        "    feature_array = gdal.Open(feature_dir).ReadAsArray()\n",
        "    # Split the feature array into chunks based on tile size\n",
        "    feature_chunks = np.array_split(feature_array, np.arange(tile_size_y_rounded, len(feature_array), tile_size_y_rounded))\n",
        "    tile_count = 1\n",
        "    # Loop through tiles and export as .tif\n",
        "    for tile in range(n_tiles):\n",
        "      feature_tile_filename = f\"{feature[:-4]}_{tile_count}.tif\"\n",
        "      # Check if tile already exists\n",
        "      feature_tile_exists = False\n",
        "      for feature_tile in os.listdir(tile_features_dir):\n",
        "        if feature_tile == feature_tile_filename: feature_tile_exists=True\n",
        "        # If feature tile does not exist:\n",
        "      if feature_tile_exists == False:\n",
        "        template_tile_dir = join(tile_templates_dir, f\"template_tile_{tile_count}.tif\")\n",
        "        export_array_as_tif(feature_chunks[tile_count-1], join(tile_features_dir,feature_tile_filename), template_tile_dir, compress = False)\n",
        "        tile_count += 1\n",
        "      # Update tile progress\n",
        "      tile_progress_index += 1\n",
        "      tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_tiles}\"\n",
        "    tile_progress_index = 0\n",
        "    # Update feature progress\n",
        "    feature_progress_index += 1\n",
        "    feature_progress_label.value = f\"Feature progress: {feature_progress_index} / {n_features}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0r89JLYW_xU"
      },
      "source": [
        "# Feature stacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaHFv1nuW5CG"
      },
      "outputs": [],
      "source": [
        "# Create feature stack arrays for each scenario\n",
        "\n",
        "# Collect scenarios with .csv feature lists\n",
        "scenarios_list = []\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    scenarios_list.append(csv[:-4])\n",
        "\n",
        "# Select scenarios to generate tiled feature stacks\n",
        "print(\"scenarios_to_stack = [\")\n",
        "for scenario in sorted(scenarios_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-uJ-EkoxIym"
      },
      "outputs": [],
      "source": [
        "scenarios_to_stack = [\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_no_degradation_since_1993\",\n",
        "  \"2021_no_disturbance_since_1993\",\n",
        "  \"2021_oldgrowth_1\",\n",
        "  \"2021_oldgrowth_2\",\n",
        "  \"2021_oldgrowth_all_land_1\",\n",
        "  \"2021_oldgrowth_all_land_2\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_no_degradation_since_1996\",\n",
        "  \"2024_no_disturbance_since_1996\",\n",
        "  \"2024_no_disturbance_since_1997\",\n",
        "  \"2024_no_disturbance_since_1998\",\n",
        "  \"2024_no_disturbance_since_1999\",\n",
        "  \"2024_no_disturbance_since_2000\",\n",
        "  \"2024_no_disturbance_since_2001\",\n",
        "  \"2024_no_disturbance_since_2002\",\n",
        "  \"2024_no_disturbance_since_2003\",\n",
        "  \"2024_no_disturbance_since_2004\",\n",
        "  \"2024_no_disturbance_since_2005\",\n",
        "  \"2024_no_disturbance_since_2006\",\n",
        "  \"2024_no_disturbance_since_2007\",\n",
        "  \"2024_no_disturbance_since_2008\",\n",
        "  \"2024_no_disturbance_since_2009\",\n",
        "  \"2024_no_disturbance_since_2010\",\n",
        "  \"2024_no_disturbance_since_2011\",\n",
        "  \"2024_no_disturbance_since_2012\",\n",
        "  \"2024_no_disturbance_since_2013\",\n",
        "  \"2024_no_disturbance_since_2014\",\n",
        "  \"2024_no_disturbance_since_2015\",\n",
        "  \"2024_no_disturbance_since_2016\",\n",
        "  \"2024_no_disturbance_since_2017\",\n",
        "  \"2024_no_disturbance_since_2018\",\n",
        "  \"2024_no_disturbance_since_2019\",\n",
        "  \"2024_no_disturbance_since_2020\",\n",
        "  \"2024_no_disturbance_since_2021\",\n",
        "  \"2024_no_disturbance_since_2022\",\n",
        "  \"2024_no_disturbance_since_2023\",\n",
        "  \"2024_no_disturbance_since_2024\",\n",
        "  \"2024_oldgrowth_1\",\n",
        "  \"2024_oldgrowth_2\",\n",
        "  \"2024_oldgrowth_all_land_1\",\n",
        "  \"2024_oldgrowth_all_land_2\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]\n",
        "\n",
        "assert set(scenarios_to_stack).issubset(scenarios_list), \"Not all selected scenarios exist.\"\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "# Set the number of stacks to the number of tiles\n",
        "n_stacks = n_tiles\n",
        "\n",
        "# Progress\n",
        "scenario_progress_index, scenario_progress_label = 0, widgets.Label(value=f\"Scenario progress: 0 / {len(scenarios_to_stack)}\")\n",
        "display(scenario_progress_label)\n",
        "stack_progress_index, stack_progress_label = 0, widgets.Label(value=f\"Tiled feature stack progress: 0 / {n_tiles}\")\n",
        "display(stack_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_stack:\n",
        "    # Define directory and features\n",
        "    scenario_feature_stacks_dir = join(tile_feature_stacks_dir, scenario)\n",
        "    makedirs(scenario_feature_stacks_dir, exist_ok=True)\n",
        "    scenario_features_csv = join(scenarios_model_dir,f\"{scenario}.csv\")\n",
        "    scenario_features = pd.Series.tolist(pd.read_csv(scenario_features_csv).iloc[:,0])\n",
        "    # Create a tile count to match the feature stack\n",
        "    tile_count = 1\n",
        "    for tile in range(n_stacks):\n",
        "      scenario_stack_filename = f\"feature_stack_{scenario}_{tile_count}.npy\"\n",
        "      # Check if feature stack already exists\n",
        "      feature_stack_exists = False\n",
        "      for feature_stack in os.listdir(scenario_feature_stacks_dir):\n",
        "        if feature_stack == scenario_stack_filename: feature_stack_exists=True\n",
        "      # If scenario prediction tile does not exist:\n",
        "      if feature_stack_exists == False:\n",
        "        scenario_tile_stack_dir = join(scenario_feature_stacks_dir, scenario_stack_filename)\n",
        "        # Create feature chunks (arrays) from tiles\n",
        "        if n_stacks == 1: feature_tiles_dirs = [f\"{features_dir}/{feature}.tif\" for feature in scenario_features]\n",
        "        else: feature_tiles_dirs = [f\"{tile_features_dir}/{feature}_{tile_count}.tif\" for feature in scenario_features]\n",
        "        feature_array_chunks = []\n",
        "        for feature in feature_tiles_dirs:\n",
        "          # Covariate raster will exist and should be ignored if prediction stage has already been attempted\n",
        "          if feature.split('/')[-1].split('.')[0] not in covariates and feature.split('/')[-1].split('.')[0] not in [f\"{cov}_{tile_count}\" for cov in covariates]:\n",
        "            feature_array_chunk = gdal.Open(feature).ReadAsArray()\n",
        "            feature_array_chunks.append(feature_array_chunk)\n",
        "        # Create a feature stack from chunks\n",
        "        feature_stack = np.dstack(feature_array_chunks)\n",
        "        feature_array_chunks = None # Flush chunks\n",
        "        stack_height, stack_width, stack_n_features = feature_stack.shape\n",
        "        # Convert feature stack to 2D numpy array with features as columns\n",
        "        feature_stack_reshaped = feature_stack.reshape(stack_height * stack_width, stack_n_features)\n",
        "        feature_stack = None # Flush stack\n",
        "        # Save as a numpy file\n",
        "        np.save(scenario_tile_stack_dir, feature_stack_reshaped)\n",
        "\n",
        "        # Force Drive sync and verify\n",
        "        subprocess.run(['sync'], check=True)\n",
        "        last_size = -1\n",
        "        for attempt in range(10):\n",
        "            time.sleep(5)\n",
        "            if os.path.exists(scenario_tile_stack_dir):\n",
        "                current_size = os.path.getsize(scenario_tile_stack_dir)\n",
        "                if current_size == last_size and current_size > 0:\n",
        "                    try:\n",
        "                        np.load(scenario_tile_stack_dir)\n",
        "                        break\n",
        "                    except: pass\n",
        "                last_size = current_size\n",
        "        else: raise RuntimeError(f\"Drive sync failed: {scenario_tile_stack_dir}\")\n",
        "\n",
        "        feature_stack_reshaped = None # Flush reshaped stack\n",
        "      # Update progress\n",
        "      tile_count += 1\n",
        "      stack_progress_index += 1\n",
        "      stack_progress_label.value = f\"Tiled feature stack progress: {stack_progress_index} / {n_stacks}\"\n",
        "    # Reset tile progress\n",
        "    stack_progress_index = 0\n",
        "    scenario_progress_index += 1\n",
        "    scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index} / {len(scenarios_to_stack)}\"\n",
        "print(\"\\nFeature stacks complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oysAS9DI7yRg"
      },
      "source": [
        "# Predict scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y92V4nKGcyE"
      },
      "outputs": [],
      "source": [
        "# This is for testing models and scenarios, or making predictions where no\n",
        "# uncertainty metric for the variate (e.g. standard error or stdev) is available.\n",
        "# If these are available, proceed to 7_uncertainty.ipynb.\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "assert n_tiles > 0, \"# There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "if n_tiles == 1: print(f\"# There is 1 template tile.\\n\")\n",
        "if n_tiles > 1: print(f\"# There are {n_tiles} template tiles.\\n\")\n",
        "\n",
        "# Collect available scenarios from the feature stack tiles directory\n",
        "scenario_stacks_list = []\n",
        "for scenario in os.listdir(tile_feature_stacks_dir):\n",
        "    if len(os.listdir(join(tile_feature_stacks_dir, scenario))) == n_tiles:\n",
        "        scenario_stacks_list.append(scenario)\n",
        "\n",
        "print(\"# Note: If you end a runtime after the creation of many large feature stacks,\")\n",
        "print(\"# it will time for the notebook to recognise their existence again due to\")\n",
        "print(\"# Google Drive latency issues. If the stacks do not appear here after some time,\")\n",
        "print(\"# run the feature stack section again until they do.\\n\")\n",
        "# Select scenarios to predict\n",
        "print(\"scenarios_to_predict = [\")\n",
        "for scenario in sorted(scenario_stacks_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRlFYaIGNXbI"
      },
      "outputs": [],
      "source": [
        "# There is 1 template tile.\n",
        "\n",
        "# Note: If you end a runtime after the creation of many large feature stacks,\n",
        "# it will time for the notebook to recognise their existence again due to\n",
        "# Google Drive latency issues. If the stacks do not appear here after some time,\n",
        "# run the feature stack section again until they do.\n",
        "\n",
        "scenarios_to_predict = [\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_no_degradation_since_1993\",\n",
        "  \"2021_no_disturbance_since_1993\",\n",
        "  \"2021_oldgrowth_1\",\n",
        "  \"2021_oldgrowth_2\",\n",
        "  \"2021_oldgrowth_all_land_1\",\n",
        "  \"2021_oldgrowth_all_land_2\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_no_degradation_since_1996\",\n",
        "  \"2024_no_disturbance_since_1996\",\n",
        "  \"2024_no_disturbance_since_1997\",\n",
        "  \"2024_no_disturbance_since_1998\",\n",
        "  \"2024_no_disturbance_since_1999\",\n",
        "  \"2024_no_disturbance_since_2000\",\n",
        "  \"2024_no_disturbance_since_2001\",\n",
        "  \"2024_no_disturbance_since_2002\",\n",
        "  \"2024_no_disturbance_since_2003\",\n",
        "  \"2024_no_disturbance_since_2004\",\n",
        "  \"2024_no_disturbance_since_2005\",\n",
        "  \"2024_no_disturbance_since_2006\",\n",
        "  \"2024_no_disturbance_since_2007\",\n",
        "  \"2024_no_disturbance_since_2008\",\n",
        "  \"2024_no_disturbance_since_2009\",\n",
        "  \"2024_no_disturbance_since_2010\",\n",
        "  \"2024_no_disturbance_since_2011\",\n",
        "  \"2024_no_disturbance_since_2012\",\n",
        "  \"2024_no_disturbance_since_2013\",\n",
        "  \"2024_no_disturbance_since_2014\",\n",
        "  \"2024_no_disturbance_since_2015\",\n",
        "  \"2024_no_disturbance_since_2016\",\n",
        "  \"2024_no_disturbance_since_2017\",\n",
        "  \"2024_no_disturbance_since_2018\",\n",
        "  \"2024_no_disturbance_since_2019\",\n",
        "  \"2024_no_disturbance_since_2020\",\n",
        "  \"2024_no_disturbance_since_2021\",\n",
        "  \"2024_no_disturbance_since_2022\",\n",
        "  \"2024_no_disturbance_since_2023\",\n",
        "  \"2024_no_disturbance_since_2024\",\n",
        "  \"2024_oldgrowth_1\",\n",
        "  \"2024_oldgrowth_2\",\n",
        "  \"2024_oldgrowth_all_land_1\",\n",
        "  \"2024_oldgrowth_all_land_2\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TibU8xTZfcuL"
      },
      "outputs": [],
      "source": [
        "# GEDI covariates need to be changed to a set value for all predictions\n",
        "add_covariates = True # Adds a selected covariate value as the feature\n",
        "sensitivity_value = 0.99\n",
        "# Higher sensitivity indicative of GEDI footprint 'quality'\n",
        "# However it may also overestimate vegetation metrics like AGBD.\n",
        "# If predictions appear to have a positive bias, lower this and run again.\n",
        "beam_value = 5\n",
        "# 5 is the first of the full beams, which appears to have the least bias on vegetation metrics.\n",
        "# Cover beams 1 - 4 underestimate. Full beams 7 - 8 overestimate. 5 - 6 tend give average values.\n",
        "\n",
        "# Detect GPU availability and set predictor type\n",
        "try:\n",
        "    test_array = cupy.array([1, 2, 3]) # Test if GPU is actually accessible\n",
        "    del test_array\n",
        "    predictor_type = 'gpu_predictor'\n",
        "    gpu_id, use_gpu = 0, True\n",
        "    print(\"GPU detected and accessible - using GPU to load the feature stack and to predict.\")\n",
        "    @contextmanager\n",
        "    def gpu_memory_context():\n",
        "        try: yield\n",
        "        finally:\n",
        "            cupy.cuda.Device().synchronize()\n",
        "            gc.collect()\n",
        "            cupy.get_default_memory_pool().free_all_blocks()\n",
        "            cupy.get_default_pinned_memory_pool().free_all_blocks()\n",
        "except:\n",
        "    predictor_type = 'cpu_predictor'\n",
        "    gpu_id = -1  # -1 means CPU\n",
        "    use_gpu = False\n",
        "    print(\"GPU not accessible - using CPU prediction\")\n",
        "xgb.set_config(verbosity=0, use_rmm=use_gpu)\n",
        "\n",
        "# Load model description to get proper parameters\n",
        "model_description_path = selected_model_json.replace('model.json', 'model_description.json')\n",
        "with open(model_description_path) as f: model_desc = json.load(f)\n",
        "model_params = eval(model_desc[\"hyperparameters\"])\n",
        "# Detect classification and multiclass from model parameters\n",
        "classification = 'objective' in model_params and ('logistic' in model_params['objective'] or 'softprob' in model_params['objective'])\n",
        "multiclass = 'num_class' in model_params and model_params['num_class'] > 2\n",
        "print(f\"Model type detected: {'Classification' if classification else 'Regression'}\")\n",
        "if classification and multiclass: print(f\"Multiclass with {model_params['num_class']} classes\")\n",
        "\n",
        "# Load model and create predictor with proper parameters\n",
        "if classification:\n",
        "    # Create XGBClassifier with original parameters\n",
        "    XGBPredictor = xgb.XGBClassifier(**model_params)\n",
        "    XGBPredictor.load_model(selected_model_json)\n",
        "    XGBPredictor.set_params(predictor=predictor_type)\n",
        "    if use_gpu: XGBPredictor.set_params(gpu_id=gpu_id)\n",
        "else:\n",
        "    # For regression, use booster approach\n",
        "    booster = xgb.Booster()\n",
        "    booster.load_model(selected_model_json)\n",
        "    booster.set_param('predictor', predictor_type)\n",
        "    if use_gpu: booster.set_param('gpu_id', gpu_id)\n",
        "    XGBPredictor = xgb.XGBRegressor(predictor=predictor_type, gpu_id=gpu_id if use_gpu else -1)\n",
        "    XGBPredictor._Booster = booster\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = [file for file in os.listdir(tile_templates_dir)\n",
        "                     if file.endswith('.tif') and file[:13] == 'template_tile']\n",
        "n_tiles = len(template_tile_list)\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "template_tile = gdal.Open(join(tile_templates_dir,'template_tile_1.tif'))\n",
        "template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "# Progress tracking\n",
        "scenario_progress_index = 0\n",
        "scenario_progress_label = widgets.Label(f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\")\n",
        "display(scenario_progress_label)\n",
        "tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "display(tile_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_predict:\n",
        "  scenario_prediction_unmasked_filename = f\"{scenario}__{selected_model}_unmasked.tif\"\n",
        "  scenario_prediction_unmasked_exists = scenario_prediction_unmasked_filename in os.listdir(scenario_predictions_unmasked_dir)\n",
        "  if not scenario_prediction_unmasked_exists:\n",
        "    scenario_feature_stack_dir = join(tile_feature_stacks_dir, scenario)\n",
        "    n_stacks = len(os.listdir(scenario_feature_stack_dir))\n",
        "    tile_cache_scenario_dir = join(tile_prediction_cache_dir, scenario_prediction_unmasked_filename[:-4])\n",
        "    makedirs(tile_cache_scenario_dir, exist_ok=True)\n",
        "    for stack in range(1, n_stacks+1):\n",
        "      scenario_tile_filename = f\"scenario_tile_{stack}.tif\"\n",
        "      scenario_tile_exists = scenario_tile_filename in os.listdir(tile_cache_scenario_dir)\n",
        "      if not scenario_tile_exists:\n",
        "        # Clear GPU memory before new stack\n",
        "        if use_gpu:\n",
        "          with gpu_memory_context(): pass\n",
        "        # Load template tile parameters\n",
        "        template_tile_dir = join(tile_templates_dir, f\"template_tile_{stack}.tif\")\n",
        "        template_tile = gdal.Open(template_tile_dir)\n",
        "        template_tile_y = template_tile.GetRasterBand(1).YSize\n",
        "        template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "        # Load stack to GPU or CPU\n",
        "        stack_filename = f\"feature_stack_{scenario}_{stack}.npy\"\n",
        "        stack_path = join(scenario_feature_stack_dir, stack_filename)\n",
        "        if use_gpu:\n",
        "          try:\n",
        "            feature_stack = cupy.load(stack_path)\n",
        "            if add_covariates: feature_stack = cupy.hstack((feature_stack,\n",
        "                  cupy.full((feature_stack.shape[0], 1), beam_value, dtype=int),\n",
        "                  cupy.full((feature_stack.shape[0], 1), sensitivity_value, dtype=float)))\n",
        "          except Exception as e:\n",
        "            if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "              print(\"GPU memory insufficient for feature stack, loading stack with CPU.\")\n",
        "              feature_stack = np.load(stack_path)\n",
        "              if add_covariates: feature_stack = np.hstack((feature_stack,\n",
        "                    np.full((feature_stack.shape[0], 1), beam_value, dtype=int),\n",
        "                    np.full((feature_stack.shape[0], 1), sensitivity_value, dtype=float)))\n",
        "              with gpu_memory_context(): pass\n",
        "            else: raise\n",
        "        else:\n",
        "          feature_stack = np.load(stack_path)\n",
        "          if add_covariates: feature_stack = np.hstack((feature_stack,\n",
        "                            np.full((feature_stack.shape[0], 1), beam_value, dtype=int),\n",
        "                            np.full((feature_stack.shape[0], 1), sensitivity_value, dtype=float)))\n",
        "        # Predict - terminate runtime if GPU prediction fails\n",
        "        try:\n",
        "            prediction = XGBPredictor.predict(feature_stack)\n",
        "            # Process predictions based on model type\n",
        "            if classification:\n",
        "                # Check if prediction is 2D (probabilities) and convert to class labels\n",
        "                if prediction.ndim > 1 and prediction.shape[1] > 1: prediction = np.argmax(prediction, axis=1)\n",
        "                elif prediction.ndim == 1: # For binary classification with single probability values\n",
        "                  prediction = np.round(prediction).astype(int)\n",
        "                  # Ensure prediction is integer type for classification\n",
        "                  prediction = prediction.astype(int)\n",
        "        except Exception as e:\n",
        "          if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "            print(\"GPU memory insufficient for prediction. Terminating runtime to save compute units, restart with TPU.\")\n",
        "            runtime.unassign()\n",
        "          else: raise\n",
        "        feature_stack = None # Flush feature stack\n",
        "        prediction_tile = prediction.reshape((template_tile_y, template_tile_x))\n",
        "        prediction = None # Flush prediction\n",
        "        # Export prediction array as .tif\n",
        "        export_array_as_tif(prediction_tile, join(tile_cache_scenario_dir, scenario_tile_filename),\n",
        "                          template = template_tile_dir, compress = False)\n",
        "        prediction_tile = None # Flush prediction tile\n",
        "      tile_progress_index += 1\n",
        "      tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_stacks}\"\n",
        "    # Prepare empty array for merging tiles\n",
        "    prediction_array = np.empty((0,template_tile_x))\n",
        "    # Read each tile .tif as an array, stack, then export as a .tif\n",
        "    for subdir in os.listdir(tile_cache_scenario_dir):\n",
        "      if subdir.endswith('.tif'):\n",
        "        tile_dir = join(tile_cache_scenario_dir, subdir)\n",
        "        prediction_array = np.vstack((prediction_array, gdal.Open(tile_dir).ReadAsArray()))\n",
        "    # Define scenario template\n",
        "    scenario_template = join(features_dir, os.listdir(features_dir)[0])\n",
        "    scenario_prediction_unmasked_dir = join(scenario_predictions_unmasked_dir, f\"{scenario_prediction_unmasked_filename}\")\n",
        "    export_array_as_tif(prediction_array, scenario_prediction_unmasked_dir, template = scenario_template, compress = True)\n",
        "    # Delete scenario tile cache directory\n",
        "    shutil.rmtree(tile_cache_scenario_dir)\n",
        "  # Reset tile progress\n",
        "  tile_progress_index = 0\n",
        "  # Update scenario progress\n",
        "  scenario_progress_index += 1\n",
        "  scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\"\n",
        "print(\"\\nScenario predictions complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diwt1peRdXpj"
      },
      "source": [
        "# Mask scenario predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQsX1NCiy0aW"
      },
      "outputs": [],
      "source": [
        "# Use polygons for masking, only areas inside the polygons will be included\n",
        "\n",
        "# Exclude existing polygons from search\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "\n",
        "print(\"mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"  '{polygon[:-5]}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daKAe6nkJ2bS"
      },
      "outputs": [],
      "source": [
        "mask_polygons = [\n",
        "  # 'project_area',\n",
        "  'gedi_area',\n",
        "  # 'peninsular_malaysia',\n",
        "  # 'pa_taman_krau',\n",
        "  # 'pa_ais',\n",
        "]\n",
        "\n",
        "# Create an inverse project area path for masking\n",
        "template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "for polygon in mask_polygons:\n",
        "  inverse_polygon_path = join(polygons_dir, f\"{polygon}_inverse.gpkg\")\n",
        "  if not exists(inverse_polygon_path):\n",
        "    polygon_path = join(polygons_dir, f\"{polygon}.gpkg\")\n",
        "    template_polygon = gpd.read_file(template_polygon_path)\n",
        "    polygon_read = gpd.read_file(polygon_path)\n",
        "    polygon_crs = polygon_read.crs.to_epsg()\n",
        "    inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "    inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "    inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "    print(f\"An inverse masking polygon for {polygon} has been created in {polygons_dir}.\")\n",
        "  else: print(f\"An inverse masking polygon for {polygon} already exists.\")\n",
        "\n",
        "\n",
        "# If only [oldgrowth_scneario]_1 exists, all disturbance from all disturbance features is removed\n",
        "# If [oldgrowth_scneario]_1 and [oldgrowth_scneario]_2 exist,\n",
        "# [oldgrowth_scneario]_1 uses a land-use proxy for pre-Landsat undisturbed forest\n",
        "# [oldgrowth_scneario]_2 simply removes all disturbance from all disturbance features\n",
        "# The final masked [scenario]_oldgrowth chooses the maximum pixel values from comparing each.\n",
        "\n",
        "# Merge oldgrowth versions by taking maximum values\n",
        "oldgrowth_version1_files = [f for f in os.listdir(scenario_predictions_unmasked_dir)\n",
        "                           if f.split('__')[0].endswith('_1') and f.endswith('_unmasked.tif')]\n",
        "for v1_file in oldgrowth_version1_files:\n",
        "  base_name = v1_file.split('__')[0][:-1] + '2'  # Replace '1' with '2'\n",
        "  rest_of_name = '__' + v1_file.split('__')[1]\n",
        "  v2_file = f\"{base_name}{rest_of_name}\"\n",
        "  merged_file = v1_file.replace('_1__', '__')\n",
        "  merged_path = join(scenario_predictions_unmasked_dir, merged_file)\n",
        "  # Skip if merged file already exists\n",
        "  if exists(merged_path): continue\n",
        "  # Check if version 2 exists\n",
        "  if exists(join(scenario_predictions_unmasked_dir, v2_file)):\n",
        "    print(f\"Merging oldgrowth versions for {v1_file.split('__')[0]}...\")\n",
        "    # Load both arrays and take maximum values\n",
        "    array1 = gdal.Open(join(scenario_predictions_unmasked_dir, v1_file)).ReadAsArray()\n",
        "    array2 = gdal.Open(join(scenario_predictions_unmasked_dir, v2_file)).ReadAsArray()\n",
        "    merged_array = np.maximum(array1, array2)\n",
        "    # Save merged file\n",
        "    export_array_as_tif(merged_array, merged_path, compress=True)\n",
        "    print(f\"Merged version exported to {merged_file}\")\n",
        "  else:\n",
        "    # Use version 1 if version 2 doesn't exist\n",
        "    shutil.copy2(join(scenario_predictions_unmasked_dir, v1_file), merged_path)\n",
        "    print(f\"Version 2 not found, copied version 1 to {merged_file}\")\n",
        "\n",
        "# Collect unmasked predictions, properly skipping oldgrowth version files\n",
        "unmasked_predictions = []\n",
        "for scenario_prediction in os.listdir(scenario_predictions_unmasked_dir):\n",
        " if '_1__' not in scenario_prediction and '_2__' not in scenario_prediction:\n",
        "    unmasked_predictions.append(scenario_prediction)\n",
        "\n",
        "# Determine last feature year for masking future scenarios\n",
        "final_feature_years = []\n",
        "for final_feature in os.listdir(feature_final_dir):\n",
        "  if final_feature.endswith('.tif') and final_feature[-9] == '_':\n",
        "    try: final_feature_years.append(int(final_feature[-8:-4]))\n",
        "    except: continue\n",
        "last_feature_year = max(final_feature_years)\n",
        "\n",
        "# Binary progress\n",
        "masking_progress_index = 0\n",
        "masking_progress_label = widgets.Label(f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\")\n",
        "display(masking_progress_label)\n",
        "\n",
        "# Mask scenarios with the relevatant mask\n",
        "for scenario_prediction in unmasked_predictions: # Loop through each unmasked scenario\n",
        "  scenario_masked_filename = f\"{scenario_prediction[:-13]}.tif\"\n",
        "  scenario_masked_dir = join(scenario_predictions_dir, scenario_masked_filename)\n",
        "  scenario_year = int(scenario_prediction[:4])\n",
        "  if not exists(scenario_masked_dir):\n",
        "    mask_exists = False\n",
        "    for mask in os.listdir(masks_dir):\n",
        "      mask_year = int(mask[12:16])\n",
        "\n",
        "      # Match area-based deforestation scenarios\n",
        "      if 'deforestation' in scenario_prediction and 'deforestation' in mask:\n",
        "          mask_middle = mask[12:-4]  # Remove \"mask_forest_\" and \".tif\"\n",
        "          if scenario_prediction.startswith(mask_middle):\n",
        "              selected_mask_filename = mask\n",
        "              selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "              mask_exists = True\n",
        "\n",
        "      elif 'deforestation' not in scenario_prediction and 'deforestation' not in mask:\n",
        "\n",
        "        # Match all oldgrowth scenarios\n",
        "        if 'oldgrowth_all_land' in mask or 'oldgrowth_all_land' in scenario_prediction:\n",
        "          if f'{scenario_year}_oldgrowth_all_land' in mask and f'{scenario_year}_oldgrowth_all_land' in scenario_prediction:\n",
        "            selected_mask_filename = mask\n",
        "            selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "            mask_exists = True\n",
        "\n",
        "        elif 'oldgrowth_all_land' not in mask or 'oldgrowth_all_land' not in scenario_prediction:\n",
        "\n",
        "          if 'disturbance_since' in scenario_prediction:\n",
        "              disturbance_scenario = scenario_masked_filename.split('__')[0]\n",
        "              disturbance_since_year = int(disturbance_scenario[-4:])-1\n",
        "              if disturbance_since_year == mask_year:\n",
        "                selected_mask_filename = mask\n",
        "                selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "                mask_exists = True\n",
        "\n",
        "          else: # Match all other historic scenarios\n",
        "            if scenario_year == mask_year:\n",
        "              selected_mask_filename = mask\n",
        "              selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "              mask_exists = True\n",
        "\n",
        "            else: # Match future scenarios with most recent forest mask\n",
        "              if scenario_year > last_feature_year and last_feature_year == mask_year:\n",
        "                selected_mask_filename = mask\n",
        "                selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "                mask_exists = True\n",
        "\n",
        "    if mask_exists == False: print(f\"A suitable mask for {scenario_prediction} does not exist.\\n\")\n",
        "    else: # Mask the scenario prediction\n",
        "      print(f\"Masking {scenario_prediction} with {selected_mask_filename}...\")\n",
        "      mask_array = gdal.Open(selected_mask_dir).ReadAsArray()\n",
        "      scenario_prediction_unmasked_dir = join(scenario_predictions_unmasked_dir, scenario_prediction)\n",
        "      scenario_prediction_array = gdal.Open(scenario_prediction_unmasked_dir).ReadAsArray()\n",
        "      # Mask where the mask array is not 1\n",
        "      scenario_masked_array = np.where(mask_array != 1, nodatavalue, scenario_prediction_array)\n",
        "      export_array_as_tif(scenario_masked_array, scenario_masked_dir, compress = True)\n",
        "      if len(mask_polygons) > 0:\n",
        "        for polygon_mask in mask_polygons:\n",
        "          inverse_gedi_area_path = join(polygons_dir, f\"{polygon_mask}_inverse.gpkg\")\n",
        "          print(f\"Masking {scenario_prediction} with {polygon_mask}...\")\n",
        "          burn_polygon_to_raster(scenario_masked_dir, inverse_gedi_area_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        # Recompress the prediction after burning the polygon masks\n",
        "        scenario_masked_array_2 = gdal.Open(scenario_masked_dir).ReadAsArray()\n",
        "        export_array_as_tif(scenario_masked_array_2, scenario_masked_dir, compress = True)\n",
        "      print(f\"{scenario_masked_filename} exported.\")\n",
        "  # Update masking progress\n",
        "  masking_progress_index += 1\n",
        "  masking_progress_label.value = f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-7ctfgdoF2"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcuArY00N9Mc"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QJ8jLRKRSfgb",
        "y_OkHaUkTCyB",
        "FmlqEP9rFapf",
        "bKfhj6mVgRvp",
        "eN8AwXXAbxAn",
        "1T9UqJrzWECr",
        "Y0r89JLYW_xU",
        "oysAS9DI7yRg",
        "diwt1peRdXpj"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "V6E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}