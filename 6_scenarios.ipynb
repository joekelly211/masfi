{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/6_scenarios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8jLRKRSfgb"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkvV803eSUQY"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEpdIjOwSiB1"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install astropy\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!pip install xgboost --upgrade\n",
        "!apt-get install -y gdal-bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQDSsvGeSi4u"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import itertools\n",
        "import json\n",
        "from os import makedirs\n",
        "from os.path import join, exists\n",
        "from osgeo import gdal\n",
        "import ipywidgets as widgets\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import psutil\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "import re\n",
        "from scipy import ndimage\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxgT3K1Sjum"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "\n",
        "feature_dir = join(base_dir, \"3_features\")\n",
        "feature_resampled_dir = join(feature_dir, \"resampled\")\n",
        "feature_final_dir = join(feature_dir, \"final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "masks_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(scenarios_dir, exist_ok=True)\n",
        "makedirs(masks_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96FAfeyooMLz"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)\n",
        "\n",
        "# Burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster, polygon, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "  with rasterio.open(raster, 'r+') as src:\n",
        "      array = src.read(1)\n",
        "      transform = src.transform\n",
        "      gdf = gpd.read_file(polygon)\n",
        "      for geom in gdf.geometry:\n",
        "          if not fixed and column_name == None:\n",
        "              column_name = gdf.columns[0]\n",
        "          if not fixed: burn_value = gdf.loc[gdf.geometry == geom, column_name].values[0]\n",
        "          else: burn_value = fixed_value\n",
        "          rasterize([(geom, burn_value)], out=array, transform=transform,\n",
        "              all_touched=all_touched, dtype=src.meta['dtype'], out_shape=src.shape)\n",
        "      src.write(array, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_OkHaUkTCyB"
      },
      "source": [
        "# Select model and scenario area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mB0XrPNYU_I"
      },
      "outputs": [],
      "source": [
        "# Select a model\n",
        "model_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file == 'model.json':\n",
        "      print(f'selected_model = \"{subdir.split(f\"{models_dir}/\",1)[1]}\"')\n",
        "      model_exists = True\n",
        "if not model_exists:\n",
        "  print(\"No model exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbXY6iP_qjYc"
      },
      "outputs": [],
      "source": [
        "selected_model = \"agbd_250112_131618\"\n",
        "categorise_target = False # If the target was categorised in 5_models\n",
        "\n",
        "# Define model directories\n",
        "selected_model_dir = join(models_dir,selected_model)\n",
        "selected_model_json = join(selected_model_dir, \"model.json\")\n",
        "selected_model_descr_dir = join(selected_model_dir, \"model_description.json\")\n",
        "selected_model_dataset_path = join(selected_model_dir, f\"{selected_model}.pkl\")\n",
        "selected_model_dataset = pd.read_pickle(selected_model_dataset_path)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(selected_model_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_target = model_dataset_description[\"selected_target\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "covariates_categorised = model_dataset_description[\"covariates_categorised\"]\n",
        "selected_features = model_dataset_description[\"selected_features\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "filter_parameter = model_dataset_description[\"filter_parameter\"]\n",
        "filter_values_to_include = model_dataset_description[\"filter_values_to_include\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "covariates = [covariate[4:] for covariate in covariates_renamed]\n",
        "\n",
        "# Create scenarios model directory\n",
        "scenarios_model_dir = join(scenarios_dir, selected_model)\n",
        "makedirs(scenarios_model_dir, exist_ok=True)\n",
        "\n",
        "# Copy model_dataset_description.json\n",
        "with open(join(scenarios_model_dir, \"model_dataset_description.json\"), \"w\") as file:\n",
        "  file.write(json.dumps(model_dataset_description))\n",
        "\n",
        "# Select a scenario area\n",
        "scenario_area_exists = False\n",
        "for subdir in os.listdir(scenarios_model_dir):\n",
        "  if not subdir.endswith('.csv') and not subdir.endswith('.json'):\n",
        "    print(f'selected_scenario_area = \"{subdir}\"')\n",
        "    scenario_area_exists = True\n",
        "if not scenario_area_exists:\n",
        "  print(f\"Create a scenario area directory in {scenarios_model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNYQ3-q2dUKl"
      },
      "outputs": [],
      "source": [
        "selected_scenario_area = \"asartr\"\n",
        "\n",
        "# Define scenario area directory\n",
        "scenario_area_dir = join(scenarios_model_dir,selected_scenario_area)\n",
        "makedirs(scenario_area_dir, exist_ok=True)\n",
        "\n",
        "# Create subdirectories\n",
        "features_dir = join(scenario_area_dir, \"features\")\n",
        "tile_templates_dir = join(scenario_area_dir, 'tile_templates')\n",
        "tile_features_dir = join(scenario_area_dir, \"tile_features\")\n",
        "tile_feature_stacks_dir = join(scenario_area_dir, \"tile_feature_stacks\")\n",
        "tile_prediction_cache_dir = join(scenario_area_dir,\"tile_prediction_cache\")\n",
        "scenario_predictions_unmasked_dir = join(scenario_area_dir,\"scenario_predictions_unmasked\")\n",
        "scenario_predictions_dir = join(scenario_area_dir, \"scenario_predictions\")\n",
        "scenario_dist_dir = join(scenario_area_dir, \"scenario_disturbance\")\n",
        "intactness_dir = join(scenario_area_dir, 'intactness')\n",
        "\n",
        "makedirs(features_dir, exist_ok=True)\n",
        "makedirs(tile_templates_dir, exist_ok=True)\n",
        "makedirs(tile_features_dir, exist_ok=True)\n",
        "makedirs(tile_feature_stacks_dir, exist_ok=True)\n",
        "makedirs(tile_prediction_cache_dir, exist_ok=True)\n",
        "makedirs(scenario_predictions_unmasked_dir, exist_ok=True)\n",
        "makedirs(scenario_predictions_dir, exist_ok=True)\n",
        "makedirs(scenario_dist_dir, exist_ok=True)\n",
        "makedirs(intactness_dir, exist_ok=True)\n",
        "\n",
        "# Copy features from the final features directory\n",
        "for feature in os.listdir(feature_final_dir):\n",
        "  if feature not in os.listdir(features_dir):\n",
        "    feature_original_path = join(feature_final_dir, feature)\n",
        "    feature_copy_path = join(features_dir, feature)\n",
        "    copyfile(feature_original_path, feature_copy_path)\n",
        "print(f\"All features present in the following directory have already been copied over: {feature_final_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3w7svaUvs07"
      },
      "source": [
        "# Define scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJZoEWycn8-x"
      },
      "outputs": [],
      "source": [
        "# Yearly scenarios may include years after the model scenario if feature data is available later than GEDI data.\n",
        "# Date of the prediction is ~December 31st, e.g. '2023' is 31/12/2023, requiring features up to 2023.\n",
        "\n",
        "model_scenario_override = None # set if cannot be automatically determined from model features\n",
        "\n",
        "yearly_features = [\"forest_with_edge_effects\", \"disturbance_with_edge_effects\"]\n",
        "\n",
        "# Remove the 'fea_' prefix from each feature\n",
        "model_features = sorted([feature[4:] for feature in selected_features])\n",
        "\n",
        "# Create a list of feature years from the model's features\n",
        "model_feature_years = []\n",
        "for feature in model_features:\n",
        "  for yearly_feature in yearly_features:\n",
        "    if yearly_feature in feature:\n",
        "      model_feature_years.append(int(feature[-4:]))\n",
        "\n",
        "# Determine the model scenario from the maximum year\n",
        "# Values from the most recent feature year (e.g. 2022) will be applied to the second most recent (e.g. 2021) as a proxy at the feature stack stage\n",
        "if model_scenario_override != None: model_scenario = model_scenario_override\n",
        "else: model_scenario = max(model_feature_years)\n",
        "model_scenario_filename = f\"{model_scenario}.csv\"\n",
        "model_scenario_dir = join(scenarios_model_dir,model_scenario_filename)\n",
        "print(f\"The maximum year used in the model is {model_scenario}, which has been created as the first scenario.\\n\")\n",
        "print(f\"The {model_scenario} scenario feature list has been saved to:\\n {model_scenario_dir}\\n\")\n",
        "print(f\"Ensure all features in this list have been copied to:\\n{features_dir}\\n\")\n",
        "\n",
        "# Save the model scenario features as a .csv\n",
        "pd.DataFrame(model_features).to_csv(model_scenario_dir, index=False)\n",
        "\n",
        "# Determine available feature years\n",
        "final_feature_years = []\n",
        "for final_feature in os.listdir(feature_final_dir):\n",
        "  if final_feature.endswith('.tif') and final_feature[-9] == '_':\n",
        "    try: final_feature_years.append(int(final_feature[-8:-4]))\n",
        "    except: continue\n",
        "\n",
        "# Find the first and last feature years\n",
        "first_feature_year = min(final_feature_years)\n",
        "last_feature_year = max(final_feature_years)\n",
        "additional_feature_years = last_feature_year - model_scenario\n",
        "print(f\"The first available feature year is {first_feature_year} and the last is {last_feature_year}.\\n\")\n",
        "\n",
        "# Calculate the range of scenario years and minimum scenario year\n",
        "model_scenario_year_range = max(model_feature_years) - (min(model_feature_years))\n",
        "minimum_yearly_scenario = first_feature_year + model_scenario_year_range\n",
        "print(f\"The earliest scenario year that can be predicted is {minimum_yearly_scenario}.\")\n",
        "print(f\"The latest scenario year that can be predicted is {last_feature_year}.\")\n",
        "print(f\"This is based on the number of yearly features used to train the model and the total availability of features.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPd_nrFYeGJ3"
      },
      "outputs": [],
      "source": [
        "# Select constant features which are the same in every scenario, e.g. topography\n",
        "print(\"constant_features = [\")\n",
        "for feature in model_features:\n",
        "  print(f'  \"{feature}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARVJed3vTgBv"
      },
      "outputs": [],
      "source": [
        "constant_features = [\n",
        "  \"coast_proximity_km\",\n",
        "  \"latitude\",\n",
        "  \"longitude\",\n",
        "  \"topo_dtm_smooth_aspect_cosine\",\n",
        "  \"topo_dtm_smooth_aspect_sine\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_03\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_07\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_11\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_03\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_07\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_11\",\n",
        "  \"topo_dtm_smooth_eastness\",\n",
        "  \"topo_dtm_smooth_elevation\",\n",
        "  \"topo_dtm_smooth_northness\",\n",
        "  \"topo_dtm_smooth_profile_curvature\",\n",
        "  \"topo_dtm_smooth_roughness_03\",\n",
        "  \"topo_dtm_smooth_roughness_07\",\n",
        "  \"topo_dtm_smooth_roughness_11\",\n",
        "  \"topo_dtm_smooth_slope\",\n",
        "  \"topo_dtm_smooth_stream_power_index_log10\",\n",
        "  \"topo_dtm_smooth_surface_area_ratio\",\n",
        "  \"topo_dtm_smooth_tangential_curvature\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_03\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_07\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_11\",\n",
        "  \"topo_dtm_smooth_topographic_ruggedness_index\",\n",
        "  \"topo_dtm_smooth_topographic_wetness_index\",\n",
        "  \"topo_dtm_unsmooth_aspect_cosine\",\n",
        "  \"topo_dtm_unsmooth_aspect_sine\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_03\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_07\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_11\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_03\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_07\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_11\",\n",
        "  \"topo_dtm_unsmooth_eastness\",\n",
        "  \"topo_dtm_unsmooth_elevation\",\n",
        "  \"topo_dtm_unsmooth_northness\",\n",
        "  \"topo_dtm_unsmooth_profile_curvature\",\n",
        "  \"topo_dtm_unsmooth_roughness_03\",\n",
        "  \"topo_dtm_unsmooth_roughness_07\",\n",
        "  \"topo_dtm_unsmooth_roughness_11\",\n",
        "  \"topo_dtm_unsmooth_slope\",\n",
        "  \"topo_dtm_unsmooth_stream_power_index_log10\",\n",
        "  \"topo_dtm_unsmooth_surface_area_ratio\",\n",
        "  \"topo_dtm_unsmooth_tangential_curvature\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_03\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_07\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_11\",\n",
        "  \"topo_dtm_unsmooth_topographic_ruggedness_index\",\n",
        "  \"topo_dtm_unsmooth_topographic_wetness_index\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg4XvxT5foVR"
      },
      "outputs": [],
      "source": [
        "# Note that areas that were forested in a historic yearly scenario but were reservoirs\n",
        "# or flooded at the time topographic feature data was collected (~2014 for GLO-30 DEM)\n",
        "# will be predicted based on a flat topography at the elevation of the water's surface.\n",
        "\n",
        "# Set scenario features as all non-constant features\n",
        "scenario_features = sorted(list(set(model_features) - set(constant_features)))\n",
        "\n",
        "# Create feature lists for all possible yearly scenarios\n",
        "for yearly_scenario in range(minimum_yearly_scenario, last_feature_year +1):\n",
        "  year_difference = model_scenario - yearly_scenario\n",
        "  yearly_scenario_features = []\n",
        "  for scenario_feature in scenario_features:\n",
        "    try:\n",
        "      year_change = int(scenario_feature[-4:]) - year_difference\n",
        "      yearly_scenario_feature = scenario_feature[:-4] + str(year_change)\n",
        "      yearly_scenario_features.append(yearly_scenario_feature)\n",
        "    except: yearly_scenario_features.append(scenario_feature)\n",
        "  # Compile yearly features and save as a .csv\n",
        "  yearly_scenario_features = sorted(yearly_scenario_features + constant_features)\n",
        "  yearly_scenario_filename = f\"{yearly_scenario}.csv\"\n",
        "  yearly_scenario_dir = join(scenarios_model_dir,yearly_scenario_filename)\n",
        "  pd.DataFrame(yearly_scenario_features).to_csv(yearly_scenario_dir, index=False)\n",
        "\n",
        "# Open the most recent yearly scenario feature list\n",
        "most_recent_scenario_csv = join(scenarios_model_dir,f\"{last_feature_year}.csv\")\n",
        "most_recent_scenario_features = pd.Series.tolist(pd.read_csv(most_recent_scenario_csv).iloc[:,0])\n",
        "\n",
        "print(f\"Lists of features for all possible yearly scenarios have been exported to {scenarios_model_dir}/.\")\n",
        "print(f\"Ensure all features in these lists have been copied to:\\n{features_dir}\\n\")\n",
        "\n",
        "# Create a 'no disturbance' feature for alternate scenarios.\n",
        "# Assumes the minimum possible value is present in the first scenario year.\n",
        "minimum_disturbance_name = f\"disturbance_with_edge_effects_0000\"\n",
        "minimum_disturbance_path = join(features_dir, f\"{minimum_disturbance_name}.tif\")\n",
        "if not exists(minimum_disturbance_path):\n",
        "  example_disturbance = join(features_dir, f\"disturbance_with_edge_effects_{first_feature_year}.tif\")\n",
        "  example_disturbance_array = gdal.Open(example_disturbance).ReadAsArray()\n",
        "  minimum_disturbance_value = example_disturbance_array.min()\n",
        "  minimum_disturbance_array = np.where(example_disturbance_array, minimum_disturbance_value, minimum_disturbance_value)\n",
        "  export_array_as_tif(minimum_disturbance_array, minimum_disturbance_path, template = example_disturbance)\n",
        "  print(f\"The minimum disturbance value is {minimum_disturbance_value}\\n,\")\n",
        "  print(f\"which has been used to create the 'minimum disturbance' feature {minimum_disturbance_name}.\")\n",
        "else: print(f\"The minimum disturbance feature {minimum_disturbance_name} already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwwIij7sL4Q_"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios remove degradation or deforestation and degradation\n",
        "# prior to the most recent yearly scenario (e.g. 2023) or the model scenario\n",
        "\n",
        "use_most_recent_scenario = True # Else use model scenario\n",
        "\n",
        "if use_most_recent_scenario:\n",
        "  alt_base_year = last_feature_year\n",
        "  alt_base_features = most_recent_scenario_features\n",
        "else:\n",
        "  alt_base_year = model_scenario\n",
        "  alt_base_features = model_features\n",
        "\n",
        "# 'No degradation' scenarios remove disturbance since the indicated year\n",
        "define_no_degradation_scenarios = True\n",
        "\n",
        "no_degradation_since = [\n",
        "    first_feature_year,\n",
        "    # 2000,\n",
        "]\n",
        "\n",
        "for year in no_degradation_since:\n",
        "  assert year >= first_feature_year, \"Years in 'no_degradation_since' must be set to or after the first feature year.\"\n",
        "\n",
        "# Create a feature list for 'no degradation' scenarios\n",
        "if define_no_degradation_scenarios:\n",
        "  for year in no_degradation_since:\n",
        "    no_degradation_features = []\n",
        "    for scenario_feature in alt_base_features:\n",
        "      if \"disturbance_with_edge_effects\" in scenario_feature:\n",
        "        scenario_feature_year = int(scenario_feature[-4:])\n",
        "        if scenario_feature_year > year: # Only remove disturbance from years specified\n",
        "          no_degradation_features.append(minimum_disturbance_name)\n",
        "        else: no_degradation_features.append(scenario_feature)\n",
        "      else: no_degradation_features.append(scenario_feature)\n",
        "    no_degradation_features = sorted(no_degradation_features) # Sort to ensure order is maintained\n",
        "    no_degradation_scenario_filename = f\"{alt_base_year}_no_degradation_since_{year}.csv\"\n",
        "    no_degradation_scenario_path = join(scenarios_model_dir, no_degradation_scenario_filename)\n",
        "    pd.DataFrame(no_degradation_features).to_csv(no_degradation_scenario_path, index=False)\n",
        "    print(f\"Feature list for a scenario without degradation between\")\n",
        "    print(f\"{alt_base_year} and {year} exported to {no_degradation_scenario_filename}.\\n\")\n",
        "else: print(\"The 'no degradation' scenarios are not enabled.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6u03s8VHfN8"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios simulate old-growth forest using a proxy area specified by the user.\n",
        "# Forest extent (i.e. 'no deforestation') can be set from yearly feature, or all historic / potential forest area.\n",
        "\n",
        "define_oldgrowth_scenarios = True\n",
        "oldgrowth_yearly_scenarios = [\n",
        "    # 2022,\n",
        "    last_feature_year\n",
        "]\n",
        "\n",
        "for year in oldgrowth_yearly_scenarios:\n",
        "  assert year in final_feature_years, \"Years in 'oldgrowth_yearly_scenarios' must be available in the final yearly features.\"\n",
        "\n",
        "simulate_all_oldgrowth = True\n",
        "all_oldgrowth_name = \"all_oldgrowth\"\n",
        "\n",
        "# The feature that best indicates oldgrowth to the model, e.g. certain protected areas\n",
        "# This will be modified to cover the entire scenario area for the old-growth scenarios\n",
        "oldgrowth_feature = 'lu_oldgrowth_with_edge_effects'\n",
        "\n",
        "# Some features may confound the old-growth proxy, e.g. protected areas that are not known to be old-growth\n",
        "# These will be removed for the old-growth scenarios\n",
        "oldgrowth_redundant_features = [\n",
        "\n",
        "]\n",
        "\n",
        "if define_oldgrowth_scenarios:\n",
        "  # Expand the oldgrowth feature to the entire scenario area\n",
        "  oldgrowth_feature_all_dir = join(features_dir, f\"{oldgrowth_feature}_all.tif\")\n",
        "  if not exists(oldgrowth_feature_all_dir):\n",
        "    oldgrowth_feature_dir = join(features_dir, f\"{oldgrowth_feature}.tif\")\n",
        "    oldgrowth_feature_array = gdal.Open(oldgrowth_feature_dir).ReadAsArray()\n",
        "    oldgrowth_feature_max_value = oldgrowth_feature_array.max()\n",
        "    print(f\"The maximum value for the oldgrowth feature '{oldgrowth_feature}' is {oldgrowth_feature_max_value}.\")\n",
        "    oldgrowth_feature_all_array = np.where(oldgrowth_feature_array, oldgrowth_feature_max_value, oldgrowth_feature_max_value)\n",
        "    oldgrowth_feature_all_dir = join(features_dir, f\"{oldgrowth_feature}_all.tif\")\n",
        "    export_array_as_tif(oldgrowth_feature_all_array, oldgrowth_feature_all_dir, template = oldgrowth_feature_dir)\n",
        "    print(f\"The oldgrowth proxy {oldgrowth_feature} has been expanded to the entire scenario area\")\n",
        "    print(f\"And exported to {oldgrowth_feature_all_dir}\")\n",
        "  else: print(f\"The oldgrowth feature '{oldgrowth_feature}_all.tif' already exists.\\n\")\n",
        "\n",
        "  # Remove the redundant features from the oldgrowth scenario area\n",
        "  for redundant_feature in oldgrowth_redundant_features:\n",
        "    redundant_feature_none_dir = join(features_dir, f\"{redundant_feature}_none.tif\")\n",
        "    if not exists(redundant_feature_none_dir):\n",
        "      redundant_feature_dir = join(features_dir, f\"{redundant_feature}.tif\")\n",
        "      redundant_feature_array = gdal.Open(redundant_feature_dir).ReadAsArray()\n",
        "      redundant_feature_min_value = redundant_feature_array.min()\n",
        "      print(f\"The minimum value for the redundant feature {redundant_feature} is {redundant_feature_min_value}.\")\n",
        "      redundant_feature_none_array = np.where(redundant_feature_array, redundant_feature_min_value, redundant_feature_min_value)\n",
        "      export_array_as_tif(redundant_feature_none_array, redundant_feature_none_dir, redundant_feature_dir)\n",
        "      print(f\"The oldgrowth redundant feature {oldgrowth_feature} has been removed from the entire scenario area\")\n",
        "      print(f\"And exported to {oldgrowth_feature_all_dir}.\")\n",
        "    else: print(f\"The oldgrowth redundant feature '{redundant_feature}_none.tif' already exists.\\n\")\n",
        "\n",
        "  # Generate 'oldgrowth' scenarios for each specified year (i.e. forest extent in that year)\n",
        "  for year in oldgrowth_yearly_scenarios:\n",
        "    oldgrowth_features = []\n",
        "    for scenario_feature in most_recent_scenario_features:\n",
        "      old_growth_scenario_year_diff = last_feature_year - year\n",
        "      if \"disturbance_with_edge_effects\" in scenario_feature:\n",
        "        oldgrowth_features.append(minimum_disturbance_name)\n",
        "      elif \"forest_with_edge_effects\" in scenario_feature:\n",
        "        scenario_feature_year = int(scenario_feature[-4:])\n",
        "        if scenario_feature_year-old_growth_scenario_year_diff > first_feature_year: # Replace with specified forest cover year\n",
        "          oldgrowth_features.append(f\"forest_with_edge_effects_{scenario_feature_year-old_growth_scenario_year_diff}\")\n",
        "        else: oldgrowth_features.append(f\"forest_with_edge_effects_{first_feature_year}\")\n",
        "      elif scenario_feature == oldgrowth_feature:\n",
        "        oldgrowth_features.append(f\"{scenario_feature}_all\")\n",
        "      elif scenario_feature in oldgrowth_redundant_features:\n",
        "        oldgrowth_features.append(f\"{scenario_feature}_none\")\n",
        "      else: oldgrowth_features.append(scenario_feature)\n",
        "    oldgrowth_features = sorted(oldgrowth_features)\n",
        "    oldgrowth_filename = f\"{year}_oldgrowth.csv\"\n",
        "    oldgrowth_dir = join(scenarios_model_dir, oldgrowth_filename)\n",
        "    pd.DataFrame(oldgrowth_features).to_csv(oldgrowth_dir, index=False)\n",
        "    print(f\"Feature list for a scenario where all forest in {year} was old-growth\")\n",
        "    print(f\"has been exported to {oldgrowth_filename}.\\n\")\n",
        "\n",
        "  # Generate 'all_oldgrowth' features and scenarios\n",
        "  if simulate_all_oldgrowth:\n",
        "    # Based on the first TMF AnnualChanges land coverage\n",
        "    # Create a forest feature for all land that exists in the first feature year (e.g. 1990)\n",
        "    forest_all_oldgrowth_name = f\"forest_with_edge_effects_{all_oldgrowth_name}\"\n",
        "    forest_all_oldgrowth_path = join(features_dir, f\"{forest_all_oldgrowth_name}.tif\")\n",
        "    first_annual_changes_filename = f\"tmf_AnnualChanges_Dec{first_feature_year}.tif\"\n",
        "    first_annual_changes_path = join(feature_resampled_dir, first_annual_changes_filename)\n",
        "    if not exists(forest_all_oldgrowth_path):\n",
        "      if exists(first_annual_changes_path):\n",
        "        first_annual_changes_array = gdal.Open(first_annual_changes_path).ReadAsArray()\n",
        "        # Convert all water values to 'nodata' and non-water values to '1'\n",
        "        forest_all_oldgrowth_array = np.where(first_annual_changes_array == 5, nodatavalue, 1)\n",
        "        # Set smoothing kernel and precision\n",
        "        kernel, precision = Gaussian2DKernel(x_stddev=1, y_stddev=1), 2\n",
        "        # Reclassify for binary differentiation after proximity conversion\n",
        "        differentiator_array = forest_all_oldgrowth_array.copy()\n",
        "        differentiator_array[differentiator_array == 1] = 10\n",
        "        # Positive proximity\n",
        "        positive_distances = ndimage.distance_transform_edt(forest_all_oldgrowth_array == 0) # target pixels\n",
        "        positive_proximity_array = np.where(positive_distances > 2, 0, positive_distances) # max distance 2\n",
        "        # Negative proximity\n",
        "        negative_distances = ndimage.distance_transform_edt(forest_all_oldgrowth_array == 1) # target pixels\n",
        "        negative_proximity_array = np.where(negative_distances > 2, 0, negative_distances) # max distance 2\n",
        "        # Sum proximities and differentiator\n",
        "        pixel_prox_summed =  differentiator_array + positive_proximity_array + negative_proximity_array\n",
        "        # Reclassify for better semantic understanding of pixel proximity\n",
        "        pixel_prox_reclassed = pixel_prox_summed.copy()\n",
        "        pixel_prox_reclass_table = [(0, 0, -4), (1, 1, -1), (1.4, 1.5, -2), (2, 2, -3), (10, 10, 3), (11, 11, 0), (11.4, 11.5, 1), (12, 12, 2)]\n",
        "        for min_value, max_value, new_value in pixel_prox_reclass_table:\n",
        "          pixel_prox_reclassed[(pixel_prox_reclassed >= min_value) & (pixel_prox_reclassed <= max_value)] = new_value\n",
        "        # Smooth binary array using 2D convolution\n",
        "        binary_smoothed = convolve(forest_all_oldgrowth_array, kernel, boundary='extend')\n",
        "        # Sum pixel proximity and smoothed binary array\n",
        "        edge_effects_array = np.round(pixel_prox_reclassed + binary_smoothed, precision)\n",
        "        # Export forest edge effects feature\n",
        "        export_array_as_tif(edge_effects_array, forest_all_oldgrowth_path)\n",
        "        print(f\"{forest_all_oldgrowth_name} has been created and saved to\\n{features_dir}\\n\")\n",
        "      else: print(f\"The first TMF annual changes year raster needed for comrec is not in the indicated directory:\\n{first_annual_changes_path}\\n\")\n",
        "    print(f\"{forest_all_oldgrowth_name} already exists in\\n{features_dir}\\n\")\n",
        "    if exists(forest_all_oldgrowth_path):\n",
        "      oldgrowth_all_features = []\n",
        "      for scenario_feature in most_recent_scenario_features:\n",
        "        if \"disturbance_with_edge_effects\" in scenario_feature:\n",
        "          oldgrowth_all_features.append(minimum_disturbance_name)\n",
        "        elif \"forest_with_edge_effects\" in scenario_feature:\n",
        "          oldgrowth_all_features.append(forest_all_oldgrowth_name)\n",
        "        elif scenario_feature == oldgrowth_feature:\n",
        "          oldgrowth_all_features.append(f\"{scenario_feature}_all\")\n",
        "        elif scenario_feature in oldgrowth_redundant_features:\n",
        "          oldgrowth_all_features.append(f\"{scenario_feature}_none\")\n",
        "        else: oldgrowth_all_features.append(scenario_feature)\n",
        "      oldgrowth_all_features = sorted(oldgrowth_all_features)\n",
        "      oldgrowth_all_filename = f\"{all_oldgrowth_name}.csv\"\n",
        "      oldgrowth_all_dir = join(scenarios_model_dir, oldgrowth_all_filename)\n",
        "      pd.DataFrame(oldgrowth_all_features).to_csv(oldgrowth_all_dir, index=False)\n",
        "      print(f\"Feature list for {all_oldgrowth_name} has been exported to {oldgrowth_all_filename}.\\n\")\n",
        "\n",
        "    # Create A forest mask for 'all oldgrowth'\n",
        "    # Also forests reservoirs since the first TMF annual changes year, though topography may be wrong\n",
        "    if exists(join(scenarios_model_dir, f\"{all_oldgrowth_name}.csv\")):\n",
        "      if exists(first_annual_changes_path):\n",
        "        oldgrowth_all_mask_path = join(masks_dir, f\"mask_forest_{all_oldgrowth_name}.tif\")\n",
        "        if not exists(oldgrowth_all_mask_path):\n",
        "          first_annual_changes_array = gdal.Open(first_annual_changes_path).ReadAsArray()\n",
        "          # Convert all water values to 'nodata' and non-water values to '1'\n",
        "          oldgrowth_all_mask_array = np.where(first_annual_changes_array == 5, nodatavalue, 1)\n",
        "          export_array_as_tif(oldgrowth_all_mask_array, oldgrowth_all_mask_path)\n",
        "          print(f\"A mask for {all_oldgrowth_name} has been created at\\n{oldgrowth_all_mask_path}\")\n",
        "        else: print(f\"A mask for {all_oldgrowth_name} already exists at\\n{oldgrowth_all_mask_path}\")\n",
        "      else: print(f\"The {first_annual_changes_filename} raster needed to mask {all_oldgrowth_name} doesn't exist.\")\n",
        "    else: print(f\"The scenario csv for {all_oldgrowth_name} doesn't exist.\")\n",
        "\n",
        "else: print(\"Old-growth scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy2nfHshozVQ"
      },
      "source": [
        "# Feature verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chCSZTV4XB8k"
      },
      "outputs": [],
      "source": [
        "# Check that all features in all scenario csvs exist\n",
        "scenario_csv_list = []\n",
        "all_features_exist = True # Changes to false if feature missing\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    csv_dir = join(scenarios_model_dir, csv)\n",
        "    csv_feature_list = pd.Series.tolist(pd.read_csv(csv_dir).iloc[:,0])\n",
        "    csv_feature_dir_list = []\n",
        "    for csv_feature in csv_feature_list:\n",
        "      if csv_feature not in covariates: csv_feature_dir_list.append(f\"{features_dir}/{csv_feature}.tif\")\n",
        "    for feature in csv_feature_dir_list:\n",
        "      if not exists(feature):\n",
        "        all_features_exist = False\n",
        "        print(f\"The following feature is missing:\\n{feature}\\n and is required for the scenario '{csv[:-4]}'\")\n",
        "\n",
        "if all_features_exist: print(\"All required features are present.\")\n",
        "print(\"Covariate features e.g. 'beam' and 'sensitivity' will be added at the prediction stage.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAqAaDfYkY7T"
      },
      "outputs": [],
      "source": [
        "# Is the scenario area equal to the original template area?\n",
        "original_template_area = True\n",
        "\n",
        "# If not, create a new template for the scenario area and upload to:\n",
        "# '6_scenarios/[model]/[scenario_area]/template.tif'\n",
        "if original_template_area: scenario_template_dir = join(areas_dir, \"template.tif\")\n",
        "else: scenario_template_dir = join(scenario_area_dir, \"template.tif\")\n",
        "print(f\"The following is being used as a template to verify scenario feature dimensions and projections:\\n{scenario_template_dir}\")\n",
        "\n",
        "\n",
        "scenario_template = gdal.Open(scenario_template_dir)\n",
        "scenario_template_dimensions, scenario_template_projection = scenario_template.GetGeoTransform(), scenario_template.GetProjection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBb-Cws3mMUy"
      },
      "outputs": [],
      "source": [
        "feature_issue = False\n",
        "for feature in os.listdir(features_dir):\n",
        "  if feature.endswith('.tif'):\n",
        "    feature_dir = join(features_dir, feature)\n",
        "    feature_open = gdal.Open(feature_dir)\n",
        "    feature_dimensions, feature_projection = feature_open.GetGeoTransform(), feature_open.GetProjection()\n",
        "    if feature_dimensions != scenario_template_dimensions:\n",
        "      print(f\"{feature} dimensions:\\n{feature_dimensions}\\ndo not match the scenario template dimensions:\\n{scenario_template_dimensions}\\n\")\n",
        "      feature_issue = True\n",
        "    if feature_projection != scenario_template_projection:\n",
        "      print(f\"{feature} projection:\\n{feature_projection}\\ndoes not match the scenario template projection:\\n{scenario_template_projection}\\n\\n\")\n",
        "      feature_issue = True\n",
        "\n",
        "if not feature_issue: print(f\"All features in the following directory have the correct dimensions and projection:\\n{features_dir}\")\n",
        "else: print(\"Correct and / or resample the feature(s).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T9UqJrzWECr"
      },
      "source": [
        "# Template tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zizOofP5dgW"
      },
      "outputs": [],
      "source": [
        "# Load the model scenario features for tile template creation\n",
        "model_scenario_features = pd.Series.tolist(pd.read_csv(model_scenario_dir).iloc[:,0])\n",
        "model_scenario_features_dirs = [features_dir + '/' + feature + '.tif' for feature in model_features]\n",
        "# Create a template feature array from the first feature that isn't a covariate (these are created later)\n",
        "template_base_path = next(r for r in model_scenario_features_dirs if all(c not in r for c in covariates))\n",
        "template_base = gdal.Open(template_base_path)\n",
        "template_base_array = template_base.ReadAsArray()\n",
        "template_base_xsize, template_base_ysize = template_base.GetRasterBand(1).XSize, template_base.GetRasterBand(1).YSize\n",
        "print(f\"The template feature is {template_base_xsize} x {template_base_ysize} pixels.\")\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles_exist = len(template_tile_list)\n",
        "\n",
        "if n_tiles_exist < 1: print(\"There are currently no template tiles. Run the next section.\")\n",
        "if n_tiles_exist >= 1:\n",
        "  tile_size_y_rounded_exist = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).YSize\n",
        "  tile_size_y_remainder_exist = gdal.Open(join(tile_templates_dir,f'template_tile_{n_tiles_exist}.tif')).GetRasterBand(1).YSize\n",
        "  if n_tiles_exist == 1: print(f\"There is a single 'tile' with a height of {tile_size_y_rounded_exist}.\"); tile_size_y_remainder_exist = 0\n",
        "  else: print(f\"There are {n_tiles_exist} template tiles, the first {n_tiles_exist-1} having a height of {tile_size_y_rounded_exist} pixels, the last {tile_size_y_remainder_exist} pixels.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njhqSH8e5cPu"
      },
      "outputs": [],
      "source": [
        "# Large scenario areas and / or numbers of features may be too much for the available memory.\n",
        "# This section defines how to split predictions into tiles that can then be merged.\n",
        "override_n_tiles = True  # Useful if the tile number has already been tested.\n",
        "n_tiles_override = 1\n",
        "tile_size_y_rounded_override = int(14910/n_tiles_override)\n",
        "\n",
        "memory_utilisation = 0.8 # Set to 0.8 to ensure crashes are avoided\n",
        "\n",
        "assert memory_utilisation > 0 and memory_utilisation <= 1, \"Set memory_utilisation to a value between 0 and 1\"\n",
        "\n",
        "# Calculate total size of feature stack\n",
        "feature_stack_size = template_base_array.size * len(model_scenario_features_dirs)\n",
        "\n",
        "# Calculate memory and the number of tiles required\n",
        "total_memory_needed = 64 / 8 * feature_stack_size\n",
        "print(f'RAM required for each prediction: ~{total_memory_needed/(1024**3):.3f} GB')\n",
        "print(f'RAM currently available: {psutil.virtual_memory().free / (1024**3):.3f} GB')\n",
        "n_tiles_temp = int(np.ceil(total_memory_needed / (psutil.virtual_memory().free * memory_utilisation)))\n",
        "\n",
        "# Calculate template tile size (split on the y axis only)\n",
        "tile_size_y_rounded = int(np.ceil(template_base_ysize/n_tiles_temp)) # Round the number of y pixels in each tile\n",
        "tile_size_y_remainder = template_base_ysize%tile_size_y_rounded # Calculate the remainder for the last tile\n",
        "n_tiles = max(1, len(range(0, template_base_ysize, tile_size_y_rounded))) # Update the number of tiles to include the remainder\n",
        "\n",
        "if override_n_tiles:\n",
        "  tile_size_y_rounded = tile_size_y_rounded_override\n",
        "  tile_size_y_remainder = template_base_ysize%tile_size_y_rounded\n",
        "  n_tiles = n_tiles_override\n",
        "  print(\"n_tiles has been overridden.\")\n",
        "\n",
        "print(f'The prediction template should be divided into {n_tiles} tiles to avoid crashing.')\n",
        "\n",
        "# Check if tiles need to be changed\n",
        "if n_tiles == n_tiles_exist and tile_size_y_rounded == tile_size_y_rounded_exist and tile_size_y_remainder == tile_size_y_remainder_exist:\n",
        "  print(\"No changes to existing tiles are required.\")\n",
        "else:\n",
        "  # Clear all tile directories\n",
        "  for tile in Path(tile_templates_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for tile in Path(tile_features_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for scenario_stack_dir in Path(tile_feature_stacks_dir).glob(\"**/*\"):\n",
        "    shutil.rmtree(scenario_stack_dir)\n",
        "  for tile in Path(tile_prediction_cache_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "\n",
        "  # Generate new tile templates based on available memory\n",
        "  tile_number = 1\n",
        "  for y_start in range(0, template_base_ysize, tile_size_y_rounded):\n",
        "    if tile_size_y_remainder != 0 and tile_number == n_tiles: tile_size_y = tile_size_y_remainder\n",
        "    else: tile_size_y = tile_size_y_rounded\n",
        "    tiling_string = \"gdal_translate -of GTIFF -srcwin \" + str(0)+ \", \" + str(y_start) + \", \" + str(template_base_xsize) + \", \" + str(tile_size_y) + \" \" + str(template_base_path) + \" \" + str(tile_templates_dir) + \"/template_tile_\" + str(tile_number) + \".tif\"\n",
        "    os.system(tiling_string)\n",
        "    tile_number += 1\n",
        "\n",
        "  print(\"Template tile creation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkNV_N7UKcMd"
      },
      "source": [
        "# Feature tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1d1ELggV4jF"
      },
      "outputs": [],
      "source": [
        "# Create feature tiles.\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "tile_size_y_rounded = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).YSize\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "if n_tiles == 1: print(\"Feature tile creation skipped. Feature stack creation will use the original features.\")\n",
        "else:\n",
        "  # Progress\n",
        "  n_features = len(os.listdir(features_dir))\n",
        "  feature_progress_index, feature_progress_label = 0, widgets.Label(value=f\"Feature progress: 0 / {n_features}\")\n",
        "  display(feature_progress_label)\n",
        "  tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "  display(tile_progress_label)\n",
        "\n",
        "  # Loop through each feature in the 6_scenarios features directory\n",
        "  for feature in os.listdir(features_dir):\n",
        "    # Create list of tile directories\n",
        "    feature_dir = join(features_dir, feature)\n",
        "    feature_array = gdal.Open(feature_dir).ReadAsArray()\n",
        "    # Split the feature array into chunks based on tile size\n",
        "    feature_chunks = np.array_split(feature_array, np.arange(tile_size_y_rounded, len(feature_array), tile_size_y_rounded))\n",
        "    tile_count = 1\n",
        "    # Loop through tiles and export as .tif\n",
        "    for tile in range(n_tiles):\n",
        "      feature_tile_filename = f\"{feature[:-4]}_{tile_count}.tif\"\n",
        "      # Check if tile already exists\n",
        "      feature_tile_exists = False\n",
        "      for feature_tile in os.listdir(tile_features_dir):\n",
        "        if feature_tile == feature_tile_filename: feature_tile_exists=True\n",
        "        # If feature tile does not exist:\n",
        "      if feature_tile_exists == False:\n",
        "        template_tile_dir = join(tile_templates_dir, f\"template_tile_{tile_count}.tif\")\n",
        "        export_array_as_tif(feature_chunks[tile_count-1], join(tile_features_dir,feature_tile_filename), template_tile_dir, compress = False)\n",
        "        tile_count += 1\n",
        "      # Update tile progress\n",
        "      tile_progress_index += 1\n",
        "      tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_tiles}\"\n",
        "    tile_progress_index = 0\n",
        "    # Update feature progress\n",
        "    feature_progress_index += 1\n",
        "    feature_progress_label.value = f\"Feature progress: {feature_progress_index} / {n_features}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0r89JLYW_xU"
      },
      "source": [
        "# Feature stacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaHFv1nuW5CG"
      },
      "outputs": [],
      "source": [
        "# Create feature stack arrays for each scenario\n",
        "\n",
        "# Collect scenarios with .csv feature lists\n",
        "scenarios_list = []\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    scenarios_list.append(csv[:-4])\n",
        "\n",
        "# Select scenarios to generate tiled feature stacks\n",
        "print(\"scenarios_to_stack = [\")\n",
        "for scenario in sorted(scenarios_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-uJ-EkoxIym"
      },
      "outputs": [],
      "source": [
        "scenarios_to_stack = [\n",
        "  # \"2018\",\n",
        "  # \"2019\",\n",
        "  # \"2020\",\n",
        "  # \"2021\",\n",
        "  # \"2022\",\n",
        "  \"2023\",\n",
        "  \"2023_no_degradation_since_1990\",\n",
        "  \"2023_oldgrowth\",\n",
        "  \"all_oldgrowth\",\n",
        "]\n",
        "\n",
        "assert set(scenarios_to_stack).issubset(scenarios_list), \"Not all selected scenarios exist.\"\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "# Progress\n",
        "scenario_progress_index, scenario_progress_label = 0, widgets.Label(value=f\"Scenario progress: 0 / {len(scenarios_to_stack)}\")\n",
        "display(scenario_progress_label)\n",
        "stack_progress_index, stack_progress_label = 0, widgets.Label(value=f\"Tiled feature stack progress: 0 / {n_tiles}\")\n",
        "display(stack_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_stack:\n",
        "    # Define directory and features\n",
        "    scenario_feature_stacks_dir = join(tile_feature_stacks_dir, scenario)\n",
        "    makedirs(scenario_feature_stacks_dir, exist_ok=True)\n",
        "    scenario_features_csv = join(scenarios_model_dir,f\"{scenario}.csv\")\n",
        "    scenario_features = pd.Series.tolist(pd.read_csv(scenario_features_csv).iloc[:,0])\n",
        "    # Set the number of stacks to the number of tiles\n",
        "    if n_tiles == 0: n_stacks = 1\n",
        "    else: n_stacks = n_tiles\n",
        "    # Create a tile count to match the feature stack\n",
        "    tile_count = 1\n",
        "    for tile in range(n_stacks):\n",
        "      scenario_stack_filename = f\"feature_stack_{scenario}_{tile_count}.npy\"\n",
        "      # Check if feature stack already exists\n",
        "      feature_stack_exists = False\n",
        "      for feature_stack in os.listdir(scenario_feature_stacks_dir):\n",
        "        if feature_stack == scenario_stack_filename: feature_stack_exists=True\n",
        "      # If scenario prediction tile does not exist:\n",
        "      if feature_stack_exists == False:\n",
        "        scenario_tile_stack_dir = join(scenario_feature_stacks_dir, scenario_stack_filename)\n",
        "        # Create feature chunks (arrays) from tiles\n",
        "        if n_stacks == 1: feature_tiles_dirs = [f\"{features_dir}/{feature}.tif\" for feature in scenario_features]\n",
        "        else: feature_tiles_dirs = [f\"{tile_features_dir}/{feature}_{tile_count}.tif\" for feature in scenario_features]\n",
        "        feature_array_chunks = []\n",
        "        for feature in feature_tiles_dirs:\n",
        "          # Covariate raster will exist and should be ignored if prediction stage has already been attempted\n",
        "          if feature.split('/')[-1].split('.')[0] not in covariates and feature.split('/')[-1].split('.')[0] not in [f\"{cov}_{tile_count}\" for cov in covariates]:\n",
        "            feature_array_chunk = gdal.Open(feature).ReadAsArray()\n",
        "            feature_array_chunks.append(feature_array_chunk)\n",
        "        # Create a feature stack from chunks\n",
        "        feature_stack = np.dstack(feature_array_chunks)\n",
        "        feature_array_chunks = None # Flush chunks\n",
        "        stack_height, stack_width, stack_n_features = feature_stack.shape\n",
        "        # Convert feature stack to 2D numpy array with features as columns\n",
        "        feature_stack_reshaped = feature_stack.reshape(stack_height * stack_width, stack_n_features)\n",
        "        feature_stack = None # Flush stack\n",
        "        # Save as a numpy file\n",
        "        np.save(scenario_tile_stack_dir, feature_stack_reshaped)\n",
        "        feature_stack_reshaped = None # Flush reshaped stack\n",
        "      # Update progress\n",
        "      tile_count += 1\n",
        "      stack_progress_index += 1\n",
        "      stack_progress_label.value = f\"Tiled feature stack progress: {stack_progress_index} / {n_stacks}\"\n",
        "    # Reset tile progress\n",
        "    stack_progress_index = 0\n",
        "    scenario_progress_index += 1\n",
        "    scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index} / {len(scenarios_to_stack)}\"\n",
        "print(\"\\nFeature stacks complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oysAS9DI7yRg"
      },
      "source": [
        "# Predict scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y92V4nKGcyE"
      },
      "outputs": [],
      "source": [
        "# This is for testing models and scenarios, or making predictions where no\n",
        "# uncertainty metric for the variate (e.g. standard error or stdev) is available.\n",
        "# If these are available, proceed to 7_predictions.ipynb.\n",
        "\n",
        "# Collect available scenarios from the feature stack tiles directory\n",
        "scenario_stacks_list = []\n",
        "for scenario in os.listdir(tile_feature_stacks_dir):\n",
        "    scenario_stacks_list.append(scenario)\n",
        "\n",
        "# Select scenarios to predict\n",
        "print(\"scenarios_to_predict = [\")\n",
        "for scenario in sorted(scenario_stacks_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRlFYaIGNXbI"
      },
      "outputs": [],
      "source": [
        "scenarios_to_predict = [\n",
        "  # \"2018\",\n",
        "  # \"2019\",\n",
        "  # \"2020\",\n",
        "  # \"2021\",\n",
        "  # \"2022\",\n",
        "  \"2023\",\n",
        "  \"2023_no_degradation_since_1990\",\n",
        "  \"2023_oldgrowth\",\n",
        "  \"all_oldgrowth\",\n",
        "]\n",
        "\n",
        "assert set(scenarios_to_predict).issubset(scenario_stacks_list), \"Not all selected scenarios exist.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TibU8xTZfcuL"
      },
      "outputs": [],
      "source": [
        "add_covariates = True # Adds a selected covariate value as the feature\n",
        "sensitivity_value = 0.99\n",
        "beam_value = 5\n",
        "# 5 is the first of the full beams, with the least bias on AGBD.\n",
        "# Cover beams 1 - 4 underestimate. Full beams 7 - 8 overestimate. 5 - 6 tend give average values.\n",
        "\n",
        "# Check for GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0': print('GPU device not found')\n",
        "else: print(f\"Found GPU at: {device_name}\")\n",
        "\n",
        "# Load model\n",
        "booster = xgb.Booster()\n",
        "booster.load_model(selected_model_json)\n",
        "if categorise_target: XGBPredictor = xgb.XGBClassifier()\n",
        "else: XGBPredictor = xgb.XGBRegressor()\n",
        "XGBPredictor._Booster = booster\n",
        "\n",
        "# Avoids issues using dataframe from CPU\n",
        "xgb.set_config(verbosity=0, use_rmm=True)\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "template_tile_x = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).XSize\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "# Scenario progress\n",
        "scenario_progress_index = 0\n",
        "scenario_progress_label = widgets.Label(f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\")\n",
        "display(scenario_progress_label)\n",
        "\n",
        "# Tile progress\n",
        "tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "display(tile_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_predict:\n",
        "  # Define scenario filename and check if exists\n",
        "  scenario_prediction_unmasked_filename = f\"{scenario}__{selected_scenario_area}_{selected_model}_unmasked.tif\"\n",
        "  scenario_prediction_unmasked_exists=False\n",
        "  for scenario_prediction in os.listdir(scenario_predictions_unmasked_dir):\n",
        "    if scenario_prediction == scenario_prediction_unmasked_filename:\n",
        "      scenario_prediction_unmasked_exists=True\n",
        "  # If scenario prediction does not exist:\n",
        "  if scenario_prediction_unmasked_exists == False:\n",
        "    # Get number of stacks\n",
        "    scenario_feature_stack_dir = join(tile_feature_stacks_dir, scenario)\n",
        "    n_stacks = len(os.listdir(scenario_feature_stack_dir))\n",
        "    # Create a tile cache directory for the prediction\n",
        "    tile_cache_scenario_dir = join(tile_prediction_cache_dir, scenario_prediction_unmasked_filename[:-4])\n",
        "    makedirs(tile_cache_scenario_dir, exist_ok=True)\n",
        "    # Create a tile count to match the feature stack chunk\n",
        "    for stack in range(1, n_stacks+1):\n",
        "      scenario_tile_filename = f\"scenario_tile_{stack}.tif\"\n",
        "      # Check if tile already exists\n",
        "      scenario_tile_exists = False\n",
        "      for scenario_tile in os.listdir(tile_cache_scenario_dir):\n",
        "        if scenario_tile == scenario_tile_filename: scenario_tile_exists=True\n",
        "      # If scenario prediction tile does not exist:\n",
        "      if scenario_tile_exists == False:\n",
        "        # Load template tile parameters\n",
        "        template_tile_dir = join(tile_templates_dir, f\"template_tile_{stack}.tif\")\n",
        "        template_tile_y = gdal.Open(template_tile_dir).GetRasterBand(1).YSize\n",
        "        template_tile_x = gdal.Open(template_tile_dir).GetRasterBand(1).XSize\n",
        "        # Load feature tile stack\n",
        "        stack_filename = f\"feature_stack_{scenario}_{stack}.npy\"\n",
        "        feature_stack = np.load(join(scenario_feature_stack_dir, stack_filename))\n",
        "        # Add covariates (sensitivity and BEAM)\n",
        "        if add_covariates: feature_stack = np.hstack((feature_stack,\n",
        "                           np.full((feature_stack.shape[0], 1), beam_value, dtype=int),\n",
        "                           np.full((feature_stack.shape[0], 1), sensitivity_value, dtype=float)\n",
        "                           ))\n",
        "        # Define prediction array and reshape\n",
        "        prediction = XGBPredictor.predict(feature_stack)\n",
        "        feature_stack = None # Flush feature stack\n",
        "        prediction_tile = prediction.reshape((template_tile_y, template_tile_x))\n",
        "        prediction = None # Flush prediction\n",
        "        # Export prediction array as .tif\n",
        "        export_array_as_tif(prediction_tile, join(tile_cache_scenario_dir, scenario_tile_filename), template = template_tile_dir, compress = False)\n",
        "        prediction_tile = None # Flush prediction tile\n",
        "        # Update progress\n",
        "      tile_progress_index += 1\n",
        "      tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_stacks}\"\n",
        "    # Prepare empty array for merging tiles\n",
        "    prediction_array = np.empty((0,template_tile_x))\n",
        "    # Read each tile .tif as an array, stack, then export as a .tif\n",
        "    for subdir in os.listdir(tile_cache_scenario_dir):\n",
        "      if subdir.endswith('.tif'):\n",
        "        tile_dir = join(tile_cache_scenario_dir, subdir)\n",
        "        prediction_array = np.vstack((prediction_array, gdal.Open(tile_dir).ReadAsArray()))\n",
        "    # Define scenario template\n",
        "    scenario_template = join(features_dir, os.listdir(features_dir)[0])\n",
        "    scenario_prediction_unmasked_dir = join(scenario_predictions_unmasked_dir, f\"{scenario_prediction_unmasked_filename}\")\n",
        "    export_array_as_tif(prediction_array, scenario_prediction_unmasked_dir, template = scenario_template, compress = True)\n",
        "    # Delete scenario tile cache directory\n",
        "    shutil.rmtree(tile_cache_scenario_dir)\n",
        "  # Reset tile progress\n",
        "  tile_progress_index = 0\n",
        "  # Update scenario progress\n",
        "  scenario_progress_index += 1\n",
        "  scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\"\n",
        "print(\"\\nScenario predictions complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diwt1peRdXpj"
      },
      "source": [
        "# Mask scenario predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQsX1NCiy0aW"
      },
      "outputs": [],
      "source": [
        "# Use polygons for masking, only areas inside the polygons will be included\n",
        "\n",
        "# Exclude existing polygons from search\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "\n",
        "print(\"mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"  '{polygon[:-5]}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daKAe6nkJ2bS"
      },
      "outputs": [],
      "source": [
        "mask_polygons = [\n",
        "  # 'project_area',\n",
        "  'gedi_area',\n",
        "  # 'peninsular_malaysia',\n",
        "  # 'pa_taman_krau',\n",
        "  # 'pa_ais',\n",
        "]\n",
        "\n",
        "# Create an inverse project area path for masking\n",
        "template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "for polygon in mask_polygons:\n",
        "  inverse_polygon_path = join(polygons_dir, f\"{polygon}_inverse.gpkg\")\n",
        "  if not exists(inverse_polygon_path):\n",
        "    polygon_path = join(polygons_dir, f\"{polygon}.gpkg\")\n",
        "    template_polygon = gpd.read_file(template_polygon_path)\n",
        "    polygon_read = gpd.read_file(polygon_path)\n",
        "    polygon_crs = polygon_read.crs.to_epsg()\n",
        "    inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "    inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "    inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "    print(f\"An inverse masking polygon for {polygon} has been created in {polygons_dir}.\")\n",
        "  else: print(f\"An inverse masking polygon for {polygon} already exists.\")\n",
        "\n",
        "unmasked_predictions = []\n",
        "for scenario_prediction in os.listdir(scenario_predictions_unmasked_dir):\n",
        "  unmasked_predictions.append(scenario_prediction)\n",
        "\n",
        "# Determine last feature year for masking future scenarios\n",
        "final_feature_years = []\n",
        "for final_feature in os.listdir(feature_final_dir):\n",
        "  if final_feature.endswith('.tif') and final_feature[-9] == '_':\n",
        "    try: final_feature_years.append(int(final_feature[-8:-4]))\n",
        "    except: continue\n",
        "last_feature_year = max(final_feature_years)\n",
        "\n",
        "# Binary progress\n",
        "masking_progress_index = 0\n",
        "masking_progress_label = widgets.Label(f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\")\n",
        "display(masking_progress_label)\n",
        "\n",
        "# Mask scenarios with the relevatant mask\n",
        "for scenario_prediction in unmasked_predictions: # Loop through each unmasked scenario\n",
        "  scenario_masked_filename = f\"{scenario_prediction[:-13]}.tif\"\n",
        "  scenario_masked_dir = join(scenario_predictions_dir, scenario_masked_filename)\n",
        "  if not exists(scenario_masked_dir):\n",
        "    mask_exists = False\n",
        "    for mask in os.listdir(masks_dir):\n",
        "      # Match all oldgrowth scenarios\n",
        "      if 'all_oldgrowth' in mask or 'all_oldgrowth' in scenario_prediction:\n",
        "        if 'all_oldgrowth' in mask and 'all_oldgrowth' in scenario_prediction:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "      else: # Match all other historic scenarios\n",
        "        scenario_year = int(scenario_prediction[:4])\n",
        "        mask_year = int(mask[12:16])\n",
        "        if scenario_year == mask_year:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "        else: # Match future scenarios with most recent forest mask\n",
        "          if scenario_year > last_feature_year and last_feature_year == mask_year:\n",
        "            selected_mask_filename = mask\n",
        "            selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "            mask_exists = True\n",
        "    if mask_exists == False: print(f\"A suitable mask for {scenario_prediction} does not exist.\\n\")\n",
        "    else: # Mask the scenario prediction\n",
        "      print(f\"Masking {scenario_prediction} with {selected_mask_filename}...\")\n",
        "      mask_array = gdal.Open(selected_mask_dir).ReadAsArray()\n",
        "      scenario_prediction_unmasked_dir = join(scenario_predictions_unmasked_dir, scenario_prediction)\n",
        "      scenario_prediction_array = gdal.Open(scenario_prediction_unmasked_dir).ReadAsArray()\n",
        "      # Mask where the mask array is not 1\n",
        "      scenario_masked_array = np.where(mask_array != 1, nodatavalue, scenario_prediction_array)\n",
        "      export_array_as_tif(scenario_masked_array, scenario_masked_dir, compress = True)\n",
        "      if len(mask_polygons) > 0:\n",
        "        for polygon_mask in mask_polygons:\n",
        "          inverse_gedi_area_path = join(polygons_dir, f\"{polygon_mask}_inverse.gpkg\")\n",
        "          print(f\"Masking {scenario_prediction} with {polygon_mask}...\")\n",
        "          burn_polygon_to_raster(scenario_masked_dir, inverse_gedi_area_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        # Recompress the prediction after burning the polygon masks\n",
        "        scenario_masked_array_2 = gdal.Open(scenario_masked_dir).ReadAsArray()\n",
        "        export_array_as_tif(scenario_masked_array_2, scenario_masked_dir, compress = True)\n",
        "      print(f\"{scenario_masked_filename} exported.\")\n",
        "  # Update masking progress\n",
        "  masking_progress_index += 1\n",
        "  masking_progress_label.value = f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECHaTfrs0n7i"
      },
      "source": [
        "# Scenario disturbance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6soEuon0qt5"
      },
      "outputs": [],
      "source": [
        "scenarios_diff_set = set()\n",
        "for masked_scenario in os.listdir(scenario_predictions_dir):\n",
        "    scenarios_diff_set.add(masked_scenario.split(\"__\")[0])\n",
        "\n",
        "# Generate all possible pairs of scenarios, including all orders\n",
        "scenario_pairs = sorted(list(itertools.permutations(scenarios_diff_set, 2)))\n",
        "\n",
        "print(\"# Select scenarios to calculate mean difference with uncertainty\")\n",
        "print(\"scenario_pairs = [\")\n",
        "for s1, s2 in scenario_pairs:\n",
        "    print(f\" ('{s1}','{s2}'),\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12skqTzu2qoO"
      },
      "outputs": [],
      "source": [
        "# Select scenarios to calculate difference\n",
        "scenario_pairs = [\n",
        "  ('2023', 'all_oldgrowth'),\n",
        "  ('2023', '2023_no_degradation_since_1990'),\n",
        "  ('2023', '2023_oldgrowth'),\n",
        "  ('2023_no_degradation_since_1990', '2023_oldgrowth'),\n",
        "  ('2023_oldgrowth', 'all_oldgrowth'),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inR5vYflAqKz"
      },
      "outputs": [],
      "source": [
        "# Rename scenario differences to disturbance types (optional)\n",
        "difference_names = {\n",
        "  ('2023', 'all_oldgrowth'):\n",
        "    '2023_degradation_deforestation_total',\n",
        "  ('2023', '2023_no_degradation_since_1990'):\n",
        "    '2023_degradation_since_1990',\n",
        "  ('2023', '2023_oldgrowth'):\n",
        "    '2023_degradation_total',\n",
        "  ('2023_no_degradation_since_1990', '2023_oldgrowth'):\n",
        "    '2023_degradation_before_1990',\n",
        "  ('2023_oldgrowth', 'all_oldgrowth'):\n",
        "    '2023_deforestation_total',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P-0bj40AxZ2"
      },
      "outputs": [],
      "source": [
        "# Function for difference\n",
        "def diff(scenario1, scenario2):\n",
        "  diff_array = scenario1 - scenario2\n",
        "  return diff_array\n",
        "\n",
        "# Loop through the scenario pairs\n",
        "for scenario1, scenario2 in scenario_pairs:\n",
        "\n",
        "  # Lookup the description from the dictionary\n",
        "  difference_name = difference_names.get((scenario1, scenario2), f\"{scenario1}_-_{scenario2}\")\n",
        "\n",
        "  # Define filenames and directories of difference .tifs\n",
        "  diff_filename = f\"{difference_name}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "  diff_dir = join(scenario_dist_dir, diff_filename)\n",
        "\n",
        "  if not exists(diff_dir):\n",
        "    print(f\"Calculating difference between {scenario1} and {scenario2}\")\n",
        "    scenario1_filename = f\"{scenario1}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "    scenario2_filename = f\"{scenario2}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "\n",
        "    # Define difference directories, assert that both exist for both scenarios\n",
        "    scenario1_dir = join(scenario_predictions_dir, scenario1_filename)\n",
        "    assert exists(scenario1_dir), f\"{scenario1_dir} does not exist.\"\n",
        "    scenario2_dir = join(scenario_predictions_dir, scenario2_filename)\n",
        "    assert exists(scenario2_dir), f\"{scenario1_dir} does not exist.\"\n",
        "\n",
        "    # Convert scenario .tifs to temporary arrays\n",
        "    scenario1_array_temp = gdal.Open(scenario1_dir).ReadAsArray()\n",
        "    scenario2_array_temp = gdal.Open(scenario2_dir).ReadAsArray()\n",
        "\n",
        "    # Fill scenario nodata values with 0 if they are not nodatavalues in the other scenario\n",
        "    scenario1_array = np.where((scenario1_array_temp == nodatavalue) & (scenario2_array_temp != nodatavalue), 0, scenario1_array_temp)\n",
        "    scenario2_array = np.where((scenario2_array_temp == nodatavalue) & (scenario1_array != nodatavalue), 0, scenario2_array_temp)\n",
        "\n",
        "    # Create difference arrays where the value is not 'nodatavalue'\n",
        "    diff_array = np.where(scenario1_array==nodatavalue, nodatavalue, diff(scenario1_array, scenario2_array))\n",
        "    export_array_as_tif(diff_array, diff_dir, template = scenario1_dir), print(f\"{diff_filename} has been exported.\")\n",
        "\n",
        "  else: print(f\"{diff_filename} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intactness"
      ],
      "metadata": {
        "id": "81OCJi98NDwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scenario_predictions_dir"
      ],
      "metadata": {
        "id": "O5i-btFpxk-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select which baseline and disturbance raster to use for calculating intactness\n",
        "# percentage and relative intactness. Ideally this is the scenario with the least disturbance\n",
        "# and the difference between that and the current reality.\n",
        "for baseline in os.listdir(scenario_predictions_dir):\n",
        "  print(f\"selected_baseline = '{baseline}'\")\n",
        "for dist in os.listdir(scenario_dist_dir):\n",
        "  print(f\"selected_dist = '{dist}'\")"
      ],
      "metadata": {
        "id": "wsend_eeNIll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_baseline = 'all_oldgrowth__asartr_agbd_250112_131618.tif'\n",
        "selected_dist = '2023_degradation_deforestation_total__asartr_agbd_250112_131618.tif'\n",
        "forest_mask_year = '2023'\n",
        "\n",
        "percentage_filename = f\"percentage_change__{selected_baseline.split('__')[0]}__{selected_dist.split('__')[0]}__{selected_dist.split('__')[1]}\"\n",
        "percentage_path = join(intactness_dir, percentage_filename)\n",
        "\n",
        "if not exists(percentage_path):\n",
        "  # Define filenames and directories\n",
        "  selected_baseline_path = join(scenario_predictions_dir, selected_baseline)\n",
        "  selected_dist_path = join(scenario_dist_dir, selected_dist)\n",
        "  selected_mask_path = join(masks_dir, f\"mask_forest_{forest_mask_year}.tif\")\n",
        "\n",
        "  # Convert to arrays\n",
        "  selected_baseline_array = gdal.Open(selected_baseline_path).ReadAsArray()\n",
        "  selected_dist_array = gdal.Open(selected_dist_path).ReadAsArray()\n",
        "  selected_mask_array = gdal.Open(selected_mask_path).ReadAsArray()\n",
        "\n",
        "  # Create percentage array where the value is not 'nodatavalue' in any of the inputs\n",
        "  percentage_array = np.where((selected_mask_array==nodatavalue) | (selected_baseline_array==nodatavalue) | (selected_dist_array==nodatavalue), nodatavalue,\n",
        "                              selected_dist_array/selected_baseline_array*100)\n",
        "  export_array_as_tif(percentage_array, percentage_path, template = selected_baseline_path)\n",
        "  print(f\"{percentage_filename} has been exported.\")\n",
        "\n",
        "else: print(f\"{percentage_filename} already exists.\")"
      ],
      "metadata": {
        "id": "87Q6aMSdNIye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use additional polygons for masking relative intactness quantiles\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"mask_polygon = '{polygon}'\")"
      ],
      "metadata": {
        "id": "YwXRZTew1r39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mask_polygon = 'forest_reserves.gpkg'\n",
        "mask_polygon = None\n",
        "\n",
        "if mask_polygon is not None:\n",
        "  # Create an inverse project area path for masking\n",
        "  template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "  inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "  if not exists(inverse_polygon_path):\n",
        "    polygon_path = join(polygons_dir, mask_polygon)\n",
        "    template_polygon = gpd.read_file(template_polygon_path)\n",
        "    polygon_read = gpd.read_file(polygon_path)\n",
        "    polygon_crs = polygon_read.crs.to_epsg()\n",
        "    inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "    inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "    inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "    print(f\"An inverse masking polygon for {polygon} has been created in {polygons_dir}.\")\n",
        "  else: print(f\"An inverse masking polygon for {polygon} already exists.\")\n",
        "\n",
        "  # Copy the percentage raster for potential masking\n",
        "  percentage_masked_filename = f\"{percentage_filename[:-4]}_masked_{mask_polygon[:-5]}.tif\"\n",
        "  percentage_masked_path = join(intactness_dir, percentage_masked_filename)\n",
        "  if not exists(percentage_masked_path):\n",
        "    print(f\"Copying {percentage_filename} for masking...\")\n",
        "    copyfile(percentage_path, percentage_masked_path)\n",
        "    print(f\"Masking {percentage_filename} with {mask_polygon}...\")\n",
        "    burn_polygon_to_raster(percentage_masked_path, inverse_polygon_path, fixed_value=nodatavalue, all_touched=False)\n",
        "    # Recompress the prediction after burning the polygon masks\n",
        "    percentage_masked_array = gdal.Open(percentage_masked_path).ReadAsArray()\n",
        "    export_array_as_tif(percentage_masked_array, percentage_masked_path, compress = True)\n",
        "    print(f\"{percentage_filename} masked.\")\n",
        "  else: print(f\"{percentage_masked_path} already exists.\")\n",
        "\n",
        "else: print(\"No additional mask will be used to calculate relative intactness.\")"
      ],
      "metadata": {
        "id": "Ru_UDUkAPw-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of quantiles for intactness rating (e.g. 10 for 1 - 10)\n",
        "num_quantiles = 10\n",
        "\n",
        "# Define paths and arrays\n",
        "if mask_polygon is None: relative_intactness_name = f'intactness__{num_quantiles}_quantiles'\n",
        "else: relative_intactness_name = f'intactness__{mask_polygon[:-5]}_{num_quantiles}_quantiles'\n",
        "relative_intactness_path = join(intactness_dir, f'{relative_intactness_name}.tif')\n",
        "if mask_polygon is None: percentage_array = gdal.Open(percentage_path).ReadAsArray()\n",
        "else: percentage_array = gdal.Open(percentage_masked_path).ReadAsArray()\n",
        "relative_intactness_array = np.empty_like(percentage_array, dtype=object)\n",
        "\n",
        "# Set all values above 0 to 0, assuming negative values are not intact\n",
        "percentage_array[percentage_array > 0] = 0\n",
        "\n",
        "# Separate valid and invalid (nodatavalue) elements\n",
        "valid_elements = percentage_array[percentage_array != nodatavalue]\n",
        "invalid_elements = percentage_array == nodatavalue\n",
        "\n",
        "# Calculate quantiles for valid elements\n",
        "quantiles = np.percentile(valid_elements, np.linspace(0, 100, num_quantiles + 1)[1:-1]) if len(valid_elements) > 0 else []\n",
        "for i in range(1, num_quantiles + 1):\n",
        "    lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "    upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "    relative_intactness_array[(percentage_array > lower_bound) & (percentage_array <= upper_bound)] = i\n",
        "# if nodatavalue is not None:\n",
        "    relative_intactness_array[invalid_elements] = nodatavalue\n",
        "export_array_as_tif(relative_intactness_array, relative_intactness_path)\n",
        "\n",
        "# Prepare data for CSV: Collect lower and upper bounds for each category\n",
        "ranges_data = {'Lower_Bound': [], 'Upper_Bound': []}\n",
        "for i in range(1, num_quantiles + 1):\n",
        "    lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "    upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "    ranges_data['Lower_Bound'].append(lower_bound)\n",
        "    ranges_data['Upper_Bound'].append(upper_bound)\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "relative_intactness_df = pd.DataFrame(ranges_data)\n",
        "relative_intactness_csv_path = os.path.join(intactness_dir, f'{relative_intactness_name}.csv')\n",
        "relative_intactness_df.to_csv(relative_intactness_csv_path, index=False)\n",
        "\n",
        "# Generate and save histogram as .png\n",
        "histogram_path = join(intactness_dir, f'{relative_intactness_name}.png')\n",
        "plt.figure()\n",
        "plt.hist(valid_elements.flatten(), bins='auto')\n",
        "plt.title(f'{relative_intactness_name} Histogram')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.savefig(histogram_path)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "wCQ_GUBiNBx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-7ctfgdoF2"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcuArY00N9Mc"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "QJ8jLRKRSfgb",
        "FkNV_N7UKcMd"
      ],
      "gpuType": "V5E1",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}