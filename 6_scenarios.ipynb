{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/6_scenarios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8jLRKRSfgb"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkvV803eSUQY"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "\n",
        "# Mount Google Drive and set base directory\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEpdIjOwSiB1"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install astropy\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!pip install xgboost --upgrade\n",
        "!apt-get install -y gdal-bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQDSsvGeSi4u"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import itertools\n",
        "import json\n",
        "from os import makedirs\n",
        "from os.path import join, exists\n",
        "from osgeo import gdal\n",
        "import ipywidgets as widgets\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import psutil\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "import re\n",
        "from scipy import ndimage\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxgT3K1Sjum"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "\n",
        "predictor_dir = join(base_dir, \"3_predictors\")\n",
        "predictor_resampled_dir = join(predictor_dir, \"resampled\")\n",
        "predictor_final_dir = join(predictor_dir, \"final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "masks_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(scenarios_dir, exist_ok=True)\n",
        "makedirs(masks_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96FAfeyooMLz"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)\n",
        "\n",
        "# Burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster, polygon, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "  with rasterio.open(raster, 'r+') as src:\n",
        "      array = src.read(1)\n",
        "      transform = src.transform\n",
        "      gdf = gpd.read_file(polygon)\n",
        "      for geom in gdf.geometry:\n",
        "          if not fixed and column_name == None:\n",
        "              column_name = gdf.columns[0]\n",
        "          if not fixed: burn_value = gdf.loc[gdf.geometry == geom, column_name].values[0]\n",
        "          else: burn_value = fixed_value\n",
        "          rasterize([(geom, burn_value)], out=array, transform=transform,\n",
        "              all_touched=all_touched, dtype=src.meta['dtype'], out_shape=src.shape)\n",
        "      src.write(array, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_OkHaUkTCyB"
      },
      "source": [
        "# Select model and scenario area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mB0XrPNYU_I"
      },
      "outputs": [],
      "source": [
        "# Select a model\n",
        "model_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file == 'model.json':\n",
        "      print(f'selected_model = \"{subdir.split(f\"{models_dir}/\",1)[1]}\"')\n",
        "      model_exists = True\n",
        "if not model_exists:\n",
        "  print(\"No model exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbXY6iP_qjYc"
      },
      "outputs": [],
      "source": [
        "selected_model = \"agbd_240926_030225\"\n",
        "categorise_variate = False # If the variate was categorised in 5_models\n",
        "\n",
        "# Define model directories\n",
        "selected_model_dir = join(models_dir,selected_model)\n",
        "selected_model_json = join(selected_model_dir, \"model.json\")\n",
        "selected_model_descr_dir = join(selected_model_dir, \"model_description.json\")\n",
        "selected_model_dataset_path = join(selected_model_dir, f\"{selected_model}.pkl\")\n",
        "selected_model_dataset = pd.read_pickle(selected_model_dataset_path)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(selected_model_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_variate = model_dataset_description[\"selected_variate\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "covariates_categorised = model_dataset_description[\"covariates_categorised\"]\n",
        "selected_predictors = model_dataset_description[\"selected_predictors\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "filter_parameter = model_dataset_description[\"filter_parameter\"]\n",
        "filter_values_to_include = model_dataset_description[\"filter_values_to_include\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "covariates = [covariate[4:] for covariate in covariates_renamed]\n",
        "\n",
        "# Create scenarios model directory\n",
        "scenarios_model_dir = join(scenarios_dir, selected_model)\n",
        "makedirs(scenarios_model_dir, exist_ok=True)\n",
        "\n",
        "# Copy model_dataset_description.json\n",
        "with open(join(scenarios_model_dir, \"model_dataset_description.json\"), \"w\") as file:\n",
        "  file.write(json.dumps(model_dataset_description))\n",
        "\n",
        "# Select a scenario area\n",
        "scenario_area_exists = False\n",
        "for subdir in os.listdir(scenarios_model_dir):\n",
        "  if not subdir.endswith('.csv') and not subdir.endswith('.json'):\n",
        "    print(f'selected_scenario_area = \"{subdir}\"')\n",
        "    scenario_area_exists = True\n",
        "if not scenario_area_exists:\n",
        "  print(f\"Create a scenario area directory in {scenarios_model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNYQ3-q2dUKl"
      },
      "outputs": [],
      "source": [
        "selected_scenario_area = \"tekai\"\n",
        "\n",
        "# Define scenario area directory\n",
        "scenario_area_dir = join(scenarios_model_dir,selected_scenario_area)\n",
        "makedirs(scenario_area_dir, exist_ok=True)\n",
        "\n",
        "# Create subdirectories\n",
        "predictors_dir = join(scenario_area_dir, \"predictors\")\n",
        "tile_templates_dir = join(scenario_area_dir, 'tile_templates')\n",
        "tile_predictors_dir = join(scenario_area_dir, \"tile_predictors\")\n",
        "tile_predictor_stacks_dir = join(scenario_area_dir, \"tile_predictor_stacks\")\n",
        "tile_prediction_cache_dir = join(scenario_area_dir,\"tile_prediction_cache\")\n",
        "scenario_predictions_unmasked_dir = join(scenario_area_dir,\"scenario_predictions_unmasked\")\n",
        "scenario_predictions_dir = join(scenario_area_dir, \"scenario_predictions\")\n",
        "scenario_diff_dir = join(scenario_area_dir, \"scenario_difference\")\n",
        "\n",
        "makedirs(predictors_dir, exist_ok=True)\n",
        "makedirs(tile_templates_dir, exist_ok=True)\n",
        "makedirs(tile_predictors_dir, exist_ok=True)\n",
        "makedirs(tile_predictor_stacks_dir, exist_ok=True)\n",
        "makedirs(tile_prediction_cache_dir, exist_ok=True)\n",
        "makedirs(scenario_predictions_unmasked_dir, exist_ok=True)\n",
        "makedirs(scenario_predictions_dir, exist_ok=True)\n",
        "makedirs(scenario_diff_dir, exist_ok=True)\n",
        "\n",
        "# Copy predictors from the final predictors directory\n",
        "for predictor in os.listdir(predictor_final_dir):\n",
        "  if predictor not in os.listdir(predictors_dir):\n",
        "    predictor_original_path = join(predictor_final_dir, predictor)\n",
        "    predictor_copy_path = join(predictors_dir, predictor)\n",
        "    copyfile(predictor_original_path, predictor_copy_path)\n",
        "print(f\"All predictors present in the following directory have already been copied over: {predictor_final_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3w7svaUvs07"
      },
      "source": [
        "# Define scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJZoEWycn8-x"
      },
      "outputs": [],
      "source": [
        "# Yearly scenarios may include years after the model scenario if predictor data is available later than GEDI data.\n",
        "# Date of the prediction is ~December 31st, e.g. '2023' is 31/12/2023, requiring predictors up to 2023.\n",
        "\n",
        "model_scenario_override = None # set if cannot be automatically determined from model predictors\n",
        "\n",
        "yearly_predictors = [\"forest_with_edge_effects\", \"disturbance_with_edge_effects\"]\n",
        "\n",
        "# Remove the 'pre_' prefix from each predictor\n",
        "model_predictors = sorted([predictor[4:] for predictor in selected_predictors])\n",
        "\n",
        "# Create a list of predictor years from the model's predictors\n",
        "model_predictor_years = []\n",
        "for predictor in model_predictors:\n",
        "  for yearly_predictor in yearly_predictors:\n",
        "    if yearly_predictor in predictor:\n",
        "      model_predictor_years.append(int(predictor[-4:]))\n",
        "\n",
        "# Determine the model scenario from the maximum year\n",
        "# Values from the most recent predictor year (e.g. 2022) will be applied to the second most recent (e.g. 2021) as a proxy at the predictor stack stage\n",
        "if model_scenario_override != None: model_scenario = model_scenario_override\n",
        "else: model_scenario = max(model_predictor_years)\n",
        "model_scenario_filename = f\"{model_scenario}.csv\"\n",
        "model_scenario_dir = join(scenarios_model_dir,model_scenario_filename)\n",
        "print(f\"The maximum year used in the model is {model_scenario}, which has been created as the first scenario.\\n\")\n",
        "print(f\"The {model_scenario} scenario predictor list has been saved to:\\n {model_scenario_dir}\\n\")\n",
        "print(f\"Ensure all predictors in this list have been copied to:\\n{predictors_dir}\\n\")\n",
        "\n",
        "# Save the model scenario predictors as a .csv\n",
        "pd.DataFrame(model_predictors).to_csv(model_scenario_dir, index=False)\n",
        "\n",
        "# Determine available predictor years\n",
        "final_predictor_years = []\n",
        "for final_predictor in os.listdir(predictor_final_dir):\n",
        "  if final_predictor.endswith('.tif') and final_predictor[-9] == '_':\n",
        "    try: final_predictor_years.append(int(final_predictor[-8:-4]))\n",
        "    except: continue\n",
        "\n",
        "# Find the first and last predictor years\n",
        "first_predictor_year = min(final_predictor_years)\n",
        "last_predictor_year = max(final_predictor_years)\n",
        "additional_predictor_years = last_predictor_year - model_scenario\n",
        "print(f\"The first available predictor year is {first_predictor_year} and the last is {last_predictor_year}.\\n\")\n",
        "\n",
        "# Calculate the range of scenario years and minimum scenario year\n",
        "model_scenario_year_range = max(model_predictor_years) - (min(model_predictor_years))\n",
        "minimum_yearly_scenario = first_predictor_year + model_scenario_year_range\n",
        "print(f\"The earliest scenario year that can be predicted is {minimum_yearly_scenario}.\")\n",
        "print(f\"The latest scenario year that can be predicted is {last_predictor_year}.\")\n",
        "print(f\"This is based on the number of yearly predictors used to train the model and the total availability of predictors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPd_nrFYeGJ3"
      },
      "outputs": [],
      "source": [
        "# Select constant predictors which are the same in every scenario, e.g. topography\n",
        "print(\"constant_predictors = [\")\n",
        "for predictor in model_predictors:\n",
        "  print(f'  \"{predictor}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARVJed3vTgBv"
      },
      "outputs": [],
      "source": [
        "constant_predictors = [\n",
        "  \"coast_proximity_km\",\n",
        "  \"latitude\",\n",
        "  \"longitude\",\n",
        "  \"topo_cor_smooth_aspect_cosine\",\n",
        "  \"topo_cor_smooth_aspect_sine\",\n",
        "  \"topo_cor_smooth_circular_variance_aspect_03\",\n",
        "  \"topo_cor_smooth_circular_variance_aspect_07\",\n",
        "  \"topo_cor_smooth_circular_variance_aspect_11\",\n",
        "  \"topo_cor_smooth_deviation_mean_elevation_03\",\n",
        "  \"topo_cor_smooth_deviation_mean_elevation_07\",\n",
        "  \"topo_cor_smooth_deviation_mean_elevation_11\",\n",
        "  \"topo_cor_smooth_eastness\",\n",
        "  \"topo_cor_smooth_elevation\",\n",
        "  \"topo_cor_smooth_northness\",\n",
        "  \"topo_cor_smooth_profile_curvature\",\n",
        "  \"topo_cor_smooth_roughness_03\",\n",
        "  \"topo_cor_smooth_roughness_07\",\n",
        "  \"topo_cor_smooth_roughness_11\",\n",
        "  \"topo_cor_smooth_slope\",\n",
        "  \"topo_cor_smooth_stream_power_index_log10\",\n",
        "  \"topo_cor_smooth_surface_area_ratio\",\n",
        "  \"topo_cor_smooth_tangential_curvature\",\n",
        "  \"topo_cor_smooth_topographic_position_index_03\",\n",
        "  \"topo_cor_smooth_topographic_position_index_07\",\n",
        "  \"topo_cor_smooth_topographic_position_index_11\",\n",
        "  \"topo_cor_smooth_topographic_ruggedness_index\",\n",
        "  \"topo_cor_smooth_topographic_wetness_index\",\n",
        "  \"topo_cor_unsmooth_aspect_cosine\",\n",
        "  \"topo_cor_unsmooth_aspect_sine\",\n",
        "  \"topo_cor_unsmooth_circular_variance_aspect_03\",\n",
        "  \"topo_cor_unsmooth_circular_variance_aspect_07\",\n",
        "  \"topo_cor_unsmooth_circular_variance_aspect_11\",\n",
        "  \"topo_cor_unsmooth_deviation_mean_elevation_03\",\n",
        "  \"topo_cor_unsmooth_deviation_mean_elevation_07\",\n",
        "  \"topo_cor_unsmooth_deviation_mean_elevation_11\",\n",
        "  \"topo_cor_unsmooth_eastness\",\n",
        "  \"topo_cor_unsmooth_elevation\",\n",
        "  \"topo_cor_unsmooth_northness\",\n",
        "  \"topo_cor_unsmooth_profile_curvature\",\n",
        "  \"topo_cor_unsmooth_roughness_03\",\n",
        "  \"topo_cor_unsmooth_roughness_07\",\n",
        "  \"topo_cor_unsmooth_roughness_11\",\n",
        "  \"topo_cor_unsmooth_slope\",\n",
        "  \"topo_cor_unsmooth_stream_power_index_log10\",\n",
        "  \"topo_cor_unsmooth_surface_area_ratio\",\n",
        "  \"topo_cor_unsmooth_tangential_curvature\",\n",
        "  \"topo_cor_unsmooth_topographic_position_index_03\",\n",
        "  \"topo_cor_unsmooth_topographic_position_index_07\",\n",
        "  \"topo_cor_unsmooth_topographic_position_index_11\",\n",
        "  \"topo_cor_unsmooth_topographic_ruggedness_index\",\n",
        "  \"topo_cor_unsmooth_topographic_wetness_index\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg4XvxT5foVR"
      },
      "outputs": [],
      "source": [
        "# Note that areas that were forested in a historic yearly scenario but were reservoirs\n",
        "# or flooded at the time topographic predictor data was collected (~2014 for GLO-30 DEM)\n",
        "# will be predicted based on a flat topography at the elevation of the water's surface.\n",
        "\n",
        "# Set scenario predictors as all non-constant predictors\n",
        "scenario_predictors = sorted(list(set(model_predictors) - set(constant_predictors)))\n",
        "\n",
        "# Create predictor lists for all possible yearly scenarios\n",
        "for yearly_scenario in range(minimum_yearly_scenario, last_predictor_year +1):\n",
        "  year_difference = model_scenario - yearly_scenario\n",
        "  yearly_scenario_predictors = []\n",
        "  for scenario_predictor in scenario_predictors:\n",
        "    try:\n",
        "      year_change = int(scenario_predictor[-4:]) - year_difference\n",
        "      yearly_scenario_predictor = scenario_predictor[:-4] + str(year_change)\n",
        "      yearly_scenario_predictors.append(yearly_scenario_predictor)\n",
        "    except: yearly_scenario_predictors.append(scenario_predictor)\n",
        "  # Compile yearly predictors and save as a .csv\n",
        "  yearly_scenario_predictors = sorted(yearly_scenario_predictors + constant_predictors)\n",
        "  yearly_scenario_filename = f\"{yearly_scenario}.csv\"\n",
        "  yearly_scenario_dir = join(scenarios_model_dir,yearly_scenario_filename)\n",
        "  pd.DataFrame(yearly_scenario_predictors).to_csv(yearly_scenario_dir, index=False)\n",
        "\n",
        "# Open the most recent yearly scenario predictor list\n",
        "most_recent_scenario_csv = join(scenarios_model_dir,f\"{last_predictor_year}.csv\")\n",
        "most_recent_scenario_predictors = pd.Series.tolist(pd.read_csv(most_recent_scenario_csv).iloc[:,0])\n",
        "\n",
        "print(f\"Lists of predictors for all possible yearly scenarios have been exported to {scenarios_model_dir}/.\")\n",
        "print(f\"Ensure all predictors in these lists have been copied to:\\n{predictors_dir}\\n\")\n",
        "\n",
        "# Create a 'no disturbance' predictor for alternate scenarios.\n",
        "# Assumes the minimum possible value is present in the first scenario year.\n",
        "minimum_disturbance_name = f\"disturbance_with_edge_effects_0000\"\n",
        "minimum_disturbance_path = join(predictors_dir, f\"{minimum_disturbance_name}.tif\")\n",
        "if not exists(minimum_disturbance_path):\n",
        "  example_disturbance = join(predictors_dir, f\"disturbance_with_edge_effects_{first_predictor_year}.tif\")\n",
        "  example_disturbance_array = gdal.Open(example_disturbance).ReadAsArray()\n",
        "  minimum_disturbance_value = example_disturbance_array.min()\n",
        "  minimum_disturbance_array = np.where(example_disturbance_array, minimum_disturbance_value, minimum_disturbance_value)\n",
        "  export_array_as_tif(minimum_disturbance_array, minimum_disturbance_path, template = example_disturbance)\n",
        "  print(f\"The minimum disturbance value is {minimum_disturbance_value}\\n,\")\n",
        "  print(f\"which has been used to create the 'minimum disturbance' predictor {minimum_disturbance_name}.\")\n",
        "else: print(f\"The minimum disturbance predictor {minimum_disturbance_name} already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwwIij7sL4Q_"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios remove degradation or deforestation and degradation\n",
        "# prior to the most recent yearly scenario (e.g. 2023)\n",
        "\n",
        "# 'No degradation' scenarios remove disturbance since the indicated year\n",
        "define_nodeg_scenarios = True\n",
        "\n",
        "no_degradation_since = [\n",
        "    first_predictor_year,\n",
        "    # 2000,\n",
        "]\n",
        "\n",
        "for year in no_degradation_since:\n",
        "  assert year >= first_predictor_year, \"Years in 'no_degradation_since' must be set to or after the first predictor year.\"\n",
        "\n",
        "\n",
        "# 'No degradation and no deforestation' scenarios remove disturbance\n",
        "# and add the forest lost since the indicated year\n",
        "define_nodeg_nodef_scenarios = True\n",
        "\n",
        "no_deforestation_or_degradation_since = [\n",
        "    first_predictor_year,\n",
        "    # 2000,\n",
        "]\n",
        "\n",
        "for year in no_deforestation_or_degradation_since:\n",
        "  assert year >= first_predictor_year, \"Years in 'no_deforestation_or_degradation_since' must be set to or after the first predictor year.\"\n",
        "\n",
        "# Create a predictor list for 'no degradation' scenarios\n",
        "if define_nodeg_scenarios:\n",
        "  for year in no_degradation_since:\n",
        "    nodeg_predictors = []\n",
        "    for scenario_predictor in most_recent_scenario_predictors:\n",
        "      if \"disturbance_with_edge_effects\" in scenario_predictor:\n",
        "        scenario_predictor_year = int(scenario_predictor[-4:])\n",
        "        if scenario_predictor_year > year: # Only remove disturbance from years specified\n",
        "          nodeg_predictors.append(minimum_disturbance_name)\n",
        "        else: nodeg_predictors.append(scenario_predictor)\n",
        "      else: nodeg_predictors.append(scenario_predictor)\n",
        "    nodeg_predictors = sorted(nodeg_predictors) # Sort to ensure order is maintained\n",
        "    nodeg_scenario_filename = f\"{last_predictor_year}_nodeg_since_{year}.csv\"\n",
        "    nodeg_scenario_path = join(scenarios_model_dir, nodeg_scenario_filename)\n",
        "    pd.DataFrame(nodeg_predictors).to_csv(nodeg_scenario_path, index=False)\n",
        "    print(f\"Predictor list for a scenario without degradation between\")\n",
        "    print(f\"{last_predictor_year} and {year} exported to {nodeg_scenario_filename}.\\n\")\n",
        "else: print(\"The 'no degradation' scenarios are not enabled.\\n\")\n",
        "\n",
        "# Create a predictor list for 'no Landsat deforestation and no Landsat degradation'\n",
        "if define_nodeg_nodef_scenarios:\n",
        "  for year in no_deforestation_or_degradation_since:\n",
        "    nodeg_nodef_predictors = []\n",
        "    for scenario_predictor in most_recent_scenario_predictors:\n",
        "      if \"disturbance_with_edge_effects\" in scenario_predictor:\n",
        "        scenario_predictor_year = int(scenario_predictor[-4:])\n",
        "        if scenario_predictor_year > year: # Only remove disturbance from years specified\n",
        "          nodeg_nodef_predictors.append(minimum_disturbance_name)\n",
        "        else: nodeg_nodef_predictors.append(scenario_predictor)\n",
        "      elif \"forest_with_edge_effects\" in scenario_predictor:\n",
        "        scenario_predictor_year = int(scenario_predictor[-4:])\n",
        "        if scenario_predictor_year > year: # Replace with specified forest cover year\n",
        "          nodeg_nodef_predictors.append(f\"forest_with_edge_effects_{year}\")\n",
        "        else: nodeg_nodef_predictors.append(scenario_predictor)\n",
        "      else: nodeg_nodef_predictors.append(scenario_predictor)\n",
        "    nodeg_nodef_predictors = sorted(nodeg_nodef_predictors) # Sort to ensure order is maintained\n",
        "    nodeg_nodef_scenario_filename = f\"{last_predictor_year}_nodeg_nodef_since_{year}.csv\"\n",
        "    nodeg_nodef_scenario_path = join(scenarios_model_dir, nodeg_nodef_scenario_filename)\n",
        "    pd.DataFrame(nodeg_nodef_predictors).to_csv(nodeg_nodef_scenario_path, index=False)\n",
        "    print(f\"Predictor list for a scenario without degradation or deforestation between\")\n",
        "    print(f\"{last_predictor_year} and {year} exported to {nodeg_nodef_scenario_filename}.\\n\")\n",
        "\n",
        "else: print(\"The 'no degradation and no deforestation' scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6u03s8VHfN8"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios simulate old-growth forest using a proxy area specified by the user.\n",
        "# Forest extent (i.e. 'no deforestation') can be set from yearly predictor, or the entire land area.\n",
        "\n",
        "define_oldgrowth_scenarios = True\n",
        "oldgrowth_yearly_scenarios = [\n",
        "    first_predictor_year,\n",
        "    # 2020,\n",
        "    last_predictor_year\n",
        "]\n",
        "\n",
        "for year in oldgrowth_yearly_scenarios:\n",
        "  assert year in final_predictor_years, \"Years in 'oldgrowth_yearly_scenarios' must be available in the final yearly predictors.\"\n",
        "\n",
        "simulate_oldgrowth_all_land = True\n",
        "oldgrowth_all_land_name = f\"{first_predictor_year}_oldgrowth_all_land\"\n",
        "\n",
        "# The predictor that best indicates oldgrowth to the model, e.g. certain protected areas\n",
        "# This will be modified to cover the entire scenario area for the old-growth scenarios\n",
        "oldgrowth_predictor = 'lu_pa_taman_negara_krau_with_edge_effects'\n",
        "\n",
        "# Some predictors may confound the old-growth proxy, e.g. protected areas that are not known to be old-growth\n",
        "# These will be removed for the old-growth scenarios\n",
        "oldgrowth_redundant_predictors = [\n",
        "'lu_ais_with_edge_effects',\n",
        "'lu_berkelah_jerantut_with_edge_effects',\n",
        "'lu_berkelah_kuantan_with_edge_effects',\n",
        "'lu_berkelah_temerloh_with_edge_effects',\n",
        "'lu_remen_chereh_with_edge_effects',\n",
        "'lu_tekai_tembeling_with_edge_effects',\n",
        "'lu_tekam_with_edge_effects',\n",
        "'lu_yong_lipis_with_edge_effects',\n",
        "'lu_yong_with_edge_effects',\n",
        "]\n",
        "\n",
        "if define_oldgrowth_scenarios:\n",
        "  # Expand the oldgrowth predictor to the entire scenario area\n",
        "  oldgrowth_predictor_all_dir = join(predictors_dir, f\"{oldgrowth_predictor}_all.tif\")\n",
        "  if not exists(oldgrowth_predictor_all_dir):\n",
        "    oldgrowth_predictor_dir = join(predictors_dir, f\"{oldgrowth_predictor}.tif\")\n",
        "    oldgrowth_predictor_array = gdal.Open(oldgrowth_predictor_dir).ReadAsArray()\n",
        "    oldgrowth_predictor_max_value = oldgrowth_predictor_array.max()\n",
        "    print(f\"The maximum value for the oldgrowth predictor '{oldgrowth_predictor}' is {oldgrowth_predictor_max_value}.\")\n",
        "    oldgrowth_predictor_all_array = np.where(oldgrowth_predictor_array, oldgrowth_predictor_max_value, oldgrowth_predictor_max_value)\n",
        "    oldgrowth_predictor_all_dir = join(predictors_dir, f\"{oldgrowth_predictor}_all.tif\")\n",
        "    export_array_as_tif(oldgrowth_predictor_all_array, oldgrowth_predictor_all_dir, template = oldgrowth_predictor_dir)\n",
        "    print(f\"The oldgrowth proxy {oldgrowth_predictor} has been expanded to the entire scenario area\")\n",
        "    print(f\"And exported to {oldgrowth_predictor_all_dir}\")\n",
        "  else: print(f\"The oldgrowth predictor '{oldgrowth_predictor}_all.tif' already exists.\\n\")\n",
        "\n",
        "  # Remove the redundant predictors from the oldgrowth scenario area\n",
        "  for redundant_predictor in oldgrowth_redundant_predictors:\n",
        "    redundant_predictor_none_dir = join(predictors_dir, f\"{redundant_predictor}_none.tif\")\n",
        "    if not exists(redundant_predictor_none_dir):\n",
        "      redundant_predictor_dir = join(predictors_dir, f\"{redundant_predictor}.tif\")\n",
        "      redundant_predictor_array = gdal.Open(redundant_predictor_dir).ReadAsArray()\n",
        "      redundant_predictor_min_value = redundant_predictor_array.min()\n",
        "      print(f\"The minimum value for the redundant predictor {redundant_predictor} is {redundant_predictor_min_value}.\")\n",
        "      redundant_predictor_none_array = np.where(redundant_predictor_array, redundant_predictor_min_value, redundant_predictor_min_value)\n",
        "      export_array_as_tif(redundant_predictor_none_array, redundant_predictor_none_dir, redundant_predictor_dir)\n",
        "      print(f\"The oldgrowth redundant predictor {oldgrowth_predictor} has been removed from the entire scenario area\")\n",
        "      print(f\"And exported to {oldgrowth_predictor_all_dir}.\")\n",
        "    else: print(f\"The oldgrowth redundant predictor '{redundant_predictor}_none.tif' already exists.\\n\")\n",
        "\n",
        "  # Generate 'oldgrowth' scenarios for each specified year (i.e. forest extent in that year)\n",
        "  for year in oldgrowth_yearly_scenarios:\n",
        "    oldgrowth_predictors = []\n",
        "    for scenario_predictor in most_recent_scenario_predictors:\n",
        "      if \"disturbance_with_edge_effects\" in scenario_predictor:\n",
        "        oldgrowth_predictors.append(minimum_disturbance_name)\n",
        "      elif \"forest_with_edge_effects\" in scenario_predictor:\n",
        "        scenario_predictor_year = int(scenario_predictor[-4:])\n",
        "        if scenario_predictor_year > year: # Replace with specified forest cover year\n",
        "          oldgrowth_predictors.append(f\"forest_with_edge_effects_{year}\")\n",
        "        else: oldgrowth_predictors.append(scenario_predictor)\n",
        "      elif scenario_predictor == oldgrowth_predictor:\n",
        "        oldgrowth_predictors.append(f\"{oldgrowth_predictor}_all\")\n",
        "      elif scenario_predictor in oldgrowth_redundant_predictors:\n",
        "        oldgrowth_predictors.append(f\"{redundant_predictor}_none\")\n",
        "      else: oldgrowth_predictors.append(scenario_predictor)\n",
        "    oldgrowth_predictors = sorted(oldgrowth_predictors)\n",
        "    oldgrowth_filename = f\"{year}_oldgrowth.csv\"\n",
        "    oldgrowth_dir = join(scenarios_model_dir, oldgrowth_filename)\n",
        "    pd.DataFrame(oldgrowth_predictors).to_csv(oldgrowth_dir, index=False)\n",
        "    print(f\"Predictor list for a scenario where all forest in {first_predictor_year} was old-growth\")\n",
        "    print(f\"has been exported to {oldgrowth_filename}.\\n\")\n",
        "\n",
        "\n",
        "  if simulate_oldgrowth_all_land:\n",
        "    # Based on the first TMF AnnualChanges land coverage\n",
        "    # Create a forest predictor all land that exists in the first predictor year (e.g. 1990)\n",
        "    forest_all_land_name = f\"forest_with_edge_effects_{first_predictor_year}_all_land\"\n",
        "    forest_all_land_path = join(predictors_dir, f\"{forest_all_land_name}.tif\")\n",
        "    first_annual_changes_filename = f\"tmf_AnnualChanges_Dec{first_predictor_year}.tif\"\n",
        "    first_annual_changes_path = join(predictor_resampled_dir, first_annual_changes_filename)\n",
        "    if not exists(forest_all_land_path):\n",
        "      if exists(first_annual_changes_path):\n",
        "        first_annual_changes_array = gdal.Open(first_annual_changes_path).ReadAsArray()\n",
        "        # Convert all water values to 'nodata' and non-water values to '1'\n",
        "        forest_all_land_array = np.where(first_annual_changes_array == 5, nodatavalue, 1)\n",
        "        # Set smoothing kernel and precision\n",
        "        kernel, precision = Gaussian2DKernel(x_stddev=1, y_stddev=1), 2\n",
        "        # Reclassify for binary differentiation after proximity conversion\n",
        "        differentiator_array = forest_all_land_array.copy()\n",
        "        differentiator_array[differentiator_array == 1] = 10\n",
        "        # Positive proximity\n",
        "        positive_distances = ndimage.distance_transform_edt(forest_all_land_array == 0) # target pixels\n",
        "        positive_proximity_array = np.where(positive_distances > 2, 0, positive_distances) # max distance 2\n",
        "        # Negative proximity\n",
        "        negative_distances = ndimage.distance_transform_edt(forest_all_land_array == 1) # target pixels\n",
        "        negative_proximity_array = np.where(negative_distances > 2, 0, negative_distances) # max distance 2\n",
        "        # Sum proximities and differentiator\n",
        "        pixel_prox_summed =  differentiator_array + positive_proximity_array + negative_proximity_array\n",
        "        # Reclassify for better semantic understanding of pixel proximity\n",
        "        pixel_prox_reclassed = pixel_prox_summed.copy()\n",
        "        pixel_prox_reclass_table = [(0, 0, -4), (1, 1, -1), (1.4, 1.5, -2), (2, 2, -3), (10, 10, 3), (11, 11, 0), (11.4, 11.5, 1), (12, 12, 2)]\n",
        "        for min_value, max_value, new_value in pixel_prox_reclass_table:\n",
        "          pixel_prox_reclassed[(pixel_prox_reclassed >= min_value) & (pixel_prox_reclassed <= max_value)] = new_value\n",
        "        # Smooth binary array using 2D convolution\n",
        "        binary_smoothed = convolve(forest_all_land_array, kernel, boundary='extend')\n",
        "        # Sum pixel proximity and smoothed binary array\n",
        "        edge_effects_array = np.round(pixel_prox_reclassed + binary_smoothed, precision)\n",
        "        # Export forest edge effects predictor\n",
        "        export_array_as_tif(edge_effects_array, forest_all_land_path)\n",
        "        print(f\"{forest_all_land_name} has been created and saved to\\n{predictors_dir}\\n\")\n",
        "      else: print(f\"The first TMF annual changes year raster needed for comrec is not in the indicated directory:\\n{first_annual_changes_path}\\n\")\n",
        "    print(f\"{forest_all_land_name} already exists in\\n{predictors_dir}\\n\")\n",
        "    if exists(forest_all_land_path):\n",
        "      oldgrowth_all_predictors = []\n",
        "      for scenario_predictor in most_recent_scenario_predictors:\n",
        "        if \"disturbance_with_edge_effects\" in scenario_predictor:\n",
        "          oldgrowth_all_predictors.append(minimum_disturbance_name)\n",
        "        elif \"forest_with_edge_effects\" in scenario_predictor:\n",
        "          oldgrowth_all_predictors.append(forest_all_land_name)\n",
        "        elif scenario_predictor == oldgrowth_predictor:\n",
        "          oldgrowth_all_predictors.append(f\"{scenario_predictor}_all\")\n",
        "        elif scenario_predictor in oldgrowth_redundant_predictors:\n",
        "          oldgrowth_all_predictors.append(f\"{scenario_predictor}_none\")\n",
        "        else: oldgrowth_all_predictors.append(scenario_predictor)\n",
        "      oldgrowth_all_predictors = sorted(oldgrowth_all_predictors)\n",
        "      oldgrowth_all_filename = f\"{oldgrowth_all_land_name}.csv\"\n",
        "      oldgrowth_all_dir = join(scenarios_model_dir, oldgrowth_all_filename)\n",
        "      pd.DataFrame(oldgrowth_all_predictors).to_csv(oldgrowth_all_dir, index=False)\n",
        "      print(f\"Predictor list for a scenario with old-growth covering everywhere that was land in {first_predictor_year}\\n\")\n",
        "      print(f\"has been exported to {oldgrowth_all_filename}.\\n\")\n",
        "\n",
        "    # Create A forest mask for 'all land' oldgrowth\n",
        "    # Also forests reservoirs since the first TMF annual changes year, though topography may be wrong\n",
        "    if exists(join(scenarios_model_dir, f\"{oldgrowth_all_land_name}.csv\")):\n",
        "      if exists(first_annual_changes_path):\n",
        "        oldgrowth_all_mask_path = join(masks_dir, f\"mask_forest_{first_predictor_year}_all_land.tif\")\n",
        "        if not exists(oldgrowth_all_mask_path):\n",
        "          first_annual_changes_array = gdal.Open(first_annual_changes_path).ReadAsArray()\n",
        "          # Convert all water values to 'nodata' and non-water values to '1'\n",
        "          oldgrowth_all_mask_array = np.where(first_annual_changes_array == 5, nodatavalue, 1)\n",
        "          export_array_as_tif(oldgrowth_all_mask_array, oldgrowth_all_mask_path)\n",
        "          print(f\"A mask for 'oldgrowth_all_land_name' has been created at\\n{oldgrowth_all_mask_path}\")\n",
        "        else: print(f\"A mask for 'oldgrowth_all_land_name' already exists at\\n{oldgrowth_all_mask_path}\")\n",
        "      else: print(f\"The {first_annual_changes_filename} raster needed to mask {oldgrowth_all_land_name} doesn't exist.\")\n",
        "    else: print(f\"The scenario csv for {oldgrowth_all_land_name} doesn't exist.\")\n",
        "\n",
        "else: print(\"Old-growth scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy2nfHshozVQ"
      },
      "source": [
        "# Predictor verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chCSZTV4XB8k"
      },
      "outputs": [],
      "source": [
        "# Check that all predictors in all scenario csvs exist\n",
        "scenario_csv_list = []\n",
        "all_predictors_exist = True # Changes to false if predictor missing\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    csv_dir = join(scenarios_model_dir, csv)\n",
        "    csv_predictor_list = pd.Series.tolist(pd.read_csv(csv_dir).iloc[:,0])\n",
        "    csv_predictor_dir_list = []\n",
        "    for csv_predictor in csv_predictor_list:\n",
        "      if csv_predictor not in covariates: csv_predictor_dir_list.append(f\"{predictors_dir}/{csv_predictor}.tif\")\n",
        "    for predictor in csv_predictor_dir_list:\n",
        "      if not exists(predictor):\n",
        "        all_predictors_exist = False\n",
        "        print(f\"The following predictor is missing:\\n{predictor}\\n and is required for the scenario '{csv[:-4]}'\")\n",
        "\n",
        "if all_predictors_exist: print(\"All required predictors are present.\")\n",
        "print(\"Covariate predictors e.g. 'beam' and 'sensitivity' will be added at the prediction stage.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAqAaDfYkY7T"
      },
      "outputs": [],
      "source": [
        "# Is the scenario area equal to the original template area?\n",
        "original_template_area = True\n",
        "\n",
        "# If not, create a new template for the scenario area and upload to:\n",
        "# '6_scenarios/[model]/[scenario_area]/template.tif'\n",
        "if original_template_area: scenario_template_dir = join(areas_dir, \"template.tif\")\n",
        "else: scenario_template_dir = join(scenario_area_dir, \"template.tif\")\n",
        "print(f\"The following is being used as a template to verify scenario predictor dimensions and projections:\\n{scenario_template_dir}\")\n",
        "\n",
        "\n",
        "scenario_template = gdal.Open(scenario_template_dir)\n",
        "scenario_template_dimensions, scenario_template_projection = scenario_template.GetGeoTransform(), scenario_template.GetProjection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBb-Cws3mMUy"
      },
      "outputs": [],
      "source": [
        "predictor_issue = False\n",
        "for predictor in os.listdir(predictors_dir):\n",
        "  if predictor.endswith('.tif'):\n",
        "    predictor_dir = join(predictors_dir, predictor)\n",
        "    predictor_open = gdal.Open(predictor_dir)\n",
        "    predictor_dimensions, predictor_projection = predictor_open.GetGeoTransform(), predictor_open.GetProjection()\n",
        "    if predictor_dimensions != scenario_template_dimensions:\n",
        "      print(f\"{predictor} dimensions:\\n{predictor_dimensions}\\ndo not match the scenario template dimensions:\\n{scenario_template_dimensions}\\n\")\n",
        "      predictor_issue = True\n",
        "    if predictor_projection != scenario_template_projection:\n",
        "      print(f\"{predictor} projection:\\n{predictor_projection}\\ndoes not match the scenario template projection:\\n{scenario_template_projection}\\n\\n\")\n",
        "      predictor_issue = True\n",
        "\n",
        "if not predictor_issue: print(f\"All predictors in the following directory have the correct dimensions and projection:\\n{predictors_dir}\")\n",
        "else: print(\"Correct and / or resample the predictor(s).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T9UqJrzWECr"
      },
      "source": [
        "# Template tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zizOofP5dgW"
      },
      "outputs": [],
      "source": [
        "# Load the model scenario predictors for tile template creation\n",
        "model_scenario_predictors = pd.Series.tolist(pd.read_csv(model_scenario_dir).iloc[:,0])\n",
        "model_scenario_predictors_dirs = [predictors_dir + '/' + predictor + '.tif' for predictor in model_predictors]\n",
        "# Create a template predictor array from the first predictor that isn't a covariate (these are created later)\n",
        "template_base_path = next(r for r in model_scenario_predictors_dirs if all(c not in r for c in covariates))\n",
        "template_base = gdal.Open(template_base_path)\n",
        "template_base_array = template_base.ReadAsArray()\n",
        "template_base_xsize, template_base_ysize = template_base.GetRasterBand(1).XSize, template_base.GetRasterBand(1).YSize\n",
        "print(f\"The template predictor is {template_base_xsize} x {template_base_ysize} pixels.\")\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles_exist = len(template_tile_list)\n",
        "\n",
        "if n_tiles_exist < 1: print(\"There are currently no template tiles. Run the next section.\")\n",
        "if n_tiles_exist >= 1:\n",
        "  tile_size_y_rounded_exist = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).YSize\n",
        "  tile_size_y_remainder_exist = gdal.Open(join(tile_templates_dir,f'template_tile_{n_tiles_exist}.tif')).GetRasterBand(1).YSize\n",
        "  if n_tiles_exist == 1: print(f\"There is a single 'tile' with a height of {tile_size_y_rounded_exist}.\"); tile_size_y_remainder_exist = 0\n",
        "  else: print(f\"There are {n_tiles_exist} template tiles, the first {n_tiles_exist-1} having a height of {tile_size_y_rounded_exist} pixels, the last {tile_size_y_remainder_exist} pixels.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njhqSH8e5cPu"
      },
      "outputs": [],
      "source": [
        "# Large scenario areas and / or numbers of predictors may be too much for the available memory.\n",
        "# This section defines how to split predictions into tiles that can then be merged.\n",
        "override_n_tiles = False # Useful if the tile number has already been tested.\n",
        "n_tiles_override = 1\n",
        "tile_size_y_rounded_override = int(14910/n_tiles_override)\n",
        "\n",
        "memory_utilisation = 0.8 # Set to 0.8 to ensure crashes are avoided\n",
        "\n",
        "assert memory_utilisation > 0 and memory_utilisation <= 1, \"Set memory_utilisation to a value between 0 and 1\"\n",
        "\n",
        "# Calculate total size of predictor stack\n",
        "predictor_stack_size = template_base_array.size * len(model_scenario_predictors_dirs)\n",
        "\n",
        "# Calculate memory and the number of tiles required\n",
        "total_memory_needed = 64 / 8 * predictor_stack_size\n",
        "print(f'RAM required for each prediction: ~{total_memory_needed/(1024**3):.3f} GB')\n",
        "print(f'RAM currently available: {psutil.virtual_memory().free / (1024**3):.3f} GB')\n",
        "n_tiles_temp = int(np.ceil(total_memory_needed / (psutil.virtual_memory().free * memory_utilisation)))\n",
        "\n",
        "# Calculate template tile size (split on the y axis only)\n",
        "tile_size_y_rounded = int(np.ceil(template_base_ysize/n_tiles_temp)) # Round the number of y pixels in each tile\n",
        "tile_size_y_remainder = template_base_ysize%tile_size_y_rounded # Calculate the remainder for the last tile\n",
        "n_tiles = max(1, len(range(0, template_base_ysize, tile_size_y_rounded))) # Update the number of tiles to include the remainder\n",
        "\n",
        "if override_n_tiles:\n",
        "  tile_size_y_rounded = tile_size_y_rounded_override\n",
        "  tile_size_y_remainder = template_base_ysize%tile_size_y_rounded\n",
        "  n_tiles = n_tiles_override\n",
        "  print(\"n_tiles has been overridden.\")\n",
        "\n",
        "print(f'The prediction template should be divided into {n_tiles} tiles to avoid crashing.')\n",
        "\n",
        "# Check if tiles need to be changed\n",
        "if n_tiles == n_tiles_exist and tile_size_y_rounded == tile_size_y_rounded_exist and tile_size_y_remainder == tile_size_y_remainder_exist:\n",
        "  print(\"No changes to existing tiles are required.\")\n",
        "else:\n",
        "  # Clear all tile directories\n",
        "  for tile in Path(tile_templates_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for tile in Path(tile_predictors_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for scenario_stack_dir in Path(tile_predictor_stacks_dir).glob(\"**/*\"):\n",
        "    shutil.rmtree(scenario_stack_dir)\n",
        "  for tile in Path(tile_prediction_cache_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "\n",
        "  # Generate new tile templates based on available memory\n",
        "  tile_number = 1\n",
        "  for y_start in range(0, template_base_ysize, tile_size_y_rounded):\n",
        "    if tile_size_y_remainder != 0 and tile_number == n_tiles: tile_size_y = tile_size_y_remainder\n",
        "    else: tile_size_y = tile_size_y_rounded\n",
        "    tiling_string = \"gdal_translate -of GTIFF -srcwin \" + str(0)+ \", \" + str(y_start) + \", \" + str(template_base_xsize) + \", \" + str(tile_size_y) + \" \" + str(template_base_path) + \" \" + str(tile_templates_dir) + \"/template_tile_\" + str(tile_number) + \".tif\"\n",
        "    os.system(tiling_string)\n",
        "    tile_number += 1\n",
        "\n",
        "  print(\"Template tile creation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkNV_N7UKcMd"
      },
      "source": [
        "# Predictor tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1d1ELggV4jF"
      },
      "outputs": [],
      "source": [
        "# Create predictor tiles.\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "tile_size_y_rounded = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).YSize\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "if n_tiles == 1: print(\"Predictor tile creation skipped. Predictor stack creation will use the original predictors.\")\n",
        "else:\n",
        "  # Progress\n",
        "  n_predictors = len(os.listdir(predictors_dir))\n",
        "  predictor_progress_index, predictor_progress_label = 0, widgets.Label(value=f\"Predictor progress: 0 / {n_predictors}\")\n",
        "  display(predictor_progress_label)\n",
        "  tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "  display(tile_progress_label)\n",
        "\n",
        "  # Loop through each predictor in the 6_scenarios predictors directory\n",
        "  for predictor in os.listdir(predictors_dir):\n",
        "    # Create list of tile directories\n",
        "    predictor_dir = join(predictors_dir, predictor)\n",
        "    predictor_array = gdal.Open(predictor_dir).ReadAsArray()\n",
        "    # Split the predictor array into chunks based on tile size\n",
        "    predictor_chunks = np.array_split(predictor_array, np.arange(tile_size_y_rounded, len(predictor_array), tile_size_y_rounded))\n",
        "    tile_count = 1\n",
        "    # Loop through tiles and export as .tif\n",
        "    for tile in range(n_tiles):\n",
        "      predictor_tile_filename = f\"{predictor[:-4]}_{tile_count}.tif\"\n",
        "      # Check if tile already exists\n",
        "      predictor_tile_exists = False\n",
        "      for predictor_tile in os.listdir(tile_predictors_dir):\n",
        "        if predictor_tile == predictor_tile_filename: predictor_tile_exists=True\n",
        "        # If predictor tile does not exist:\n",
        "      if predictor_tile_exists == False:\n",
        "        template_tile_dir = join(tile_templates_dir, f\"template_tile_{tile_count}.tif\")\n",
        "        export_array_as_tif(predictor_chunks[tile_count-1], join(tile_predictors_dir,predictor_tile_filename), template_tile_dir, compress = False)\n",
        "        tile_count += 1\n",
        "      # Update tile progress\n",
        "      tile_progress_index += 1\n",
        "      tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_tiles}\"\n",
        "    tile_progress_index = 0\n",
        "    # Update predictor progress\n",
        "    predictor_progress_index += 1\n",
        "    predictor_progress_label.value = f\"Predictor progress: {predictor_progress_index} / {n_predictors}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0r89JLYW_xU"
      },
      "source": [
        "# Predictor stacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaHFv1nuW5CG"
      },
      "outputs": [],
      "source": [
        "# Create predictor stack arrays for each scenario\n",
        "\n",
        "# Collect scenarios with .csv predictor lists\n",
        "scenarios_list = []\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    scenarios_list.append(csv[:-4])\n",
        "\n",
        "# Select scenarios to generate tiled predictor stacks\n",
        "print(\"scenarios_to_stack = [\")\n",
        "for scenario in sorted(scenarios_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f-uJ-EkoxIym"
      },
      "outputs": [],
      "source": [
        "scenarios_to_stack = [\n",
        "  \"1990_oldgrowth\",\n",
        "  \"1990_oldgrowth_all_land\",\n",
        "  # \"2016\",\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2023_nodeg_nodef_since_1990\",\n",
        "  \"2023_nodeg_since_1990\",\n",
        "  \"2023_oldgrowth\",\n",
        "]\n",
        "\n",
        "assert set(scenarios_to_stack).issubset(scenarios_list), \"Not all selected scenarios exist.\"\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "# Progress\n",
        "scenario_progress_index, scenario_progress_label = 0, widgets.Label(value=f\"Scenario progress: 0 / {len(scenarios_to_stack)}\")\n",
        "display(scenario_progress_label)\n",
        "stack_progress_index, stack_progress_label = 0, widgets.Label(value=f\"Tiled predictor stack progress: 0 / {n_tiles}\")\n",
        "display(stack_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_stack:\n",
        "    # Define directory and predictors\n",
        "    scenario_predictor_stacks_dir = join(tile_predictor_stacks_dir, scenario)\n",
        "    makedirs(scenario_predictor_stacks_dir, exist_ok=True)\n",
        "    scenario_predictors_csv = join(scenarios_model_dir,f\"{scenario}.csv\")\n",
        "    scenario_predictors = pd.Series.tolist(pd.read_csv(scenario_predictors_csv).iloc[:,0])\n",
        "    # Set the number of stacks to the number of tiles\n",
        "    if n_tiles == 0: n_stacks = 1\n",
        "    else: n_stacks = n_tiles\n",
        "    # Create a tile count to match the predictor stack\n",
        "    tile_count = 1\n",
        "    for tile in range(n_stacks):\n",
        "      scenario_stack_filename = f\"predictor_stack_{scenario}_{tile_count}.npy\"\n",
        "      # Check if predictor stack already exists\n",
        "      predictor_stack_exists = False\n",
        "      for predictor_stack in os.listdir(scenario_predictor_stacks_dir):\n",
        "        if predictor_stack == scenario_stack_filename: predictor_stack_exists=True\n",
        "      # If scenario prediction tile does not exist:\n",
        "      if predictor_stack_exists == False:\n",
        "        scenario_tile_stack_dir = join(scenario_predictor_stacks_dir, scenario_stack_filename)\n",
        "        # Create predictor chunks (arrays) from tiles\n",
        "        if n_stacks == 1: predictor_tiles_dirs = [f\"{predictors_dir}/{predictor}.tif\" for predictor in scenario_predictors]\n",
        "        else: predictor_tiles_dirs = [f\"{tile_predictors_dir}/{predictor}_{tile_count}.tif\" for predictor in scenario_predictors]\n",
        "        predictor_array_chunks = []\n",
        "        for predictor in predictor_tiles_dirs:\n",
        "          # Covariate raster will exist and should be ignored if prediction stage has already been attempted\n",
        "          if predictor.split('/')[-1].split('.')[0] not in covariates and predictor.split('/')[-1].split('.')[0] not in [f\"{cov}_{tile_count}\" for cov in covariates]:\n",
        "            predictor_array_chunk = gdal.Open(predictor).ReadAsArray()\n",
        "            predictor_array_chunks.append(predictor_array_chunk)\n",
        "        # Create a predictor stack from chunks\n",
        "        predictor_stack = np.dstack(predictor_array_chunks)\n",
        "        predictor_array_chunks = None # Flush chunks\n",
        "        stack_height, stack_width, stack_n_predictors = predictor_stack.shape\n",
        "        # Convert predictor stack to 2D numpy array with predictors as columns\n",
        "        predictor_stack_reshaped = predictor_stack.reshape(stack_height * stack_width, stack_n_predictors)\n",
        "        predictor_stack = None # Flush stack\n",
        "        # Save as a numpy file\n",
        "        np.save(scenario_tile_stack_dir, predictor_stack_reshaped)\n",
        "        predictor_stack_reshaped = None # Flush reshaped stack\n",
        "      # Update progress\n",
        "      tile_count += 1\n",
        "      stack_progress_index += 1\n",
        "      stack_progress_label.value = f\"Tiled predictor stack progress: {stack_progress_index} / {n_stacks}\"\n",
        "    # Reset tile progress\n",
        "    stack_progress_index = 0\n",
        "    scenario_progress_index += 1\n",
        "    scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index} / {len(scenarios_to_stack)}\"\n",
        "print(\"\\nPredictor stacks complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oysAS9DI7yRg"
      },
      "source": [
        "# Predict scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y92V4nKGcyE"
      },
      "outputs": [],
      "source": [
        "# Collect available scenarios from the predictor stack tiles directory\n",
        "scenario_stacks_list = []\n",
        "for scenario in os.listdir(tile_predictor_stacks_dir):\n",
        "    scenario_stacks_list.append(scenario)\n",
        "\n",
        "# Select scenarios to predict\n",
        "print(\"scenarios_to_predict = [\")\n",
        "for scenario in sorted(scenario_stacks_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRlFYaIGNXbI"
      },
      "outputs": [],
      "source": [
        "scenarios_to_predict = [\n",
        "  \"1990_oldgrowth\",\n",
        "  \"1990_oldgrowth_all_land\",\n",
        "  # \"2016\",\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2023_nodeg_nodef_since_1990\",\n",
        "  \"2023_nodeg_since_1990\",\n",
        "  \"2023_oldgrowth\",\n",
        "]\n",
        "\n",
        "assert set(scenarios_to_predict).issubset(scenario_stacks_list), \"Not all selected scenarios exist.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TibU8xTZfcuL"
      },
      "outputs": [],
      "source": [
        "add_covariates = True # Adds a selected covariate value as the predictor\n",
        "sensitivity_value = 0.99\n",
        "beam_value = 5\n",
        "# 5 is the first of the full beams, with the least bias on AGBD.\n",
        "# Cover beams 1 - 4 underestimate. Full beams 7 - 8 overestimate. 5 - 6 tend give average values.\n",
        "\n",
        "# Check for GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0': print('GPU device not found')\n",
        "else: print(f\"Found GPU at: {device_name}\")\n",
        "\n",
        "# Load model\n",
        "\n",
        "# Reload hyperparameters\n",
        "with open(selected_model_descr_dir) as model_description_json:\n",
        "  model_description = json.load(model_description_json)\n",
        "final_hyperparameters = eval(model_description[\"hyperparameters\"])\n",
        "\n",
        "# Remove early stopping and replace with mean n_estimators\n",
        "if \"early_stopping_rounds\" in final_hyperparameters:\n",
        "  final_hyperparameters = {k:v for k, v in final_hyperparameters.items() if k != \"early_stopping_rounds\"}\n",
        "  final_hyperparameters[\"n_estimators\"] = round(model_description[\"n_estimators mean\"])\n",
        "\n",
        "if categorise_variate: XGBPredictor = xgb.XGBClassifier(**final_hyperparameters)\n",
        "else: XGBPredictor = xgb.XGBRegressor(**final_hyperparameters)\n",
        "XGBPredictor.load_model(fname=selected_model_json)\n",
        "\n",
        "# Avoids issues using dataframe from CPU\n",
        "xgb.set_config(verbosity=0, use_rmm=True)\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "template_tile_x = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).XSize\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "# Scenario progress\n",
        "scenario_progress_index = 0\n",
        "scenario_progress_label = widgets.Label(f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\")\n",
        "display(scenario_progress_label)\n",
        "\n",
        "# Tile progress\n",
        "tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "display(tile_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_predict:\n",
        "  # Define scenario filename and check if exists\n",
        "  scenario_prediction_unmasked_filename = f\"{scenario}__{selected_scenario_area}_{selected_model}_unmasked.tif\"\n",
        "  scenario_prediction_unmasked_exists=False\n",
        "  for scenario_prediction in os.listdir(scenario_predictions_unmasked_dir):\n",
        "    if scenario_prediction == scenario_prediction_unmasked_filename:\n",
        "      scenario_prediction_unmasked_exists=True\n",
        "  # If scenario prediction does not exist:\n",
        "  if scenario_prediction_unmasked_exists == False:\n",
        "    # Get number of stacks\n",
        "    scenario_predictor_stack_dir = join(tile_predictor_stacks_dir, scenario)\n",
        "    n_stacks = len(os.listdir(scenario_predictor_stack_dir))\n",
        "    # Create a tile cache directory for the prediction\n",
        "    tile_cache_scenario_dir = join(tile_prediction_cache_dir, scenario_prediction_unmasked_filename[:-4])\n",
        "    makedirs(tile_cache_scenario_dir, exist_ok=True)\n",
        "    # Create a tile count to match the predictor stack chunk\n",
        "    for stack in range(1, n_stacks+1):\n",
        "      scenario_tile_filename = f\"scenario_tile_{stack}.tif\"\n",
        "      # Check if tile already exists\n",
        "      scenario_tile_exists = False\n",
        "      for scenario_tile in os.listdir(tile_cache_scenario_dir):\n",
        "        if scenario_tile == scenario_tile_filename: scenario_tile_exists=True\n",
        "      # If scenario prediction tile does not exist:\n",
        "      if scenario_tile_exists == False:\n",
        "        # Load template tile parameters\n",
        "        template_tile_dir = join(tile_templates_dir, f\"template_tile_{stack}.tif\")\n",
        "        template_tile_y = gdal.Open(template_tile_dir).GetRasterBand(1).YSize\n",
        "        template_tile_x = gdal.Open(template_tile_dir).GetRasterBand(1).XSize\n",
        "        # Load predictor tile stack\n",
        "        stack_filename = f\"predictor_stack_{scenario}_{stack}.npy\"\n",
        "        predictor_stack = np.load(join(scenario_predictor_stack_dir, stack_filename))\n",
        "        # Add covariates (sensitivity and BEAM)\n",
        "        if add_covariates: predictor_stack = np.hstack((predictor_stack,\n",
        "                           np.full((predictor_stack.shape[0], 1), beam_value, dtype=int),\n",
        "                           np.full((predictor_stack.shape[0], 1), sensitivity_value, dtype=float)\n",
        "                           ))\n",
        "        # Define prediction array and reshape\n",
        "        prediction = XGBPredictor.predict(predictor_stack)\n",
        "        predictor_stack = None # Flush predictor stack\n",
        "        prediction_tile = prediction.reshape((template_tile_y, template_tile_x))\n",
        "        prediction = None # Flush prediction\n",
        "        # Export prediction array as .tif\n",
        "        export_array_as_tif(prediction_tile, join(tile_cache_scenario_dir, scenario_tile_filename), template = template_tile_dir, compress = False)\n",
        "        prediction_tile = None # Flush prediction tile\n",
        "        # Update progress\n",
        "      tile_progress_index += 1\n",
        "      tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_stacks}\"\n",
        "    # Prepare empty array for merging tiles\n",
        "    prediction_array = np.empty((0,template_tile_x))\n",
        "    # Read each tile .tif as an array, stack, then export as a .tif\n",
        "    for subdir in os.listdir(tile_cache_scenario_dir):\n",
        "      if subdir.endswith('.tif'):\n",
        "        tile_dir = join(tile_cache_scenario_dir, subdir)\n",
        "        prediction_array = np.vstack((prediction_array, gdal.Open(tile_dir).ReadAsArray()))\n",
        "    # Define scenario template\n",
        "    scenario_template = join(predictors_dir, os.listdir(predictors_dir)[0])\n",
        "    scenario_prediction_unmasked_dir = join(scenario_predictions_unmasked_dir, f\"{scenario_prediction_unmasked_filename}\")\n",
        "    export_array_as_tif(prediction_array, scenario_prediction_unmasked_dir, template = scenario_template, compress = True)\n",
        "    # Delete scenario tile cache directory\n",
        "    shutil.rmtree(tile_cache_scenario_dir)\n",
        "  # Reset tile progress\n",
        "  tile_progress_index = 0\n",
        "  # Update scenario progress\n",
        "  scenario_progress_index += 1\n",
        "  scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\"\n",
        "print(\"\\nScenario predictions complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diwt1peRdXpj"
      },
      "source": [
        "# Mask scenario predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQsX1NCiy0aW"
      },
      "outputs": [],
      "source": [
        "# Use polygons for masking, only areas inside the polygons will be included\n",
        "\n",
        "# Exclude existing polygons from search\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "\n",
        "print(\"mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"  '{polygon[:-5]}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daKAe6nkJ2bS"
      },
      "outputs": [],
      "source": [
        "mask_polygons = [\n",
        "  # 'project_area',\n",
        "  'gedi_area',\n",
        "  # 'peninsular_malaysia',\n",
        "  # 'pa_taman_krau',\n",
        "  # 'pa_ais',\n",
        "]\n",
        "\n",
        "# Create an inverse project area path for masking\n",
        "template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "for polygon in mask_polygons:\n",
        "  inverse_polygon_path = join(polygons_dir, f\"{polygon}_inverse.gpkg\")\n",
        "  if not exists(inverse_polygon_path):\n",
        "    polygon_path = join(polygons_dir, f\"{polygon}.gpkg\")\n",
        "    template_polygon = gpd.read_file(template_polygon_path)\n",
        "    polygon_read = gpd.read_file(polygon_path)\n",
        "    polygon_crs = polygon_read.crs.to_epsg()\n",
        "    inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "    inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "    inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "    print(f\"An inverse masking polygon for {polygon} has been created in {polygons_dir}.\")\n",
        "  else: print(f\"An inverse masking polygon for {polygon} already exists.\")\n",
        "\n",
        "unmasked_predictions = []\n",
        "for scenario_prediction in os.listdir(scenario_predictions_unmasked_dir):\n",
        "  unmasked_predictions.append(scenario_prediction)\n",
        "\n",
        "# Determine last predictor year for masking future scenarios\n",
        "final_predictor_years = []\n",
        "for final_predictor in os.listdir(predictor_final_dir):\n",
        "  if final_predictor.endswith('.tif') and final_predictor[-9] == '_':\n",
        "    try: final_predictor_years.append(int(final_predictor[-8:-4]))\n",
        "    except: continue\n",
        "last_predictor_year = max(final_predictor_years)\n",
        "\n",
        "# Binary progress\n",
        "masking_progress_index = 0\n",
        "masking_progress_label = widgets.Label(f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\")\n",
        "display(masking_progress_label)\n",
        "\n",
        "# Mask scenarios with the relevatant mask\n",
        "for scenario_prediction in unmasked_predictions: # Loop through each unmasked scenario\n",
        "  if 'nodeg_nodef' in scenario_prediction: scenario_year = int(scenario_prediction.split('__')[0][-4:])\n",
        "  else: scenario_year = int(scenario_prediction[:4])\n",
        "  scenario_masked_filename = f\"{scenario_prediction[:-13]}.tif\"\n",
        "  scenario_masked_dir = join(scenario_predictions_dir, scenario_masked_filename)\n",
        "  if not exists(scenario_masked_dir):\n",
        "    mask_exists = False\n",
        "    for mask in os.listdir(masks_dir):\n",
        "      mask_year = int(mask[12:16])\n",
        "      # Match oldgrowth all land scenarios\n",
        "      if 'all_land' in mask:\n",
        "        if 'all_land' in scenario_prediction:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "      else: # Match all other historic scenarios\n",
        "        if scenario_year == mask_year:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "        else: # Match future scenarios with most recent forest mask\n",
        "          if scenario_year > last_predictor_year and last_predictor_year == mask_year:\n",
        "            selected_mask_filename = mask\n",
        "            selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "            mask_exists = True\n",
        "    if mask_exists == False: print(f\"A suitable mask for {scenario_prediction} does not exist.\\n\")\n",
        "    else: # Mask the scenario prediction\n",
        "      print(f\"Masking {scenario_prediction} with {selected_mask_filename}...\")\n",
        "      mask_array = gdal.Open(selected_mask_dir).ReadAsArray()\n",
        "      scenario_prediction_unmasked_dir = join(scenario_predictions_unmasked_dir, scenario_prediction)\n",
        "      scenario_prediction_array = gdal.Open(scenario_prediction_unmasked_dir).ReadAsArray()\n",
        "      # Mask where the mask array is not 1\n",
        "      scenario_masked_array = np.where(mask_array != 1, nodatavalue, scenario_prediction_array)\n",
        "      export_array_as_tif(scenario_masked_array, scenario_masked_dir, compress = True)\n",
        "      if len(mask_polygons) > 0:\n",
        "        for polygon_mask in mask_polygons:\n",
        "          inverse_gedi_area_path = join(polygons_dir, f\"{polygon_mask}_inverse.gpkg\")\n",
        "          print(f\"Masking {scenario_prediction} with {polygon_mask}...\")\n",
        "          burn_polygon_to_raster(scenario_masked_dir, inverse_gedi_area_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        # Recompress the prediction after burning the polygon masks\n",
        "        scenario_masked_array_2 = gdal.Open(scenario_masked_dir).ReadAsArray()\n",
        "        export_array_as_tif(scenario_masked_array_2, scenario_masked_dir, compress = True)\n",
        "      print(f\"{scenario_masked_filename} exported.\")\n",
        "  # Update masking progress\n",
        "  masking_progress_index += 1\n",
        "  masking_progress_label.value = f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECHaTfrs0n7i"
      },
      "source": [
        "# Scenario difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6soEuon0qt5"
      },
      "outputs": [],
      "source": [
        "scenarios_diff_set = set()\n",
        "for masked_scenario in os.listdir(scenario_predictions_dir):\n",
        "    scenarios_diff_set.add(masked_scenario.split(\"__\")[0])\n",
        "\n",
        "# Generate all possible pairs of scenarios, including all orders\n",
        "scenario_pairs = sorted(list(itertools.permutations(scenarios_diff_set, 2)))\n",
        "\n",
        "print(\"# Select scenarios to calculate mean difference with uncertainty\")\n",
        "print(\"scenario_pairs = [\")\n",
        "for s1, s2 in scenario_pairs:\n",
        "    print(f\" ('{s1}','{s2}'),\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12skqTzu2qoO"
      },
      "outputs": [],
      "source": [
        "# Select scenarios to calculate mean difference with uncertainty\n",
        "scenario_pairs = [\n",
        " ('1990_oldgrowth','1990_oldgrowth_all_land'),\n",
        " ('2023','1990_oldgrowth_all_land'),\n",
        " ('2023','2023_nodeg_nodef_since_1990'),\n",
        " ('2023','2023_nodeg_since_1990'),\n",
        " ('2023','2023_oldgrowth'),\n",
        " ('2023_nodeg_nodef_since_1990','1990_oldgrowth_all_land'),\n",
        " ('2023_nodeg_since_1990','2023_oldgrowth'),\n",
        " ('2023_oldgrowth','1990_oldgrowth'),\n",
        " ('2023_oldgrowth','1990_oldgrowth_all_land'),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inR5vYflAqKz"
      },
      "outputs": [],
      "source": [
        "# Rename scenario differences for semantic meaning (optional)\n",
        "difference_names = {\n",
        "    ('1990_oldgrowth', '1990_oldgrowth_all_land'): 'deforestation_before_1990',\n",
        "    ('2023', '1990_oldgrowth_all_land'): 'degradation_deforestation_total',\n",
        "    ('2023', '2023_nodeg_nodef_since_1990'): 'degradation_deforestation_since_1990',\n",
        "    ('2023', '2023_nodeg_since_1990'): 'degradation_since_1990',\n",
        "    ('2023', '2023_oldgrowth'): 'degradation_total',\n",
        "    ('2023_nodeg_nodef_since_1990', '1990_oldgrowth_all_land'): 'degradation_deforestation_before_1990',\n",
        "    ('2023_nodeg_since_1990', '2023_oldgrowth'): 'degradation_before_1990',\n",
        "    ('2023_oldgrowth', '1990_oldgrowth'): 'deforestation_since_1990',\n",
        "    ('2023_oldgrowth', '1990_oldgrowth_all_land'): 'deforestation_total'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P-0bj40AxZ2"
      },
      "outputs": [],
      "source": [
        "# Function for difference\n",
        "def diff(scenario1, scenario2):\n",
        "  diff_array = scenario1 - scenario2\n",
        "  return diff_array\n",
        "\n",
        "# Loop through the scenario pairs\n",
        "for scenario1, scenario2 in scenario_pairs:\n",
        "\n",
        "  # Lookup the description from the dictionary\n",
        "  difference_name = difference_names.get((scenario1, scenario2), f\"{scenario1}_-_{scenario2}\")\n",
        "\n",
        "  # Define filenames and directories of difference .tifs\n",
        "  diff_filename = f\"{difference_name}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "  diff_dir = join(scenario_diff_dir, diff_filename)\n",
        "\n",
        "  if not exists(diff_dir):\n",
        "    print(f\"Calculating difference between {scenario1} and {scenario2}\")\n",
        "    scenario1_filename = f\"{scenario1}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "    scenario2_filename = f\"{scenario2}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "\n",
        "    # Define difference directories, assert that both exist for both scenarios\n",
        "    scenario1_dir = join(scenario_predictions_dir, scenario1_filename)\n",
        "    assert exists(scenario1_dir), f\"{scenario1_dir} does not exist.\"\n",
        "    scenario2_dir = join(scenario_predictions_dir, scenario2_filename)\n",
        "    assert exists(scenario2_dir), f\"{scenario1_dir} does not exist.\"\n",
        "\n",
        "    # Convert scenario .tifs to temporary arrays\n",
        "    scenario1_array_temp = gdal.Open(scenario1_dir).ReadAsArray()\n",
        "    scenario2_array_temp = gdal.Open(scenario2_dir).ReadAsArray()\n",
        "\n",
        "    # Fill scenario nodata values with 0 if they are not nodatavalues in the other scenario\n",
        "    scenario1_array = np.where((scenario1_array_temp == nodatavalue) & (scenario2_array_temp != nodatavalue), 0, scenario1_array_temp)\n",
        "    scenario2_array = np.where((scenario2_array_temp == nodatavalue) & (scenario1_array != nodatavalue), 0, scenario2_array_temp)\n",
        "\n",
        "    # Create difference arrays where the value is not 'nodatavalue'\n",
        "    diff_array = np.where(scenario1_array==nodatavalue, nodatavalue, diff(scenario1_array, scenario2_array))\n",
        "    export_array_as_tif(diff_array, diff_dir, template = scenario1_dir), print(f\"{diff_filename} has been exported.\")\n",
        "\n",
        "  else: print(f\"{diff_filename} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-7ctfgdoF2"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcuArY00N9Mc"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "QJ8jLRKRSfgb",
        "y_OkHaUkTCyB",
        "X3w7svaUvs07",
        "xy2nfHshozVQ",
        "1T9UqJrzWECr",
        "FkNV_N7UKcMd"
      ],
      "gpuType": "V28",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}