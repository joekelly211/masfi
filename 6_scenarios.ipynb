{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/dev/6_scenarios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8jLRKRSfgb"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkvV803eSUQY"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi_asartr\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi_asartr'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEpdIjOwSiB1"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install astropy\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!pip install tensorflow\n",
        "!pip install xgboost --upgrade\n",
        "!apt-get install -y gdal-bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQDSsvGeSi4u"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import itertools\n",
        "import json\n",
        "from os import makedirs\n",
        "from os.path import join, exists\n",
        "from osgeo import gdal\n",
        "import ipywidgets as widgets\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import psutil\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "import re\n",
        "from scipy import ndimage\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxgT3K1Sjum"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "\n",
        "feature_dir = join(base_dir, \"3_features\")\n",
        "feature_resampled_dir = join(feature_dir, \"resampled\")\n",
        "feature_final_dir = join(feature_dir, \"final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "masks_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(scenarios_dir, exist_ok=True)\n",
        "makedirs(masks_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96FAfeyooMLz"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)\n",
        "\n",
        "# Burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster, polygon, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "  with rasterio.open(raster, 'r+') as src:\n",
        "      array = src.read(1)\n",
        "      transform = src.transform\n",
        "      gdf = gpd.read_file(polygon)\n",
        "      for geom in gdf.geometry:\n",
        "          if not fixed and column_name == None:\n",
        "              column_name = gdf.columns[0]\n",
        "          if not fixed: burn_value = gdf.loc[gdf.geometry == geom, column_name].values[0]\n",
        "          else: burn_value = fixed_value\n",
        "          rasterize([(geom, burn_value)], out=array, transform=transform,\n",
        "              all_touched=all_touched, dtype=src.meta['dtype'], out_shape=src.shape)\n",
        "      src.write(array, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_OkHaUkTCyB"
      },
      "source": [
        "# Select model and scenario area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mB0XrPNYU_I"
      },
      "outputs": [],
      "source": [
        "# Select a model\n",
        "model_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file == 'model.json':\n",
        "      print(f'selected_model = \"{subdir.split(f\"{models_dir}/\",1)[1]}\"')\n",
        "      model_exists = True\n",
        "if not model_exists:\n",
        "  print(\"No model exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbXY6iP_qjYc"
      },
      "outputs": [],
      "source": [
        "selected_model = \"agbd_historic_250429_223033\"\n",
        "categorise_target = False # If the target was categorised in 5_models\n",
        "\n",
        "# Define model directories\n",
        "selected_model_dir = join(models_dir,selected_model)\n",
        "selected_model_json = join(selected_model_dir, \"model.json\")\n",
        "selected_model_descr_dir = join(selected_model_dir, \"model_description.json\")\n",
        "selected_model_dataset_path = join(selected_model_dir, f\"{selected_model}.pkl\")\n",
        "selected_model_dataset = pd.read_pickle(selected_model_dataset_path)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(selected_model_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_target = model_dataset_description[\"selected_target\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "covariates_categorised = model_dataset_description[\"covariates_categorised\"]\n",
        "selected_features = model_dataset_description[\"selected_features\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "filter_parameter = model_dataset_description[\"filter_parameter\"]\n",
        "filter_values_to_include = model_dataset_description[\"filter_values_to_include\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "covariates = [covariate[4:] for covariate in covariates_renamed]\n",
        "\n",
        "# Create scenarios model directory\n",
        "scenarios_model_dir = join(scenarios_dir, selected_model)\n",
        "makedirs(scenarios_model_dir, exist_ok=True)\n",
        "\n",
        "# Copy model_dataset_description.json\n",
        "with open(join(scenarios_model_dir, \"model_dataset_description.json\"), \"w\") as file:\n",
        "  file.write(json.dumps(model_dataset_description))\n",
        "\n",
        "# Select a scenario area\n",
        "scenario_area_exists = False\n",
        "for subdir in os.listdir(scenarios_model_dir):\n",
        "  if not subdir.endswith('.csv') and not subdir.endswith('.json'):\n",
        "    print(f'selected_scenario_area = \"{subdir}\"')\n",
        "    scenario_area_exists = True\n",
        "if not scenario_area_exists:\n",
        "  print(f\"Create a scenario area directory in {scenarios_model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNYQ3-q2dUKl"
      },
      "outputs": [],
      "source": [
        "selected_scenario_area = \"asartr\"\n",
        "\n",
        "# Define scenario area directory\n",
        "scenario_area_dir = join(scenarios_model_dir,selected_scenario_area)\n",
        "makedirs(scenario_area_dir, exist_ok=True)\n",
        "\n",
        "# Create subdirectories\n",
        "features_dir = join(scenario_area_dir, \"features\")\n",
        "tile_templates_dir = join(scenario_area_dir, 'tile_templates')\n",
        "tile_features_dir = join(scenario_area_dir, \"tile_features\")\n",
        "tile_feature_stacks_dir = join(scenario_area_dir, \"tile_feature_stacks\")\n",
        "tile_prediction_cache_dir = join(scenario_area_dir,\"tile_prediction_cache\")\n",
        "scenario_predictions_unmasked_dir = join(scenario_area_dir,\"scenario_predictions_unmasked\")\n",
        "scenario_predictions_dir = join(scenario_area_dir, \"scenario_predictions\")\n",
        "scenario_dist_dir = join(scenario_area_dir, \"scenario_disturbance\")\n",
        "intactness_dir = join(scenario_area_dir, 'intactness')\n",
        "\n",
        "makedirs(features_dir, exist_ok=True)\n",
        "makedirs(tile_templates_dir, exist_ok=True)\n",
        "makedirs(tile_features_dir, exist_ok=True)\n",
        "makedirs(tile_feature_stacks_dir, exist_ok=True)\n",
        "makedirs(tile_prediction_cache_dir, exist_ok=True)\n",
        "makedirs(scenario_predictions_unmasked_dir, exist_ok=True)\n",
        "makedirs(scenario_predictions_dir, exist_ok=True)\n",
        "makedirs(scenario_dist_dir, exist_ok=True)\n",
        "makedirs(intactness_dir, exist_ok=True)\n",
        "\n",
        "# Copy features from the final features directory\n",
        "for feature in os.listdir(feature_final_dir):\n",
        "  if feature not in os.listdir(features_dir):\n",
        "    feature_original_path = join(feature_final_dir, feature)\n",
        "    feature_copy_path = join(features_dir, feature)\n",
        "    copyfile(feature_original_path, feature_copy_path)\n",
        "print(f\"All features present in the following directory have already been copied over: {feature_final_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3w7svaUvs07"
      },
      "source": [
        "# Define scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJZoEWycn8-x"
      },
      "outputs": [],
      "source": [
        "# Yearly scenarios may include years after the model scenario if feature data is available later than GEDI data.\n",
        "# Date of the prediction is ~December 31st, e.g. '2024' is 31/12/2024, requiring features up to 2024.\n",
        "\n",
        "model_scenario_override = None # set if cannot be automatically determined from model features\n",
        "\n",
        "yearly_features = [\"forest_with_edge_effects\", \"disturbance_with_edge_effects\"]\n",
        "\n",
        "# Remove the 'fea_' prefix from each feature\n",
        "model_features = sorted([feature[4:] for feature in selected_features])\n",
        "\n",
        "# Create a list of feature years from the model's features\n",
        "model_feature_years = []\n",
        "for feature in model_features:\n",
        "  for yearly_feature in yearly_features:\n",
        "    if yearly_feature in feature:\n",
        "      model_feature_years.append(int(feature[-4:]))\n",
        "\n",
        "# Determine the model scenario from the maximum year\n",
        "if model_scenario_override != None: model_scenario = model_scenario_override\n",
        "else: model_scenario = max(model_feature_years)\n",
        "model_scenario_filename = f\"{model_scenario}.csv\"\n",
        "model_scenario_dir = join(scenarios_model_dir,model_scenario_filename)\n",
        "print(f\"The maximum year used in the model is {model_scenario}, which has been created as the first scenario.\\n\")\n",
        "print(f\"The {model_scenario} scenario feature list has been saved to:\\n {model_scenario_dir}\\n\")\n",
        "print(f\"Ensure all features in this list have been copied to:\\n{features_dir}\\n\")\n",
        "\n",
        "# Save the model scenario features as a .csv\n",
        "pd.DataFrame(model_features).to_csv(model_scenario_dir, index=False)\n",
        "\n",
        "# Determine available feature years\n",
        "final_feature_years = []\n",
        "for final_feature in os.listdir(feature_final_dir):\n",
        "  if final_feature.endswith('.tif') and final_feature[-9] == '_':\n",
        "    try: final_feature_years.append(int(final_feature[-8:-4]))\n",
        "    except: continue\n",
        "\n",
        "# Find the first and last feature years\n",
        "first_feature_year = min(final_feature_years)\n",
        "last_feature_year = max(final_feature_years)\n",
        "additional_feature_years = last_feature_year - model_scenario\n",
        "print(f\"The first available feature year is {first_feature_year} and the last is {last_feature_year}.\\n\")\n",
        "\n",
        "# Calculate the range of scenario years and minimum scenario year\n",
        "model_scenario_year_range = max(model_feature_years) - (min(model_feature_years))\n",
        "minimum_yearly_scenario = first_feature_year + model_scenario_year_range\n",
        "print(f\"The earliest scenario year that can be predicted is {minimum_yearly_scenario}.\")\n",
        "print(f\"The latest scenario year that can be predicted is {last_feature_year}.\")\n",
        "print(f\"This is based on the number of yearly features used to train the model and the total availability of features.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPd_nrFYeGJ3"
      },
      "outputs": [],
      "source": [
        "# Select constant features which are the same in every scenario, e.g. topography\n",
        "print(\"constant_features = [\")\n",
        "for feature in model_features:\n",
        "  print(f'  \"{feature}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARVJed3vTgBv"
      },
      "outputs": [],
      "source": [
        "constant_features = [\n",
        "  \"coast_proximity_km\",\n",
        "  \"latitude\",\n",
        "  \"longitude\",\n",
        "  \"topo_dtm_smooth_aspect_cosine\",\n",
        "  \"topo_dtm_smooth_aspect_sine\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_03\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_07\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_11\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_03\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_07\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_11\",\n",
        "  \"topo_dtm_smooth_eastness\",\n",
        "  \"topo_dtm_smooth_elevation\",\n",
        "  \"topo_dtm_smooth_northness\",\n",
        "  \"topo_dtm_smooth_profile_curvature\",\n",
        "  \"topo_dtm_smooth_roughness_03\",\n",
        "  \"topo_dtm_smooth_roughness_07\",\n",
        "  \"topo_dtm_smooth_roughness_11\",\n",
        "  \"topo_dtm_smooth_slope\",\n",
        "  \"topo_dtm_smooth_stream_power_index_log10\",\n",
        "  \"topo_dtm_smooth_surface_area_ratio\",\n",
        "  \"topo_dtm_smooth_tangential_curvature\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_03\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_07\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_11\",\n",
        "  \"topo_dtm_smooth_topographic_ruggedness_index\",\n",
        "  \"topo_dtm_smooth_topographic_wetness_index\",\n",
        "  \"topo_dtm_unsmooth_aspect_cosine\",\n",
        "  \"topo_dtm_unsmooth_aspect_sine\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_03\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_07\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_11\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_03\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_07\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_11\",\n",
        "  \"topo_dtm_unsmooth_eastness\",\n",
        "  \"topo_dtm_unsmooth_elevation\",\n",
        "  \"topo_dtm_unsmooth_northness\",\n",
        "  \"topo_dtm_unsmooth_profile_curvature\",\n",
        "  \"topo_dtm_unsmooth_roughness_03\",\n",
        "  \"topo_dtm_unsmooth_roughness_07\",\n",
        "  \"topo_dtm_unsmooth_roughness_11\",\n",
        "  \"topo_dtm_unsmooth_slope\",\n",
        "  \"topo_dtm_unsmooth_stream_power_index_log10\",\n",
        "  \"topo_dtm_unsmooth_surface_area_ratio\",\n",
        "  \"topo_dtm_unsmooth_tangential_curvature\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_03\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_07\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_11\",\n",
        "  \"topo_dtm_unsmooth_topographic_ruggedness_index\",\n",
        "  \"topo_dtm_unsmooth_topographic_wetness_index\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg4XvxT5foVR"
      },
      "outputs": [],
      "source": [
        "# Note that areas that were forested in a historic yearly scenario but were reservoirs\n",
        "# or flooded at the time topographic feature data was collected (~2014 for GLO-30 DEM)\n",
        "# will be predicted based on a flat topography at the elevation of the water's surface.\n",
        "\n",
        "# Set scenario features as all non-constant features\n",
        "scenario_features = sorted(list(set(model_features) - set(constant_features)))\n",
        "\n",
        "# Create feature lists for all possible yearly scenarios\n",
        "for yearly_scenario in range(minimum_yearly_scenario, last_feature_year +1):\n",
        "  year_difference = model_scenario - yearly_scenario\n",
        "  yearly_scenario_features = []\n",
        "  for scenario_feature in scenario_features:\n",
        "    try:\n",
        "      year_change = int(scenario_feature[-4:]) - year_difference\n",
        "      yearly_scenario_feature = scenario_feature[:-4] + str(year_change)\n",
        "      yearly_scenario_features.append(yearly_scenario_feature)\n",
        "    except: yearly_scenario_features.append(scenario_feature)\n",
        "  # Compile yearly features and save as a .csv\n",
        "  yearly_scenario_features = sorted(yearly_scenario_features + constant_features)\n",
        "  yearly_scenario_filename = f\"{yearly_scenario}.csv\"\n",
        "  yearly_scenario_dir = join(scenarios_model_dir,yearly_scenario_filename)\n",
        "  pd.DataFrame(yearly_scenario_features).to_csv(yearly_scenario_dir, index=False)\n",
        "\n",
        "# Open the most recent yearly scenario feature list\n",
        "most_recent_scenario_csv = join(scenarios_model_dir,f\"{last_feature_year}.csv\")\n",
        "most_recent_scenario_features = pd.Series.tolist(pd.read_csv(most_recent_scenario_csv).iloc[:,0])\n",
        "\n",
        "print(f\"Lists of features for all possible yearly scenarios have been exported to {scenarios_model_dir}/.\")\n",
        "print(f\"Ensure all features in these lists have been copied to:\\n{features_dir}\\n\")\n",
        "\n",
        "# Create a 'no disturbance' feature for alternate scenarios.\n",
        "# Assumes the minimum possible value is present in the first scenario year.\n",
        "minimum_disturbance_name = f\"disturbance_with_edge_effects_0000\"\n",
        "minimum_disturbance_path = join(features_dir, f\"{minimum_disturbance_name}.tif\")\n",
        "if not exists(minimum_disturbance_path):\n",
        "  example_disturbance = join(features_dir, f\"disturbance_with_edge_effects_{first_feature_year}.tif\")\n",
        "  example_disturbance_array = gdal.Open(example_disturbance).ReadAsArray()\n",
        "  minimum_disturbance_value = example_disturbance_array.min()\n",
        "  minimum_disturbance_array = np.where(example_disturbance_array, minimum_disturbance_value, minimum_disturbance_value)\n",
        "  export_array_as_tif(minimum_disturbance_array, minimum_disturbance_path, template = example_disturbance)\n",
        "  print(f\"The minimum disturbance value is {minimum_disturbance_value}\\n,\")\n",
        "  print(f\"which has been used to create the 'minimum disturbance' feature {minimum_disturbance_name}.\")\n",
        "else: print(f\"The minimum disturbance feature {minimum_disturbance_name} already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rK4JtDSAtn6"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios remove degradation for specific time ranges\n",
        "\n",
        "# Define ranges for 'no degradation' scenarios\n",
        "define_no_degradation_scenarios = True\n",
        "\n",
        "# No degradation ranges as tuples of (start_year, end_year)\n",
        "no_degradation_ranges = [\n",
        "    (1991, 2014),\n",
        "    (2015, 2024),\n",
        "    (2022, 2022),\n",
        "    (2023, 2023),\n",
        "    (2024, 2024),\n",
        "    (2022, 2024),\n",
        "    (2023, 2024),\n",
        "]\n",
        "\n",
        "# Create a feature list for 'no degradation' scenarios\n",
        "if define_no_degradation_scenarios:\n",
        "  for start_year, end_year in no_degradation_ranges:\n",
        "    assert end_year <= last_feature_year, \"End years must be at or before the last feature year.\"\n",
        "    assert end_year >= minimum_yearly_scenario, \"End years must be at or after the minimum yearly scenario.\"\n",
        "    assert start_year >= first_feature_year, \"Start years must be at or after the first feature year.\"\n",
        "    assert start_year >= end_year - model_scenario_year_range, \"Start years must be within the model scenario range of the end year.\"\n",
        "    assert start_year <= end_year, \"The start year must less than or equal to the end year.\"\n",
        "\n",
        "    # Determine base features based on the end year of the range\n",
        "    scenario_features_csv = join(scenarios_model_dir, f\"{end_year}.csv\")\n",
        "    base_features = pd.Series.tolist(pd.read_csv(scenario_features_csv).iloc[:,0])\n",
        "\n",
        "    no_degradation_features = []\n",
        "    for scenario_feature in base_features:\n",
        "      if \"disturbance_with_edge_effects\" in scenario_feature:\n",
        "        scenario_feature_year = int(scenario_feature[-4:])\n",
        "        # Replace disturbance feature if it falls within the specified range\n",
        "        if scenario_feature_year >= start_year:\n",
        "          no_degradation_features.append(minimum_disturbance_name)\n",
        "        else:\n",
        "          no_degradation_features.append(scenario_feature)\n",
        "      else:\n",
        "        no_degradation_features.append(scenario_feature)\n",
        "\n",
        "    no_degradation_scenario_filename = f\"{end_year}_no_degradation_since_{start_year}.csv\"\n",
        "    no_degradation_scenario_path = join(scenarios_model_dir, no_degradation_scenario_filename)\n",
        "    pd.DataFrame(no_degradation_features).to_csv(no_degradation_scenario_path, index=False)\n",
        "    print(f\"Feature list for a scenario without degradation between {start_year} and {end_year} exported to {no_degradation_scenario_filename}.\")\n",
        "else:\n",
        "  print(\"The 'no degradation' scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFy1yJekgw0v"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios replace degradation history of an 'actual year' with an 'alternate year'\n",
        "# Forest extent remains the same. This allows deforestation alone (without the effect of degradation and recovery)\n",
        "# To be measured by, e.g., 2024_with_2014_degradation - 2014 = 2024_deforestation_actual_since_2014.\n",
        "# Otherwise can only measure deforestation assuming a year was old-growth, e.g.\n",
        "# 2024_oldgrowth - 2014_oldgrowth = 2024_deforestation_oldgrowth_since_2014\n",
        "\n",
        "# Define ranges for 'alternate degradation' scenarios\n",
        "define_alternate_degradation_scenarios = True\n",
        "\n",
        "# Alternate degradation scenarios as tuples of (alternate_year, actual_year)\n",
        "alternate_degradation_ranges = [\n",
        "    # (1990, 2014), # Not possible without 1990 scenario\n",
        "    (2014, 2024),\n",
        "    (2021, 2022),\n",
        "    (2022, 2023),\n",
        "    (2023, 2024),\n",
        "    (2021, 2024),\n",
        "    (2022, 2024),\n",
        "\n",
        "]\n",
        "\n",
        "# Create a feature list for 'no degradation' scenarios\n",
        "if define_alternate_degradation_scenarios:\n",
        "  for alternate_year, actual_year in alternate_degradation_ranges:\n",
        "    assert actual_year <= last_feature_year, \"Actual years must be at or before the last feature year.\"\n",
        "    assert actual_year >= minimum_yearly_scenario + 1, \"Actual years must be at least one year after the minimum yearly scenario.\"\n",
        "    assert alternate_year >= first_feature_year, \"Alternate years must be at or after the first feature year.\"\n",
        "    assert alternate_year >= actual_year - model_scenario_year_range, \"Alternate years must be within the model scenario range of the end year.\"\n",
        "    assert alternate_year < actual_year, \"Alternate years must be before the actual year.\"\n",
        "\n",
        "    # Determine base features based on the end year of the range\n",
        "    scenario_features_csv = join(scenarios_model_dir, f\"{actual_year}.csv\")\n",
        "    base_features = pd.Series.tolist(pd.read_csv(scenario_features_csv).iloc[:,0])\n",
        "    actual_alternate_diff = actual_year - alternate_year\n",
        "\n",
        "    alternate_degradation_features = []\n",
        "    for scenario_feature in base_features:\n",
        "      if \"disturbance_with_edge_effects\" in scenario_feature:\n",
        "        # Replace actual disturbance feature with alternate disturbance feature\n",
        "        scenario_feature_year = int(scenario_feature[-4:])\n",
        "        alternate_feature_year = scenario_feature_year - actual_alternate_diff\n",
        "        alternate_feature = scenario_feature.replace(str(scenario_feature_year), str(alternate_feature_year))\n",
        "        alternate_degradation_features.append(alternate_feature)\n",
        "      else:\n",
        "        alternate_degradation_features.append(scenario_feature)\n",
        "\n",
        "    alternate_degradation_scenario_filename = f\"{actual_year}_alternate_degradation_{alternate_year}.csv\"\n",
        "    alternate_degradation_scenario_path = join(scenarios_model_dir, alternate_degradation_scenario_filename)\n",
        "    pd.DataFrame(alternate_degradation_features).to_csv(alternate_degradation_scenario_path, index=False)\n",
        "    print(f\"Feature list for a {alternate_year} scenario with {actual_year} degradationm exported to {alternate_degradation_scenario_filename}.\")\n",
        "else:\n",
        "  print(\"The 'alternate degradation' scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6u03s8VHfN8"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios simulate old-growth forest using a proxy area specified by the user.\n",
        "# Forest extent (i.e. 'no deforestation') can be set from yearly feature, or all historic / potential forest area.\n",
        "\n",
        "define_oldgrowth_scenarios = True\n",
        "oldgrowth_yearly_scenarios = [\n",
        "    1990,\n",
        "    2014,\n",
        "    2021,\n",
        "    2022,\n",
        "    2023,\n",
        "    last_feature_year\n",
        "]\n",
        "\n",
        "for year in oldgrowth_yearly_scenarios:\n",
        "  assert year in final_feature_years, \"Years in 'oldgrowth_yearly_scenarios' must be available in the final yearly features.\"\n",
        "\n",
        "simulate_all_oldgrowth = True\n",
        "all_oldgrowth_name = \"all_oldgrowth\"\n",
        "\n",
        "# The feature that best indicates oldgrowth to the model, e.g. certain protected areas\n",
        "# This will be modified to cover the entire scenario area for the old-growth scenarios\n",
        "oldgrowth_feature = 'lu_oldgrowth_with_edge_effects'\n",
        "\n",
        "# Some features may confound the old-growth proxy, e.g. protected areas that are not known to be old-growth\n",
        "# These will be removed for the old-growth scenarios\n",
        "oldgrowth_redundant_features = [\n",
        "\n",
        "]\n",
        "\n",
        "if define_oldgrowth_scenarios:\n",
        "  # Expand the oldgrowth feature to the entire scenario area\n",
        "  oldgrowth_feature_all_dir = join(features_dir, f\"{oldgrowth_feature}_all.tif\")\n",
        "  if not exists(oldgrowth_feature_all_dir):\n",
        "    oldgrowth_feature_dir = join(features_dir, f\"{oldgrowth_feature}.tif\")\n",
        "    oldgrowth_feature_array = gdal.Open(oldgrowth_feature_dir).ReadAsArray()\n",
        "    oldgrowth_feature_max_value = oldgrowth_feature_array.max()\n",
        "    print(f\"The maximum value for the oldgrowth feature '{oldgrowth_feature}' is {oldgrowth_feature_max_value}.\")\n",
        "    oldgrowth_feature_all_array = np.where(oldgrowth_feature_array, oldgrowth_feature_max_value, oldgrowth_feature_max_value)\n",
        "    oldgrowth_feature_all_dir = join(features_dir, f\"{oldgrowth_feature}_all.tif\")\n",
        "    export_array_as_tif(oldgrowth_feature_all_array, oldgrowth_feature_all_dir, template = oldgrowth_feature_dir)\n",
        "    print(f\"The oldgrowth proxy {oldgrowth_feature} has been expanded to the entire scenario area\")\n",
        "    print(f\"And exported to {oldgrowth_feature_all_dir}\")\n",
        "  else: print(f\"The oldgrowth feature '{oldgrowth_feature}_all.tif' already exists.\\n\")\n",
        "\n",
        "  # Remove the redundant features from the oldgrowth scenario area\n",
        "  for redundant_feature in oldgrowth_redundant_features:\n",
        "    redundant_feature_none_dir = join(features_dir, f\"{redundant_feature}_none.tif\")\n",
        "    if not exists(redundant_feature_none_dir):\n",
        "      redundant_feature_dir = join(features_dir, f\"{redundant_feature}.tif\")\n",
        "      redundant_feature_array = gdal.Open(redundant_feature_dir).ReadAsArray()\n",
        "      redundant_feature_min_value = redundant_feature_array.min()\n",
        "      print(f\"The minimum value for the redundant feature {redundant_feature} is {redundant_feature_min_value}.\")\n",
        "      redundant_feature_none_array = np.where(redundant_feature_array, redundant_feature_min_value, redundant_feature_min_value)\n",
        "      export_array_as_tif(redundant_feature_none_array, redundant_feature_none_dir, redundant_feature_dir)\n",
        "      print(f\"The oldgrowth redundant feature {oldgrowth_feature} has been removed from the entire scenario area\")\n",
        "      print(f\"And exported to {oldgrowth_feature_all_dir}.\")\n",
        "    else: print(f\"The oldgrowth redundant feature '{redundant_feature}_none.tif' already exists.\\n\")\n",
        "\n",
        "  # Generate 'oldgrowth' scenarios for each specified year (i.e. forest extent in that year)\n",
        "  for year in oldgrowth_yearly_scenarios:\n",
        "    oldgrowth_features = []\n",
        "    for scenario_feature in most_recent_scenario_features:\n",
        "      old_growth_scenario_year_diff = last_feature_year - year\n",
        "      if \"disturbance_with_edge_effects\" in scenario_feature:\n",
        "        oldgrowth_features.append(minimum_disturbance_name)\n",
        "      elif \"forest_with_edge_effects\" in scenario_feature:\n",
        "        scenario_feature_year = int(scenario_feature[-4:])\n",
        "        if scenario_feature_year-old_growth_scenario_year_diff > first_feature_year: # Replace with specified forest cover year\n",
        "          oldgrowth_features.append(f\"forest_with_edge_effects_{scenario_feature_year-old_growth_scenario_year_diff}\")\n",
        "        else: oldgrowth_features.append(f\"forest_with_edge_effects_{first_feature_year}\")\n",
        "      elif scenario_feature == oldgrowth_feature:\n",
        "        oldgrowth_features.append(f\"{scenario_feature}_all\")\n",
        "      elif scenario_feature in oldgrowth_redundant_features:\n",
        "        oldgrowth_features.append(f\"{scenario_feature}_none\")\n",
        "      else: oldgrowth_features.append(scenario_feature)\n",
        "    oldgrowth_filename = f\"{year}_oldgrowth.csv\"\n",
        "    oldgrowth_dir = join(scenarios_model_dir, oldgrowth_filename)\n",
        "    pd.DataFrame(oldgrowth_features).to_csv(oldgrowth_dir, index=False)\n",
        "    print(f\"Feature list for a scenario where all forest in {year} was old-growth\")\n",
        "    print(f\"has been exported to {oldgrowth_filename}.\\n\")\n",
        "\n",
        "  # Generate 'all_oldgrowth' features and scenarios\n",
        "  if simulate_all_oldgrowth:\n",
        "    # Based on the first TMF AnnualChanges land coverage\n",
        "    # Create a forest feature for all land that exists in the first feature year (e.g. 1990)\n",
        "    forest_all_oldgrowth_name = f\"forest_with_edge_effects_{all_oldgrowth_name}\"\n",
        "    forest_all_oldgrowth_path = join(features_dir, f\"{forest_all_oldgrowth_name}.tif\")\n",
        "    first_annual_changes_filename = f\"tmf_AnnualChanges_Dec{first_feature_year}.tif\"\n",
        "    first_annual_changes_path = join(feature_resampled_dir, first_annual_changes_filename)\n",
        "    if not exists(forest_all_oldgrowth_path):\n",
        "      if exists(first_annual_changes_path):\n",
        "        first_annual_changes_array = gdal.Open(first_annual_changes_path).ReadAsArray()\n",
        "        # Convert all water values to 'nodata' and non-water values to '1'\n",
        "        forest_all_oldgrowth_array = np.where(first_annual_changes_array == 5, nodatavalue, 1)\n",
        "        # Set smoothing kernel and precision\n",
        "        kernel, precision = Gaussian2DKernel(x_stddev=1, y_stddev=1), 2\n",
        "        # Reclassify for binary differentiation after proximity conversion\n",
        "        differentiator_array = forest_all_oldgrowth_array.copy()\n",
        "        differentiator_array[differentiator_array == 1] = 10\n",
        "        # Positive proximity\n",
        "        positive_distances = ndimage.distance_transform_edt(forest_all_oldgrowth_array == 0) # target pixels\n",
        "        positive_proximity_array = np.where(positive_distances > 2, 0, positive_distances) # max distance 2\n",
        "        # Negative proximity\n",
        "        negative_distances = ndimage.distance_transform_edt(forest_all_oldgrowth_array == 1) # target pixels\n",
        "        negative_proximity_array = np.where(negative_distances > 2, 0, negative_distances) # max distance 2\n",
        "        # Sum proximities and differentiator\n",
        "        pixel_prox_summed =  differentiator_array + positive_proximity_array + negative_proximity_array\n",
        "        # Reclassify for better semantic understanding of pixel proximity\n",
        "        pixel_prox_reclassed = pixel_prox_summed.copy()\n",
        "        pixel_prox_reclass_table = [(0, 0, -4), (1, 1, -1), (1.4, 1.5, -2), (2, 2, -3), (10, 10, 3), (11, 11, 0), (11.4, 11.5, 1), (12, 12, 2)]\n",
        "        for min_value, max_value, new_value in pixel_prox_reclass_table:\n",
        "          pixel_prox_reclassed[(pixel_prox_reclassed >= min_value) & (pixel_prox_reclassed <= max_value)] = new_value\n",
        "        # Smooth binary array using 2D convolution\n",
        "        binary_smoothed = convolve(forest_all_oldgrowth_array, kernel, boundary='extend')\n",
        "        # Sum pixel proximity and smoothed binary array\n",
        "        edge_effects_array = np.round(pixel_prox_reclassed + binary_smoothed, precision)\n",
        "        # Export forest edge effects feature\n",
        "        export_array_as_tif(edge_effects_array, forest_all_oldgrowth_path)\n",
        "        print(f\"{forest_all_oldgrowth_name} has been created and saved to\\n{features_dir}\\n\")\n",
        "      else: print(f\"The first TMF annual changes year raster needed for comrec is not in the indicated directory:\\n{first_annual_changes_path}\\n\")\n",
        "    print(f\"{forest_all_oldgrowth_name} already exists in\\n{features_dir}\\n\")\n",
        "    if exists(forest_all_oldgrowth_path):\n",
        "      oldgrowth_all_features = []\n",
        "      for scenario_feature in most_recent_scenario_features:\n",
        "        if \"disturbance_with_edge_effects\" in scenario_feature:\n",
        "          oldgrowth_all_features.append(minimum_disturbance_name)\n",
        "        elif \"forest_with_edge_effects\" in scenario_feature:\n",
        "          oldgrowth_all_features.append(forest_all_oldgrowth_name)\n",
        "        elif scenario_feature == oldgrowth_feature:\n",
        "          oldgrowth_all_features.append(f\"{scenario_feature}_all\")\n",
        "        elif scenario_feature in oldgrowth_redundant_features:\n",
        "          oldgrowth_all_features.append(f\"{scenario_feature}_none\")\n",
        "        else: oldgrowth_all_features.append(scenario_feature)\n",
        "      oldgrowth_all_filename = f\"{all_oldgrowth_name}.csv\"\n",
        "      oldgrowth_all_dir = join(scenarios_model_dir, oldgrowth_all_filename)\n",
        "      pd.DataFrame(oldgrowth_all_features).to_csv(oldgrowth_all_dir, index=False)\n",
        "      print(f\"Feature list for {all_oldgrowth_name} has been exported to {oldgrowth_all_filename}.\\n\")\n",
        "\n",
        "    # Create A forest mask for 'all oldgrowth'\n",
        "    # Also forests reservoirs since the first TMF annual changes year, though topography may be wrong\n",
        "    if exists(join(scenarios_model_dir, f\"{all_oldgrowth_name}.csv\")):\n",
        "      if exists(first_annual_changes_path):\n",
        "        oldgrowth_all_mask_path = join(masks_dir, f\"mask_forest_{all_oldgrowth_name}.tif\")\n",
        "        if not exists(oldgrowth_all_mask_path):\n",
        "          first_annual_changes_array = gdal.Open(first_annual_changes_path).ReadAsArray()\n",
        "          # Convert all water values to 'nodata' and non-water values to '1'\n",
        "          oldgrowth_all_mask_array = np.where(first_annual_changes_array == 5, nodatavalue, 1)\n",
        "          export_array_as_tif(oldgrowth_all_mask_array, oldgrowth_all_mask_path)\n",
        "          print(f\"A mask for {all_oldgrowth_name} has been created at\\n{oldgrowth_all_mask_path}\")\n",
        "        else: print(f\"A mask for {all_oldgrowth_name} already exists at\\n{oldgrowth_all_mask_path}\")\n",
        "      else: print(f\"The {first_annual_changes_filename} raster needed to mask {all_oldgrowth_name} doesn't exist.\")\n",
        "    else: print(f\"The scenario csv for {all_oldgrowth_name} doesn't exist.\")\n",
        "\n",
        "else: print(\"Old-growth scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy2nfHshozVQ"
      },
      "source": [
        "# Feature verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chCSZTV4XB8k"
      },
      "outputs": [],
      "source": [
        "# Check that all features in all scenario csvs exist\n",
        "scenario_csv_list = []\n",
        "all_features_exist = True # Changes to false if feature missing\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    csv_dir = join(scenarios_model_dir, csv)\n",
        "    csv_feature_list = pd.Series.tolist(pd.read_csv(csv_dir).iloc[:,0])\n",
        "    csv_feature_dir_list = []\n",
        "    for csv_feature in csv_feature_list:\n",
        "      if csv_feature not in covariates: csv_feature_dir_list.append(f\"{features_dir}/{csv_feature}.tif\")\n",
        "    for feature in csv_feature_dir_list:\n",
        "      if not exists(feature):\n",
        "        all_features_exist = False\n",
        "        print(f\"The following feature is missing:\\n{feature}\\n and is required for the scenario '{csv[:-4]}'\")\n",
        "\n",
        "if all_features_exist: print(\"All required features are present.\")\n",
        "print(\"Covariate features e.g. 'beam' and 'sensitivity' will be added at the prediction stage.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAqAaDfYkY7T"
      },
      "outputs": [],
      "source": [
        "# Is the scenario area equal to the original template area?\n",
        "original_template_area = True\n",
        "\n",
        "# If not, create a new template for the scenario area and upload to:\n",
        "# '6_scenarios/[model]/[scenario_area]/template.tif'\n",
        "if original_template_area: scenario_template_dir = join(areas_dir, \"template.tif\")\n",
        "else: scenario_template_dir = join(scenario_area_dir, \"template.tif\")\n",
        "print(f\"The following is being used as a template to verify scenario feature dimensions and projections:\\n{scenario_template_dir}\")\n",
        "\n",
        "\n",
        "scenario_template = gdal.Open(scenario_template_dir)\n",
        "scenario_template_dimensions, scenario_template_projection = scenario_template.GetGeoTransform(), scenario_template.GetProjection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBb-Cws3mMUy"
      },
      "outputs": [],
      "source": [
        "feature_issue = False\n",
        "for feature in os.listdir(features_dir):\n",
        "  if feature.endswith('.tif'):\n",
        "    feature_dir = join(features_dir, feature)\n",
        "    feature_open = gdal.Open(feature_dir)\n",
        "    feature_dimensions, feature_projection = feature_open.GetGeoTransform(), feature_open.GetProjection()\n",
        "    if feature_dimensions != scenario_template_dimensions:\n",
        "      print(f\"{feature} dimensions:\\n{feature_dimensions}\\ndo not match the scenario template dimensions:\\n{scenario_template_dimensions}\\n\")\n",
        "      feature_issue = True\n",
        "    if feature_projection != scenario_template_projection:\n",
        "      print(f\"{feature} projection:\\n{feature_projection}\\ndoes not match the scenario template projection:\\n{scenario_template_projection}\\n\\n\")\n",
        "      feature_issue = True\n",
        "\n",
        "if not feature_issue: print(f\"All features in the following directory have the correct dimensions and projection:\\n{features_dir}\")\n",
        "else: print(\"Correct and / or resample the feature(s).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T9UqJrzWECr"
      },
      "source": [
        "# Template tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zizOofP5dgW"
      },
      "outputs": [],
      "source": [
        "# Load the model scenario features for tile template creation\n",
        "model_scenario_features = pd.Series.tolist(pd.read_csv(model_scenario_dir).iloc[:,0])\n",
        "model_scenario_features_dirs = [features_dir + '/' + feature + '.tif' for feature in model_features]\n",
        "# Create a template feature array from the first feature that isn't a covariate (these are created later)\n",
        "template_base_path = next(r for r in model_scenario_features_dirs if all(c not in r for c in covariates))\n",
        "template_base = gdal.Open(template_base_path)\n",
        "template_base_array = template_base.ReadAsArray()\n",
        "template_base_xsize, template_base_ysize = template_base.GetRasterBand(1).XSize, template_base.GetRasterBand(1).YSize\n",
        "print(f\"The template feature is {template_base_xsize} x {template_base_ysize} pixels.\")\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles_exist = len(template_tile_list)\n",
        "\n",
        "if n_tiles_exist < 1: print(\"There are currently no template tiles. Run the next section.\")\n",
        "if n_tiles_exist >= 1:\n",
        "  tile_size_y_rounded_exist = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).YSize\n",
        "  tile_size_y_remainder_exist = gdal.Open(join(tile_templates_dir,f'template_tile_{n_tiles_exist}.tif')).GetRasterBand(1).YSize\n",
        "  if n_tiles_exist == 1: print(f\"There is a single 'tile' with a height of {tile_size_y_rounded_exist}.\"); tile_size_y_remainder_exist = 0\n",
        "  else: print(f\"There are {n_tiles_exist} template tiles, the first {n_tiles_exist-1} having a height of {tile_size_y_rounded_exist} pixels, the last {tile_size_y_remainder_exist} pixels.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njhqSH8e5cPu"
      },
      "outputs": [],
      "source": [
        "# Large scenario areas and / or numbers of features may be too much for the available memory.\n",
        "# This section defines how to split predictions into tiles that can then be merged.\n",
        "override_n_tiles = True  # Useful if the tile number has already been tested.\n",
        "n_tiles_override = 1\n",
        "tile_size_y_rounded_override = int(14910/n_tiles_override)\n",
        "\n",
        "memory_utilisation = 0.8 # Set to 0.8 to ensure crashes are avoided\n",
        "\n",
        "assert memory_utilisation > 0 and memory_utilisation <= 1, \"Set memory_utilisation to a value between 0 and 1\"\n",
        "\n",
        "# Calculate total size of feature stack\n",
        "feature_stack_size = template_base_array.size * len(model_scenario_features_dirs)\n",
        "\n",
        "# Calculate memory and the number of tiles required\n",
        "total_memory_needed = 64 / 8 * feature_stack_size\n",
        "print(f'RAM required for each prediction: ~{total_memory_needed/(1024**3):.3f} GB')\n",
        "print(f'RAM currently available: {psutil.virtual_memory().free / (1024**3):.3f} GB')\n",
        "n_tiles_temp = int(np.ceil(total_memory_needed / (psutil.virtual_memory().free * memory_utilisation)))\n",
        "\n",
        "# Calculate template tile size (split on the y axis only)\n",
        "tile_size_y_rounded = int(np.ceil(template_base_ysize/n_tiles_temp)) # Round the number of y pixels in each tile\n",
        "tile_size_y_remainder = template_base_ysize%tile_size_y_rounded # Calculate the remainder for the last tile\n",
        "n_tiles = max(1, len(range(0, template_base_ysize, tile_size_y_rounded))) # Update the number of tiles to include the remainder\n",
        "\n",
        "if override_n_tiles:\n",
        "  tile_size_y_rounded = tile_size_y_rounded_override\n",
        "  tile_size_y_remainder = template_base_ysize%tile_size_y_rounded\n",
        "  n_tiles = n_tiles_override\n",
        "  print(\"n_tiles has been overridden.\")\n",
        "\n",
        "print(f'The prediction template should be divided into {n_tiles} tiles to avoid crashing.')\n",
        "\n",
        "# Check if tiles need to be changed\n",
        "change_tiles = True\n",
        "if override_n_tiles:\n",
        "  if n_tiles == n_tiles_exist: change_tiles = False\n",
        "if n_tiles == n_tiles_exist and tile_size_y_rounded == tile_size_y_rounded_exist and tile_size_y_remainder == tile_size_y_remainder_exist:\n",
        "  change_tiles = False\n",
        "\n",
        "if change_tiles:\n",
        "  # Clear all tile directories\n",
        "  for tile in Path(tile_templates_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for tile in Path(tile_features_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for scenario_stack_dir in Path(tile_feature_stacks_dir).glob(\"**/*\"):\n",
        "    shutil.rmtree(scenario_stack_dir)\n",
        "  for tile in Path(tile_prediction_cache_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "\n",
        "  # Generate new tile templates based on available memory\n",
        "  tile_number = 1\n",
        "  for y_start in range(0, template_base_ysize, tile_size_y_rounded):\n",
        "    if tile_size_y_remainder != 0 and tile_number == n_tiles: tile_size_y = tile_size_y_remainder\n",
        "    else: tile_size_y = tile_size_y_rounded\n",
        "    tiling_string = \"gdal_translate -of GTIFF -srcwin \" + str(0)+ \", \" + str(y_start) + \", \" + str(template_base_xsize) + \", \" + str(tile_size_y) + \" \" + str(template_base_path) + \" \" + str(tile_templates_dir) + \"/template_tile_\" + str(tile_number) + \".tif\"\n",
        "    os.system(tiling_string)\n",
        "    tile_number += 1\n",
        "  print(\"Template tile creation complete.\")\n",
        "\n",
        "else: print(\"No changes to existing tiles are required.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkNV_N7UKcMd"
      },
      "source": [
        "# Feature tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1d1ELggV4jF"
      },
      "outputs": [],
      "source": [
        "# Create feature tiles.\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "tile_size_y_rounded = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).YSize\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "if n_tiles == 1: print(\"Feature tile creation skipped. Feature stack creation will use the original features.\")\n",
        "else:\n",
        "  # Progress\n",
        "  n_features = len(os.listdir(features_dir))\n",
        "  feature_progress_index, feature_progress_label = 0, widgets.Label(value=f\"Feature progress: 0 / {n_features}\")\n",
        "  display(feature_progress_label)\n",
        "  tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "  display(tile_progress_label)\n",
        "\n",
        "  # Loop through each feature in the 6_scenarios features directory\n",
        "  for feature in os.listdir(features_dir):\n",
        "    # Create list of tile directories\n",
        "    feature_dir = join(features_dir, feature)\n",
        "    feature_array = gdal.Open(feature_dir).ReadAsArray()\n",
        "    # Split the feature array into chunks based on tile size\n",
        "    feature_chunks = np.array_split(feature_array, np.arange(tile_size_y_rounded, len(feature_array), tile_size_y_rounded))\n",
        "    tile_count = 1\n",
        "    # Loop through tiles and export as .tif\n",
        "    for tile in range(n_tiles):\n",
        "      feature_tile_filename = f\"{feature[:-4]}_{tile_count}.tif\"\n",
        "      # Check if tile already exists\n",
        "      feature_tile_exists = False\n",
        "      for feature_tile in os.listdir(tile_features_dir):\n",
        "        if feature_tile == feature_tile_filename: feature_tile_exists=True\n",
        "        # If feature tile does not exist:\n",
        "      if feature_tile_exists == False:\n",
        "        template_tile_dir = join(tile_templates_dir, f\"template_tile_{tile_count}.tif\")\n",
        "        export_array_as_tif(feature_chunks[tile_count-1], join(tile_features_dir,feature_tile_filename), template_tile_dir, compress = False)\n",
        "        tile_count += 1\n",
        "      # Update tile progress\n",
        "      tile_progress_index += 1\n",
        "      tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_tiles}\"\n",
        "    tile_progress_index = 0\n",
        "    # Update feature progress\n",
        "    feature_progress_index += 1\n",
        "    feature_progress_label.value = f\"Feature progress: {feature_progress_index} / {n_features}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0r89JLYW_xU"
      },
      "source": [
        "# Feature stacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaHFv1nuW5CG"
      },
      "outputs": [],
      "source": [
        "# Create feature stack arrays for each scenario\n",
        "\n",
        "# Collect scenarios with .csv feature lists\n",
        "scenarios_list = []\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    scenarios_list.append(csv[:-4])\n",
        "\n",
        "# Select scenarios to generate tiled feature stacks\n",
        "print(\"scenarios_to_stack = [\")\n",
        "for scenario in sorted(scenarios_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-uJ-EkoxIym"
      },
      "outputs": [],
      "source": [
        "scenarios_to_stack = [\n",
        "  \"1990_oldgrowth\",\n",
        "  \"2014\",\n",
        "  \"2014_no_degradation_since_1991\",\n",
        "  \"2014_oldgrowth\",\n",
        "  \"2015\",\n",
        "  \"2016\",\n",
        "  \"2017\",\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_oldgrowth\",\n",
        "  \"2022\",\n",
        "  \"2022_alternate_degradation_2021\",\n",
        "  \"2022_no_degradation_since_2022\",\n",
        "  \"2022_oldgrowth\",\n",
        "  \"2023\",\n",
        "  \"2023_alternate_degradation_2022\",\n",
        "  \"2023_no_degradation_since_2023\",\n",
        "  \"2023_oldgrowth\",\n",
        "  \"2024\",\n",
        "  \"2024_alternate_degradation_2014\",\n",
        "  \"2024_alternate_degradation_2021\",\n",
        "  \"2024_alternate_degradation_2022\",\n",
        "  \"2024_alternate_degradation_2023\",\n",
        "  \"2024_no_degradation_since_2015\",\n",
        "  \"2024_no_degradation_since_2022\",\n",
        "  \"2024_no_degradation_since_2023\",\n",
        "  \"2024_no_degradation_since_2024\",\n",
        "  \"2024_oldgrowth\",\n",
        "  \"all_oldgrowth\",\n",
        "]\n",
        "\n",
        "assert set(scenarios_to_stack).issubset(scenarios_list), \"Not all selected scenarios exist.\"\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "# Progress\n",
        "scenario_progress_index, scenario_progress_label = 0, widgets.Label(value=f\"Scenario progress: 0 / {len(scenarios_to_stack)}\")\n",
        "display(scenario_progress_label)\n",
        "stack_progress_index, stack_progress_label = 0, widgets.Label(value=f\"Tiled feature stack progress: 0 / {n_tiles}\")\n",
        "display(stack_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_stack:\n",
        "    # Define directory and features\n",
        "    scenario_feature_stacks_dir = join(tile_feature_stacks_dir, scenario)\n",
        "    makedirs(scenario_feature_stacks_dir, exist_ok=True)\n",
        "    scenario_features_csv = join(scenarios_model_dir,f\"{scenario}.csv\")\n",
        "    scenario_features = pd.Series.tolist(pd.read_csv(scenario_features_csv).iloc[:,0])\n",
        "    # Set the number of stacks to the number of tiles\n",
        "    if n_tiles == 0: n_stacks = 1\n",
        "    else: n_stacks = n_tiles\n",
        "    # Create a tile count to match the feature stack\n",
        "    tile_count = 1\n",
        "    for tile in range(n_stacks):\n",
        "      scenario_stack_filename = f\"feature_stack_{scenario}_{tile_count}.npy\"\n",
        "      # Check if feature stack already exists\n",
        "      feature_stack_exists = False\n",
        "      for feature_stack in os.listdir(scenario_feature_stacks_dir):\n",
        "        if feature_stack == scenario_stack_filename: feature_stack_exists=True\n",
        "      # If scenario prediction tile does not exist:\n",
        "      if feature_stack_exists == False:\n",
        "        scenario_tile_stack_dir = join(scenario_feature_stacks_dir, scenario_stack_filename)\n",
        "        # Create feature chunks (arrays) from tiles\n",
        "        if n_stacks == 1: feature_tiles_dirs = [f\"{features_dir}/{feature}.tif\" for feature in scenario_features]\n",
        "        else: feature_tiles_dirs = [f\"{tile_features_dir}/{feature}_{tile_count}.tif\" for feature in scenario_features]\n",
        "        feature_array_chunks = []\n",
        "        for feature in feature_tiles_dirs:\n",
        "          # Covariate raster will exist and should be ignored if prediction stage has already been attempted\n",
        "          if feature.split('/')[-1].split('.')[0] not in covariates and feature.split('/')[-1].split('.')[0] not in [f\"{cov}_{tile_count}\" for cov in covariates]:\n",
        "            feature_array_chunk = gdal.Open(feature).ReadAsArray()\n",
        "            feature_array_chunks.append(feature_array_chunk)\n",
        "        # Create a feature stack from chunks\n",
        "        feature_stack = np.dstack(feature_array_chunks)\n",
        "        feature_array_chunks = None # Flush chunks\n",
        "        stack_height, stack_width, stack_n_features = feature_stack.shape\n",
        "        # Convert feature stack to 2D numpy array with features as columns\n",
        "        feature_stack_reshaped = feature_stack.reshape(stack_height * stack_width, stack_n_features)\n",
        "        feature_stack = None # Flush stack\n",
        "        # Save as a numpy file\n",
        "        np.save(scenario_tile_stack_dir, feature_stack_reshaped)\n",
        "        feature_stack_reshaped = None # Flush reshaped stack\n",
        "      # Update progress\n",
        "      tile_count += 1\n",
        "      stack_progress_index += 1\n",
        "      stack_progress_label.value = f\"Tiled feature stack progress: {stack_progress_index} / {n_stacks}\"\n",
        "    # Reset tile progress\n",
        "    stack_progress_index = 0\n",
        "    scenario_progress_index += 1\n",
        "    scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index} / {len(scenarios_to_stack)}\"\n",
        "print(\"\\nFeature stacks complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oysAS9DI7yRg"
      },
      "source": [
        "# Predict scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y92V4nKGcyE"
      },
      "outputs": [],
      "source": [
        "# This is for testing models and scenarios, or making predictions where no\n",
        "# uncertainty metric for the variate (e.g. standard error or stdev) is available.\n",
        "# If these are available, proceed to 7_predictions.ipynb.\n",
        "\n",
        "# Collect available scenarios from the feature stack tiles directory\n",
        "scenario_stacks_list = []\n",
        "for scenario in os.listdir(tile_feature_stacks_dir):\n",
        "    scenario_stacks_list.append(scenario)\n",
        "\n",
        "# Select scenarios to predict\n",
        "print(\"scenarios_to_predict = [\")\n",
        "for scenario in sorted(scenario_stacks_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRlFYaIGNXbI"
      },
      "outputs": [],
      "source": [
        "scenarios_to_predict = [\n",
        "  \"1990_oldgrowth\",\n",
        "  \"2014\",\n",
        "  \"2014_no_degradation_since_1991\",\n",
        "  \"2014_oldgrowth\",\n",
        "  \"2015\",\n",
        "  \"2016\",\n",
        "  \"2017\",\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2021_oldgrowth\",\n",
        "  \"2022\",\n",
        "  \"2022_alternate_degradation_2021\",\n",
        "  \"2022_no_degradation_since_2022\",\n",
        "  \"2022_oldgrowth\",\n",
        "  \"2023\",\n",
        "  \"2023_alternate_degradation_2022\",\n",
        "  \"2023_no_degradation_since_2023\",\n",
        "  \"2023_oldgrowth\",\n",
        "  \"2024\",\n",
        "  \"2024_alternate_degradation_2014\",\n",
        "  \"2024_alternate_degradation_2021\",\n",
        "  \"2024_alternate_degradation_2022\",\n",
        "  \"2024_alternate_degradation_2023\",\n",
        "  \"2024_no_degradation_since_2015\",\n",
        "  \"2024_no_degradation_since_2022\",\n",
        "  \"2024_no_degradation_since_2023\",\n",
        "  \"2024_no_degradation_since_2024\",\n",
        "  \"2024_oldgrowth\",\n",
        "  \"all_oldgrowth\",\n",
        "]\n",
        "\n",
        "assert set(scenarios_to_predict).issubset(scenario_stacks_list), \"Not all selected scenarios exist.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TibU8xTZfcuL"
      },
      "outputs": [],
      "source": [
        "add_covariates = True # Adds a selected covariate value as the feature\n",
        "sensitivity_value = 0.99\n",
        "beam_value = 5\n",
        "# 5 is the first of the full beams, with the least bias on AGBD.\n",
        "# Cover beams 1 - 4 underestimate. Full beams 7 - 8 overestimate. 5 - 6 tend give average values.\n",
        "\n",
        "# Check for GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0': print('GPU device not found')\n",
        "else: print(f\"Found GPU at: {device_name}\")\n",
        "\n",
        "# Load model\n",
        "booster = xgb.Booster()\n",
        "booster.load_model(selected_model_json)\n",
        "if categorise_target: XGBPredictor = xgb.XGBClassifier()\n",
        "else: XGBPredictor = xgb.XGBRegressor()\n",
        "XGBPredictor._Booster = booster\n",
        "\n",
        "# Avoids issues using dataframe from CPU\n",
        "xgb.set_config(verbosity=0, use_rmm=True)\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "template_tile_x = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).XSize\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "# Scenario progress\n",
        "scenario_progress_index = 0\n",
        "scenario_progress_label = widgets.Label(f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\")\n",
        "display(scenario_progress_label)\n",
        "\n",
        "# Tile progress\n",
        "tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "display(tile_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_predict:\n",
        "  # Define scenario filename and check if exists\n",
        "  scenario_prediction_unmasked_filename = f\"{scenario}__{selected_scenario_area}_{selected_model}_unmasked.tif\"\n",
        "  scenario_prediction_unmasked_exists=False\n",
        "  for scenario_prediction in os.listdir(scenario_predictions_unmasked_dir):\n",
        "    if scenario_prediction == scenario_prediction_unmasked_filename:\n",
        "      scenario_prediction_unmasked_exists=True\n",
        "  # If scenario prediction does not exist:\n",
        "  if scenario_prediction_unmasked_exists == False:\n",
        "    # Get number of stacks\n",
        "    scenario_feature_stack_dir = join(tile_feature_stacks_dir, scenario)\n",
        "    n_stacks = len(os.listdir(scenario_feature_stack_dir))\n",
        "    # Create a tile cache directory for the prediction\n",
        "    tile_cache_scenario_dir = join(tile_prediction_cache_dir, scenario_prediction_unmasked_filename[:-4])\n",
        "    makedirs(tile_cache_scenario_dir, exist_ok=True)\n",
        "    # Create a tile count to match the feature stack chunk\n",
        "    for stack in range(1, n_stacks+1):\n",
        "      scenario_tile_filename = f\"scenario_tile_{stack}.tif\"\n",
        "      # Check if tile already exists\n",
        "      scenario_tile_exists = False\n",
        "      for scenario_tile in os.listdir(tile_cache_scenario_dir):\n",
        "        if scenario_tile == scenario_tile_filename: scenario_tile_exists=True\n",
        "      # If scenario prediction tile does not exist:\n",
        "      if scenario_tile_exists == False:\n",
        "        # Load template tile parameters\n",
        "        template_tile_dir = join(tile_templates_dir, f\"template_tile_{stack}.tif\")\n",
        "        template_tile_y = gdal.Open(template_tile_dir).GetRasterBand(1).YSize\n",
        "        template_tile_x = gdal.Open(template_tile_dir).GetRasterBand(1).XSize\n",
        "        # Load feature tile stack\n",
        "        stack_filename = f\"feature_stack_{scenario}_{stack}.npy\"\n",
        "        feature_stack = np.load(join(scenario_feature_stack_dir, stack_filename))\n",
        "        # Add covariates (sensitivity and BEAM)\n",
        "        if add_covariates: feature_stack = np.hstack((feature_stack,\n",
        "                           np.full((feature_stack.shape[0], 1), beam_value, dtype=int),\n",
        "                           np.full((feature_stack.shape[0], 1), sensitivity_value, dtype=float)\n",
        "                           ))\n",
        "        # Define prediction array and reshape\n",
        "        prediction = XGBPredictor.predict(feature_stack)\n",
        "        feature_stack = None # Flush feature stack\n",
        "        prediction_tile = prediction.reshape((template_tile_y, template_tile_x))\n",
        "        prediction = None # Flush prediction\n",
        "        # Export prediction array as .tif\n",
        "        export_array_as_tif(prediction_tile, join(tile_cache_scenario_dir, scenario_tile_filename), template = template_tile_dir, compress = False)\n",
        "        prediction_tile = None # Flush prediction tile\n",
        "        # Update progress\n",
        "      tile_progress_index += 1\n",
        "      tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_stacks}\"\n",
        "    # Prepare empty array for merging tiles\n",
        "    prediction_array = np.empty((0,template_tile_x))\n",
        "    # Read each tile .tif as an array, stack, then export as a .tif\n",
        "    for subdir in os.listdir(tile_cache_scenario_dir):\n",
        "      if subdir.endswith('.tif'):\n",
        "        tile_dir = join(tile_cache_scenario_dir, subdir)\n",
        "        prediction_array = np.vstack((prediction_array, gdal.Open(tile_dir).ReadAsArray()))\n",
        "    # Define scenario template\n",
        "    scenario_template = join(features_dir, os.listdir(features_dir)[0])\n",
        "    scenario_prediction_unmasked_dir = join(scenario_predictions_unmasked_dir, f\"{scenario_prediction_unmasked_filename}\")\n",
        "    export_array_as_tif(prediction_array, scenario_prediction_unmasked_dir, template = scenario_template, compress = True)\n",
        "    # Delete scenario tile cache directory\n",
        "    shutil.rmtree(tile_cache_scenario_dir)\n",
        "  # Reset tile progress\n",
        "  tile_progress_index = 0\n",
        "  # Update scenario progress\n",
        "  scenario_progress_index += 1\n",
        "  scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\"\n",
        "print(\"\\nScenario predictions complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diwt1peRdXpj"
      },
      "source": [
        "# Mask scenario predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQsX1NCiy0aW"
      },
      "outputs": [],
      "source": [
        "# Use polygons for masking, only areas inside the polygons will be included\n",
        "\n",
        "# Exclude existing polygons from search\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "\n",
        "print(\"mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"  '{polygon[:-5]}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daKAe6nkJ2bS"
      },
      "outputs": [],
      "source": [
        "mask_polygons = [\n",
        "  # 'project_area',\n",
        "  'gedi_area',\n",
        "  # 'peninsular_malaysia',\n",
        "  # 'pa_taman_krau',\n",
        "  # 'pa_ais',\n",
        "]\n",
        "\n",
        "# Create an inverse project area path for masking\n",
        "template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "for polygon in mask_polygons:\n",
        "  inverse_polygon_path = join(polygons_dir, f\"{polygon}_inverse.gpkg\")\n",
        "  if not exists(inverse_polygon_path):\n",
        "    polygon_path = join(polygons_dir, f\"{polygon}.gpkg\")\n",
        "    template_polygon = gpd.read_file(template_polygon_path)\n",
        "    polygon_read = gpd.read_file(polygon_path)\n",
        "    polygon_crs = polygon_read.crs.to_epsg()\n",
        "    inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "    inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "    inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "    print(f\"An inverse masking polygon for {polygon} has been created in {polygons_dir}.\")\n",
        "  else: print(f\"An inverse masking polygon for {polygon} already exists.\")\n",
        "\n",
        "unmasked_predictions = []\n",
        "for scenario_prediction in os.listdir(scenario_predictions_unmasked_dir):\n",
        "  unmasked_predictions.append(scenario_prediction)\n",
        "\n",
        "# Determine last feature year for masking future scenarios\n",
        "final_feature_years = []\n",
        "for final_feature in os.listdir(feature_final_dir):\n",
        "  if final_feature.endswith('.tif') and final_feature[-9] == '_':\n",
        "    try: final_feature_years.append(int(final_feature[-8:-4]))\n",
        "    except: continue\n",
        "last_feature_year = max(final_feature_years)\n",
        "\n",
        "# Binary progress\n",
        "masking_progress_index = 0\n",
        "masking_progress_label = widgets.Label(f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\")\n",
        "display(masking_progress_label)\n",
        "\n",
        "# Mask scenarios with the relevatant mask\n",
        "for scenario_prediction in unmasked_predictions: # Loop through each unmasked scenario\n",
        "  scenario_masked_filename = f\"{scenario_prediction[:-13]}.tif\"\n",
        "  scenario_masked_dir = join(scenario_predictions_dir, scenario_masked_filename)\n",
        "  if not exists(scenario_masked_dir):\n",
        "    mask_exists = False\n",
        "    for mask in os.listdir(masks_dir):\n",
        "      # Match all oldgrowth scenarios\n",
        "      if 'all_oldgrowth' in mask or 'all_oldgrowth' in scenario_prediction:\n",
        "        if 'all_oldgrowth' in mask and 'all_oldgrowth' in scenario_prediction:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "      else: # Match all other historic scenarios\n",
        "        scenario_year = int(scenario_prediction[:4])\n",
        "        mask_year = int(mask[12:16])\n",
        "        if scenario_year == mask_year:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "        else: # Match future scenarios with most recent forest mask\n",
        "          if scenario_year > last_feature_year and last_feature_year == mask_year:\n",
        "            selected_mask_filename = mask\n",
        "            selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "            mask_exists = True\n",
        "    if mask_exists == False: print(f\"A suitable mask for {scenario_prediction} does not exist.\\n\")\n",
        "    else: # Mask the scenario prediction\n",
        "      print(f\"Masking {scenario_prediction} with {selected_mask_filename}...\")\n",
        "      mask_array = gdal.Open(selected_mask_dir).ReadAsArray()\n",
        "      scenario_prediction_unmasked_dir = join(scenario_predictions_unmasked_dir, scenario_prediction)\n",
        "      scenario_prediction_array = gdal.Open(scenario_prediction_unmasked_dir).ReadAsArray()\n",
        "      # Mask where the mask array is not 1\n",
        "      scenario_masked_array = np.where(mask_array != 1, nodatavalue, scenario_prediction_array)\n",
        "      export_array_as_tif(scenario_masked_array, scenario_masked_dir, compress = True)\n",
        "      if len(mask_polygons) > 0:\n",
        "        for polygon_mask in mask_polygons:\n",
        "          inverse_gedi_area_path = join(polygons_dir, f\"{polygon_mask}_inverse.gpkg\")\n",
        "          print(f\"Masking {scenario_prediction} with {polygon_mask}...\")\n",
        "          burn_polygon_to_raster(scenario_masked_dir, inverse_gedi_area_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        # Recompress the prediction after burning the polygon masks\n",
        "        scenario_masked_array_2 = gdal.Open(scenario_masked_dir).ReadAsArray()\n",
        "        export_array_as_tif(scenario_masked_array_2, scenario_masked_dir, compress = True)\n",
        "      print(f\"{scenario_masked_filename} exported.\")\n",
        "  # Update masking progress\n",
        "  masking_progress_index += 1\n",
        "  masking_progress_label.value = f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECHaTfrs0n7i"
      },
      "source": [
        "# Scenario disturbance / change"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool to identify required scenarios for forest disturbance/change mapping\n",
        "use_tool = True\n",
        "\n",
        "def select_forest_scenarios():\n",
        "    # Initialize variables\n",
        "    calculation_note = None\n",
        "    is_before_calculation = False\n",
        "\n",
        "    # Print header\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"\\nFOREST DISTURBANCE / CHANGE SELECTOR\\n\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Step 1: Collect disturbance type\n",
        "    disturbance_prompt = (\n",
        "        \"Select disturbance type:\\n\"\n",
        "        \"1. Degradation\\n\"\n",
        "        \"2. Deforestation\\n\"\n",
        "        \"3. Degradation and deforestation (total disturbance)\\n\"\n",
        "        \"4. Change\\n\\n\"\n",
        "        \"Enter your choice (1-4): \"\n",
        "    )\n",
        "    disturbance_type = input(disturbance_prompt)\n",
        "\n",
        "    # Handle Change type (separate workflow)\n",
        "    if disturbance_type == \"4\":\n",
        "        print(\"\\n\")\n",
        "        year_of_interest = input(\"Enter year of interest: \")\n",
        "        print(\"\\n\")\n",
        "        baseline_year = input(\"Enter baseline year (must be before year of interest): \")\n",
        "\n",
        "        # Validate year order\n",
        "        if int(baseline_year) >= int(year_of_interest):\n",
        "            print(\"\\nError: Baseline year must be before year of interest\")\n",
        "            return None\n",
        "\n",
        "        selected_difference = f\"{year_of_interest}_change_{baseline_year}\"\n",
        "        scenario_pair = (year_of_interest, baseline_year)\n",
        "        other_requirements = []\n",
        "\n",
        "    else:\n",
        "        # Step 2: Year of interest\n",
        "        print(\"\\n\")\n",
        "        year_of_interest = input(\"Enter year of interest: \")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # Step 3: Collect baseline type\n",
        "        baseline_prompt = (\n",
        "            \"Select baseline type:\\n\"\n",
        "            \"1. Total (since a hypothetical old-growth state)\\n\"\n",
        "            \"2. Since a baseline year\\n\"\n",
        "            \"3. Before a baseline year\\n\\n\"\n",
        "            \"Enter your choice (1-3): \"\n",
        "        )\n",
        "        baseline_type = input(baseline_prompt)\n",
        "\n",
        "        # Initialize result variables\n",
        "        selected_difference = None\n",
        "        scenario_pair = None\n",
        "        other_requirements = []\n",
        "        is_before_calculation = (baseline_type == \"3\")\n",
        "\n",
        "        # Process based on baseline type\n",
        "        if baseline_type == \"1\":  # Total (oldgrowth baseline)\n",
        "            if disturbance_type == \"1\":  # Degradation\n",
        "                selected_difference = f\"{year_of_interest}_degradation_total\"\n",
        "                scenario_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth\")\n",
        "\n",
        "            elif disturbance_type == \"2\":  # Deforestation\n",
        "                selected_difference = f\"{year_of_interest}_deforestation_total\"\n",
        "                scenario_pair = (f\"{year_of_interest}_oldgrowth\", \"all_oldgrowth\")\n",
        "\n",
        "            elif disturbance_type == \"3\":  # Degradation and deforestation\n",
        "                # Define component requirements\n",
        "                deg_total_diff = f\"{year_of_interest}_degradation_total\"\n",
        "                deg_total_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth\")\n",
        "                def_total_diff = f\"{year_of_interest}_deforestation_total\"\n",
        "                def_total_pair = (f\"{year_of_interest}_oldgrowth\", \"all_oldgrowth\")\n",
        "\n",
        "                other_requirements.append((deg_total_diff, deg_total_pair))\n",
        "                other_requirements.append((def_total_diff, def_total_pair))\n",
        "\n",
        "                calculation_note = f\"Equivalent to {year_of_interest} - all_oldgrowth\"\n",
        "                selected_difference = f\"{year_of_interest}_degradation_deforestation_total\"\n",
        "                scenario_pair = (year_of_interest, \"all_oldgrowth\")\n",
        "\n",
        "        elif baseline_type == \"2\":  # Since a baseline year\n",
        "            # Get and validate baseline year\n",
        "            print(\"\\n\")\n",
        "            baseline_year = input(\"Enter baseline year (must be before year of interest): \")\n",
        "            if int(baseline_year) >= int(year_of_interest):\n",
        "                print(\"\\nError: Baseline year must be before year of interest\")\n",
        "                return None\n",
        "\n",
        "            baseline_year_plus1 = str(int(baseline_year) + 1)\n",
        "\n",
        "            if disturbance_type == \"1\":  # Degradation\n",
        "                selected_difference = f\"{year_of_interest}_degradation_since_{baseline_year_plus1}\"\n",
        "                scenario_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{baseline_year_plus1}\")\n",
        "\n",
        "            elif disturbance_type == \"2\":  # Deforestation\n",
        "                selected_difference = f\"{year_of_interest}_deforestation_since_{baseline_year_plus1}\"\n",
        "                scenario_pair = (f\"{year_of_interest}_alternate_degradation_{baseline_year}\", baseline_year)\n",
        "\n",
        "            elif disturbance_type == \"3\":  # Degradation and deforestation since\n",
        "                # Define component requirements\n",
        "                degradation_since_diff = f\"{year_of_interest}_degradation_since_{baseline_year_plus1}\"\n",
        "                degradation_since_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{baseline_year_plus1}\")\n",
        "\n",
        "                deforestation_since_diff = f\"{year_of_interest}_deforestation_since_{baseline_year_plus1}\"\n",
        "                deforestation_since_pair = (f\"{year_of_interest}_alternate_degradation_{baseline_year}\", baseline_year)\n",
        "\n",
        "                other_requirements.append((degradation_since_diff, degradation_since_pair))\n",
        "                other_requirements.append((deforestation_since_diff, deforestation_since_pair))\n",
        "\n",
        "                selected_difference = f\"{year_of_interest}_degradation_deforestation_since_{baseline_year_plus1}\"\n",
        "                scenario_pair = (year_of_interest, baseline_year)\n",
        "\n",
        "        elif baseline_type == \"3\":  # Before a baseline year\n",
        "            # Get and validate baseline year\n",
        "            print(\"\\n\")\n",
        "            baseline_year = input(\"Enter baseline year (must be before year of interest): \")\n",
        "            if int(baseline_year) >= int(year_of_interest):\n",
        "                print(\"\\nError: Baseline year must be before year of interest\")\n",
        "                return None\n",
        "\n",
        "            baseline_year_plus1 = str(int(baseline_year) + 1)\n",
        "\n",
        "            if disturbance_type == \"1\":  # Degradation\n",
        "                selected_difference = f\"{year_of_interest}_degradation_before_{baseline_year_plus1}\"\n",
        "\n",
        "                # Define required difference components\n",
        "                since_difference = f\"{year_of_interest}_degradation_since_{baseline_year_plus1}\"\n",
        "                since_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{baseline_year_plus1}\")\n",
        "\n",
        "                total_difference = f\"{year_of_interest}_degradation_total\"\n",
        "                total_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth\")\n",
        "\n",
        "                other_requirements.append((since_difference, since_pair))\n",
        "                other_requirements.append((total_difference, total_pair))\n",
        "\n",
        "            elif disturbance_type == \"2\":  # Deforestation\n",
        "                selected_difference = f\"{year_of_interest}_deforestation_before_{baseline_year_plus1}\"\n",
        "\n",
        "                # Define required difference components\n",
        "                since_difference = f\"{year_of_interest}_deforestation_since_{baseline_year_plus1}\"\n",
        "                since_pair = (f\"{year_of_interest}_alternate_degradation_{baseline_year}\", baseline_year)\n",
        "\n",
        "                total_difference = f\"{year_of_interest}_deforestation_total\"\n",
        "                total_pair = (f\"{year_of_interest}_oldgrowth\", \"all_oldgrowth\")\n",
        "\n",
        "                other_requirements.append((since_difference, since_pair))\n",
        "                other_requirements.append((total_difference, total_pair))\n",
        "\n",
        "            elif disturbance_type == \"3\":  # Degradation and deforestation before\n",
        "                selected_difference = f\"{year_of_interest}_degradation_deforestation_before_{baseline_year_plus1}\"\n",
        "\n",
        "                # Define requirements for degradation_before\n",
        "                degradation_since_diff = f\"{year_of_interest}_degradation_since_{baseline_year_plus1}\"\n",
        "                degradation_since_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{baseline_year_plus1}\")\n",
        "\n",
        "                degradation_total_diff = f\"{year_of_interest}_degradation_total\"\n",
        "                degradation_total_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth\")\n",
        "\n",
        "                # Define requirements for deforestation_before\n",
        "                deforestation_since_diff = f\"{year_of_interest}_deforestation_since_{baseline_year_plus1}\"\n",
        "                deforestation_since_pair = (f\"{year_of_interest}_alternate_degradation_{baseline_year}\", baseline_year)\n",
        "\n",
        "                deforestation_total_diff = f\"{year_of_interest}_deforestation_total\"\n",
        "                deforestation_total_pair = (f\"{year_of_interest}_oldgrowth\", \"all_oldgrowth\")\n",
        "\n",
        "                # Add all requirements\n",
        "                other_requirements.append((degradation_since_diff, degradation_since_pair))\n",
        "                other_requirements.append((degradation_total_diff, degradation_total_pair))\n",
        "                other_requirements.append((deforestation_since_diff, deforestation_since_pair))\n",
        "                other_requirements.append((deforestation_total_diff, deforestation_total_pair))\n",
        "\n",
        "        else:\n",
        "            print(\"\\nError: Invalid baseline type selection.\")\n",
        "            return None\n",
        "\n",
        "    # Build result display\n",
        "    result_text = []\n",
        "    result_text.append(\"\\n\" + \"=\"*50)\n",
        "    result_text.append(f\"\\nSELECTED DIFFERENCE: {selected_difference}\")\n",
        "\n",
        "    # Add required differences and their scenario pairs\n",
        "    if other_requirements:\n",
        "        result_text.append(f\"\\nRequired difference and scenario pairs:\")\n",
        "        for diff, pair in other_requirements:\n",
        "            result_text.append(f\"'{diff}' {pair}\")\n",
        "\n",
        "        if calculation_note:\n",
        "            result_text.append(f\"\\n{calculation_note}\")\n",
        "    # Only include scenario pair when no other requirements\n",
        "    elif scenario_pair:\n",
        "        result_text.append(f\"\\nScenario pair required: {scenario_pair}\")\n",
        "\n",
        "    result_text.append(\"\\n\" + \"=\"*50)\n",
        "\n",
        "    # Print results with fewer new lines\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"\\n\".join(result_text))\n",
        "\n",
        "    # Return appropriate values\n",
        "    if is_before_calculation:\n",
        "        return selected_difference, other_requirements\n",
        "    else:\n",
        "        return selected_difference, scenario_pair, other_requirements\n",
        "\n",
        "# Run the function\n",
        "if use_tool:\n",
        "  if __name__ == \"__main__\":\n",
        "      select_forest_scenarios()"
      ],
      "metadata": {
        "id": "qquUGaydwWZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build dictionaries of disturbance / change options based on available files\n",
        "\n",
        "# Extract all available scenarios from directory\n",
        "scenarios = set()\n",
        "for file in os.listdir(scenario_predictions_dir):\n",
        "    scenarios.add(file.split(\"__\")[0])\n",
        "\n",
        "# Extract and categorize years from scenarios\n",
        "years = set()\n",
        "plain_years = set()  # Years as standalone scenarios (e.g. \"2014\")\n",
        "oldgrowth_years = set()  # Years with oldgrowth variants\n",
        "\n",
        "for s in scenarios:\n",
        "    if s.isdigit():\n",
        "        years.add(int(s))\n",
        "        plain_years.add(int(s))\n",
        "    elif \"_oldgrowth\" in s:\n",
        "        year = s.split(\"_oldgrowth\")[0]\n",
        "        if year.isdigit():\n",
        "            years.add(int(year))\n",
        "            oldgrowth_years.add(int(year))\n",
        "    elif any(pattern in s for pattern in [\"_no_degradation_since_\", \"_alternate_degradation_\"]):\n",
        "        year = s.split(\"_\")[0]\n",
        "        if year.isdigit():\n",
        "            years.add(int(year))\n",
        "\n",
        "years_sorted = sorted(list(years))\n",
        "\n",
        "# Track scenario availability and dependencies\n",
        "deforest_since_scenarios = set()\n",
        "direct_degradation_pairs = set()\n",
        "direct_deforestation_pairs = set()\n",
        "\n",
        "# Output dictionaries\n",
        "scenario_difference_dictionary = {}\n",
        "\n",
        "print(\"# Differences in scenario_difference_dictionary and in before_baseline_dictionary are \")\n",
        "print(\"# calculated by subtracting the second scenario / difference from the first. The \")\n",
        "print(\"# differences in degradation_deforestation_dictionary are summed.\")\n",
        "print(\"\")\n",
        "print(\"scenario_difference_dictionary = {\")\n",
        "print(\"\")\n",
        "\n",
        "# 1. Process oldgrowth baseline sections for direct differences\n",
        "oldgrowth_entries = []\n",
        "for year in years_sorted:\n",
        "    y_str = str(year)\n",
        "\n",
        "    # Only create section if all required metrics are possible\n",
        "    if (year in plain_years and\n",
        "        year in oldgrowth_years and\n",
        "        \"all_oldgrowth\" in scenarios):\n",
        "\n",
        "        if not oldgrowth_entries:\n",
        "            oldgrowth_entries.append(\"# Disturbance using oldgrowth as a baseline\")\n",
        "\n",
        "        oldgrowth_entries.append(f\"  ('{y_str}', '{y_str}_oldgrowth'):\")\n",
        "        oldgrowth_entries.append(f\"    '{y_str}_degradation_total',\")\n",
        "        scenario_difference_dictionary[(y_str, f\"{y_str}_oldgrowth\")] = f\"{y_str}_degradation_total\"\n",
        "\n",
        "        oldgrowth_entries.append(f\"  ('{y_str}_oldgrowth', 'all_oldgrowth'):\")\n",
        "        oldgrowth_entries.append(f\"    '{y_str}_deforestation_total',\")\n",
        "        scenario_difference_dictionary[(f\"{y_str}_oldgrowth\", \"all_oldgrowth\")] = f\"{y_str}_deforestation_total\"\n",
        "        oldgrowth_entries.append(\"\")\n",
        "\n",
        "# Print oldgrowth entries if any exist\n",
        "if oldgrowth_entries:\n",
        "    print(\"\\n\".join(oldgrowth_entries))\n",
        "\n",
        "# 2. Track deforestation scenarios for dependencies\n",
        "for year_a in years_sorted:\n",
        "    for year_b in years_sorted:\n",
        "        if year_a <= year_b:\n",
        "            continue\n",
        "\n",
        "        a_str, b_str = str(year_a), str(year_b)\n",
        "        b_plus1 = str(year_b + 1)\n",
        "\n",
        "        # Check for deforestation_since scenarios\n",
        "        if (year_b in plain_years and f\"{a_str}_alternate_degradation_{b_str}\" in scenarios):\n",
        "            deforest_since_scenarios.add(f\"{a_str}_deforestation_since_{b_plus1}\")\n",
        "\n",
        "# 3. Process year-to-year comparisons for direct differences by baseline year\n",
        "baseline_entries = {}\n",
        "\n",
        "for year_a in years_sorted:\n",
        "    for year_b in years_sorted:\n",
        "        if year_a <= year_b:\n",
        "            continue\n",
        "\n",
        "        a_str, b_str = str(year_a), str(year_b)\n",
        "        b_plus1 = str(year_b + 1)\n",
        "\n",
        "        deforest_since_key = f\"{a_str}_deforestation_since_{b_plus1}\"\n",
        "        has_deforestation_since = deforest_since_key in deforest_since_scenarios\n",
        "\n",
        "        entries = []\n",
        "\n",
        "        # 1. Degradation since [year_b+1]\n",
        "        if (year_a in plain_years and f\"{a_str}_no_degradation_since_{b_plus1}\" in scenarios):\n",
        "            entries.append((\n",
        "                f\"  ('{a_str}', '{a_str}_no_degradation_since_{b_plus1}'):\",\n",
        "                f\"    '{a_str}_degradation_since_{b_plus1}',\"\n",
        "            ))\n",
        "            scenario_difference_dictionary[(a_str, f\"{a_str}_no_degradation_since_{b_plus1}\")] = f\"{a_str}_degradation_since_{b_plus1}\"\n",
        "            direct_degradation_pairs.add((a_str, b_plus1))\n",
        "\n",
        "        # 2. Deforestation since [year_b+1]\n",
        "        if has_deforestation_since:\n",
        "            entries.append((\n",
        "                f\"  ('{a_str}_alternate_degradation_{b_str}', '{b_str}'):\",\n",
        "                f\"    '{a_str}_deforestation_since_{b_plus1}',\"\n",
        "            ))\n",
        "            scenario_difference_dictionary[(f\"{a_str}_alternate_degradation_{b_str}\", b_str)] = f\"{a_str}_deforestation_since_{b_plus1}\"\n",
        "            direct_deforestation_pairs.add((a_str, b_plus1))\n",
        "\n",
        "        # Only add section if at least one comparison exists\n",
        "        if entries:\n",
        "            if b_str not in baseline_entries:\n",
        "                baseline_entries[b_str] = []\n",
        "\n",
        "            # Add section header and entries\n",
        "            section_text = [f\"# Disturbance by {a_str}, using {b_str} as a baseline\"]\n",
        "            for line1, line2 in sorted(entries, key=lambda x: x[1]):\n",
        "                section_text.append(line1)\n",
        "                section_text.append(line2)\n",
        "            section_text.append(\"\")\n",
        "\n",
        "            baseline_entries[b_str].append((a_str, section_text))\n",
        "\n",
        "# Print baseline entries if any exist\n",
        "if baseline_entries:\n",
        "    for b_str in sorted(baseline_entries.keys()):\n",
        "        for a_str, section in sorted(baseline_entries[b_str], key=lambda x: x[0]):\n",
        "            print(\"\\n\".join(section))\n",
        "\n",
        "# 4. Process year-to-year change sections\n",
        "years_available = sorted([y for y in years_sorted if y in plain_years])\n",
        "\n",
        "if len(years_available) >= 2:\n",
        "    # A. Single-year consecutive changes\n",
        "    print(\"# Change between single years\")\n",
        "    for i in range(1, len(years_available)):\n",
        "        current = str(years_available[i])\n",
        "        previous = str(years_available[i-1])\n",
        "\n",
        "        print(f\"  ('{current}', '{previous}'):\")\n",
        "        print(f\"    '{current}_change_{previous}',\")\n",
        "        scenario_difference_dictionary[(current, previous)] = f\"{current}_change_{previous}\"\n",
        "    print(\"\")\n",
        "\n",
        "    # B. Multi-year comparison (earliest to latest only)\n",
        "    if len(years_available) > 2:\n",
        "        earliest = str(years_available[0])\n",
        "        latest = str(years_available[-1])\n",
        "\n",
        "        print(\"# Change between multiple years\")\n",
        "        print(\"# Add any other desired year combinations manually using the pattern below\")\n",
        "        print(f\"  ('{latest}', '{earliest}'):\")\n",
        "        print(f\"    '{latest}_change_{earliest}',\")\n",
        "        scenario_difference_dictionary[(latest, earliest)] = f\"{latest}_change_{earliest}\"\n",
        "        print(\"\")\n",
        "\n",
        "print(\"}\")\n",
        "\n",
        "# Dictionary for calculated 'before' differences\n",
        "print(\"\")\n",
        "print(\"before_baseline_dictionary = {\")\n",
        "\n",
        "# Collection for degradation before metrics\n",
        "degradation_before_entries = []\n",
        "for year_a, year_b_plus1 in sorted(direct_degradation_pairs):\n",
        "    # Check if required components exist\n",
        "    if (year_a, f\"{year_a}_oldgrowth\") in scenario_difference_dictionary:\n",
        "        deg_since = f\"{year_a}_degradation_since_{year_b_plus1}\"\n",
        "        deg_total = f\"{year_a}_degradation_total\"\n",
        "        deg_before = f\"{year_a}_degradation_before_{year_b_plus1}\"\n",
        "\n",
        "        degradation_before_entries.append(f\"  '{deg_before}': ('{deg_total}', '{deg_since}'),\")\n",
        "\n",
        "# Print degradation before metrics if any exist\n",
        "if degradation_before_entries:\n",
        "    print(\"\\n# Degradation before metrics (Total - Since)\")\n",
        "    for entry in degradation_before_entries:\n",
        "        print(entry)\n",
        "\n",
        "# Collection for deforestation before metrics\n",
        "deforestation_before_entries = []\n",
        "for year_a, year_b_plus1 in sorted(direct_deforestation_pairs):\n",
        "    # Check if required components exist\n",
        "    if (f\"{year_a}_oldgrowth\", \"all_oldgrowth\") in scenario_difference_dictionary:\n",
        "        def_since = f\"{year_a}_deforestation_since_{year_b_plus1}\"\n",
        "        def_total = f\"{year_a}_deforestation_total\"\n",
        "        def_before = f\"{year_a}_deforestation_before_{year_b_plus1}\"\n",
        "\n",
        "        deforestation_before_entries.append(f\"  '{def_before}': ('{def_total}', '{def_since}'),\")\n",
        "\n",
        "# Print deforestation before metrics if any exist\n",
        "if deforestation_before_entries:\n",
        "    if degradation_before_entries:\n",
        "        print(\"\")\n",
        "    print(\"# Deforestation before metrics (Total - Since)\")\n",
        "    for entry in deforestation_before_entries:\n",
        "        print(entry)\n",
        "\n",
        "print(\"}\")\n",
        "\n",
        "# Dictionary for combined degradation and deforestation calculations\n",
        "print(\"\")\n",
        "print(\"degradation_deforestation_dictionary = {\")\n",
        "\n",
        "# Collection for combined disturbance totals\n",
        "combined_total_entries = []\n",
        "for year in years_sorted:\n",
        "    y_str = str(year)\n",
        "\n",
        "    # Check if required components exist\n",
        "    if ((y_str, f\"{y_str}_oldgrowth\") in scenario_difference_dictionary and\n",
        "        (f\"{y_str}_oldgrowth\", \"all_oldgrowth\") in scenario_difference_dictionary):\n",
        "        deg_total = f\"{y_str}_degradation_total\"\n",
        "        def_total = f\"{y_str}_deforestation_total\"\n",
        "        combined = f\"{y_str}_degradation_deforestation_total\"\n",
        "\n",
        "        combined_total_entries.append(f\"  '{combined}': ('{deg_total}', '{def_total}'),\")\n",
        "\n",
        "# Print combined total entries if any exist\n",
        "if combined_total_entries:\n",
        "    print(\"\\n# Combined degradation and deforestation totals\")\n",
        "    for entry in combined_total_entries:\n",
        "        print(entry)\n",
        "\n",
        "# Collection for combined since metrics\n",
        "combined_pairs = direct_degradation_pairs.intersection(direct_deforestation_pairs)\n",
        "combined_since_entries = []\n",
        "\n",
        "for year_a, year_b_plus1 in sorted(combined_pairs):\n",
        "    # Verify components exist\n",
        "    deg_since = f\"{year_a}_degradation_since_{year_b_plus1}\"\n",
        "    def_since = f\"{year_a}_deforestation_since_{year_b_plus1}\"\n",
        "    combined = f\"{year_a}_degradation_deforestation_since_{year_b_plus1}\"\n",
        "\n",
        "    combined_since_entries.append(f\"  '{combined}': ('{deg_since}', '{def_since}'),\")\n",
        "\n",
        "# Print combined since entries if any exist\n",
        "if combined_since_entries:\n",
        "    if combined_total_entries:\n",
        "        print(\"\")\n",
        "    print(\"# Combined degradation and deforestation since\")\n",
        "    for entry in combined_since_entries:\n",
        "        print(entry)\n",
        "\n",
        "# Collection for combined before metrics\n",
        "combined_before_entries = []\n",
        "for year_a, year_b_plus1 in sorted(combined_pairs):\n",
        "    # Check if individual before metrics defined\n",
        "    deg_before = f\"{year_a}_degradation_before_{year_b_plus1}\"\n",
        "    def_before = f\"{year_a}_deforestation_before_{year_b_plus1}\"\n",
        "\n",
        "    # Only include if components would exist\n",
        "    if ((year_a, f\"{year_a}_oldgrowth\") in scenario_difference_dictionary and\n",
        "        (f\"{year_a}_oldgrowth\", \"all_oldgrowth\") in scenario_difference_dictionary):\n",
        "        combined = f\"{year_a}_degradation_deforestation_before_{year_b_plus1}\"\n",
        "\n",
        "        combined_before_entries.append(f\"  '{combined}': ('{deg_before}', '{def_before}'),\")\n",
        "\n",
        "# Print combined before entries if any exist\n",
        "if combined_before_entries:\n",
        "    if combined_total_entries or combined_since_entries:\n",
        "        print(\"\")\n",
        "    print(\"# Combined degradation and deforestation before\")\n",
        "    for entry in combined_before_entries:\n",
        "        print(entry)\n",
        "\n",
        "print(\"}\")"
      ],
      "metadata": {
        "id": "85lfEfse8qkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Differences in scenario_difference_dictionary and in before_baseline_dictionary are\n",
        "# calculated by subtracting the second scenario / difference from the first. The\n",
        "# differences in degradation_deforestation_dictionary are summed.\n",
        "\n",
        "scenario_difference_dictionary = {\n",
        "\n",
        "# Disturbance using oldgrowth as a baseline\n",
        "  ('2014', '2014_oldgrowth'):\n",
        "    '2014_degradation_total',\n",
        "  ('2014_oldgrowth', 'all_oldgrowth'):\n",
        "    '2014_deforestation_total',\n",
        "\n",
        "  ('2021', '2021_oldgrowth'):\n",
        "    '2021_degradation_total',\n",
        "  ('2021_oldgrowth', 'all_oldgrowth'):\n",
        "    '2021_deforestation_total',\n",
        "\n",
        "  ('2022', '2022_oldgrowth'):\n",
        "    '2022_degradation_total',\n",
        "  ('2022_oldgrowth', 'all_oldgrowth'):\n",
        "    '2022_deforestation_total',\n",
        "\n",
        "  ('2023', '2023_oldgrowth'):\n",
        "    '2023_degradation_total',\n",
        "  ('2023_oldgrowth', 'all_oldgrowth'):\n",
        "    '2023_deforestation_total',\n",
        "\n",
        "  ('2024', '2024_oldgrowth'):\n",
        "    '2024_degradation_total',\n",
        "  ('2024_oldgrowth', 'all_oldgrowth'):\n",
        "    '2024_deforestation_total',\n",
        "\n",
        "# Disturbance by 2014, using 1990 as a baseline\n",
        "  ('2014', '2014_no_degradation_since_1991'):\n",
        "    '2014_degradation_since_1991',\n",
        "\n",
        "# Disturbance by 2024, using 2014 as a baseline\n",
        "  ('2024_alternate_degradation_2014', '2014'):\n",
        "    '2024_deforestation_since_2015',\n",
        "  ('2024', '2024_no_degradation_since_2015'):\n",
        "    '2024_degradation_since_2015',\n",
        "\n",
        "# Disturbance by 2022, using 2021 as a baseline\n",
        "  ('2022_alternate_degradation_2021', '2021'):\n",
        "    '2022_deforestation_since_2022',\n",
        "  ('2022', '2022_no_degradation_since_2022'):\n",
        "    '2022_degradation_since_2022',\n",
        "\n",
        "# Disturbance by 2024, using 2021 as a baseline\n",
        "  ('2024_alternate_degradation_2021', '2021'):\n",
        "    '2024_deforestation_since_2022',\n",
        "  ('2024', '2024_no_degradation_since_2022'):\n",
        "    '2024_degradation_since_2022',\n",
        "\n",
        "# Disturbance by 2023, using 2022 as a baseline\n",
        "  ('2023_alternate_degradation_2022', '2022'):\n",
        "    '2023_deforestation_since_2023',\n",
        "  ('2023', '2023_no_degradation_since_2023'):\n",
        "    '2023_degradation_since_2023',\n",
        "\n",
        "# Disturbance by 2024, using 2022 as a baseline\n",
        "  ('2024_alternate_degradation_2022', '2022'):\n",
        "    '2024_deforestation_since_2023',\n",
        "  ('2024', '2024_no_degradation_since_2023'):\n",
        "    '2024_degradation_since_2023',\n",
        "\n",
        "# Disturbance by 2024, using 2023 as a baseline\n",
        "  ('2024_alternate_degradation_2023', '2023'):\n",
        "    '2024_deforestation_since_2024',\n",
        "  ('2024', '2024_no_degradation_since_2024'):\n",
        "    '2024_degradation_since_2024',\n",
        "\n",
        "# Change between single years\n",
        "  ('2015', '2014'):\n",
        "    '2015_change_2014',\n",
        "  ('2016', '2015'):\n",
        "    '2016_change_2015',\n",
        "  ('2017', '2016'):\n",
        "    '2017_change_2016',\n",
        "  ('2018', '2017'):\n",
        "    '2018_change_2017',\n",
        "  ('2019', '2018'):\n",
        "    '2019_change_2018',\n",
        "  ('2020', '2019'):\n",
        "    '2020_change_2019',\n",
        "  ('2021', '2020'):\n",
        "    '2021_change_2020',\n",
        "  ('2022', '2021'):\n",
        "    '2022_change_2021',\n",
        "  ('2023', '2022'):\n",
        "    '2023_change_2022',\n",
        "  ('2024', '2023'):\n",
        "    '2024_change_2023',\n",
        "\n",
        "# Change between multiple years\n",
        "# Add any other desired year combinations manually using the pattern below\n",
        "  ('2024', '2014'):\n",
        "    '2024_change_2014',\n",
        "\n",
        "}\n",
        "\n",
        "before_baseline_dictionary = {\n",
        "\n",
        "# Degradation before metrics (Total - Since)\n",
        "  '2014_degradation_before_1991': ('2014_degradation_total', '2014_degradation_since_1991'),\n",
        "  '2022_degradation_before_2022': ('2022_degradation_total', '2022_degradation_since_2022'),\n",
        "  '2023_degradation_before_2023': ('2023_degradation_total', '2023_degradation_since_2023'),\n",
        "  '2024_degradation_before_2015': ('2024_degradation_total', '2024_degradation_since_2015'),\n",
        "  '2024_degradation_before_2022': ('2024_degradation_total', '2024_degradation_since_2022'),\n",
        "  '2024_degradation_before_2023': ('2024_degradation_total', '2024_degradation_since_2023'),\n",
        "  '2024_degradation_before_2024': ('2024_degradation_total', '2024_degradation_since_2024'),\n",
        "\n",
        "# Deforestation before metrics (Total - Since)\n",
        "  '2022_deforestation_before_2022': ('2022_deforestation_total', '2022_deforestation_since_2022'),\n",
        "  '2023_deforestation_before_2023': ('2023_deforestation_total', '2023_deforestation_since_2023'),\n",
        "  '2024_deforestation_before_2015': ('2024_deforestation_total', '2024_deforestation_since_2015'),\n",
        "  '2024_deforestation_before_2022': ('2024_deforestation_total', '2024_deforestation_since_2022'),\n",
        "  '2024_deforestation_before_2023': ('2024_deforestation_total', '2024_deforestation_since_2023'),\n",
        "  '2024_deforestation_before_2024': ('2024_deforestation_total', '2024_deforestation_since_2024'),\n",
        "}\n",
        "\n",
        "degradation_deforestation_dictionary = {\n",
        "\n",
        "# Combined degradation and deforestation totals\n",
        "  '2014_degradation_deforestation_total': ('2014_degradation_total', '2014_deforestation_total'),\n",
        "  '2021_degradation_deforestation_total': ('2021_degradation_total', '2021_deforestation_total'),\n",
        "  '2022_degradation_deforestation_total': ('2022_degradation_total', '2022_deforestation_total'),\n",
        "  '2023_degradation_deforestation_total': ('2023_degradation_total', '2023_deforestation_total'),\n",
        "  '2024_degradation_deforestation_total': ('2024_degradation_total', '2024_deforestation_total'),\n",
        "\n",
        "# Combined degradation and deforestation since\n",
        "  '2022_degradation_deforestation_since_2022': ('2022_degradation_since_2022', '2022_deforestation_since_2022'),\n",
        "  '2023_degradation_deforestation_since_2023': ('2023_degradation_since_2023', '2023_deforestation_since_2023'),\n",
        "  '2024_degradation_deforestation_since_2015': ('2024_degradation_since_2015', '2024_deforestation_since_2015'),\n",
        "  '2024_degradation_deforestation_since_2022': ('2024_degradation_since_2022', '2024_deforestation_since_2022'),\n",
        "  '2024_degradation_deforestation_since_2023': ('2024_degradation_since_2023', '2024_deforestation_since_2023'),\n",
        "  '2024_degradation_deforestation_since_2024': ('2024_degradation_since_2024', '2024_deforestation_since_2024'),\n",
        "\n",
        "# Combined degradation and deforestation before\n",
        "  '2022_degradation_deforestation_before_2022': ('2022_degradation_before_2022', '2022_deforestation_before_2022'),\n",
        "  '2023_degradation_deforestation_before_2023': ('2023_degradation_before_2023', '2023_deforestation_before_2023'),\n",
        "  '2024_degradation_deforestation_before_2015': ('2024_degradation_before_2015', '2024_deforestation_before_2015'),\n",
        "  '2024_degradation_deforestation_before_2022': ('2024_degradation_before_2022', '2024_deforestation_before_2022'),\n",
        "  '2024_degradation_deforestation_before_2023': ('2024_degradation_before_2023', '2024_deforestation_before_2023'),\n",
        "  '2024_degradation_deforestation_before_2024': ('2024_degradation_before_2024', '2024_deforestation_before_2024'),\n",
        "}"
      ],
      "metadata": {
        "id": "gFNUYdP3Fibv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P-0bj40AxZ2"
      },
      "outputs": [],
      "source": [
        "# Functions for differences and sums\n",
        "def subtract_arrays(array1, array2):\n",
        "    diff_array = array1 - array2\n",
        "    return diff_array\n",
        "\n",
        "def sum_arrays(array1, array2):\n",
        "    sum_array = array1 + array2\n",
        "    return sum_array\n",
        "\n",
        "# Set up single progress indicator for all operations\n",
        "total_operations = len(scenario_difference_dictionary) + len(before_baseline_dictionary) + len(degradation_deforestation_dictionary)\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Difference calculation progress: {progress_index}/{total_operations}\")\n",
        "\n",
        "display(progress_label)\n",
        "\n",
        "# 1. Process direct scenario differences\n",
        "for (scenario1, scenario2), difference_name in scenario_difference_dictionary.items():\n",
        "    # Define filenames and directories of difference .tifs\n",
        "    diff_filename = f\"{difference_name}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "    diff_dir = join(scenario_dist_dir, diff_filename)\n",
        "\n",
        "    if not exists(diff_dir):\n",
        "        scenario1_filename = f\"{scenario1}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "        scenario2_filename = f\"{scenario2}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "\n",
        "        # Define difference directories, assert that both exist for both scenarios\n",
        "        scenario1_dir = join(scenario_predictions_dir, scenario1_filename)\n",
        "        assert exists(scenario1_dir), f\"{scenario1_dir} does not exist.\"\n",
        "        scenario2_dir = join(scenario_predictions_dir, scenario2_filename)\n",
        "        assert exists(scenario2_dir), f\"{scenario2_dir} does not exist.\"\n",
        "\n",
        "        # Convert scenario .tifs to temporary arrays\n",
        "        scenario1_array_temp = gdal.Open(scenario1_dir).ReadAsArray()\n",
        "        scenario2_array_temp = gdal.Open(scenario2_dir).ReadAsArray()\n",
        "\n",
        "        # Fill scenario nodata values with 0 if they are not nodatavalues in the other scenario\n",
        "        scenario1_array = np.where((scenario1_array_temp == nodatavalue) & (scenario2_array_temp != nodatavalue), 0, scenario1_array_temp)\n",
        "        scenario2_array = np.where((scenario2_array_temp == nodatavalue) & (scenario1_array != nodatavalue), 0, scenario2_array_temp)\n",
        "\n",
        "        # Create difference arrays where the value is not 'nodatavalue'\n",
        "        diff_array = np.where(scenario1_array==nodatavalue, nodatavalue, subtract_arrays(scenario1_array, scenario2_array))\n",
        "        export_array_as_tif(diff_array, diff_dir, template = scenario1_dir)\n",
        "\n",
        "    # Update progress\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Difference calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "# 2. Process before baseline differences (require subtracting one difference from another)\n",
        "for difference_name, (diff1_name, diff2_name) in before_baseline_dictionary.items():\n",
        "    # Define filenames and directories of difference .tifs\n",
        "    output_filename = f\"{difference_name}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "    output_dir = join(scenario_dist_dir, output_filename)\n",
        "\n",
        "    if not exists(output_dir):\n",
        "        diff1_filename = f\"{diff1_name}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "        diff2_filename = f\"{diff2_name}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "\n",
        "        # Define difference directories, assert that both exist\n",
        "        diff1_dir = join(scenario_dist_dir, diff1_filename)\n",
        "        assert exists(diff1_dir), f\"{diff1_dir} does not exist.\"\n",
        "        diff2_dir = join(scenario_dist_dir, diff2_filename)\n",
        "        assert exists(diff2_dir), f\"{diff2_dir} does not exist.\"\n",
        "\n",
        "        # Convert difference .tifs to temporary arrays\n",
        "        diff1_array_temp = gdal.Open(diff1_dir).ReadAsArray()\n",
        "        diff2_array_temp = gdal.Open(diff2_dir).ReadAsArray()\n",
        "\n",
        "        # Fill difference nodata values with 0 if they are not nodatavalues in the other difference\n",
        "        diff1_array = np.where((diff1_array_temp == nodatavalue) & (diff2_array_temp != nodatavalue), 0, diff1_array_temp)\n",
        "        diff2_array = np.where((diff2_array_temp == nodatavalue) & (diff1_array != nodatavalue), 0, diff2_array_temp)\n",
        "\n",
        "        # Create difference arrays where the value is not 'nodatavalue'\n",
        "        result_array = np.where(diff1_array==nodatavalue, nodatavalue, subtract_arrays(diff1_array, diff2_array))\n",
        "        export_array_as_tif(result_array, output_dir, template = diff1_dir)\n",
        "\n",
        "    # Update progress\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Difference calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "# 3. Process combined degradation and deforestation (require summing two differences)\n",
        "for difference_name, (diff1_name, diff2_name) in degradation_deforestation_dictionary.items():\n",
        "    # Define filenames and directories of difference .tifs\n",
        "    output_filename = f\"{difference_name}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "    output_dir = join(scenario_dist_dir, output_filename)\n",
        "\n",
        "    if not exists(output_dir):\n",
        "        diff1_filename = f\"{diff1_name}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "        diff2_filename = f\"{diff2_name}__{selected_scenario_area}_{selected_model}.tif\"\n",
        "\n",
        "        # Define difference directories, assert that both exist\n",
        "        diff1_dir = join(scenario_dist_dir, diff1_filename)\n",
        "        assert exists(diff1_dir), f\"{diff1_dir} does not exist.\"\n",
        "        diff2_dir = join(scenario_dist_dir, diff2_filename)\n",
        "        assert exists(diff2_dir), f\"{diff2_dir} does not exist.\"\n",
        "\n",
        "        # Convert difference .tifs to temporary arrays\n",
        "        diff1_array_temp = gdal.Open(diff1_dir).ReadAsArray()\n",
        "        diff2_array_temp = gdal.Open(diff2_dir).ReadAsArray()\n",
        "\n",
        "        # Fill difference nodata values with 0 if they are not nodatavalues in the other difference\n",
        "        diff1_array = np.where((diff1_array_temp == nodatavalue) & (diff2_array_temp != nodatavalue), 0, diff1_array_temp)\n",
        "        diff2_array = np.where((diff2_array_temp == nodatavalue) & (diff1_array != nodatavalue), 0, diff2_array_temp)\n",
        "\n",
        "        # Create sum arrays where the value is not 'nodatavalue' in both\n",
        "        result_array = np.where(diff1_array==nodatavalue, nodatavalue, sum_arrays(diff1_array, diff2_array))\n",
        "        export_array_as_tif(result_array, output_dir, template = diff1_dir)\n",
        "\n",
        "    # Update progress\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Difference calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "print(\"All differences calculated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81OCJi98NDwj"
      },
      "source": [
        "# Intactness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsend_eeNIll"
      },
      "outputs": [],
      "source": [
        "# Select which baseline and disturbance raster to use for calculating intactness\n",
        "# percentage and relative intactness. Ideally this is the scenario with the least disturbance\n",
        "# and the difference between that and the current reality.\n",
        "for baseline in os.listdir(scenario_predictions_dir):\n",
        "  print(f\"selected_baseline = '{baseline}'\")\n",
        "for dist in os.listdir(scenario_dist_dir):\n",
        "  print(f\"selected_dist = '{dist}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87Q6aMSdNIye"
      },
      "outputs": [],
      "source": [
        "selected_baseline = 'all_oldgrowth__asartr_agbd_historic_250429_223033.tif'\n",
        "selected_dist = '2024_degradation_deforestation_total__asartr_agbd_historic_250429_223033.tif'\n",
        "forest_mask_year = '2024'\n",
        "\n",
        "percentage_filename = f\"percentage_change__{selected_baseline.split('__')[0]}__{selected_dist.split('__')[0]}__{selected_dist.split('__')[1]}\"\n",
        "percentage_path = join(intactness_dir, percentage_filename)\n",
        "\n",
        "if not exists(percentage_path):\n",
        "  # Define filenames and directories\n",
        "  selected_baseline_path = join(scenario_predictions_dir, selected_baseline)\n",
        "  selected_dist_path = join(scenario_dist_dir, selected_dist)\n",
        "  selected_mask_path = join(masks_dir, f\"mask_forest_{forest_mask_year}.tif\")\n",
        "\n",
        "  # Convert to arrays\n",
        "  selected_baseline_array = gdal.Open(selected_baseline_path).ReadAsArray()\n",
        "  selected_dist_array = gdal.Open(selected_dist_path).ReadAsArray()\n",
        "  selected_mask_array = gdal.Open(selected_mask_path).ReadAsArray()\n",
        "\n",
        "  # Create percentage array where the value is not 'nodatavalue' in any of the inputs\n",
        "  percentage_array = np.where((selected_mask_array==nodatavalue) | (selected_baseline_array==nodatavalue) | (selected_dist_array==nodatavalue), nodatavalue,\n",
        "                              selected_dist_array/selected_baseline_array*100)\n",
        "  export_array_as_tif(percentage_array, percentage_path, template = selected_baseline_path)\n",
        "  print(f\"{percentage_filename} has been exported.\")\n",
        "\n",
        "else: print(f\"{percentage_filename} already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwXRZTew1r39"
      },
      "outputs": [],
      "source": [
        "# Use additional polygons for masking relative intactness quantiles\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"mask_polygon = '{polygon}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru_UDUkAPw-8"
      },
      "outputs": [],
      "source": [
        "mask_polygon = 'asartr_phase_2.gpkg'\n",
        "# mask_polygon = None\n",
        "\n",
        "if mask_polygon is not None:\n",
        "  # Create an inverse project area path for masking\n",
        "  template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "  inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "  if not exists(inverse_polygon_path):\n",
        "    polygon_path = join(polygons_dir, mask_polygon)\n",
        "    template_polygon = gpd.read_file(template_polygon_path)\n",
        "    polygon_read = gpd.read_file(polygon_path)\n",
        "    polygon_crs = polygon_read.crs.to_epsg()\n",
        "    inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "    inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "    inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "    print(f\"An inverse masking polygon for {polygon} has been created in {polygons_dir}.\")\n",
        "  else: print(f\"An inverse masking polygon for {polygon} already exists.\")\n",
        "\n",
        "  # Copy the percentage raster for potential masking\n",
        "  percentage_masked_filename = f\"{percentage_filename[:-4]}_masked_{mask_polygon[:-5]}.tif\"\n",
        "  percentage_masked_path = join(intactness_dir, percentage_masked_filename)\n",
        "  if not exists(percentage_masked_path):\n",
        "    print(f\"Copying {percentage_filename} for masking...\")\n",
        "    copyfile(percentage_path, percentage_masked_path)\n",
        "    print(f\"Masking {percentage_filename} with {mask_polygon}...\")\n",
        "    burn_polygon_to_raster(percentage_masked_path, inverse_polygon_path, fixed_value=nodatavalue, all_touched=False)\n",
        "    # Recompress the prediction after burning the polygon masks\n",
        "    percentage_masked_array = gdal.Open(percentage_masked_path).ReadAsArray()\n",
        "    export_array_as_tif(percentage_masked_array, percentage_masked_path, compress = True)\n",
        "    print(f\"{percentage_filename} masked.\")\n",
        "  else: print(f\"{percentage_masked_path} already exists.\")\n",
        "\n",
        "else: print(\"No additional mask will be used to calculate relative intactness.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCQ_GUBiNBx3"
      },
      "outputs": [],
      "source": [
        "# Define number of quantiles for intactness rating (e.g. 10 for 1 - 10)\n",
        "num_quantiles = 10\n",
        "\n",
        "# Define paths and arrays\n",
        "if mask_polygon is None: relative_intactness_name = f'intactness__{num_quantiles}_quantiles'\n",
        "else: relative_intactness_name = f'intactness__{mask_polygon[:-5]}_{num_quantiles}_quantiles'\n",
        "relative_intactness_path = join(intactness_dir, f'{relative_intactness_name}.tif')\n",
        "if mask_polygon is None: percentage_array = gdal.Open(percentage_path).ReadAsArray()\n",
        "else: percentage_array = gdal.Open(percentage_masked_path).ReadAsArray()\n",
        "relative_intactness_array = np.empty_like(percentage_array, dtype=object)\n",
        "\n",
        "# Set all values above 0 to 0, assuming negative values are not intact\n",
        "percentage_array[percentage_array > 0] = 0\n",
        "\n",
        "# Separate valid and invalid (nodatavalue) elements\n",
        "valid_elements = percentage_array[percentage_array != nodatavalue]\n",
        "invalid_elements = percentage_array == nodatavalue\n",
        "\n",
        "# Calculate quantiles for valid elements\n",
        "quantiles = np.percentile(valid_elements, np.linspace(0, 100, num_quantiles + 1)[1:-1]) if len(valid_elements) > 0 else []\n",
        "for i in range(1, num_quantiles + 1):\n",
        "    lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "    upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "    relative_intactness_array[(percentage_array > lower_bound) & (percentage_array <= upper_bound)] = i\n",
        "# if nodatavalue is not None:\n",
        "    relative_intactness_array[invalid_elements] = nodatavalue\n",
        "export_array_as_tif(relative_intactness_array, relative_intactness_path)\n",
        "\n",
        "# Prepare data for CSV: Collect lower and upper bounds for each category\n",
        "ranges_data = {'Lower_Bound': [], 'Upper_Bound': []}\n",
        "for i in range(1, num_quantiles + 1):\n",
        "    lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "    upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "    ranges_data['Lower_Bound'].append(lower_bound)\n",
        "    ranges_data['Upper_Bound'].append(upper_bound)\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "relative_intactness_df = pd.DataFrame(ranges_data)\n",
        "relative_intactness_csv_path = os.path.join(intactness_dir, f'{relative_intactness_name}.csv')\n",
        "relative_intactness_df.to_csv(relative_intactness_csv_path, index=False)\n",
        "\n",
        "# Generate and save histogram as .png\n",
        "histogram_path = join(intactness_dir, f'{relative_intactness_name}.png')\n",
        "plt.figure()\n",
        "plt.hist(valid_elements.flatten(), bins='auto')\n",
        "plt.title(f'{relative_intactness_name} Histogram')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.savefig(histogram_path)\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-7ctfgdoF2"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcuArY00N9Mc"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "y_OkHaUkTCyB",
        "X3w7svaUvs07",
        "xy2nfHshozVQ",
        "1T9UqJrzWECr",
        "FkNV_N7UKcMd",
        "diwt1peRdXpj"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}