{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/6_scenarios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8jLRKRSfgb"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkvV803eSUQY"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEpdIjOwSiB1"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install astropy\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!pip install xgboost\n",
        "!apt-get install -y gdal-bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQDSsvGeSi4u"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "from contextlib import contextmanager\n",
        "try: import cupy # Only works on GPU runtime\n",
        "except: None\n",
        "import gc\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import json\n",
        "import math\n",
        "from os import makedirs\n",
        "from os.path import join, exists\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import psutil\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "import re\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "import shutil\n",
        "from shutil import copyfile, rmtree\n",
        "import subprocess\n",
        "import time\n",
        "import xgboost as xgb\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxgT3K1Sjum"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "\n",
        "feature_dir = join(base_dir, \"3_features\")\n",
        "feature_resampled_dir = join(feature_dir, \"resampled\")\n",
        "feature_binary_dir = join(feature_dir, \"binary\")\n",
        "feature_final_dir = join(feature_dir, \"final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "masks_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(scenarios_dir, exist_ok=True)\n",
        "makedirs(masks_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96FAfeyooMLz"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = vector = None\n",
        "    try:\n",
        "        raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "        vector = ogr.Open(polygon_path)\n",
        "        if not raster or not vector:\n",
        "            raise ValueError(\"Cannot open input files\")\n",
        "        layer = vector.GetLayer()\n",
        "        options = [\"ALL_TOUCHED=TRUE\"] if all_touched else []\n",
        "        if fixed:\n",
        "            gdal.RasterizeLayer(raster, [1], layer, burn_values=[fixed_value], options=options)\n",
        "        else:\n",
        "            attr_name = column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()\n",
        "            options.append(f\"ATTRIBUTE={attr_name}\")\n",
        "            gdal.RasterizeLayer(raster, [1], layer, options=options)\n",
        "    finally:\n",
        "        if raster: raster.FlushCache()\n",
        "        raster = vector = None\n",
        "\n",
        "# Global function: edge effects\n",
        "# Provides spatial awareness analogous to CNN receptive fields for tabular models\n",
        "# Data_type: 'binary' or 'continuous'.\n",
        "cell_size_y_path = join(areas_dir, 'cell_size_y.tif')\n",
        "cell_size_x_path = join(areas_dir, 'cell_size_x.tif')\n",
        "def edge_effects(array, data_type, cell_size_x_path, cell_size_y_path, threshold_metres):\n",
        "    # Determine pixel size from cell size rasters.\n",
        "    cell_size_x = gdal.Open(cell_size_x_path)\n",
        "    cell_size_x_array = cell_size_x.ReadAsArray()\n",
        "    cell_size_x = None\n",
        "    cell_size_y = gdal.Open(cell_size_y_path)\n",
        "    cell_size_y_array = cell_size_y.ReadAsArray()\n",
        "    cell_size_y = None\n",
        "    mean_cell_resolution = np.mean([np.mean(cell_size_x_array), np.mean(cell_size_y_array)])\n",
        "    # Maximum pixel distance for kernel extent.\n",
        "    max_pixel_distance = threshold_metres / mean_cell_resolution\n",
        "    # 2D Gaussian weight distribution follows chi-squared with df=2.\n",
        "    # Cumulative probability within radius r: P = 1 - exp(-r² / 2σ²).\n",
        "    # Solving for r at P=0.95: r = σ * sqrt(-2 * ln(0.05)) ≈ 2.45σ.\n",
        "    # Setting r = max_pixel_distance ensures 95% of kernel weight falls within threshold.\n",
        "    gaussian_stdev = max_pixel_distance / 2.45\n",
        "    kernel_radius = int(np.ceil(max_pixel_distance))\n",
        "    kernel_size = 2 * kernel_radius + 1\n",
        "    # Gaussian kernel for spatial weighting.\n",
        "    kernel = Gaussian2DKernel(x_stddev=gaussian_stdev, y_stddev=gaussian_stdev,\n",
        "                              x_size=kernel_size, y_size=kernel_size)\n",
        "    # Circular mask enforces ecological threshold as hard boundary.\n",
        "    # Square kernels would include pixels beyond threshold at corners.\n",
        "    y, x = np.ogrid[:kernel_size, :kernel_size]\n",
        "    centre = kernel_radius\n",
        "    distance_from_centre = np.sqrt((x - centre)**2 + (y - centre)**2)\n",
        "    circular_mask = distance_from_centre <= max_pixel_distance\n",
        "    # Apply mask and renormalise to sum to 1.\n",
        "    # Renormalisation ensures consistent weighting after truncation.\n",
        "    kernel_array = kernel.array.copy()\n",
        "    kernel_array[~circular_mask] = 0\n",
        "    kernel_array /= kernel_array.sum()\n",
        "    # Gaussian smoothing captures local spatial context.\n",
        "    # For binary: represents local class density within threshold.\n",
        "    # For continuous: represents local weighted mean within threshold.\n",
        "    # boundary='extend' extrapolates edge values beyond raster extent.\n",
        "    smoothed = convolve(array.astype(float), kernel_array, boundary='extend')\n",
        "    if data_type == 'continuous': return smoothed # Without rounding\n",
        "    if data_type == 'binary': smoothed = np.round(smoothed, 2) # Round\n",
        "    # Binary data: compute signed distance to class boundary.\n",
        "    # Euclidean distance transform gives centre-to-centre pixel distance.\n",
        "    dist_from_ones = distance_transform_edt(array == 0)\n",
        "    dist_from_zeros = distance_transform_edt(array == 1)\n",
        "    # Convert to distance from pixel centre to class boundary.\n",
        "    # Class boundary lies between adjacent pixels of different classes.\n",
        "    # Subtracting 0.5 pixels approximates centre-to-boundary distance.\n",
        "    # Sign encodes class membership: positive = class 1, negative = class 0.\n",
        "    # Magnitude encodes proximity to boundary (edge effects zone).\n",
        "    signed_distance = np.where(\n",
        "        array == 1,\n",
        "        np.maximum(dist_from_zeros - 0.5, 0) * mean_cell_resolution,\n",
        "        -np.maximum(dist_from_ones - 0.5, 0) * mean_cell_resolution\n",
        "    )\n",
        "    # Cap at threshold: pixels beyond are interior, not edge-influenced.\n",
        "    # Round to integer metres for cleaner feature representation.\n",
        "    signed_distance = np.round(np.clip(signed_distance, -threshold_metres, threshold_metres)).astype(np.int16)\n",
        "    return signed_distance, smoothed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_OkHaUkTCyB"
      },
      "source": [
        "# Select model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mB0XrPNYU_I"
      },
      "outputs": [],
      "source": [
        "# Select a model\n",
        "model_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file == 'model.json':\n",
        "      print(f'selected_model = \"{subdir.split(f\"{models_dir}/\",1)[1]}\"')\n",
        "      model_exists = True\n",
        "if not model_exists:\n",
        "  print(\"No model exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbXY6iP_qjYc"
      },
      "outputs": [],
      "source": [
        "selected_model = \"agbd_251203_161707\"\n",
        "\n",
        "# This must be True when using AlphaEarth features.\n",
        "# Alternate scenarios cannot be created with AlphaEarth's embeddings.\n",
        "# Mixing Alpha Earth with features of lower resolution has not been tested.\n",
        "alpha_earth = False\n",
        "\n",
        "# Set this to True for anything (e.g. elevation) with only a single prediction\n",
        "single_prediction = False\n",
        "\n",
        "# Define model directories\n",
        "selected_model_dir = join(models_dir,selected_model)\n",
        "selected_model_shap_dir = join(selected_model_dir, \"shap\")\n",
        "selected_model_json = join(selected_model_dir, \"model.json\")\n",
        "selected_model_descr_dir = join(selected_model_dir, \"model_description.json\")\n",
        "selected_model_dataset_path = join(selected_model_dir, f\"{selected_model}.pkl\")\n",
        "selected_model_dataset = pd.read_pickle(selected_model_dataset_path)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(selected_model_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_target = model_dataset_description[\"selected_target\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "selected_features = model_dataset_description[\"selected_features\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_features_mappings = model_dataset_description[\"categorical_features_mappings\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "filter_parameter = model_dataset_description[\"filter_parameter\"]\n",
        "filter_values_to_include = model_dataset_description[\"filter_values_to_include\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "covariates = [covariate[4:] for covariate in covariates_renamed]\n",
        "\n",
        "# Create scenarios model directory\n",
        "scenarios_model_dir = join(scenarios_dir, selected_model)\n",
        "makedirs(scenarios_model_dir, exist_ok=True)\n",
        "\n",
        "# Copy model_dataset_description.json\n",
        "with open(join(scenarios_model_dir, \"model_dataset_description.json\"), \"w\") as file:\n",
        "  file.write(json.dumps(model_dataset_description))\n",
        "\n",
        "# Create subdirectories\n",
        "tile_templates_dir = join(scenarios_model_dir, 'tile_templates')\n",
        "tile_features_dir = join(scenarios_model_dir, \"tile_features\")\n",
        "tile_feature_stacks_dir = join(scenarios_model_dir, \"tile_feature_stacks\")\n",
        "tile_prediction_cache_dir = join(scenarios_model_dir,\"tile_prediction_cache\")\n",
        "scenario_predictions_unmasked_dir = join(scenarios_model_dir,\"scenario_predictions_unmasked\")\n",
        "scenario_predictions_dir = join(scenarios_model_dir, \"scenario_predictions\")\n",
        "\n",
        "makedirs(tile_templates_dir, exist_ok=True)\n",
        "makedirs(tile_features_dir, exist_ok=True)\n",
        "makedirs(tile_feature_stacks_dir, exist_ok=True)\n",
        "makedirs(tile_prediction_cache_dir, exist_ok=True)\n",
        "makedirs(scenario_predictions_unmasked_dir, exist_ok=True)\n",
        "makedirs(scenario_predictions_dir, exist_ok=True)\n",
        "\n",
        "if alpha_earth or single_prediction: features_dir = feature_final_dir\n",
        "else: # Copy features from the final features directory.\n",
        "  # These will be added to for alternate scenarios.\n",
        "  features_dir = join(scenarios_model_dir, \"features\")\n",
        "  makedirs(features_dir, exist_ok=True)\n",
        "  for feature in os.listdir(feature_final_dir):\n",
        "    if feature not in os.listdir(features_dir):\n",
        "      if 'alpha_earth' not in feature:\n",
        "        feature_original_path = join(feature_final_dir, feature)\n",
        "        feature_copy_path = join(features_dir, feature)\n",
        "        copyfile(feature_original_path, feature_copy_path)\n",
        "  print(f\"All features present in the following directory have already been copied over: {feature_final_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmlqEP9rFapf"
      },
      "source": [
        "# Define yearly scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJZoEWycn8-x"
      },
      "outputs": [],
      "source": [
        "# Yearly scenarios may include years after the model scenario if feature data is available later than GEDI data.\n",
        "# Date of the prediction is ~December 31st, e.g. '2024' is 31/12/2024, requiring features up to 2024.\n",
        "\n",
        "model_scenario_override = None # set if cannot be automatically determined from model features\n",
        "\n",
        "yearly_features = [\"forest\", \"disturbance\", \"alpha_earth\"]\n",
        "\n",
        "# Remove the 'fea_' prefix from each feature\n",
        "model_features = sorted([feature[4:] for feature in selected_features])\n",
        "\n",
        "# Create a list of feature years from the model's features\n",
        "model_feature_years = []\n",
        "for feature in model_features:\n",
        "  for yearly_feature in yearly_features:\n",
        "    if yearly_feature in feature:\n",
        "      model_feature_years.append(int(feature[-4:]))\n",
        "\n",
        "# Determine the model scenario from the maximum year\n",
        "if model_scenario_override != None: model_scenario = model_scenario_override\n",
        "else: model_scenario = max(model_feature_years)\n",
        "model_scenario_filename = f\"{model_scenario}.csv\"\n",
        "model_scenario_dir = join(scenarios_model_dir, model_scenario_filename)\n",
        "print(f\"The maximum year used in the model is {model_scenario}, which has been created as the first scenario.\\n\")\n",
        "print(f\"The {model_scenario} scenario feature list has been saved to:\\n {model_scenario_dir}\\n\")\n",
        "print(f\"Ensure all features in this list have been copied to:\\n{features_dir}\\n\")\n",
        "\n",
        "# Save the model scenario features as a .csv\n",
        "pd.DataFrame(model_features).to_csv(model_scenario_dir, index=False)\n",
        "\n",
        "# Determine available feature years\n",
        "final_feature_years = []\n",
        "for final_feature in os.listdir(feature_final_dir):\n",
        "  if alpha_earth:\n",
        "    if final_feature.endswith('.tif') and 'alpha_earth' in final_feature:\n",
        "      try: final_feature_years.append(int(final_feature[-8:-4]))\n",
        "      except: continue\n",
        "  else:\n",
        "    if final_feature.endswith('.tif') and final_feature[-9] == '_':\n",
        "      try: final_feature_years.append(int(final_feature[-8:-4]))\n",
        "      except: continue\n",
        "\n",
        "# Find the first and last feature years\n",
        "first_feature_year = min(final_feature_years)\n",
        "last_feature_year = max(final_feature_years)\n",
        "additional_feature_years = last_feature_year - model_scenario\n",
        "print(f\"The first available feature year is {first_feature_year} and the last is {last_feature_year}.\\n\")\n",
        "\n",
        "# Calculate the range of scenario years and minimum scenario year\n",
        "model_scenario_year_range = max(model_feature_years) - (min(model_feature_years))\n",
        "minimum_yearly_scenario = first_feature_year + model_scenario_year_range\n",
        "print(f\"The earliest scenario year that can be predicted is {minimum_yearly_scenario}.\")\n",
        "print(f\"The latest scenario year that can be predicted is {last_feature_year}.\")\n",
        "print(f\"This is based on the number of yearly features used to train the model and the total availability of features.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPd_nrFYeGJ3"
      },
      "outputs": [],
      "source": [
        "# Select static features which are the same in every scenario, e.g. topography\n",
        "print(\"static_features = [\")\n",
        "for feature in model_features:\n",
        "  if \"beam\" not in feature and \"sensitivity\" not in feature:\n",
        "    print(f'  \"{feature}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARVJed3vTgBv"
      },
      "outputs": [],
      "source": [
        "# # GEDI elevation\n",
        "\n",
        "# static_features = [\n",
        "#   \"coast_proximity_km\",\n",
        "#   \"latitude\",\n",
        "#   \"longitude\",\n",
        "#   \"topo_dsm_smooth_aspect_cosine\",\n",
        "#   \"topo_dsm_smooth_aspect_sine\",\n",
        "#   \"topo_dsm_smooth_circular_variance_aspect_03\",\n",
        "#   \"topo_dsm_smooth_circular_variance_aspect_07\",\n",
        "#   \"topo_dsm_smooth_circular_variance_aspect_11\",\n",
        "#   \"topo_dsm_smooth_deviation_mean_elevation_03\",\n",
        "#   \"topo_dsm_smooth_deviation_mean_elevation_07\",\n",
        "#   \"topo_dsm_smooth_deviation_mean_elevation_11\",\n",
        "#   \"topo_dsm_smooth_eastness\",\n",
        "#   \"topo_dsm_smooth_elevation\",\n",
        "#   \"topo_dsm_smooth_northness\",\n",
        "#   \"topo_dsm_smooth_profile_curvature\",\n",
        "#   \"topo_dsm_smooth_roughness_03\",\n",
        "#   \"topo_dsm_smooth_roughness_07\",\n",
        "#   \"topo_dsm_smooth_roughness_11\",\n",
        "#   \"topo_dsm_smooth_slope\",\n",
        "#   \"topo_dsm_smooth_stream_power_index_log10\",\n",
        "#   \"topo_dsm_smooth_surface_area_ratio\",\n",
        "#   \"topo_dsm_smooth_tangential_curvature\",\n",
        "#   \"topo_dsm_smooth_topographic_position_index_03\",\n",
        "#   \"topo_dsm_smooth_topographic_position_index_07\",\n",
        "#   \"topo_dsm_smooth_topographic_position_index_11\",\n",
        "#   \"topo_dsm_smooth_topographic_ruggedness_index\",\n",
        "#   \"topo_dsm_smooth_topographic_wetness_index\",\n",
        "#   \"topo_dsm_unsmooth_aspect_cosine\",\n",
        "#   \"topo_dsm_unsmooth_aspect_sine\",\n",
        "#   \"topo_dsm_unsmooth_circular_variance_aspect_03\",\n",
        "#   \"topo_dsm_unsmooth_circular_variance_aspect_07\",\n",
        "#   \"topo_dsm_unsmooth_circular_variance_aspect_11\",\n",
        "#   \"topo_dsm_unsmooth_deviation_mean_elevation_03\",\n",
        "#   \"topo_dsm_unsmooth_deviation_mean_elevation_07\",\n",
        "#   \"topo_dsm_unsmooth_deviation_mean_elevation_11\",\n",
        "#   \"topo_dsm_unsmooth_eastness\",\n",
        "#   \"topo_dsm_unsmooth_elevation\",\n",
        "#   \"topo_dsm_unsmooth_northness\",\n",
        "#   \"topo_dsm_unsmooth_profile_curvature\",\n",
        "#   \"topo_dsm_unsmooth_roughness_03\",\n",
        "#   \"topo_dsm_unsmooth_roughness_07\",\n",
        "#   \"topo_dsm_unsmooth_roughness_11\",\n",
        "#   \"topo_dsm_unsmooth_slope\",\n",
        "#   \"topo_dsm_unsmooth_stream_power_index_log10\",\n",
        "#   \"topo_dsm_unsmooth_surface_area_ratio\",\n",
        "#   \"topo_dsm_unsmooth_tangential_curvature\",\n",
        "#   \"topo_dsm_unsmooth_topographic_position_index_03\",\n",
        "#   \"topo_dsm_unsmooth_topographic_position_index_07\",\n",
        "#   \"topo_dsm_unsmooth_topographic_position_index_11\",\n",
        "#   \"topo_dsm_unsmooth_topographic_ruggedness_index\",\n",
        "#   \"topo_dsm_unsmooth_topographic_wetness_index\",\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "static_features = [\n",
        "  \"coast_proximity_km\",\n",
        "  \"latitude\",\n",
        "  \"longitude\",\n",
        "  \"topo_dtm_smooth_aspect_cosine\",\n",
        "  \"topo_dtm_smooth_aspect_sine\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_03\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_07\",\n",
        "  \"topo_dtm_smooth_circular_variance_aspect_11\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_03\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_07\",\n",
        "  \"topo_dtm_smooth_deviation_mean_elevation_11\",\n",
        "  \"topo_dtm_smooth_eastness\",\n",
        "  \"topo_dtm_smooth_elevation\",\n",
        "  \"topo_dtm_smooth_northness\",\n",
        "  \"topo_dtm_smooth_profile_curvature\",\n",
        "  \"topo_dtm_smooth_roughness_03\",\n",
        "  \"topo_dtm_smooth_roughness_07\",\n",
        "  \"topo_dtm_smooth_roughness_11\",\n",
        "  \"topo_dtm_smooth_slope\",\n",
        "  \"topo_dtm_smooth_stream_power_index_log10\",\n",
        "  \"topo_dtm_smooth_surface_area_ratio\",\n",
        "  \"topo_dtm_smooth_tangential_curvature\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_03\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_07\",\n",
        "  \"topo_dtm_smooth_topographic_position_index_11\",\n",
        "  \"topo_dtm_smooth_topographic_ruggedness_index\",\n",
        "  \"topo_dtm_smooth_topographic_wetness_index\",\n",
        "  \"topo_dtm_unsmooth_aspect_cosine\",\n",
        "  \"topo_dtm_unsmooth_aspect_sine\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_03\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_07\",\n",
        "  \"topo_dtm_unsmooth_circular_variance_aspect_11\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_03\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_07\",\n",
        "  \"topo_dtm_unsmooth_deviation_mean_elevation_11\",\n",
        "  \"topo_dtm_unsmooth_eastness\",\n",
        "  \"topo_dtm_unsmooth_elevation\",\n",
        "  \"topo_dtm_unsmooth_northness\",\n",
        "  \"topo_dtm_unsmooth_profile_curvature\",\n",
        "  \"topo_dtm_unsmooth_roughness_03\",\n",
        "  \"topo_dtm_unsmooth_roughness_07\",\n",
        "  \"topo_dtm_unsmooth_roughness_11\",\n",
        "  \"topo_dtm_unsmooth_slope\",\n",
        "  \"topo_dtm_unsmooth_stream_power_index_log10\",\n",
        "  \"topo_dtm_unsmooth_surface_area_ratio\",\n",
        "  \"topo_dtm_unsmooth_tangential_curvature\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_03\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_07\",\n",
        "  \"topo_dtm_unsmooth_topographic_position_index_11\",\n",
        "  \"topo_dtm_unsmooth_topographic_ruggedness_index\",\n",
        "  \"topo_dtm_unsmooth_topographic_wetness_index\",\n",
        "]"
      ],
      "metadata": {
        "id": "djq-5Tvpaapn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg4XvxT5foVR"
      },
      "outputs": [],
      "source": [
        "# Note that areas that were forested in a historic yearly scenario but were reservoirs\n",
        "# or flooded at the time topographic feature data was collected (~2014 for GLO-30 DEM)\n",
        "# will be predicted based on a flat topography at the elevation of the water's surface.\n",
        "\n",
        "# Set scenario features as all non-static features\n",
        "scenario_features = sorted(list(set(model_features) - set(static_features)))\n",
        "\n",
        "# Create feature lists for all possible yearly scenarios\n",
        "for yearly_scenario in range(minimum_yearly_scenario, last_feature_year +1):\n",
        "  year_difference = model_scenario - yearly_scenario\n",
        "  yearly_scenario_features = []\n",
        "  for scenario_feature in scenario_features:\n",
        "    try:\n",
        "      year_change = int(scenario_feature[-4:]) - year_difference\n",
        "      yearly_scenario_feature = scenario_feature[:-4] + str(year_change)\n",
        "      yearly_scenario_features.append(yearly_scenario_feature)\n",
        "    except: yearly_scenario_features.append(scenario_feature)\n",
        "  # Compile yearly features and save as a .csv\n",
        "  yearly_scenario_features = sorted(yearly_scenario_features + static_features)\n",
        "  yearly_scenario_filename = f\"{yearly_scenario}.csv\"\n",
        "  yearly_scenario_dir = join(scenarios_model_dir,yearly_scenario_filename)\n",
        "  pd.DataFrame(yearly_scenario_features).to_csv(yearly_scenario_dir, index=False)\n",
        "\n",
        "# Open the most recent yearly scenario feature list\n",
        "most_recent_scenario_csv = join(scenarios_model_dir,f\"{last_feature_year}.csv\")\n",
        "most_recent_scenario_features = pd.Series.tolist(pd.read_csv(most_recent_scenario_csv).iloc[:,0])\n",
        "\n",
        "print(f\"Lists of features for all possible yearly scenarios have been exported to {scenarios_model_dir}/.\")\n",
        "print(f\"Ensure all features in these lists have been copied to:\\n{features_dir}\\n\")\n",
        "\n",
        "\n",
        "# Create 'no disturbance' edge distance and local density features for alternate scenarios.\n",
        "# Assumes the minimum possible value is present in the first scenario year.\n",
        "if any('disturbance_edge_distance' in feature for feature in scenario_features):\n",
        "  minimum_disturbance_distance_name = f\"disturbance_edge_distance_0000\"\n",
        "  minimum_disturbance_path = join(features_dir, f\"{minimum_disturbance_distance_name}.tif\")\n",
        "  if not exists(minimum_disturbance_path):\n",
        "    example_disturbance = join(features_dir, f\"disturbance_edge_distance_{first_feature_year}.tif\")\n",
        "    example_disturbance = gdal.Open(example_disturbance)\n",
        "    example_disturbance_array = example_disturbance.ReadAsArray()\n",
        "    example_disturbance = None\n",
        "    minimum_disturbance_value = example_disturbance_array.min()\n",
        "    minimum_disturbance_array = np.full_like(example_disturbance_array, minimum_disturbance_value)\n",
        "    export_array_as_tif(minimum_disturbance_array, minimum_disturbance_path, template = example_disturbance)\n",
        "    print(f\"The minimum disturbance value is {minimum_disturbance_value}\\n,\")\n",
        "    print(f\"which has been used to create the 'minimum disturbance' feature {minimum_disturbance_distance_name}.\")\n",
        "  else: print(f\"The minimum disturbance feature {minimum_disturbance_distance_name} already exists.\")\n",
        "if any('disturbance_local_density' in feature for feature in scenario_features):\n",
        "  minimum_disturbance_density_name = f\"disturbance_local_density_0000\"\n",
        "  minimum_disturbance_path = join(features_dir, f\"{minimum_disturbance_density_name}.tif\")\n",
        "  if not exists(minimum_disturbance_path):\n",
        "    example_disturbance = join(features_dir, f\"disturbance_local_density_{first_feature_year}.tif\")\n",
        "    example_disturbance = gdal.Open(example_disturbance)\n",
        "    example_disturbance_array = example_disturbance.ReadAsArray()\n",
        "    example_disturbance = None\n",
        "    minimum_disturbance_value = example_disturbance_array.min()\n",
        "    minimum_disturbance_array = np.full_like(example_disturbance_array, minimum_disturbance_value)\n",
        "    export_array_as_tif(minimum_disturbance_array, minimum_disturbance_path, template = example_disturbance)\n",
        "    print(f\"The minimum disturbance value is {minimum_disturbance_value}\\n,\")\n",
        "    print(f\"which has been used to create the 'minimum disturbance' feature {minimum_disturbance_density_name}.\")\n",
        "  else: print(f\"The minimum disturbance feature {minimum_disturbance_density_name} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3w7svaUvs07"
      },
      "source": [
        "# Define alternate scenarios (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmcpUriAFfZ6"
      },
      "source": [
        "## Disturbance requirement tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWA1QsHGQw_P"
      },
      "outputs": [],
      "source": [
        "# Tool to identify required scenarios for forest AGBD disturbance mapping\n",
        "use_tool = False\n",
        "\n",
        "def select_forest_scenarios():\n",
        "    # Initialize variables\n",
        "    calculation_note = None\n",
        "    is_specific_effects_calculation = False\n",
        "\n",
        "    # Print header\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"\\nFOREST DISTURBANCE SELECTOR\\n\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Step 1: Collect disturbance type\n",
        "    disturbance_prompt = (\n",
        "        \"Select disturbance type:\\n\"\n",
        "        \"1. Degradation\\n\"\n",
        "        \"2. Deforestation\\n\"\n",
        "        \"3. Disturbance (degradation + deforestation)\\n\\n\"\n",
        "        \"Enter your choice (1-3): \"\n",
        "    )\n",
        "    disturbance_type = input(disturbance_prompt)\n",
        "\n",
        "    # Step 2: Year of interest\n",
        "    print(\"\\n\")\n",
        "    year_of_interest = input(\"Enter year of interest: \")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Step 3: Collect baseline type\n",
        "    baseline_prompt = (\n",
        "        \"Select baseline type:\\n\"\n",
        "        \"1. Since oldgrowth state\\n\"\n",
        "        \"2. Since a baseline year\\n\"\n",
        "        \"3. Effect before first available disturbance year\\n\"\n",
        "        \"4. Effect of a specific year\\n\"\n",
        "        \"5. Effect in the same year\\n\\n\"\n",
        "        \"Enter your choice (1-5): \"\n",
        "    )\n",
        "    baseline_type = input(baseline_prompt)\n",
        "\n",
        "    # Initialize result variables\n",
        "    selected_difference = None\n",
        "    scenario_pair = None\n",
        "    other_requirements = []\n",
        "    is_specific_effects_calculation = (baseline_type in [\"3\", \"4\", \"5\"])\n",
        "\n",
        "    # Process based on baseline type\n",
        "    if baseline_type == \"1\":  # Since oldgrowth state\n",
        "        if disturbance_type == \"1\":  # Degradation\n",
        "            selected_difference = f\"{year_of_interest}_degradation_since_oldgrowth\"\n",
        "            scenario_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth\")\n",
        "\n",
        "        elif disturbance_type == \"2\":  # Deforestation\n",
        "            selected_difference = f\"{year_of_interest}_deforestation_since_oldgrowth\"\n",
        "            # This requires both degradation_since_oldgrowth and disturbance_since_oldgrowth\n",
        "            deg_oldgrowth_diff = f\"{year_of_interest}_degradation_since_oldgrowth\"\n",
        "            deg_oldgrowth_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth\")\n",
        "            dist_oldgrowth_diff = f\"{year_of_interest}_disturbance_since_oldgrowth\"\n",
        "            dist_oldgrowth_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth_all_land\")\n",
        "\n",
        "            other_requirements.append((deg_oldgrowth_diff, deg_oldgrowth_pair))\n",
        "            other_requirements.append((dist_oldgrowth_diff, dist_oldgrowth_pair))\n",
        "            calculation_note = f\"Calculated as {dist_oldgrowth_diff} - {deg_oldgrowth_diff}\"\n",
        "\n",
        "        elif disturbance_type == \"3\":  # Total disturbance (degradation + deforestation)\n",
        "            selected_difference = f\"{year_of_interest}_disturbance_since_oldgrowth\"\n",
        "            scenario_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth_all_land\")\n",
        "\n",
        "    elif baseline_type == \"2\":  # Since a baseline year\n",
        "        # Get and validate baseline year\n",
        "        print(\"\\n\")\n",
        "        baseline_year = input(\"Enter baseline year (must be before year of interest): \")\n",
        "        if int(baseline_year) >= int(year_of_interest):\n",
        "            print(\"\\nError: Baseline year must be before year of interest\")\n",
        "            return None\n",
        "\n",
        "        baseline_year_plus1 = str(int(baseline_year) + 1)\n",
        "\n",
        "        if disturbance_type == \"1\":  # Degradation\n",
        "            selected_difference = f\"{year_of_interest}_degradation_since_{baseline_year_plus1}\"\n",
        "            scenario_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{baseline_year_plus1}\")\n",
        "\n",
        "        elif disturbance_type == \"2\":  # Deforestation\n",
        "            selected_difference = f\"{year_of_interest}_deforestation_since_{baseline_year_plus1}\"\n",
        "            # This requires both degradation_since and disturbance_since for the same period\n",
        "            deg_since_diff = f\"{year_of_interest}_degradation_since_{baseline_year_plus1}\"\n",
        "            deg_since_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{baseline_year_plus1}\")\n",
        "            dist_since_diff = f\"{year_of_interest}_disturbance_since_{baseline_year_plus1}\"\n",
        "            dist_since_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{baseline_year_plus1}\")\n",
        "\n",
        "            other_requirements.append((deg_since_diff, deg_since_pair))\n",
        "            other_requirements.append((dist_since_diff, dist_since_pair))\n",
        "            calculation_note = f\"Calculated as {dist_since_diff} - {deg_since_diff}\"\n",
        "\n",
        "        elif disturbance_type == \"3\":  # Total disturbance since baseline\n",
        "            selected_difference = f\"{year_of_interest}_disturbance_since_{baseline_year_plus1}\"\n",
        "            scenario_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{baseline_year_plus1}\")\n",
        "\n",
        "    elif baseline_type == \"3\":  # Effect before first available disturbance year\n",
        "        # Get and validate first available disturbance year\n",
        "        print(\"\\n\")\n",
        "        first_year = input(\"Enter first available disturbance year in your data: \")\n",
        "        if int(first_year) >= int(year_of_interest):\n",
        "            print(\"\\nError: first available disturbance year must be before year of interest\")\n",
        "            return None\n",
        "\n",
        "        if disturbance_type == \"1\":  # Degradation\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_degradation_before_{first_year}\"\n",
        "\n",
        "            # Define required difference components\n",
        "            since_first = f\"{year_of_interest}_degradation_since_{first_year}\"\n",
        "            since_first_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{first_year}\")\n",
        "\n",
        "            oldgrowth_difference = f\"{year_of_interest}_degradation_since_oldgrowth\"\n",
        "            oldgrowth_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth\")\n",
        "\n",
        "            other_requirements.append((since_first, since_first_pair))\n",
        "            other_requirements.append((oldgrowth_difference, oldgrowth_pair))\n",
        "            calculation_note = f\"Calculated as {oldgrowth_difference} - {since_first}\"\n",
        "\n",
        "        elif disturbance_type == \"2\":  # Deforestation\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_deforestation_before_{first_year}\"\n",
        "\n",
        "            # Define required difference components for deforestation_since_first\n",
        "            deg_since_first = f\"{year_of_interest}_degradation_since_{first_year}\"\n",
        "            deg_since_first_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{first_year}\")\n",
        "            dist_since_first = f\"{year_of_interest}_disturbance_since_{first_year}\"\n",
        "            dist_since_first_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{first_year}\")\n",
        "\n",
        "            # Define required difference components for deforestation_since_oldgrowth\n",
        "            deg_oldgrowth_diff = f\"{year_of_interest}_degradation_since_oldgrowth\"\n",
        "            deg_oldgrowth_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth\")\n",
        "            dist_oldgrowth_diff = f\"{year_of_interest}_disturbance_since_oldgrowth\"\n",
        "            dist_oldgrowth_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth_all_land\")\n",
        "\n",
        "            other_requirements.append((deg_since_first, deg_since_first_pair))\n",
        "            other_requirements.append((dist_since_first, dist_since_first_pair))\n",
        "            other_requirements.append((deg_oldgrowth_diff, deg_oldgrowth_pair))\n",
        "            other_requirements.append((dist_oldgrowth_diff, dist_oldgrowth_pair))\n",
        "\n",
        "            defor_since_first = f\"{year_of_interest}_deforestation_since_{first_year}\"\n",
        "            defor_since_oldgrowth = f\"{year_of_interest}_deforestation_since_oldgrowth\"\n",
        "            calculation_note = f\"Calculated as {defor_since_oldgrowth} - {defor_since_first}\"\n",
        "\n",
        "        elif disturbance_type == \"3\":  # Total disturbance before first year\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_disturbance_before_{first_year}\"\n",
        "\n",
        "            # Define required difference components\n",
        "            since_first = f\"{year_of_interest}_disturbance_since_{first_year}\"\n",
        "            since_first_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{first_year}\")\n",
        "\n",
        "            oldgrowth_difference = f\"{year_of_interest}_disturbance_since_oldgrowth\"\n",
        "            oldgrowth_pair = (year_of_interest, f\"{year_of_interest}_oldgrowth_all_land\")\n",
        "\n",
        "            other_requirements.append((since_first, since_first_pair))\n",
        "            other_requirements.append((oldgrowth_difference, oldgrowth_pair))\n",
        "            calculation_note = f\"Calculated as {oldgrowth_difference} - {since_first}\"\n",
        "\n",
        "    elif baseline_type == \"4\":  # Effect of a specific year\n",
        "        # Get and validate specific year\n",
        "        print(\"\\n\")\n",
        "        specific_year = input(\"Enter the specific year whose effect you want to measure: \")\n",
        "        if int(specific_year) >= int(year_of_interest):\n",
        "            print(\"\\nError: Specific year must be before year of interest\")\n",
        "            return None\n",
        "\n",
        "        specific_year_plus1 = str(int(specific_year) + 1)\n",
        "\n",
        "        if disturbance_type == \"1\":  # Degradation\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_degradation_in_{specific_year}\"\n",
        "\n",
        "            # Define required difference components\n",
        "            since_specific = f\"{year_of_interest}_degradation_since_{specific_year}\"\n",
        "            since_specific_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{specific_year}\")\n",
        "\n",
        "            since_after = f\"{year_of_interest}_degradation_since_{specific_year_plus1}\"\n",
        "            since_after_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{specific_year_plus1}\")\n",
        "\n",
        "            other_requirements.append((since_specific, since_specific_pair))\n",
        "            other_requirements.append((since_after, since_after_pair))\n",
        "            calculation_note = f\"Calculated as {since_specific} - {since_after}\"\n",
        "\n",
        "        elif disturbance_type == \"2\":  # Deforestation\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_deforestation_in_{specific_year}\"\n",
        "\n",
        "            # Define components for since_specific\n",
        "            deg_since_specific = f\"{year_of_interest}_degradation_since_{specific_year}\"\n",
        "            deg_since_specific_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{specific_year}\")\n",
        "            dist_since_specific = f\"{year_of_interest}_disturbance_since_{specific_year}\"\n",
        "            dist_since_specific_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{specific_year}\")\n",
        "\n",
        "            # Define components for since_after\n",
        "            deg_since_after = f\"{year_of_interest}_degradation_since_{specific_year_plus1}\"\n",
        "            deg_since_after_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{specific_year_plus1}\")\n",
        "            dist_since_after = f\"{year_of_interest}_disturbance_since_{specific_year_plus1}\"\n",
        "            dist_since_after_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{specific_year_plus1}\")\n",
        "\n",
        "            other_requirements.append((deg_since_specific, deg_since_specific_pair))\n",
        "            other_requirements.append((dist_since_specific, dist_since_specific_pair))\n",
        "            other_requirements.append((deg_since_after, deg_since_after_pair))\n",
        "            other_requirements.append((dist_since_after, dist_since_after_pair))\n",
        "\n",
        "            defor_since_specific = f\"{year_of_interest}_deforestation_since_{specific_year}\"\n",
        "            defor_since_after = f\"{year_of_interest}_deforestation_since_{specific_year_plus1}\"\n",
        "            calculation_note = f\"Calculated as {defor_since_specific} - {defor_since_after}\"\n",
        "\n",
        "        elif disturbance_type == \"3\":  # Total disturbance effect\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_disturbance_in_{specific_year}\"\n",
        "\n",
        "            # Define required difference components\n",
        "            since_specific = f\"{year_of_interest}_disturbance_since_{specific_year}\"\n",
        "            since_specific_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{specific_year}\")\n",
        "\n",
        "            since_after = f\"{year_of_interest}_disturbance_since_{specific_year_plus1}\"\n",
        "            since_after_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{specific_year_plus1}\")\n",
        "\n",
        "            other_requirements.append((since_specific, since_specific_pair))\n",
        "            other_requirements.append((since_after, since_after_pair))\n",
        "            calculation_note = f\"Calculated as {since_specific} - {since_after}\"\n",
        "\n",
        "    elif baseline_type == \"5\":  # Effect in the same year\n",
        "        if disturbance_type == \"1\":  # Degradation\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_degradation_in_{year_of_interest}\"\n",
        "\n",
        "            # This is a copy operation\n",
        "            source_difference = f\"{year_of_interest}_degradation_since_{year_of_interest}\"\n",
        "            source_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{year_of_interest}\")\n",
        "\n",
        "            other_requirements.append((source_difference, source_pair))\n",
        "            calculation_note = f\"Copy and rename {source_difference}\"\n",
        "\n",
        "        elif disturbance_type == \"2\":  # Deforestation\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_deforestation_in_{year_of_interest}\"\n",
        "\n",
        "            # Define components for same-year deforestation\n",
        "            deg_same_year = f\"{year_of_interest}_degradation_since_{year_of_interest}\"\n",
        "            deg_same_year_pair = (year_of_interest, f\"{year_of_interest}_no_degradation_since_{year_of_interest}\")\n",
        "            dist_same_year = f\"{year_of_interest}_disturbance_since_{year_of_interest}\"\n",
        "            dist_same_year_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{year_of_interest}\")\n",
        "\n",
        "            other_requirements.append((deg_same_year, deg_same_year_pair))\n",
        "            other_requirements.append((dist_same_year, dist_same_year_pair))\n",
        "\n",
        "            defor_same_year = f\"{year_of_interest}_deforestation_since_{year_of_interest}\"\n",
        "            calculation_note = f\"Copy and rename {defor_same_year}\"\n",
        "\n",
        "        elif disturbance_type == \"3\":  # Total disturbance in same year\n",
        "            selected_difference = f\"{year_of_interest}_effect_of_disturbance_in_{year_of_interest}\"\n",
        "\n",
        "            # This is a copy operation\n",
        "            source_difference = f\"{year_of_interest}_disturbance_since_{year_of_interest}\"\n",
        "            source_pair = (year_of_interest, f\"{year_of_interest}_no_disturbance_since_{year_of_interest}\")\n",
        "\n",
        "            other_requirements.append((source_difference, source_pair))\n",
        "            calculation_note = f\"Copy and rename {source_difference}\"\n",
        "\n",
        "    else:\n",
        "        print(\"\\nError: Invalid baseline type selection.\")\n",
        "        return None\n",
        "\n",
        "    # Build result display\n",
        "    result_text = []\n",
        "    result_text.append(\"\\n\" + \"=\"*50)\n",
        "    result_text.append(f\"\\nSELECTED DIFFERENCE: {selected_difference}\")\n",
        "\n",
        "    # Add required differences and their scenario pairs\n",
        "    if other_requirements:\n",
        "        result_text.append(f\"\\nRequired difference and scenario pairs:\")\n",
        "        for diff, pair in other_requirements:\n",
        "            result_text.append(f\"'{diff}' {pair}\")\n",
        "\n",
        "        if calculation_note:\n",
        "            result_text.append(f\"\\n{calculation_note}\")\n",
        "    # Only include scenario pair when no other requirements\n",
        "    elif scenario_pair:\n",
        "        result_text.append(f\"\\nScenario pair required: {scenario_pair}\")\n",
        "\n",
        "    result_text.append(\"\\n\" + \"=\"*50)\n",
        "\n",
        "    # Print results with fewer new lines\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"\\n\".join(result_text))\n",
        "\n",
        "    # Return appropriate values\n",
        "    if is_specific_effects_calculation:\n",
        "        return selected_difference, other_requirements\n",
        "    else:\n",
        "        return selected_difference, scenario_pair, other_requirements\n",
        "\n",
        "# Run the function\n",
        "if use_tool:\n",
        "  if alpha_earth: print(\"Alternate scenarios cannot be created with Alpha Earth features.\")\n",
        "  else:\n",
        "    if __name__ == \"__main__\":\n",
        "        select_forest_scenarios()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKfhj6mVgRvp"
      },
      "source": [
        "## No degradation scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rK4JtDSAtn6"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios remove degradation for specific time ranges\n",
        "\n",
        "# Define ranges for 'no degradation' scenarios\n",
        "define_no_degradation_scenarios = True\n",
        "\n",
        "# No degradation ranges as tuples of (start_year, end_year)\n",
        "no_degradation_ranges = [\n",
        "    (1993, 2021),\n",
        "    (1996, 2024),\n",
        "    # (1997, 2024),\n",
        "    # (1998, 2024),\n",
        "    # (1999, 2024),\n",
        "    # (2000, 2024),\n",
        "    # (2001, 2024),\n",
        "    # (2002, 2024),\n",
        "    # (2003, 2024),\n",
        "    # (2004, 2024),\n",
        "    # (2005, 2024),\n",
        "    # (2006, 2024),\n",
        "    # (2007, 2024),\n",
        "    # (2008, 2024),\n",
        "    # (2009, 2024),\n",
        "    # (2010, 2024),\n",
        "    # (2011, 2024),\n",
        "    # (2012, 2024),\n",
        "    # (2013, 2024),\n",
        "    # (2014, 2024),\n",
        "    # (2015, 2024),\n",
        "    # (2016, 2024),\n",
        "    # (2017, 2024),\n",
        "    # (2018, 2024),\n",
        "    # (2019, 2024),\n",
        "    # (2020, 2024),\n",
        "    # (2021, 2024),\n",
        "    # (2022, 2024),\n",
        "    # (2023, 2024),\n",
        "    # (2024, 2024),\n",
        "]\n",
        "\n",
        "# Create a feature list for 'no degradation' scenarios\n",
        "if define_no_degradation_scenarios:\n",
        "  if alpha_earth: print(\"Alternate scenarios cannot be created with Alpha Earth features.\")\n",
        "  else:\n",
        "    for start_year, end_year in no_degradation_ranges:\n",
        "      assert end_year <= last_feature_year, \"End years must be at or before the last feature year.\"\n",
        "      assert end_year >= minimum_yearly_scenario, \"End years must be at or after the minimum yearly scenario.\"\n",
        "      assert start_year >= first_feature_year, \"Start years must be at or after the first feature year.\"\n",
        "      assert start_year >= end_year - model_scenario_year_range, \"Start years must be within the model scenario range of the end year.\"\n",
        "      assert start_year <= end_year, \"The start year must less than or equal to the end year.\"\n",
        "\n",
        "      # Determine base features based on the end year of the range\n",
        "      scenario_features_csv = join(scenarios_model_dir, f\"{end_year}.csv\")\n",
        "      base_features = pd.Series.tolist(pd.read_csv(scenario_features_csv).iloc[:,0])\n",
        "\n",
        "      no_degradation_features = []\n",
        "      for scenario_feature in base_features:\n",
        "        if \"disturbance_edge_distance\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          # Replace disturbance feature if it falls within the specified range\n",
        "          if scenario_feature_year >= start_year: no_degradation_features.append(minimum_disturbance_distance_name)\n",
        "          else: no_degradation_features.append(scenario_feature)\n",
        "        elif \"disturbance_local_density\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          # Replace disturbance feature if it falls within the specified range\n",
        "          if scenario_feature_year >= start_year: no_degradation_features.append(minimum_disturbance_density_name)\n",
        "          else: no_degradation_features.append(scenario_feature)\n",
        "        else: no_degradation_features.append(scenario_feature)\n",
        "\n",
        "      no_degradation_scenario_filename = f\"{end_year}_no_degradation_since_{start_year}.csv\"\n",
        "      no_degradation_scenario_path = join(scenarios_model_dir, no_degradation_scenario_filename)\n",
        "      pd.DataFrame(no_degradation_features).to_csv(no_degradation_scenario_path, index=False)\n",
        "      print(f\"Feature list for a scenario without degradation between {start_year} and {end_year} exported to {no_degradation_scenario_filename}.\")\n",
        "else:\n",
        "  print(\"The 'no degradation' scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN8AwXXAbxAn"
      },
      "source": [
        "## No disturbance scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQmk9oahbwd-"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios remove both degradation and deforestation for specific time ranges\n",
        "\n",
        "# Define ranges for 'no disturbance' scenarios\n",
        "define_no_disturbance_scenarios = True\n",
        "\n",
        "# No disturbance ranges as tuples of (start_year, end_year)\n",
        "no_disturbance_ranges = [\n",
        "    (1993, 2021),\n",
        "    (1996, 2024),\n",
        "    (1997, 2024),\n",
        "    (1998, 2024),\n",
        "    (1999, 2024),\n",
        "    (2000, 2024),\n",
        "    (2001, 2024),\n",
        "    (2002, 2024),\n",
        "    (2003, 2024),\n",
        "    (2004, 2024),\n",
        "    (2005, 2024),\n",
        "    (2006, 2024),\n",
        "    (2007, 2024),\n",
        "    (2008, 2024),\n",
        "    (2009, 2024),\n",
        "    (2010, 2024),\n",
        "    (2011, 2024),\n",
        "    (2012, 2024),\n",
        "    (2013, 2024),\n",
        "    (2014, 2024),\n",
        "    (2015, 2024),\n",
        "    (2016, 2024),\n",
        "    (2017, 2024),\n",
        "    (2018, 2024),\n",
        "    (2019, 2024),\n",
        "    (2020, 2024),\n",
        "    (2021, 2024),\n",
        "    (2022, 2024),\n",
        "    (2023, 2024),\n",
        "    (2024, 2024),\n",
        "]\n",
        "\n",
        "# Create a feature list for 'no disturbance' scenarios\n",
        "if define_no_disturbance_scenarios:\n",
        "  if alpha_earth: print(\"Alternate scenarios cannot be created with Alpha Earth features.\")\n",
        "  else:\n",
        "    for start_year, end_year in no_disturbance_ranges:\n",
        "      assert end_year <= last_feature_year, \"End years must be at or before the last feature year.\"\n",
        "      assert end_year >= minimum_yearly_scenario, \"End years must be at or after the minimum yearly scenario.\"\n",
        "      assert start_year >= first_feature_year, \"Start years must be at or after the first feature year.\"\n",
        "      assert start_year >= end_year - model_scenario_year_range, \"Start years must be within the model scenario range of the end year.\"\n",
        "      assert start_year <= end_year, \"The start year must less than or equal to the end year.\"\n",
        "\n",
        "      # Determine base features based on the end year of the range\n",
        "      scenario_features_csv = join(scenarios_model_dir, f\"{end_year}.csv\")\n",
        "      base_features = pd.Series.tolist(pd.read_csv(scenario_features_csv).iloc[:,0])\n",
        "\n",
        "      no_disturbance_features = []\n",
        "      for scenario_feature in base_features:\n",
        "        if \"disturbance_edge_distance\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          if scenario_feature_year >= start_year: no_disturbance_features.append(minimum_disturbance_distance_name)\n",
        "          else: no_disturbance_features.append(scenario_feature)\n",
        "        elif \"disturbance_local_density\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          if scenario_feature_year >= start_year: no_disturbance_features.append(minimum_disturbance_density_name)\n",
        "          else: no_disturbance_features.append(scenario_feature)\n",
        "        elif \"forest_edge_distance\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          forest_year = start_year - 1\n",
        "          alternate_forest = f\"forest_edge_distance_{forest_year}\"\n",
        "          if scenario_feature_year > forest_year: no_disturbance_features.append(alternate_forest)\n",
        "          else: no_disturbance_features.append(scenario_feature)\n",
        "        elif \"forest_local_density\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          forest_year = start_year - 1\n",
        "          alternate_forest = f\"forest_local_density_{forest_year}\"\n",
        "          if scenario_feature_year > forest_year: no_disturbance_features.append(alternate_forest)\n",
        "          else: no_disturbance_features.append(scenario_feature)\n",
        "        else: no_disturbance_features.append(scenario_feature)\n",
        "\n",
        "      no_disturbance_scenario_filename = f\"{end_year}_no_disturbance_since_{start_year}.csv\"\n",
        "      no_disturbance_scenario_path = join(scenarios_model_dir, no_disturbance_scenario_filename)\n",
        "      pd.DataFrame(no_disturbance_features).to_csv(no_disturbance_scenario_path, index=False)\n",
        "      print(f\"Feature list for a scenario without disturbance between {start_year} and {end_year} exported to {no_disturbance_scenario_filename}.\")\n",
        "else:\n",
        "  print(\"The 'no disturbance' scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTBj-edFgbKY"
      },
      "source": [
        "## Oldgrowth scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5355gl2awb3"
      },
      "outputs": [],
      "source": [
        "# List of land-use base feature names. All but one are 'redundant'.\n",
        "# One should be selected as 'oldgrowth_feature'.\n",
        "# Both _edge_distance and _local_density variants will be modified.\n",
        "print(\"oldgrowth_redundant_features = [\")\n",
        "seen_bases = set()\n",
        "for feature in model_features:\n",
        "    if \"lu_\" in feature:\n",
        "        if \"_edge_distance\" in feature: base_name = feature.replace(\"_edge_distance\", \"\")\n",
        "        elif \"_local_density\" in feature: base_name = feature.replace(\"_local_density\", \"\")\n",
        "        else: continue\n",
        "        if base_name not in seen_bases:\n",
        "            seen_bases.add(base_name)\n",
        "            print(f'  \"{base_name}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6u03s8VHfN8"
      },
      "outputs": [],
      "source": [
        "# These alternate scenarios simulate old-growth forest using a proxy area specified by the user.\n",
        "# Forest extent (i.e. 'no deforestation') can be set from yearly feature, or all historic / potential forest area.\n",
        "# A second version of the scenario without the oldgrowth proxy will be created for comparison.\n",
        "# In rare cases, areas with unexpectedly high AGBD will have a lower AGBD estimate with the oldgrowth proxy,\n",
        "# and the highest estimate will be used for that pixel.\n",
        "\n",
        "define_oldgrowth_scenarios = True\n",
        "oldgrowth_yearly_scenarios = [\n",
        "    2021,\n",
        "    last_feature_year\n",
        "]\n",
        "\n",
        "for year in oldgrowth_yearly_scenarios:\n",
        "  assert year in final_feature_years, \"Years in 'oldgrowth_yearly_scenarios' must be available in the final yearly features.\"\n",
        "\n",
        "# oldgrowth_all_land will be created for each year in oldgrowth_yearly_scenarios.\n",
        "simulate_oldgrowth_all_land = True\n",
        "\n",
        "# Base name of the feature that best indicates oldgrowth to the model.\n",
        "# Both _edge_distance and _local_density variants will be modified.\n",
        "oldgrowth_feature = 'lu_old-growth_protected_areas'\n",
        "\n",
        "# Base names of features that may confound the old-growth proxy.\n",
        "# Both _edge_distance and _local_density variants will be removed for the old-growth scenarios.\n",
        "oldgrowth_redundant_features = [\n",
        "    \"lu_ais\",\n",
        "    \"lu_berkelah_jerantut\",\n",
        "    \"lu_berkelah_kuantan\",\n",
        "    \"lu_berkelah_temerloh\",\n",
        "    \"lu_old-growth_protected_areas\",\n",
        "    \"lu_remen_chereh\",\n",
        "    \"lu_tekai_tembeling\",\n",
        "    \"lu_tekam\",\n",
        "    \"lu_yong_lipis\",\n",
        "    \"lu_yong\",\n",
        "]\n",
        "\n",
        "# Set the edge effect distance for the alternate scenario features\n",
        "# This should match the distance used in '3_features_lcluc.ipynb'.\n",
        "edge_effect_distance = 120\n",
        "\n",
        "if define_oldgrowth_scenarios:\n",
        "  if alpha_earth: print(\"Alternate scenarios cannot be created with Alpha Earth features.\")\n",
        "  else:\n",
        "    # Expand the oldgrowth feature to the entire template area.\n",
        "    # For edge_distance: set all pixels to +threshold (interior of class 1).\n",
        "    # For local_density: set all pixels to 1.0 (100% class 1 coverage).\n",
        "    for suffix in [\"_edge_distance\",\n",
        "                  #  \"_local_density\"\n",
        "                   ]:\n",
        "      feature_name = f\"{oldgrowth_feature}{suffix}\"\n",
        "      feature_all_path = join(features_dir, f\"{feature_name}_all.tif\")\n",
        "      if not exists(feature_all_path):\n",
        "        feature_path = join(features_dir, f\"{feature_name}.tif\")\n",
        "        feature = gdal.Open(feature_path)\n",
        "        feature_array = feature.ReadAsArray()\n",
        "        feature = None\n",
        "        if suffix == \"_edge_distance\":\n",
        "          all_value = feature_array.max()\n",
        "        else:\n",
        "          all_value = 1.0\n",
        "        print(f\"The 'all' value for '{feature_name}' is {all_value}.\")\n",
        "        feature_all_array = np.full_like(feature_array, all_value)\n",
        "        export_array_as_tif(feature_all_array, feature_all_path, template=feature_path)\n",
        "        print(f\"Exported to {feature_all_path}\")\n",
        "      else: print(f\"'{feature_name}_all.tif' already exists.\\n\")\n",
        "\n",
        "    # Remove the redundant features from the oldgrowth template area.\n",
        "    # For edge_distance: set all pixels to -threshold (interior of class 0).\n",
        "    # For local_density: set all pixels to 0.0 (0% class 1 coverage).\n",
        "    for redundant_feature in oldgrowth_redundant_features:\n",
        "      for suffix in [\"_edge_distance\",\n",
        "                    #  \"_local_density\"\n",
        "                     ]:\n",
        "        feature_name = f\"{redundant_feature}{suffix}\"\n",
        "        feature_none_path = join(features_dir, f\"{feature_name}_none.tif\")\n",
        "        if not exists(feature_none_path):\n",
        "          feature_path = join(features_dir, f\"{feature_name}.tif\")\n",
        "          feature = gdal.Open(feature_path)\n",
        "          feature_array = feature.ReadAsArray()\n",
        "          feature = None\n",
        "          if suffix == \"_edge_distance\":\n",
        "            none_value = feature_array.min()\n",
        "          else:\n",
        "            none_value = 0.0\n",
        "          print(f\"The 'none' value for '{feature_name}' is {none_value}.\")\n",
        "          feature_none_array = np.full_like(feature_array, none_value)\n",
        "          export_array_as_tif(feature_none_array, feature_none_path, template=feature_path)\n",
        "          print(f\"Exported to {feature_none_path}\")\n",
        "        else: print(f\"'{feature_name}_none.tif' already exists.\\n\")\n",
        "\n",
        "    # Generate 'oldgrowth' scenarios for each specified year (i.e. forest extent in that year).\n",
        "    for year in oldgrowth_yearly_scenarios:\n",
        "      oldgrowth_features_1 = []\n",
        "      oldgrowth_features_2 = []\n",
        "      old_growth_scenario_year_diff = last_feature_year - year\n",
        "      for scenario_feature in most_recent_scenario_features:\n",
        "        if \"disturbance_edge_distance\" in scenario_feature:\n",
        "          feature_1 = feature_2 = minimum_disturbance_distance_name\n",
        "        elif \"disturbance_local_density\" in scenario_feature:\n",
        "          feature_1 = feature_2 = minimum_disturbance_density_name\n",
        "        elif \"forest_edge_distance\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          new_year = (scenario_feature_year - old_growth_scenario_year_diff > first_feature_year) and \\\n",
        "                    scenario_feature_year - old_growth_scenario_year_diff or first_feature_year\n",
        "          feature_1 = feature_2 = f\"forest_edge_distance_{new_year}\"\n",
        "        elif \"forest_local_density\" in scenario_feature:\n",
        "          scenario_feature_year = int(scenario_feature[-4:])\n",
        "          new_year = (scenario_feature_year - old_growth_scenario_year_diff > first_feature_year) and \\\n",
        "                    scenario_feature_year - old_growth_scenario_year_diff or first_feature_year\n",
        "          feature_1 = feature_2 = f\"forest_local_density_{new_year}\"\n",
        "        elif scenario_feature.startswith(oldgrowth_feature):\n",
        "          feature_1 = f\"{scenario_feature}_all\"\n",
        "          feature_2 = scenario_feature\n",
        "        elif any(scenario_feature.startswith(rf) for rf in oldgrowth_redundant_features):\n",
        "          feature_1 = f\"{scenario_feature}_none\"\n",
        "          feature_2 = scenario_feature\n",
        "        else: feature_1 = feature_2 = scenario_feature\n",
        "        oldgrowth_features_1.append(feature_1)\n",
        "        oldgrowth_features_2.append(feature_2)\n",
        "\n",
        "      # Compare feature lists and save appropriate CSVs.\n",
        "      if oldgrowth_features_1 == oldgrowth_features_2:\n",
        "        filename = f\"{year}_oldgrowth_1.csv\"\n",
        "        pd.DataFrame(oldgrowth_features_1).to_csv(join(scenarios_model_dir, filename), index=False)\n",
        "        print(f\"Feature lists were identical, only saved {filename}\")\n",
        "      else:\n",
        "        for suffix, features in [(\"1\", oldgrowth_features_1), (\"2\", oldgrowth_features_2)]:\n",
        "          filename = f\"{year}_oldgrowth_{suffix}.csv\"\n",
        "          pd.DataFrame(features).to_csv(join(scenarios_model_dir, filename), index=False)\n",
        "        print(f\"Feature lists for scenarios where all forest in {year} was old-growth\")\n",
        "        print(f\"have been exported to {year}_oldgrowth_1.csv and {year}_oldgrowth_2.csv.\\n\")\n",
        "\n",
        "    # Generate 'oldgrowth_all_land' features and scenarios.\n",
        "    if simulate_oldgrowth_all_land:\n",
        "      for year in oldgrowth_yearly_scenarios:\n",
        "        oldgrowth_all_land_name = f\"{year}_oldgrowth_all_land\"\n",
        "        annual_changes_filename = f\"tmf_AnnualChanges_Dec{year}.tif\"\n",
        "        annual_changes_path = join(feature_resampled_dir, annual_changes_filename)\n",
        "\n",
        "        # Create forest features for all land (both edge_distance and local_density).\n",
        "        forest_distance_name = f\"forest_edge_distance_{oldgrowth_all_land_name}\"\n",
        "        forest_distance_path = join(features_dir, f\"{forest_distance_name}.tif\")\n",
        "        forest_density_name = f\"forest_local_density_{oldgrowth_all_land_name}\"\n",
        "        forest_density_path = join(features_dir, f\"{forest_density_name}.tif\")\n",
        "\n",
        "        if not exists(forest_distance_path) or not exists(forest_density_path):\n",
        "          if exists(annual_changes_path):\n",
        "            annual_changes = gdal.Open(annual_changes_path)\n",
        "            annual_changes_array = annual_changes.ReadAsArray()\n",
        "            annual_changes = None\n",
        "            # Convert water (5) to 0, non-water to 1.\n",
        "            forest_binary_array = np.where(annual_changes_array == 5, 0, 1)\n",
        "            distance_array, density_array = edge_effects(\n",
        "                forest_binary_array, 'binary',\n",
        "                cell_size_x_path, cell_size_y_path,\n",
        "                edge_effect_distance\n",
        "            )\n",
        "            if not exists(forest_distance_path):\n",
        "              export_array_as_tif(distance_array, forest_distance_path)\n",
        "              print(f\"{forest_distance_name} has been created and saved to\\n{features_dir}\\n\")\n",
        "            if not exists(forest_density_path):\n",
        "              export_array_as_tif(density_array, forest_density_path)\n",
        "              print(f\"{forest_density_name} has been created and saved to\\n{features_dir}\\n\")\n",
        "          else: print(f\"The TMF annual changes {year} raster needed for '{oldgrowth_all_land_name}' is not in:\\n{annual_changes_path}\\n\")\n",
        "        else:\n",
        "          print(f\"{forest_distance_name} already exists in\\n{features_dir}\\n\")\n",
        "          print(f\"{forest_density_name} already exists in\\n{features_dir}\\n\")\n",
        "\n",
        "        if exists(forest_distance_path) and exists(forest_density_path):\n",
        "          oldgrowth_all_features_1 = []\n",
        "          oldgrowth_all_features_2 = []\n",
        "          for scenario_feature in most_recent_scenario_features:\n",
        "            if \"disturbance_edge_distance\" in scenario_feature:\n",
        "              feature_1 = feature_2 = minimum_disturbance_distance_name\n",
        "            elif \"disturbance_local_density\" in scenario_feature:\n",
        "              feature_1 = feature_2 = minimum_disturbance_density_name\n",
        "            elif \"forest_edge_distance\" in scenario_feature:\n",
        "              feature_1 = feature_2 = forest_distance_name\n",
        "            elif \"forest_local_density\" in scenario_feature:\n",
        "              feature_1 = feature_2 = forest_density_name\n",
        "            elif scenario_feature.startswith(oldgrowth_feature):\n",
        "              feature_1 = f\"{scenario_feature}_all\"\n",
        "              feature_2 = scenario_feature\n",
        "            elif any(scenario_feature.startswith(rf) for rf in oldgrowth_redundant_features):\n",
        "              feature_1 = f\"{scenario_feature}_none\"\n",
        "              feature_2 = scenario_feature\n",
        "            else: feature_1 = feature_2 = scenario_feature\n",
        "            oldgrowth_all_features_1.append(feature_1)\n",
        "            oldgrowth_all_features_2.append(feature_2)\n",
        "\n",
        "          # Compare feature lists and save appropriate CSVs.\n",
        "          if oldgrowth_all_features_1 == oldgrowth_all_features_2:\n",
        "            filename = f\"{oldgrowth_all_land_name}_1.csv\"\n",
        "            pd.DataFrame(oldgrowth_all_features_1).to_csv(join(scenarios_model_dir, filename), index=False)\n",
        "            print(f\"Feature lists were identical, only saved {filename}\")\n",
        "          else:\n",
        "            for suffix, features in [(\"1\", oldgrowth_all_features_1), (\"2\", oldgrowth_all_features_2)]:\n",
        "              filename = f\"{oldgrowth_all_land_name}_{suffix}.csv\"\n",
        "              pd.DataFrame(features).to_csv(join(scenarios_model_dir, filename), index=False)\n",
        "            print(f\"Feature lists for {oldgrowth_all_land_name} have been exported to {oldgrowth_all_land_name}_1.csv and {oldgrowth_all_land_name}_2.csv.\\n\")\n",
        "\n",
        "        # Create an all-land forest mask.\n",
        "        oldgrowth_all_mask_path = join(masks_dir, f\"mask_forest_{oldgrowth_all_land_name}.tif\")\n",
        "        if not exists(oldgrowth_all_mask_path):\n",
        "          annual_changes = gdal.Open(annual_changes_path)\n",
        "          annual_changes_array = annual_changes.ReadAsArray()\n",
        "          annual_changes = None\n",
        "          oldgrowth_all_mask_array = np.where(annual_changes_array == 5, nodatavalue, 1)\n",
        "          export_array_as_tif(oldgrowth_all_mask_array, oldgrowth_all_mask_path)\n",
        "          print(f\"A mask for {oldgrowth_all_land_name} has been created at\\n{oldgrowth_all_mask_path}\\n\")\n",
        "        else: print(f\"A mask for {oldgrowth_all_land_name} already exists at\\n{oldgrowth_all_mask_path}\\n\")\n",
        "\n",
        "else: print(\"Old-growth scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9w2IzcgVNvx"
      },
      "source": [
        "## Area-based disturbance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQRgPdbiVNVF"
      },
      "outputs": [],
      "source": [
        "# Use polygons to select areas for alternate scenarios of area-based disturbance\n",
        "define_area_based_disturbance = True\n",
        "\n",
        "# Set the edge effect distance for the alternate scenario features\n",
        "# This should match the distance used in '3_features_lcluc.ipynb'.\n",
        "edge_effect_distance = 120\n",
        "\n",
        "if define_area_based_disturbance:\n",
        "  if alpha_earth: print(\"Alternate scenarios cannot be created with Alpha Earth features.\")\n",
        "  else:\n",
        "    # Exclude existing polygons from search\n",
        "    polygons_to_exclude = ['project_area.gpkg', 'project_area_buffered_bbox.gpkg', 'gedi_area.gpkg', 'template.gpkg']\n",
        "    print(\"# Modify this dictionary by:\")\n",
        "    print(\"# 1) Commenting out any polygons not being used for disturbance.\")\n",
        "    print(\"# 2) Changing the type from 'deforestation' to 'degradation' if necessary.\")\n",
        "    print(\"# 3) Changing the tuple years from 'range' to 'discrete' to specify individual years.\")\n",
        "    print(\"# 4) If years are discrete, add one or more. If a range, add the start and end year.\")\n",
        "    print(\"# 5) Changing the alternate scenario year for each area if needed.\")\n",
        "    print(\"# 6) Copy and paste lines for multiple scenarios with the same area (different disturbance types, different years).\")\n",
        "    print(\"# Remember to ensure all keys are unique - if you copy and paste, manually change the keys.\\n\")\n",
        "\n",
        "    # Exclude existing polygons from search\n",
        "    polygons_to_exclude = ['project_area.gpkg', 'gedi_area.gpkg', 'template.gpkg']\n",
        "    exclude_lu_polygons = False\n",
        "\n",
        "    print(\"disturbance_polygons = {\")\n",
        "    index = 1\n",
        "    first_disturbance_year = last_feature_year - model_scenario_year_range\n",
        "    for polygon in sorted(os.listdir(polygons_dir)):\n",
        "      if polygon not in polygons_to_exclude and 'inverse' not in polygon and 'buffered' not in polygon:\n",
        "        if not exclude_lu_polygons:\n",
        "          print(f\"    {index}: ['{polygon[:-5]}', 'deforestation','range', ({first_disturbance_year}, {last_feature_year}), {last_feature_year}],\")\n",
        "          index += 1\n",
        "        if exclude_lu_polygons and 'lu_' not in polygon:\n",
        "          print(f\"    {index}: ['{polygon[:-5]}', 'deforestation','range', ({first_disturbance_year}, {last_feature_year}), {last_feature_year}],\")\n",
        "          index += 1\n",
        "    print(\"}\\n\")\n",
        "\n",
        "else: print(\"Area-based disturbance scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI_Ck3HQVWLo"
      },
      "outputs": [],
      "source": [
        "# The alternate year is set to 2024.\n",
        "# The years for alternate area-based disturbance can be between 1996 and 2024\n",
        "\n",
        "# Modify this dictionary by:\n",
        "# 1) Commenting out any polygons not being used for disturbance.\n",
        "# 2) Changing the type from 'deforestation' to 'degradation' if necessary.\n",
        "# 3) Changing the tuple years from 'range' to 'discrete' to specify individual years.\n",
        "# 4) If years are discrete, add one or more. If a range, add the start and end year.\n",
        "# 5) Copy and paste lines for multiple scenarios with the same area (different disturbance types, different years).\n",
        "# Remember to ensure all keys are unique - if you copy and paste, manually change the keys.\n",
        "\n",
        "if define_area_based_disturbance:\n",
        "    if alpha_earth: print(\"Alternate scenarios cannot be created with Alpha Earth features.\")\n",
        "    else:\n",
        "      disturbance_polygons = {\n",
        "          3: ['road_mat_daling', 'deforestation','range', (2023, 2024), 2024],\n",
        "      }\n",
        "\n",
        "      # Validate disturbance types, year types and available years.\n",
        "      for area_index, value in disturbance_polygons.items():\n",
        "          polygon_name = value[0]\n",
        "          disturbance_type = value[1]\n",
        "          year_type, years_data = value[2], value[3]\n",
        "          alternate_scenario_year = value[4]\n",
        "\n",
        "          # Calculate first available disturbance year for this area's alternate scenario year\n",
        "          first_disturbance_year = alternate_scenario_year - model_scenario_year_range\n",
        "\n",
        "          # Validate alternate scenario year\n",
        "          assert alternate_scenario_year >= minimum_yearly_scenario, f\"Alternate scenario year for {polygon_name} must be between {minimum_yearly_scenario} and {last_feature_year}.\"\n",
        "          assert alternate_scenario_year <= last_feature_year, f\"Alternate scenario year for {polygon_name} must be between {minimum_yearly_scenario} and {last_feature_year}.\"\n",
        "          # Validate disturbance types and year types\n",
        "          assert disturbance_type in ['deforestation', 'degradation'], f\"Disturbance type for {polygon_name} must be 'deforestation' or 'degradation'.\"\n",
        "          if year_type == 'range':\n",
        "            start_year, end_year = years_data\n",
        "            assert start_year <= end_year, f\"The start year for {polygon_name} {disturbance_type} must be before the end year.\"\n",
        "\n",
        "          # Validate deforestation constraints\n",
        "          if disturbance_type == 'deforestation':\n",
        "            assert year_type == 'range', f\"Year type for {polygon_name} deforestation must be 'range'.\"\n",
        "            assert end_year == alternate_scenario_year, f\"Deforestation in {polygon_name} must end in the alternate scenario year {alternate_scenario_year}. Deforestation is considered permanent land-cover change.\"\n",
        "            assert start_year >= first_disturbance_year, f\"The start year for deforestation in {polygon_name} must be >= the first available disturbance year {first_disturbance_year}.\"\n",
        "            all_years = list(range(start_year, end_year + 1))\n",
        "\n",
        "          # Validate degradation constraints\n",
        "          if disturbance_type == 'degradation':\n",
        "            assert year_type in ['range', 'discrete'], f\"Year type for {polygon_name} degradation must be 'range' or 'discrete'.\"\n",
        "            if year_type == 'range':\n",
        "                all_years = list(range(start_year, end_year + 1))\n",
        "            else: all_years = list(years_data)\n",
        "            for year in all_years:\n",
        "                assert year <= alternate_scenario_year, f\"Years for {polygon_name} degradation (check {year}) must be <= the alternate scenario year {alternate_scenario_year}.\"\n",
        "                assert year >= first_disturbance_year, f\"Years for {polygon_name} degradation (check {year}) must >= the first available disturbance year {first_disturbance_year}.\"\n",
        "\n",
        "          # Simplify dictionary\n",
        "          disturbance_polygons[area_index] = [polygon_name, disturbance_type, all_years, alternate_scenario_year]\n",
        "\n",
        "      print(\"The 'disturbance_polygons' dictionary is valid.\")\n",
        "else: print(\"Area-based disturbance scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enzNAFf17h6P"
      },
      "outputs": [],
      "source": [
        "if define_area_based_disturbance:\n",
        "    if alpha_earth: print(\"Alternate scenarios cannot be created with Alpha Earth features.\")\n",
        "    else:\n",
        "      # Define the projects CRS to check the area polygon matches\n",
        "      crs_epsg = 4326\n",
        "      # This setting buffers any deforestation area to add degradation around it.\n",
        "      # At least 1 pixel distance (e.g. 30 m) might be realistic. Otherwise set to None.\n",
        "      buffer_distance_metres = 30\n",
        "\n",
        "      # Define a temporary directory for copying binary rasters and burning the area's polygon\n",
        "      binary_temp_dir = join(scenarios_model_dir, 'binary_temp')\n",
        "      makedirs(binary_temp_dir, exist_ok=True)\n",
        "\n",
        "      # Calculate progress totals before processing\n",
        "      total_areas = len(disturbance_polygons)\n",
        "      total_rasters = 0\n",
        "\n",
        "      for area_index, parameters in disturbance_polygons.items():\n",
        "          polygon_name = parameters[0]\n",
        "          disturbance_type = parameters[1]\n",
        "          disturbance_years = parameters[2]\n",
        "\n",
        "          # Raster counting: degradation = years x 2, deforestation = years x 5\n",
        "          # (edge_distance + local_density for disturbance, plus same for forest, plus mask)\n",
        "          if disturbance_type == 'degradation':\n",
        "              area_raster_count = len(disturbance_years) * 2\n",
        "          elif disturbance_type == 'deforestation':\n",
        "              area_raster_count = len(disturbance_years) * 5\n",
        "\n",
        "          total_rasters += area_raster_count\n",
        "\n",
        "      # Progress indicators\n",
        "      area_progress_index, area_progress_label = 0, widgets.Label(value=f\"Area progress: 0 / {total_areas}\")\n",
        "      display(area_progress_label)\n",
        "      raster_progress_index, raster_progress_label = 0, widgets.Label(value=f\"Raster progress: 0 / {total_rasters}\")\n",
        "      display(raster_progress_label)\n",
        "\n",
        "      for area_index, parameters in disturbance_polygons.items():\n",
        "\n",
        "          # Extract alternate area-based disturbance parameters\n",
        "          area_disturbance_features = []\n",
        "          polygon_name = parameters[0]\n",
        "          disturbance_type = parameters[1]\n",
        "          disturbance_years = parameters[2]\n",
        "          alternate_scenario_year = parameters[3]\n",
        "\n",
        "          # Determine base features by the alternate scenario's year for this area\n",
        "          alternate_year_scenario_csv = join(scenarios_model_dir, f\"{alternate_scenario_year}.csv\")\n",
        "          base_features = pd.Series.tolist(pd.read_csv(alternate_year_scenario_csv).iloc[:,0])\n",
        "\n",
        "          # Define area polygon\n",
        "          area_polygon_path = join(polygons_dir, f\"{polygon_name}.gpkg\")\n",
        "          if disturbance_type == 'deforestation' and buffer_distance_metres:\n",
        "            area_buffered_path = join(polygons_dir, f\"{polygon_name}_buffered_{buffer_distance_metres}.gpkg\")\n",
        "            if not exists(area_buffered_path):\n",
        "              area_polygon = gpd.read_file(join(polygons_dir, f\"{polygon_name}.gpkg\"))\n",
        "              if area_polygon.crs.to_epsg() == crs_epsg:\n",
        "                # Suppress warning about not being a geographic CRS, as we account for this.\n",
        "                # However larger buffers or project areas near the poles might still need to be converted.\n",
        "                warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "                # Get the centroid of the project polygon\n",
        "                area_polygon_centroid = area_polygon.centroid.values[0]\n",
        "                # Convert the buffer distance from meters to decimal degrees based on the location at the centroid\n",
        "                buffer_distance_degrees = buffer_distance_metres / (111320 * abs(math.cos(math.radians(area_polygon_centroid.y))))\n",
        "                # Buffer the polygon and save\n",
        "                area_polygon_buffered = area_polygon.buffer(buffer_distance_degrees)\n",
        "                gdf = gpd.GeoDataFrame(geometry=area_polygon_buffered, crs=f\"EPSG:{crs_epsg}\")\n",
        "                gdf.to_file(area_buffered_path, driver='GPKG')\n",
        "                print(f\"Buffered the project area to {buffer_distance_metres} and exported to the polygons directory.\")\n",
        "              else: print(f\"Reproject {polygon_name}.gpkg to EPSG:4326.\")\n",
        "          else: area_buffered_path = None\n",
        "\n",
        "          # Track which binary rasters have been processed to avoid duplicate edge_effects calls\n",
        "          processed_disturbance_years = set()\n",
        "          processed_forest_years = set()\n",
        "\n",
        "          for scenario_feature in base_features:\n",
        "              # Handle disturbance edge_distance features\n",
        "              if \"disturbance_edge_distance\" in scenario_feature:\n",
        "                  scenario_feature_year = int(scenario_feature[-4:])\n",
        "                  if scenario_feature_year in disturbance_years:\n",
        "                    # Define feature names for both edge_distance and local_density\n",
        "                    if disturbance_type == 'deforestation':\n",
        "                      distance_name = f\"disturbance_edge_distance_{scenario_feature_year}_{polygon_name}_deforestation_{buffer_distance_metres}m_buffer\"\n",
        "                      density_name = f\"disturbance_local_density_{scenario_feature_year}_{polygon_name}_deforestation_{buffer_distance_metres}m_buffer\"\n",
        "                    else:\n",
        "                      distance_name = f\"disturbance_edge_distance_{scenario_feature_year}_{polygon_name}_degradation\"\n",
        "                      density_name = f\"disturbance_local_density_{scenario_feature_year}_{polygon_name}_degradation\"\n",
        "                    distance_path = join(features_dir, f\"{distance_name}.tif\")\n",
        "                    density_path = join(features_dir, f\"{density_name}.tif\")\n",
        "\n",
        "                    if not exists(distance_path) or not exists(density_path):\n",
        "                      # Copy the disturbance binary raster for burning '1' to the polygon area\n",
        "                      binary_raster_name = f\"disturbance_binary_{scenario_feature_year}.tif\"\n",
        "                      binary_raster_path = join(feature_binary_dir, binary_raster_name)\n",
        "                      binary_raster_temp_path = join(binary_temp_dir, binary_raster_name)\n",
        "                      copyfile(binary_raster_path, binary_raster_temp_path)\n",
        "                      if area_buffered_path: burn_polygon_to_raster(binary_raster_temp_path, area_buffered_path, fixed_value=1, all_touched=True)\n",
        "                      else: burn_polygon_to_raster(binary_raster_temp_path, area_polygon_path, fixed_value=1, all_touched=True)\n",
        "                      # Apply edge effects and export both arrays\n",
        "                      binary_burned = gdal.Open(binary_raster_temp_path)\n",
        "                      binary_burned_array = binary_burned.ReadAsArray()\n",
        "                      binary_burned = None\n",
        "                      distance_array, density_array = edge_effects(binary_burned_array, 'binary', cell_size_x_path, cell_size_y_path, edge_effect_distance)\n",
        "                      if not exists(distance_path): export_array_as_tif(distance_array, distance_path)\n",
        "                      if not exists(density_path): export_array_as_tif(density_array, density_path)\n",
        "\n",
        "                    area_disturbance_features.append(distance_name)\n",
        "                    processed_disturbance_years.add(scenario_feature_year)\n",
        "                    raster_progress_index += 2\n",
        "                    raster_progress_label.value = f\"Raster progress: {raster_progress_index} / {total_rasters}\"\n",
        "                  else: area_disturbance_features.append(scenario_feature)\n",
        "\n",
        "              # Handle disturbance local_density features\n",
        "              elif \"disturbance_local_density\" in scenario_feature:\n",
        "                  scenario_feature_year = int(scenario_feature[-4:])\n",
        "                  if scenario_feature_year in disturbance_years:\n",
        "                    # Feature was created when processing edge_distance\n",
        "                    if disturbance_type == 'deforestation':\n",
        "                      density_name = f\"disturbance_local_density_{scenario_feature_year}_{polygon_name}_deforestation_{buffer_distance_metres}m_buffer\"\n",
        "                    else:\n",
        "                      density_name = f\"disturbance_local_density_{scenario_feature_year}_{polygon_name}_degradation\"\n",
        "                    area_disturbance_features.append(density_name)\n",
        "                  else: area_disturbance_features.append(scenario_feature)\n",
        "\n",
        "              # Handle forest edge_distance features (deforestation only)\n",
        "              elif \"forest_edge_distance\" in scenario_feature:\n",
        "                  if disturbance_type == 'deforestation':\n",
        "                      scenario_feature_year = int(scenario_feature[-4:])\n",
        "                      if scenario_feature_year in disturbance_years:\n",
        "                        # Define feature names for both edge_distance and local_density\n",
        "                        distance_name = f\"forest_edge_distance_{scenario_feature_year}_{polygon_name}_deforestation\"\n",
        "                        density_name = f\"forest_local_density_{scenario_feature_year}_{polygon_name}_deforestation\"\n",
        "                        distance_path = join(features_dir, f\"{distance_name}.tif\")\n",
        "                        density_path = join(features_dir, f\"{density_name}.tif\")\n",
        "\n",
        "                        if not exists(distance_path) or not exists(density_path):\n",
        "                          # Copy the forest binary raster for burning '0' to the polygon area\n",
        "                          binary_raster_name = f\"forest_binary_{scenario_feature_year}.tif\"\n",
        "                          binary_raster_path = join(feature_binary_dir, binary_raster_name)\n",
        "                          binary_raster_temp_path = join(binary_temp_dir, binary_raster_name)\n",
        "                          copyfile(binary_raster_path, binary_raster_temp_path)\n",
        "                          burn_polygon_to_raster(binary_raster_temp_path, area_polygon_path, fixed_value=0, all_touched=True)\n",
        "                          # Apply edge effects and export both arrays\n",
        "                          binary_burned = gdal.Open(binary_raster_temp_path)\n",
        "                          binary_burned_array = binary_burned.ReadAsArray()\n",
        "                          binary_burned = None\n",
        "                          distance_array, density_array = edge_effects(binary_burned_array, 'binary', cell_size_x_path, cell_size_y_path, edge_effect_distance)\n",
        "                          if not exists(distance_path): export_array_as_tif(distance_array, distance_path)\n",
        "                          if not exists(density_path): export_array_as_tif(density_array, density_path)\n",
        "\n",
        "                        area_disturbance_features.append(distance_name)\n",
        "                        processed_forest_years.add(scenario_feature_year)\n",
        "                        raster_progress_index += 2\n",
        "                        raster_progress_label.value = f\"Raster progress: {raster_progress_index} / {total_rasters}\"\n",
        "                      else: area_disturbance_features.append(scenario_feature)\n",
        "                  else: area_disturbance_features.append(scenario_feature)\n",
        "\n",
        "              # Handle forest local_density features (deforestation only)\n",
        "              elif \"forest_local_density\" in scenario_feature:\n",
        "                  if disturbance_type == 'deforestation':\n",
        "                      scenario_feature_year = int(scenario_feature[-4:])\n",
        "                      if scenario_feature_year in disturbance_years:\n",
        "                        # Feature was created when processing edge_distance\n",
        "                        density_name = f\"forest_local_density_{scenario_feature_year}_{polygon_name}_deforestation\"\n",
        "                        area_disturbance_features.append(density_name)\n",
        "                      else: area_disturbance_features.append(scenario_feature)\n",
        "                  else: area_disturbance_features.append(scenario_feature)\n",
        "\n",
        "              else: area_disturbance_features.append(scenario_feature)\n",
        "\n",
        "          # Add name affix based on whether years are a range or discrete\n",
        "          if disturbance_type == \"deforestation\": year_affix = f'_{min(disturbance_years)}'\n",
        "          else:\n",
        "            if len(disturbance_years) != (max(disturbance_years) - min(disturbance_years) + 1):\n",
        "                sorted_years = sorted(disturbance_years)\n",
        "                parts, start = [], sorted_years[0]\n",
        "                for i, year in enumerate(sorted_years[1:] + [None], 1):\n",
        "                    if year != sorted_years[i-1] + 1:\n",
        "                        end = sorted_years[i-1]\n",
        "                        parts.append(f\"{start}-{end}\" if start != end else str(start))\n",
        "                        start = year\n",
        "                year_affix = \"_\" + \"_\".join(parts)\n",
        "            else: year_affix = f'_{min(disturbance_years)}-{max(disturbance_years)}'\n",
        "\n",
        "          if disturbance_type == 'deforestation':\n",
        "            if buffer_distance_metres:\n",
        "              area_disturbance_scenario_name = f\"{alternate_scenario_year}_{polygon_name}_deforestation{year_affix}_{buffer_distance_metres}m_degradation_buffer\"\n",
        "            else: area_disturbance_scenario_name = f\"{alternate_scenario_year}_{polygon_name}_deforestation{year_affix}_0m_degradation_buffer\"\n",
        "            # Create a new forest mask for the area-based disturbance scenario\n",
        "            mask_raster_path = join(masks_dir, f\"mask_forest_{alternate_scenario_year}_{polygon_name}_deforestation.tif\")\n",
        "            if not exists(mask_raster_path):\n",
        "              # Ensure original forest binary is copied to temp and burned with polygon\n",
        "              scenario_year_forest_binary_path = join(binary_temp_dir, f\"forest_binary_{alternate_scenario_year}.tif\")\n",
        "              forest_binary_source = join(feature_binary_dir, f\"forest_binary_{alternate_scenario_year}.tif\")\n",
        "              copyfile(forest_binary_source, scenario_year_forest_binary_path)\n",
        "              burn_polygon_to_raster(scenario_year_forest_binary_path, area_polygon_path, fixed_value=0, all_touched=True)\n",
        "              # Create mask from burned forest data\n",
        "              scenario_year_forest_binary = gdal.Open(scenario_year_forest_binary_path)\n",
        "              scenario_year_forest_binary_array = scenario_year_forest_binary.ReadAsArray()\n",
        "              scenario_year_forest_binary = None\n",
        "              mask_array = np.where(scenario_year_forest_binary_array == 0, nodatavalue, 1)\n",
        "              export_array_as_tif(mask_array, mask_raster_path)\n",
        "              print(f\"A mask raster has been created for {area_disturbance_scenario_name}.\")\n",
        "            raster_progress_index += 1\n",
        "            raster_progress_label.value = f\"Raster progress: {raster_progress_index} / {total_rasters}\"\n",
        "\n",
        "          else: area_disturbance_scenario_name = f\"{alternate_scenario_year}_{polygon_name}_degradation{year_affix}\"\n",
        "\n",
        "          # Clear temporary binary raster folder\n",
        "          for temp_file in os.listdir(binary_temp_dir): os.remove(join(binary_temp_dir, temp_file))\n",
        "\n",
        "          # Export the alternate area-based disturbance scenario\n",
        "          no_degradation_scenario_path = join(scenarios_model_dir, f\"{area_disturbance_scenario_name}.csv\")\n",
        "          pd.DataFrame(area_disturbance_features).to_csv(no_degradation_scenario_path, index=False)\n",
        "          print(f\"Feature list for {area_disturbance_scenario_name} has been exported.\")\n",
        "\n",
        "          # Update area progress\n",
        "          area_progress_index += 1\n",
        "          area_progress_label.value = f\"Area progress: {area_progress_index} / {total_areas}\"\n",
        "\n",
        "      print(\"\\nAlternate area-based disturbance scenarios complete.\")\n",
        "else: print(\"Area-based disturbance scenarios are not enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy2nfHshozVQ"
      },
      "source": [
        "# Feature verification (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chCSZTV4XB8k"
      },
      "outputs": [],
      "source": [
        "# Check all features in scenario .csvs exist\n",
        "scenario_csv_list = []\n",
        "all_features_exist = True # Changes to false if feature missing\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    csv_dir = join(scenarios_model_dir, csv)\n",
        "    csv_feature_list = pd.Series.tolist(pd.read_csv(csv_dir).iloc[:,0])\n",
        "    csv_feature_dir_list = []\n",
        "    for csv_feature in csv_feature_list:\n",
        "      if csv_feature not in covariates: csv_feature_dir_list.append(f\"{features_dir}/{csv_feature}.tif\")\n",
        "    for feature in csv_feature_dir_list:\n",
        "      if not exists(feature):\n",
        "        all_features_exist = False\n",
        "        print(f\"The following feature is missing:\\n{feature}\\n and is required for the scenario '{csv[:-4]}'\")\n",
        "\n",
        "if all_features_exist: print(\"All required features are present.\")\n",
        "print(\"Covariate features e.g. 'beam' and 'sensitivity' will be added at the prediction stage.\")\n",
        "\n",
        "# Check all features against template dimensions\n",
        "if not alpha_earth: # Higher resolution than template\n",
        "  scenario_template = gdal.Open(template_tif_path)\n",
        "  scenario_template_dimensions, scenario_template_projection = scenario_template.GetGeoTransform(), scenario_template.GetProjection()\n",
        "  scenario_template = None\n",
        "  feature_issue = False\n",
        "  for feature in os.listdir(features_dir):\n",
        "    if feature.endswith('.tif'):\n",
        "      feature_dir = join(features_dir, feature)\n",
        "      feature = gdal.Open(feature_dir)\n",
        "      feature_dimensions, feature_projection = feature.GetGeoTransform(), feature.GetProjection()\n",
        "      feature = None\n",
        "      if feature_dimensions != scenario_template_dimensions:\n",
        "        print(f\"{feature} dimensions:\\n{feature_dimensions}\\ndo not match the scenario template dimensions:\\n{scenario_template_dimensions}\\n\")\n",
        "        feature_issue = True\n",
        "      if feature_projection != scenario_template_projection:\n",
        "        print(f\"{feature} projection:\\n{feature_projection}\\ndoes not match the scenario template projection:\\n{scenario_template_projection}\\n\\n\")\n",
        "        feature_issue = True\n",
        "  if not feature_issue: print(f\"All features in the following directory have the correct dimensions and projection:\\n{features_dir}\")\n",
        "  else: print(\"Correct and / or resample the feature(s).\")\n",
        "else: print(\"AlphaEarth features cannot be mixed with other features unless resampled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T9UqJrzWECr"
      },
      "source": [
        "# Tiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zizOofP5dgW"
      },
      "outputs": [],
      "source": [
        "# Load the model scenario features for template tile creation\n",
        "model_scenario_features = pd.Series.tolist(pd.read_csv(model_scenario_dir).iloc[:,0])\n",
        "model_scenario_features_dirs = [features_dir + '/' + feature + '.tif' for feature in model_scenario_features]\n",
        "# Create a template feature array from the first feature that isn't a covariate (these are created later)\n",
        "template_base_path = next(r for r in model_scenario_features_dirs if all(c not in r for c in covariates))\n",
        "template_base = gdal.Open(template_base_path)\n",
        "template_base_array = template_base.ReadAsArray()\n",
        "template_base_xsize, template_base_ysize = template_base.GetRasterBand(1).XSize, template_base.GetRasterBand(1).YSize\n",
        "template_base = None\n",
        "print(f\"The template feature is {template_base_xsize} x {template_base_ysize} pixels.\")\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles_exist = len(template_tile_list)\n",
        "\n",
        "if n_tiles_exist < 1: print(\"There are currently no template tiles. Run the next section.\")\n",
        "if n_tiles_exist >= 1:\n",
        "  tile_size_y_rounded_exist_ds = gdal.Open(join(tile_templates_dir,'template_tile_1.tif'))\n",
        "  tile_size_y_rounded_exist = tile_size_y_rounded_exist_ds.GetRasterBand(1).YSize\n",
        "  tile_size_y_rounded_exist_ds = None\n",
        "  tile_size_y_remainder_exist_ds = gdal.Open(join(tile_templates_dir,f'template_tile_{n_tiles_exist}.tif'))\n",
        "  tile_size_y_remainder_exist = tile_size_y_remainder_exist_ds.GetRasterBand(1).YSize\n",
        "  tile_size_y_remainder_exist_ds = None\n",
        "  if n_tiles_exist == 1:\n",
        "    print(f\"There is a single 'tile' with a height of {tile_size_y_rounded_exist} pixels.\")\n",
        "    tile_size_y_remainder_exist = 0\n",
        "  else:  print(f\"There are {n_tiles_exist} template tiles, the first {n_tiles_exist-1} having a height of {tile_size_y_rounded_exist} pixels, the last {tile_size_y_remainder_exist} pixels.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njhqSH8e5cPu"
      },
      "outputs": [],
      "source": [
        "# Large template areas and / or numbers of features may be too much for the available memory.\n",
        "# This section defines how to split predictions into tiles that can then be merged.\n",
        "override_n_tiles = True  # Useful if the tile number has already been tested.\n",
        "n_tiles_override = 1\n",
        "tile_size_y_rounded_override = int(20000/n_tiles_override)\n",
        "\n",
        "memory_utilisation = 0.8 # Set to 0.8 to ensure crashes are avoided\n",
        "\n",
        "assert memory_utilisation > 0 and memory_utilisation <= 1, \"Set memory_utilisation to a value between 0 and 1\"\n",
        "\n",
        "# Calculate total size of feature stack\n",
        "feature_stack_size = template_base_array.size * len(model_scenario_features_dirs)\n",
        "\n",
        "# Calculate memory and the number of tiles required\n",
        "total_memory_needed = 64 / 8 * feature_stack_size\n",
        "print(f'RAM required for each prediction: ~{total_memory_needed/(1024**3):.3f} GB')\n",
        "print(f'RAM currently available: {psutil.virtual_memory().free / (1024**3):.3f} GB')\n",
        "n_tiles_temp = int(np.ceil(total_memory_needed / (psutil.virtual_memory().free * memory_utilisation)))\n",
        "\n",
        "# Calculate template tile size (split on the y axis only)\n",
        "tile_size_y_rounded = int(np.ceil(template_base_ysize/n_tiles_temp)) # Round the number of y pixels in each tile\n",
        "tile_size_y_remainder = template_base_ysize%tile_size_y_rounded # Calculate the remainder for the last tile\n",
        "n_tiles = max(1, len(range(0, template_base_ysize, tile_size_y_rounded))) # Update the number of tiles to include the remainder\n",
        "\n",
        "if override_n_tiles:\n",
        "  tile_size_y_rounded = tile_size_y_rounded_override\n",
        "  tile_size_y_remainder = template_base_ysize%tile_size_y_rounded\n",
        "  n_tiles = n_tiles_override\n",
        "  print(\"n_tiles has been overridden.\")\n",
        "\n",
        "print(f'The prediction template should be divided into {n_tiles} tiles to avoid crashing.')\n",
        "\n",
        "# Check if tiles need to be changed\n",
        "change_tiles = True\n",
        "if override_n_tiles:\n",
        "  if n_tiles == n_tiles_exist: change_tiles = False\n",
        "if n_tiles == n_tiles_exist and tile_size_y_rounded == tile_size_y_rounded_exist and tile_size_y_remainder == tile_size_y_remainder_exist:\n",
        "  change_tiles = False\n",
        "\n",
        "if change_tiles:\n",
        "  # Clear all tile directories\n",
        "  for tile in Path(tile_templates_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for tile in Path(tile_features_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for scenario_stack_dir in Path(tile_feature_stacks_dir).glob(\"**/*\"):\n",
        "    shutil.rmtree(scenario_stack_dir)\n",
        "  for tile in Path(tile_prediction_cache_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "\n",
        "  # Generate new tile templates based on available memory\n",
        "  tile_number = 1\n",
        "  for y_start in range(0, template_base_ysize, tile_size_y_rounded):\n",
        "    if tile_size_y_remainder != 0 and tile_number == n_tiles: tile_size_y = tile_size_y_remainder\n",
        "    else: tile_size_y = tile_size_y_rounded\n",
        "    tiling_string = \"gdal_translate -of GTIFF -srcwin \" + str(0)+ \", \" + str(y_start) + \", \" + str(template_base_xsize) + \", \" + str(tile_size_y) + \" \" + str(template_base_path) + \" \" + str(tile_templates_dir) + \"/template_tile_\" + str(tile_number) + \".tif\"\n",
        "    os.system(tiling_string)\n",
        "    tile_number += 1\n",
        "  print(\"Template tile creation complete.\")\n",
        "\n",
        "else: print(\"No changes to existing tiles are required.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P_nom3fcuJu"
      },
      "outputs": [],
      "source": [
        "# Create feature tiles.\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "template_tile_1 = gdal.Open(join(tile_templates_dir, 'template_tile_1.tif'))\n",
        "c = template_tile_1.GetRasterBand(1).YSize\n",
        "template_tile_1 = None\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "if n_tiles == 1: print(\"Feature tile creation skipped. Feature stack creation will use the original features.\")\n",
        "else:\n",
        "  # Progress\n",
        "  n_features = len(os.listdir(features_dir))\n",
        "  feature_progress_index, feature_progress_label = 0, widgets.Label(value=f\"Feature progress: 0 / {n_features}\")\n",
        "  display(feature_progress_label)\n",
        "  tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "  display(tile_progress_label)\n",
        "\n",
        "  # Loop through each feature in the 6_scenarios features directory\n",
        "  for feature in os.listdir(features_dir):\n",
        "    # Create list of tile directories\n",
        "    feature_dir = join(features_dir, feature)\n",
        "    feature = gdal.Open(feature_dir)\n",
        "    feature_array = feature.ReadAsArray()\n",
        "    feature = None\n",
        "    # Split the feature array into chunks based on tile size\n",
        "    feature_chunks = np.array_split(feature_array, np.arange(tile_size_y_rounded, len(feature_array), tile_size_y_rounded))\n",
        "    tile_count = 1\n",
        "    # Loop through tiles and export as .tif\n",
        "    for tile in range(n_tiles):\n",
        "      feature_tile_filename = f\"{feature[:-4]}_{tile_count}.tif\"\n",
        "      # Check if tile already exists\n",
        "      feature_tile_exists = False\n",
        "      for feature_tile in os.listdir(tile_features_dir):\n",
        "        if feature_tile == feature_tile_filename: feature_tile_exists=True\n",
        "        # If feature tile does not exist:\n",
        "      if feature_tile_exists == False:\n",
        "        template_tile_dir = join(tile_templates_dir, f\"template_tile_{tile_count}.tif\")\n",
        "        export_array_as_tif(feature_chunks[tile_count-1], join(tile_features_dir,feature_tile_filename), template_tile_dir, compress = False)\n",
        "        tile_count += 1\n",
        "      # Update tile progress\n",
        "      tile_progress_index += 1\n",
        "      tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_tiles}\"\n",
        "    tile_progress_index = 0\n",
        "    # Update feature progress\n",
        "    feature_progress_index += 1\n",
        "    feature_progress_label.value = f\"Feature progress: {feature_progress_index} / {n_features}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0r89JLYW_xU"
      },
      "source": [
        "# Feature stacks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load neutral effect values for covariates from SHAP analysis.\n",
        "# Used to minimise covariate bias when making spatial predictions.\n",
        "# Otherwise set a 'manual override' at a suitable value.\n",
        "\n",
        "# Manual overrides (applied after SHAP values loaded)\n",
        "covariate_overrides = {\n",
        "    # 'fea_beam': 5,\n",
        "    # 'fea_sensitivity': 0.99,\n",
        "}\n",
        "\n",
        "covariate_values = {}\n",
        "feature_analysis_path = join(selected_model_shap_dir, 'shap_feature_analysis.csv')\n",
        "if covariates_renamed:\n",
        "    print(f\"Covariates defined in model: {covariates_renamed}\\n\")\n",
        "    if exists(feature_analysis_path):\n",
        "        feature_analysis = pd.read_csv(feature_analysis_path)\n",
        "        print(\"Neutral effect values from SHAP analysis:\")\n",
        "        for cov in covariates_renamed:\n",
        "            row = feature_analysis[feature_analysis['Dataset name'] == cov]\n",
        "            if not row.empty:\n",
        "                neutral_val = row['Neutral_Effect_Value'].values[0]\n",
        "                shap_at_neutral = row['SHAP_at_Neutral'].values[0]\n",
        "                covariate_values[cov] = neutral_val\n",
        "                print(f\"  {cov}: {neutral_val:.4f} (SHAP at neutral: {shap_at_neutral:.4f})\")\n",
        "            else: print(f\"  {cov}: not found in SHAP analysis\")\n",
        "    else: print(\"SHAP feature analysis not found. Set values in covariate_overrides.\")\n",
        "\n",
        "    # Apply overrides and verify completeness\n",
        "    covariate_values.update(covariate_overrides)\n",
        "    if covariate_overrides: print(f\"\\nManual overrides applied: {covariate_overrides}\")\n",
        "    missing = [c for c in covariates_renamed if c not in covariate_values]\n",
        "    assert not missing, f\"Missing covariate values: {missing}. Set in covariate_overrides.\"\n",
        "\n",
        "    # Cast to float32 for consistency with feature stack dtype\n",
        "    covariate_values = {k: np.float32(v) for k, v in covariate_values.items()}\n",
        "    print(f\"Final covariate values: {covariate_values}\")\n",
        "else: print(\"No covariates defined in model.\")"
      ],
      "metadata": {
        "id": "yhSrhXg47Yfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaHFv1nuW5CG"
      },
      "outputs": [],
      "source": [
        "# Create feature stack arrays for each scenario\n",
        "# Collect scenarios with .csv feature lists\n",
        "scenarios_list = []\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    scenarios_list.append(csv[:-4])\n",
        "# Select scenarios to generate tiled feature stacks\n",
        "print(\"scenarios_to_stack = [\")\n",
        "for scenario in sorted(scenarios_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-uJ-EkoxIym"
      },
      "outputs": [],
      "source": [
        "scenarios_to_stack = [\n",
        "  # \"2015\",\n",
        "  \"2018\",\n",
        "  # \"2019\",\n",
        "  # \"2020\",\n",
        "  # \"2021\",\n",
        "  # \"2021_no_degradation_since_1993\",\n",
        "  # \"2021_no_disturbance_since_1993\",\n",
        "  # \"2021_oldgrowth_1\",\n",
        "  # \"2021_oldgrowth_2\",\n",
        "  # \"2021_oldgrowth_all_land_1\",\n",
        "  # \"2021_oldgrowth_all_land_2\",\n",
        "  # \"2022\",\n",
        "  # \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_no_degradation_since_1996\",\n",
        "  \"2024_no_disturbance_since_1996\",\n",
        "  # \"2024_no_disturbance_since_1997\",\n",
        "  # \"2024_no_disturbance_since_1998\",\n",
        "  # \"2024_no_disturbance_since_1999\",\n",
        "  # \"2024_no_disturbance_since_2000\",\n",
        "  # \"2024_no_disturbance_since_2001\",\n",
        "  # \"2024_no_disturbance_since_2002\",\n",
        "  # \"2024_no_disturbance_since_2003\",\n",
        "  # \"2024_no_disturbance_since_2004\",\n",
        "  # \"2024_no_disturbance_since_2005\",\n",
        "  # \"2024_no_disturbance_since_2006\",\n",
        "  # \"2024_no_disturbance_since_2007\",\n",
        "  # \"2024_no_disturbance_since_2008\",\n",
        "  # \"2024_no_disturbance_since_2009\",\n",
        "  # \"2024_no_disturbance_since_2010\",\n",
        "  # \"2024_no_disturbance_since_2011\",\n",
        "  # \"2024_no_disturbance_since_2012\",\n",
        "  # \"2024_no_disturbance_since_2013\",\n",
        "  # \"2024_no_disturbance_since_2014\",\n",
        "  # \"2024_no_disturbance_since_2015\",\n",
        "  # \"2024_no_disturbance_since_2016\",\n",
        "  # \"2024_no_disturbance_since_2017\",\n",
        "  # \"2024_no_disturbance_since_2018\",\n",
        "  # \"2024_no_disturbance_since_2019\",\n",
        "  # \"2024_no_disturbance_since_2020\",\n",
        "  # \"2024_no_disturbance_since_2021\",\n",
        "  # \"2024_no_disturbance_since_2022\",\n",
        "  # \"2024_no_disturbance_since_2023\",\n",
        "  # \"2024_no_disturbance_since_2024\",\n",
        "  \"2024_oldgrowth_1\",\n",
        "  \"2024_oldgrowth_2\",\n",
        "  \"2024_oldgrowth_all_land_1\",\n",
        "  \"2024_oldgrowth_all_land_2\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]\n",
        "\n",
        "assert set(scenarios_to_stack).issubset(scenarios_list), \"Not all selected scenarios exist.\"\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "n_stacks = n_tiles\n",
        "covariate_set = set(covariates)\n",
        "\n",
        "# Progress\n",
        "scenario_progress_index, scenario_progress_label = 0, widgets.Label(value=f\"Scenario progress: 0 / {len(scenarios_to_stack)}\")\n",
        "display(scenario_progress_label)\n",
        "stack_progress_index, stack_progress_label = 0, widgets.Label(value=f\"Tiled feature stack progress: 0 / {n_tiles}\")\n",
        "display(stack_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_stack:\n",
        "    # Define directory and features\n",
        "    scenario_feature_stacks_dir = join(tile_feature_stacks_dir, scenario)\n",
        "    makedirs(scenario_feature_stacks_dir, exist_ok=True)\n",
        "    scenario_features_csv = join(scenarios_model_dir, f\"{scenario}.csv\")\n",
        "    scenario_features = pd.Series.tolist(pd.read_csv(scenario_features_csv).iloc[:,0])\n",
        "    # Create a tile count to match the feature stack\n",
        "    tile_count = 1\n",
        "    for tile in range(n_stacks):\n",
        "      scenario_stack_filename = f\"feature_stack_{scenario}_{tile_count}.npy\"\n",
        "      scenario_tile_stack_dir = join(scenario_feature_stacks_dir, scenario_stack_filename)\n",
        "      # Check if feature stack already exists\n",
        "      if not exists(scenario_tile_stack_dir):\n",
        "        # Create feature chunks (arrays) from tiles\n",
        "        if n_stacks == 1:\n",
        "          feature_tiles_dirs = [f\"{features_dir}/{feature}.tif\" for feature in scenario_features]\n",
        "        else:\n",
        "          feature_tiles_dirs = [f\"{tile_features_dir}/{feature}_{tile_count}.tif\" for feature in scenario_features]\n",
        "        # Filter out covariates before reading\n",
        "        covariate_tile_set = {f\"{cov}_{tile_count}\" for cov in covariate_set}\n",
        "        valid_feature_tiles = []\n",
        "        for feature in feature_tiles_dirs:\n",
        "          fname = feature.split('/')[-1].split('.')[0]\n",
        "          if fname not in covariate_set and fname not in covariate_tile_set:\n",
        "            valid_feature_tiles.append(feature)\n",
        "        # Use VRT to read all features at once - much faster than individual reads\n",
        "        if not valid_feature_tiles: raise ValueError(f\"No non-covariate features found for scenario {scenario}, tile {tile_count}\")\n",
        "        vrt_options = gdal.BuildVRTOptions(separate=True)\n",
        "        vrt = gdal.BuildVRT('', valid_feature_tiles, options=vrt_options)\n",
        "        if vrt is None:\n",
        "            raise RuntimeError(f\"Failed to build VRT for scenario {scenario}, tile {tile_count}\")\n",
        "        feature_stack = vrt.ReadAsArray()\n",
        "        vrt = None\n",
        "        # Transpose and reshape to final form, ensure float32\n",
        "        stack_n_features, stack_height, stack_width = feature_stack.shape\n",
        "        feature_stack_reshaped = feature_stack.transpose(1, 2, 0).reshape(stack_height * stack_width, stack_n_features)\n",
        "        feature_stack = None\n",
        "        if feature_stack_reshaped.dtype != np.float32:\n",
        "            feature_stack_reshaped = feature_stack_reshaped.astype(np.float32)\n",
        "        # Append covariate columns in order defined by covariates_renamed\n",
        "        if covariates_renamed:\n",
        "            n_rows = feature_stack_reshaped.shape[0]\n",
        "            covariate_arrays = []\n",
        "            for cov in covariates_renamed:\n",
        "                val = covariate_values[cov]\n",
        "                arr = np.full(n_rows, val, dtype=np.float32)\n",
        "                covariate_arrays.append(arr.reshape(-1, 1))\n",
        "            covariate_block = np.hstack(covariate_arrays)\n",
        "            feature_stack_reshaped = np.hstack([feature_stack_reshaped, covariate_block])\n",
        "            covariate_arrays = covariate_block = None\n",
        "        # Save as a numpy file\n",
        "        np.save(scenario_tile_stack_dir, feature_stack_reshaped)\n",
        "\n",
        "        # Force Drive sync and verify\n",
        "        subprocess.run(['sync'], check=True)\n",
        "        last_size = -1\n",
        "        for attempt in range(10):\n",
        "            time.sleep(5)\n",
        "            if os.path.exists(scenario_tile_stack_dir):\n",
        "                current_size = os.path.getsize(scenario_tile_stack_dir)\n",
        "                if current_size == last_size and current_size > 0:\n",
        "                    try:\n",
        "                        np.load(scenario_tile_stack_dir)\n",
        "                        break\n",
        "                    except: pass\n",
        "                last_size = current_size\n",
        "        else: raise RuntimeError(f\"Drive sync failed: {scenario_tile_stack_dir}\")\n",
        "\n",
        "        feature_stack_reshaped = None\n",
        "      # Update progress\n",
        "      tile_count += 1\n",
        "      stack_progress_index += 1\n",
        "      stack_progress_label.value = f\"Tiled feature stack progress: {stack_progress_index} / {n_stacks}\"\n",
        "    # Reset tile progress\n",
        "    stack_progress_index = 0\n",
        "    scenario_progress_index += 1\n",
        "    scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index} / {len(scenarios_to_stack)}\"\n",
        "print(\"\\nFeature stacks complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oysAS9DI7yRg"
      },
      "source": [
        "# Predict scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y92V4nKGcyE"
      },
      "outputs": [],
      "source": [
        "# This is for testing models and scenarios, or making predictions where no\n",
        "# uncertainty metric for the variate (e.g. standard error or stdev) is available.\n",
        "# If these are available, proceed to 7_uncertainty.ipynb.\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "assert n_tiles > 0, \"# There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "if n_tiles == 1: print(f\"# There is 1 template tile.\\n\")\n",
        "if n_tiles > 1: print(f\"# There are {n_tiles} template tiles.\\n\")\n",
        "\n",
        "# Collect available scenarios from the feature stack tiles directory\n",
        "scenario_stacks_list = []\n",
        "for scenario in os.listdir(tile_feature_stacks_dir):\n",
        "    npy_files = [f for f in os.listdir(join(tile_feature_stacks_dir, scenario)) if f.endswith('.npy')]\n",
        "    if len(npy_files) == n_tiles:\n",
        "        scenario_stacks_list.append(scenario)\n",
        "\n",
        "print(\"# Note: If you end a runtime after the creation of many large feature stacks,\")\n",
        "print(\"# it will time for the notebook to recognise their existence again due to\")\n",
        "print(\"# Google Drive latency issues. If the stacks do not appear here after some time,\")\n",
        "print(\"# run the feature stack section again until they do.\\n\")\n",
        "# Select scenarios to predict\n",
        "print(\"scenarios_to_predict = [\")\n",
        "for scenario in sorted(scenario_stacks_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRlFYaIGNXbI"
      },
      "outputs": [],
      "source": [
        "# There is 1 template tile.\n",
        "\n",
        "# Note: If you end a runtime after the creation of many large feature stacks,\n",
        "# it will time for the notebook to recognise their existence again due to\n",
        "# Google Drive latency issues. If the stacks do not appear here after some time,\n",
        "# run the feature stack section again until they do.\n",
        "\n",
        "scenarios_to_predict = [\n",
        "  # \"2015\",\n",
        "  \"2018\",\n",
        "  # \"2019\",\n",
        "  # \"2020\",\n",
        "  # \"2021\",\n",
        "  # \"2021_no_degradation_since_1993\",\n",
        "  # \"2021_no_disturbance_since_1993\",\n",
        "  # \"2021_oldgrowth_1\",\n",
        "  # \"2021_oldgrowth_2\",\n",
        "  # \"2021_oldgrowth_all_land_1\",\n",
        "  # \"2021_oldgrowth_all_land_2\",\n",
        "  # \"2022\",\n",
        "  # \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_no_degradation_since_1996\",\n",
        "  \"2024_no_disturbance_since_1996\",\n",
        "  # \"2024_no_disturbance_since_1997\",\n",
        "  # \"2024_no_disturbance_since_1998\",\n",
        "  # \"2024_no_disturbance_since_1999\",\n",
        "  # \"2024_no_disturbance_since_2000\",\n",
        "  # \"2024_no_disturbance_since_2001\",\n",
        "  # \"2024_no_disturbance_since_2002\",\n",
        "  # \"2024_no_disturbance_since_2003\",\n",
        "  # \"2024_no_disturbance_since_2004\",\n",
        "  # \"2024_no_disturbance_since_2005\",\n",
        "  # \"2024_no_disturbance_since_2006\",\n",
        "  # \"2024_no_disturbance_since_2007\",\n",
        "  # \"2024_no_disturbance_since_2008\",\n",
        "  # \"2024_no_disturbance_since_2009\",\n",
        "  # \"2024_no_disturbance_since_2010\",\n",
        "  # \"2024_no_disturbance_since_2011\",\n",
        "  # \"2024_no_disturbance_since_2012\",\n",
        "  # \"2024_no_disturbance_since_2013\",\n",
        "  # \"2024_no_disturbance_since_2014\",\n",
        "  # \"2024_no_disturbance_since_2015\",\n",
        "  # \"2024_no_disturbance_since_2016\",\n",
        "  # \"2024_no_disturbance_since_2017\",\n",
        "  # \"2024_no_disturbance_since_2018\",\n",
        "  # \"2024_no_disturbance_since_2019\",\n",
        "  # \"2024_no_disturbance_since_2020\",\n",
        "  # \"2024_no_disturbance_since_2021\",\n",
        "  # \"2024_no_disturbance_since_2022\",\n",
        "  # \"2024_no_disturbance_since_2023\",\n",
        "  # \"2024_no_disturbance_since_2024\",\n",
        "  \"2024_oldgrowth_1\",\n",
        "  \"2024_oldgrowth_2\",\n",
        "  \"2024_oldgrowth_all_land_1\",\n",
        "  \"2024_oldgrowth_all_land_2\",\n",
        "  \"2024_road_mat_daling_deforestation_2023_30m_degradation_buffer\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TibU8xTZfcuL"
      },
      "outputs": [],
      "source": [
        "# Probabilities instead of classes IF binary classification\n",
        "predict_probabilities = False\n",
        "\n",
        "# Classification threshold IF binary classification\n",
        "classification_threshold = 0.5\n",
        "\n",
        "# Detect GPU availability and set predictor type. Note that XGBoost inference is\n",
        "# not much faster on GPU than CPU, and transferring large feature stacks from\n",
        "# CPU to GPU memory can actually make it much slower. TPU is not used, but the\n",
        "# TPU Colab runtime can provide more memory and more CPU workers.\n",
        "try:\n",
        "    test_array = cupy.array([1, 2, 3])\n",
        "    del test_array\n",
        "    predictor_type = 'gpu_predictor'\n",
        "    gpu_id, use_gpu = 0, True\n",
        "    print(\"GPU detected and accessible - using GPU to load the feature stack and to predict.\")\n",
        "except:\n",
        "    predictor_type = 'cpu_predictor'\n",
        "    gpu_id, use_gpu = -1, False\n",
        "    print(\"GPU not accessible - using CPU prediction\")\n",
        "xgb.set_config(verbosity=0, use_rmm=use_gpu)\n",
        "\n",
        "# Load model and detect type\n",
        "booster = xgb.Booster()\n",
        "booster.load_model(selected_model_json)\n",
        "model_config = json.loads(booster.save_config())\n",
        "\n",
        "objective_name = model_config['learner']['objective']['name']\n",
        "num_class = int(model_config['learner']['learner_model_param'].get('num_class', '0'))\n",
        "classification = any(obj_type in objective_name for obj_type in ['logistic', 'softprob', 'softmax'])\n",
        "multiclass = classification and num_class > 2\n",
        "if classification and multiclass: print(f\"Model type: Multiclass classification ({num_class} classes)\")\n",
        "elif classification: print(\"Model type: Binary classification\")\n",
        "else: print(\"Model type: Regression\")\n",
        "\n",
        "# Build feature_types list matching selected_features order\n",
        "feature_types = []\n",
        "for feat in selected_features:\n",
        "    if feat in categorical_columns:\n",
        "        feature_types.append('c')\n",
        "    else:\n",
        "        feature_types.append('q')\n",
        "\n",
        "# Select appropriate predictor type and set feature_types\n",
        "if classification:\n",
        "    XGBPredictor = xgb.XGBClassifier()\n",
        "    XGBPredictor.load_model(selected_model_json)\n",
        "    XGBPredictor.set_params(predictor=predictor_type, feature_types=feature_types)\n",
        "    if use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "else:\n",
        "    XGBPredictor = xgb.XGBRegressor()\n",
        "    XGBPredictor.load_model(selected_model_json)\n",
        "    XGBPredictor.set_params(predictor=predictor_type, feature_types=feature_types)\n",
        "    if use_gpu: XGBPredictor.set_params(device='cuda:0')\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = [file for file in os.listdir(tile_templates_dir)\n",
        "                     if file.endswith('.tif') and file[:13] == 'template_tile']\n",
        "n_tiles = len(template_tile_list)\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "template_tile = gdal.Open(join(tile_templates_dir,'template_tile_1.tif'))\n",
        "template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "template_tile = None\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "if alpha_earth: template_base_path = next(r for r in model_scenario_features_dirs if all(c not in r for c in covariates))\n",
        "else: template_base_path = template_tif_path\n",
        "\n",
        "# Progress tracking\n",
        "scenario_progress_index = 0\n",
        "scenario_progress_label = widgets.Label(f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\")\n",
        "display(scenario_progress_label)\n",
        "tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "display(tile_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_predict:\n",
        "  scenario_prediction_unmasked_filename = f\"{scenario}__{selected_model}_unmasked.tif\"\n",
        "  scenario_prediction_unmasked_exists = scenario_prediction_unmasked_filename in os.listdir(scenario_predictions_unmasked_dir)\n",
        "  if not scenario_prediction_unmasked_exists:\n",
        "    scenario_feature_stack_dir = join(tile_feature_stacks_dir, scenario)\n",
        "    n_stacks = len([f for f in os.listdir(scenario_feature_stack_dir) if f.endswith('.npy')])\n",
        "    tile_cache_scenario_dir = join(tile_prediction_cache_dir, scenario_prediction_unmasked_filename[:-4])\n",
        "    makedirs(tile_cache_scenario_dir, exist_ok=True)\n",
        "    for stack in range(1, n_stacks+1):\n",
        "      scenario_tile_filename = f\"scenario_tile_{stack}.tif\"\n",
        "      scenario_tile_exists = scenario_tile_filename in os.listdir(tile_cache_scenario_dir)\n",
        "      if not scenario_tile_exists:\n",
        "        # Load template tile parameters\n",
        "        template_tile_dir = join(tile_templates_dir, f\"template_tile_{stack}.tif\")\n",
        "        template_tile = gdal.Open(template_tile_dir)\n",
        "        template_tile_y = template_tile.GetRasterBand(1).YSize\n",
        "        template_tile_x = template_tile.GetRasterBand(1).XSize\n",
        "        template_tile = None\n",
        "        # Load stack to GPU or CPU\n",
        "        stack_filename = f\"feature_stack_{scenario}_{stack}.npy\"\n",
        "        stack_path = join(scenario_feature_stack_dir, stack_filename)\n",
        "        if use_gpu:\n",
        "          try: feature_stack = cupy.asarray(np.load(stack_path))\n",
        "          except Exception as e:\n",
        "              if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "                  print(\"GPU memory insufficient, switching to CPU for this tile.\")\n",
        "                  cupy.get_default_memory_pool().free_all_blocks()\n",
        "                  gc.collect()\n",
        "                  feature_stack = np.load(stack_path)\n",
        "                  XGBPredictor.set_params(device='cpu', predictor='cpu_predictor', n_jobs=-1)\n",
        "              else: raise\n",
        "        else:\n",
        "          feature_stack = np.load(stack_path)\n",
        "        # Predict - terminate runtime if GPU prediction fails\n",
        "        try:\n",
        "            if classification and predict_probabilities and not multiclass:\n",
        "                # Get probability of class 1 for binary classification\n",
        "                prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                prediction = prediction_proba[:, 1]  # Probability of class 1\n",
        "            else:\n",
        "                if classification and not multiclass:\n",
        "                    # Use predict_proba for better accuracy in binary classification\n",
        "                    prediction_proba = XGBPredictor.predict_proba(feature_stack)\n",
        "                    prediction = (prediction_proba[:, 1] > classification_threshold).astype(int)\n",
        "                else:\n",
        "                    prediction = XGBPredictor.predict(feature_stack)\n",
        "                    if classification:\n",
        "                        # Check if prediction is 2D (probabilities) and convert to class labels\n",
        "                        if prediction.ndim > 1 and prediction.shape[1] > 1: prediction = np.argmax(prediction, axis=1)\n",
        "                        # Ensure prediction is integer type for classification\n",
        "                        prediction = prediction.astype(int)\n",
        "        except Exception as e:\n",
        "            if \"Memory allocation error\" in str(e) or \"Out of memory\" in str(e):\n",
        "                print(\"GPU memory insufficient for prediction. Terminating runtime to save compute units, restart with TPU.\")\n",
        "                runtime.unassign()\n",
        "            else: raise\n",
        "        feature_stack = None\n",
        "        prediction_tile = prediction.reshape((template_tile_y, template_tile_x))\n",
        "        prediction = None\n",
        "        # Export prediction array as .tif\n",
        "        export_array_as_tif(prediction_tile, join(tile_cache_scenario_dir, scenario_tile_filename),\n",
        "                          template = template_tile_dir, compress = False)\n",
        "        prediction_tile = None\n",
        "      tile_progress_index += 1\n",
        "      tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_stacks}\"\n",
        "    # Prepare empty array for merging tiles\n",
        "    prediction_array = np.empty((0,template_tile_x))\n",
        "    # Read each tile .tif as an array, stack, then export as a .tif\n",
        "    for subdir in os.listdir(tile_cache_scenario_dir):\n",
        "      if subdir.endswith('.tif'):\n",
        "        tile_dir = join(tile_cache_scenario_dir, subdir)\n",
        "        tile = gdal.Open(tile_dir)\n",
        "        tile_array = tile.ReadAsArray()\n",
        "        prediction_array = np.vstack((prediction_array, tile_array))\n",
        "        tile = None\n",
        "    scenario_prediction_unmasked_dir = join(scenario_predictions_unmasked_dir, f\"{scenario_prediction_unmasked_filename}\")\n",
        "    export_array_as_tif(prediction_array, scenario_prediction_unmasked_dir, template = template_base_path, compress = True)\n",
        "    # Delete scenario tile cache directory\n",
        "    shutil.rmtree(tile_cache_scenario_dir)\n",
        "  # Reset tile progress\n",
        "  tile_progress_index = 0\n",
        "  # Update scenario progress\n",
        "  scenario_progress_index += 1\n",
        "  scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\"\n",
        "print(\"\\nScenario predictions complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diwt1peRdXpj"
      },
      "source": [
        "# Mask scenario predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQsX1NCiy0aW"
      },
      "outputs": [],
      "source": [
        "# Use polygons for masking, only areas inside the polygons will be included\n",
        "\n",
        "# Exclude existing polygons from search\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "\n",
        "print(\"mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"  '{polygon[:-5]}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daKAe6nkJ2bS"
      },
      "outputs": [],
      "source": [
        "mask_polygons = [\n",
        "  # 'project_area',\n",
        "  'gedi_area',\n",
        "  # 'peninsular_malaysia',\n",
        "  # 'pa_taman_krau',\n",
        "  # 'pa_ais',\n",
        "]\n",
        "\n",
        "# GEDI L4A AGBD precision is 0\n",
        "precision = 0\n",
        "\n",
        "# Create an inverse project area path for masking\n",
        "template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "for polygon in mask_polygons:\n",
        "  inverse_polygon_path = join(polygons_dir, f\"{polygon}_inverse.gpkg\")\n",
        "  if not exists(inverse_polygon_path):\n",
        "    polygon_path = join(polygons_dir, f\"{polygon}.gpkg\")\n",
        "    template_polygon = gpd.read_file(template_polygon_path)\n",
        "    polygon_read = gpd.read_file(polygon_path)\n",
        "    polygon_crs = polygon_read.crs.to_epsg()\n",
        "    inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "    inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "    inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "    print(f\"An inverse masking polygon for {polygon} has been created in {polygons_dir}.\")\n",
        "  else: print(f\"An inverse masking polygon for {polygon} already exists.\")\n",
        "\n",
        "\n",
        "# If only [oldgrowth_scneario]_1 exists, all disturbance from all disturbance features is removed\n",
        "# If [oldgrowth_scneario]_1 and [oldgrowth_scneario]_2 exist,\n",
        "# [oldgrowth_scneario]_1 uses a land-use proxy for pre-Landsat undisturbed forest\n",
        "# [oldgrowth_scneario]_2 simply removes all disturbance from all disturbance features\n",
        "# The final masked [scenario]_oldgrowth chooses the maximum pixel values from comparing each.\n",
        "\n",
        "if alpha_earth: template_base_path = next(r for r in model_scenario_features_dirs if all(c not in r for c in covariates))\n",
        "else: template_base_path = template_tif_path\n",
        "\n",
        "# Merge oldgrowth versions by taking maximum values\n",
        "oldgrowth_version1_files = [f for f in os.listdir(scenario_predictions_unmasked_dir)\n",
        "                           if ('_oldgrowth_1__' in f or '_oldgrowth_all_land_1__' in f) and\n",
        "                              f.endswith('_unmasked.tif')]\n",
        "for v1_file in oldgrowth_version1_files:\n",
        "  base_name = v1_file.split('__')[0][:-1] + '2'  # Replace '1' with '2'\n",
        "  rest_of_name = '__' + v1_file.split('__')[1]\n",
        "  v2_file = f\"{base_name}{rest_of_name}\"\n",
        "  merged_file = v1_file.replace('_1__', '__')\n",
        "  merged_path = join(scenario_predictions_unmasked_dir, merged_file)\n",
        "  # Skip if merged file already exists\n",
        "  if exists(merged_path): continue\n",
        "  # Check if version 2 exists\n",
        "  if exists(join(scenario_predictions_unmasked_dir, v2_file)):\n",
        "    print(f\"Merging oldgrowth versions for {v1_file.split('__')[0]}...\")\n",
        "    # Load both arrays and take maximum values\n",
        "    oldgrowth_1 = gdal.Open(join(scenario_predictions_unmasked_dir, v1_file))\n",
        "    oldgrowth_1_array = oldgrowth_1.ReadAsArray()\n",
        "    oldgrowth_1 = None\n",
        "    oldgrowth_2 = gdal.Open(join(scenario_predictions_unmasked_dir, v2_file))\n",
        "    oldgrowth_2_array = oldgrowth_2.ReadAsArray()\n",
        "    oldgrowth_2 = None\n",
        "    merged_array = np.maximum(oldgrowth_1_array, oldgrowth_2_array)\n",
        "    # Save merged file\n",
        "    export_array_as_tif(merged_array, merged_path, template = template_base_path)\n",
        "    print(f\"Merged version exported to {merged_file}\")\n",
        "  else:\n",
        "    # Use version 1 if version 2 doesn't exist\n",
        "    shutil.copy2(join(scenario_predictions_unmasked_dir, v1_file), merged_path)\n",
        "    print(f\"Version 2 not found, copied version 1 to {merged_file}\")\n",
        "\n",
        "# Collect unmasked predictions, properly skipping oldgrowth version files\n",
        "unmasked_predictions = []\n",
        "for scenario_prediction in os.listdir(scenario_predictions_unmasked_dir):\n",
        "  if '_oldgrowth_1__' in scenario_prediction or '_oldgrowth_2__' in scenario_prediction:\n",
        "    continue\n",
        "  if '_oldgrowth_all_land_1__' in scenario_prediction or '_oldgrowth_all_land_2__' in scenario_prediction:\n",
        "    continue\n",
        "  unmasked_predictions.append(scenario_prediction)\n",
        "\n",
        "# Determine last feature year for masking future scenarios\n",
        "final_feature_years = []\n",
        "for final_feature in os.listdir(feature_final_dir):\n",
        "  if final_feature.endswith('.tif') and final_feature[-9] == '_':\n",
        "    try: final_feature_years.append(int(final_feature[-8:-4]))\n",
        "    except: continue\n",
        "last_feature_year = max(final_feature_years)\n",
        "\n",
        "# Binary progress\n",
        "masking_progress_index = 0\n",
        "masking_progress_label = widgets.Label(f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\")\n",
        "display(masking_progress_label)\n",
        "\n",
        "# Mask scenario statistics with the relevant mask\n",
        "for scenario_prediction in unmasked_predictions: # Loop through each unmasked scenario\n",
        "  scenario_masked_filename = f\"{scenario_prediction[:-13]}.tif\"\n",
        "  scenario_masked_dir = join(scenario_predictions_dir, scenario_masked_filename)\n",
        "  scenario_year = int(scenario_prediction[:4])\n",
        "  if not exists(scenario_masked_dir):\n",
        "    mask_exists = False\n",
        "    for mask in sorted(os.listdir(masks_dir)):\n",
        "      mask_year = int(mask[12:16])\n",
        "\n",
        "      # Match 'all land' old-growth scenarios\n",
        "      if 'oldgrowth_all_land' in scenario_prediction:\n",
        "        if f'{scenario_year}_oldgrowth_all_land' in mask:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "          break\n",
        "\n",
        "      # Match area-based deforestation scenarios\n",
        "      elif 'deforestation' in scenario_prediction:\n",
        "        if 'deforestation' in mask:\n",
        "          mask_middle = mask[12:-4]  # Remove \"mask_forest_\" and \".tif\"\n",
        "          if scenario_prediction.startswith(mask_middle):\n",
        "            selected_mask_filename = mask\n",
        "            selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "            mask_exists = True\n",
        "            break\n",
        "\n",
        "      # Match 'no disturbance since' scenarios\n",
        "      # Use the mask from the last year of disturbance, as this represents forest extent Dec 31st\n",
        "      elif 'no_disturbance_since' in scenario_prediction:\n",
        "        disturbance_since_year = int(scenario_prediction.split('__')[0][-4:])\n",
        "        if (disturbance_since_year - 1) == mask_year and 'oldgrowth_all_land' not in mask and 'deforestation' not in mask:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "          break\n",
        "\n",
        "      # Match future scenarios with most recent forest mask\n",
        "      elif scenario_year > last_feature_year:\n",
        "        if last_feature_year == mask_year and 'oldgrowth_all_land' not in mask and 'deforestation' not in mask:\n",
        "          selected_mask_filename = mask\n",
        "          selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "          mask_exists = True\n",
        "          break\n",
        "\n",
        "      # Match all other historic scenarios and degradation scenarios\n",
        "      # Degradation scenarios use the same mask as historic, as there is no additional deforestation\n",
        "      elif scenario_year == mask_year and 'oldgrowth_all_land' not in mask and 'deforestation' not in mask:\n",
        "        selected_mask_filename = mask\n",
        "        selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "        mask_exists = True\n",
        "        break\n",
        "\n",
        "    if mask_exists == False: print(f\"A suitable mask for {scenario_prediction} does not exist.\\n\")\n",
        "    else: # Mask the scenario prediction\n",
        "      print(f\"Masking {scenario_prediction} with {selected_mask_filename}...\")\n",
        "\n",
        "      if alpha_earth: # Will require resampling the mask\n",
        "          template = gdal.Open(template_base_path)\n",
        "          mask_array = gdal.Warp('', selected_mask_dir, format='MEM',\n",
        "                                width=template.RasterXSize, height=template.RasterYSize,\n",
        "                                outputBounds=[template.GetGeoTransform()[0],\n",
        "                                            template.GetGeoTransform()[3] - template.RasterYSize * abs(template.GetGeoTransform()[5]),\n",
        "                                            template.GetGeoTransform()[0] + template.RasterXSize * template.GetGeoTransform()[1],\n",
        "                                            template.GetGeoTransform()[3]],\n",
        "                                resampleAlg='near').ReadAsArray()\n",
        "          template = None\n",
        "      else:\n",
        "        mask = gdal.Open(selected_mask_dir)\n",
        "        mask_array = mask.ReadAsArray()\n",
        "        mask = None\n",
        "\n",
        "      scenario_prediction_unmasked_dir = join(scenario_predictions_unmasked_dir, scenario_prediction)\n",
        "      scenario_prediction = gdal.Open(scenario_prediction_unmasked_dir)\n",
        "      scenario_prediction_array = scenario_prediction.ReadAsArray()\n",
        "      scenario_prediction_array = np.round(scenario_prediction_array, precision)\n",
        "      scenario_prediction = None\n",
        "      # Mask where the mask array is not 1\n",
        "      scenario_masked_array = np.where(mask_array != 1, nodatavalue, scenario_prediction_array)\n",
        "      export_array_as_tif(scenario_masked_array, scenario_masked_dir, template = template_base_path, compress = True)\n",
        "      if len(mask_polygons) > 0:\n",
        "        for polygon_mask in mask_polygons:\n",
        "          inverse_gedi_area_path = join(polygons_dir, f\"{polygon_mask}_inverse.gpkg\")\n",
        "          print(f\"Masking {scenario_masked_filename} with {polygon_mask}...\")\n",
        "          burn_polygon_to_raster(scenario_masked_dir, inverse_gedi_area_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        # Recompress the prediction after burning the polygon masks\n",
        "        scenario_masked = gdal.Open(scenario_masked_dir)\n",
        "        scenario_masked_array_2 = scenario_masked.ReadAsArray()\n",
        "        scenario_masked = None\n",
        "        export_array_as_tif(scenario_masked_array_2, scenario_masked_dir, template = template_base_path, compress = True)\n",
        "      print(f\"{scenario_masked_filename} exported.\")\n",
        "  # Update masking progress\n",
        "  masking_progress_index += 1\n",
        "  masking_progress_label.value = f\"Masking progress: {masking_progress_index}/{len(unmasked_predictions)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-7ctfgdoF2"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcuArY00N9Mc"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QJ8jLRKRSfgb",
        "y_OkHaUkTCyB",
        "FmlqEP9rFapf",
        "X3w7svaUvs07",
        "xy2nfHshozVQ",
        "1T9UqJrzWECr",
        "Y0r89JLYW_xU"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}