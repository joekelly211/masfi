{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/6_scenarios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8jLRKRSfgb"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkvV803eSUQY"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "\n",
        "# Mount Google Drive and set base directory\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEpdIjOwSiB1"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install astropy\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!pip install xgboost --upgrade\n",
        "!apt-get install -y gdal-bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQDSsvGeSi4u"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import json\n",
        "from os import makedirs\n",
        "from os.path import join, exists\n",
        "from osgeo import gdal\n",
        "import ipywidgets as widgets\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import psutil\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "import re\n",
        "from scipy import ndimage\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "from skranger.ensemble import RangerForestRegressor\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxgT3K1Sjum"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "masks_dir = join(areas_dir, \"masks\")\n",
        "predictor_dir = join(base_dir, \"3_predictors\")\n",
        "predictor_resampled_dir = join(predictor_dir, \"resampled\")\n",
        "predictor_binary_dir = join(predictor_dir, \"binary\")\n",
        "predictor_final_dir = join(predictor_dir, \"final\")\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(scenarios_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96FAfeyooMLz"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)\n",
        "\n",
        "# Burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster, polygon, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "  with rasterio.open(raster, 'r+') as src:\n",
        "      array = src.read(1)\n",
        "      transform = src.transform\n",
        "      gdf = gpd.read_file(polygon)\n",
        "      for geom in gdf.geometry:\n",
        "          if not fixed and column_name == None:\n",
        "              column_name = gdf.columns[0]\n",
        "          if not fixed: burn_value = gdf.loc[gdf.geometry == geom, column_name].values[0]\n",
        "          else: burn_value = fixed_value\n",
        "          rasterize([(geom, burn_value)], out=array, transform=transform,\n",
        "              all_touched=all_touched, dtype=src.meta['dtype'], out_shape=src.shape)\n",
        "      src.write(array, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_OkHaUkTCyB"
      },
      "source": [
        "# Select model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mB0XrPNYU_I"
      },
      "outputs": [],
      "source": [
        "# Select a model\n",
        "model_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file == 'model.json':\n",
        "      print(f'selected_model = \"{subdir.split(f\"{models_dir}/\",1)[1]}\"')\n",
        "      model_exists = True\n",
        "if not model_exists:\n",
        "  print(\"No model exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4pZnNpCGiTv"
      },
      "outputs": [],
      "source": [
        "selected_model = \"agbd_240718_164421\"\n",
        "categorise_variate = False # If the variate was categorised in 5_models\n",
        "\n",
        "# Define model directories\n",
        "selected_model_dir = join(models_dir,selected_model)\n",
        "selected_model_json = join(selected_model_dir, \"model.json\")\n",
        "selected_model_descr_dir = join(selected_model_dir, \"model_description.json\")\n",
        "selected_model_dataset_path = join(selected_model_dir, f\"{selected_model}.pkl\")\n",
        "selected_model_dataset = pd.read_pickle(selected_model_dataset_path)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(selected_model_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_variate = model_dataset_description[\"selected_variate\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "covariates_categorised = model_dataset_description[\"covariates_categorised\"]\n",
        "selected_predictors = model_dataset_description[\"selected_predictors\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "filter_parameter = model_dataset_description[\"filter_parameter\"]\n",
        "filter_values_to_include = model_dataset_description[\"filter_values_to_include\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "# Reload hyperparameters\n",
        "with open(selected_model_descr_dir) as model_description_json:\n",
        "  model_description = json.load(model_description_json)\n",
        "final_hyperparameters = eval(model_description[\"hyperparameters\"])\n",
        "\n",
        "# Remove early stopping and replace with mean n_estimators\n",
        "if \"early_stopping_rounds\" in final_hyperparameters:\n",
        "  final_hyperparameters = {k:v for k, v in final_hyperparameters.items() if k != \"early_stopping_rounds\"}\n",
        "  final_hyperparameters[\"n_estimators\"] = round(model_description[\"n_estimators mean\"])\n",
        "\n",
        "# Create scenarios model directory\n",
        "scenarios_model_dir = join(scenarios_dir, selected_model)\n",
        "makedirs(scenarios_model_dir, exist_ok=True)\n",
        "\n",
        "# Copy model_dataset_description.json\n",
        "with open(join(scenarios_model_dir, \"model_dataset_description.json\"), \"w\") as file:\n",
        "  file.write(json.dumps(model_dataset_description))\n",
        "\n",
        "# Define model\n",
        "XGBPredictor = xgb.XGBRegressor(**final_hyperparameters)\n",
        "XGBPredictor.load_model(fname=selected_model_json)\n",
        "\n",
        "# Avoids issues using dataframe from CPU\n",
        "xgb.set_config(verbosity=0, use_rmm=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EktN7Mqok75"
      },
      "source": [
        "# Select scenario area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du2Q1YUJklzl"
      },
      "outputs": [],
      "source": [
        "# Select a scenario area\n",
        "scenario_area_exists = False\n",
        "for subdir in os.listdir(scenarios_model_dir):\n",
        "  if not subdir.endswith('.csv') and not subdir.endswith('.json'):\n",
        "    print(f'selected_scenario_area = \"{subdir}\"')\n",
        "    scenario_area_exists = True\n",
        "if not scenario_area_exists:\n",
        "  print(f\"Create a scenario area directory in {scenarios_model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJZoEWycn8-x"
      },
      "outputs": [],
      "source": [
        "selected_scenario_area = \"terengganu\"\n",
        "\n",
        "model_scenario_override = 2022 # set if cannot be automatically determined from model predictors\n",
        "\n",
        "yearly_predictors = [\"forest_with_edge_effects\", \"disturbance_with_edge_effects\"]\n",
        "\n",
        "# Define scenario area directory\n",
        "scenario_area_dir = join(scenarios_model_dir,selected_scenario_area)\n",
        "makedirs(scenario_area_dir, exist_ok=True)\n",
        "\n",
        "# Create subdirectories\n",
        "predictors_dir = join(scenario_area_dir, \"predictors\")\n",
        "tile_templates_dir = join(scenario_area_dir, 'tile_templates')\n",
        "tile_predictors_dir = join(scenario_area_dir, \"tile_predictors\")\n",
        "tile_predictor_stacks_dir = join(scenario_area_dir, \"tile_predictor_stacks\")\n",
        "tile_prediction_cache_dir = join(scenario_area_dir,\"tile_prediction_cache\")\n",
        "scenario_predictions_unmasked_dir = join(scenario_area_dir,\"scenario_predictions_unmasked\")\n",
        "scenario_predictions_dir = join(scenario_area_dir, \"scenario_predictions\")\n",
        "\n",
        "makedirs(predictors_dir, exist_ok=True)\n",
        "makedirs(tile_templates_dir, exist_ok=True)\n",
        "makedirs(tile_predictors_dir, exist_ok=True)\n",
        "makedirs(tile_predictor_stacks_dir, exist_ok=True)\n",
        "makedirs(tile_prediction_cache_dir, exist_ok=True)\n",
        "makedirs(scenario_predictions_unmasked_dir, exist_ok=True)\n",
        "makedirs(scenario_predictions_dir, exist_ok=True)\n",
        "\n",
        "# Remove the 'pre_' prefix from each predictor\n",
        "model_predictors = sorted([predictor[4:] for predictor in selected_predictors])\n",
        "\n",
        "# Create a list of predictor years from the model's predictors\n",
        "model_predictor_years = []\n",
        "for predictor in model_predictors:\n",
        "  for yearly_predictor in yearly_predictors:\n",
        "    if yearly_predictor in predictor:\n",
        "      model_predictor_years.append(int(predictor[-4:]))\n",
        "\n",
        "# Determine the model scenario from the maximum year\n",
        "# Values from the most recent predictor year (e.g. 2022) will be applied to the second most recent (e.g. 2021) as a proxy at the predictor stack stage\n",
        "if model_scenario_override != None: model_scenario = model_scenario_override\n",
        "else: model_scenario = max(model_predictor_years) + 1\n",
        "model_scenario_filename = f\"{model_scenario}.csv\"\n",
        "model_scenario_dir = join(scenarios_model_dir,model_scenario_filename)\n",
        "print(f\"The maximum year used in the model is {model_scenario}, which has been created as the first scenario.\\n\")\n",
        "print(f\"The {model_scenario} scenario predictor list has been saved to:\\n {model_scenario_dir}\\n\")\n",
        "print(f\"Ensure all predictors in this list, as well as for any additional scenarios, are uploaded to:\\n{predictors_dir}\")\n",
        "\n",
        "# Save the model scenario predictors as a .csv\n",
        "pd.DataFrame(model_predictors).to_csv(model_scenario_dir, index=False)\n",
        "\n",
        "# Copy predictor from the final predictors directory\n",
        "for predictor in os.listdir(predictor_final_dir):\n",
        "  if predictor not in os.listdir(predictors_dir):\n",
        "    predictor_original_path = join(predictor_final_dir, predictor)\n",
        "    predictor_copy_path = join(predictors_dir, predictor)\n",
        "    copyfile(predictor_original_path, predictor_copy_path)\n",
        "print(f\"All predictors present in the following directory have already been copied over: {predictor_final_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3w7svaUvs07"
      },
      "source": [
        "# Define historic scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X6mjtAyF9R7"
      },
      "outputs": [],
      "source": [
        "# Select constant predictors (which are the same in all scenarios)\n",
        "print(\"constant_predictors = [\")\n",
        "for predictor in model_predictors:\n",
        "  print(f'  \"{predictor}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yc1oUmyJTSy"
      },
      "outputs": [],
      "source": [
        "constant_predictors = [\n",
        "  \"coast_proximity_km\",\n",
        "  # \"disturbance_with_edge_effects_2004\",\n",
        "  # \"disturbance_with_edge_effects_2005\",\n",
        "  # \"disturbance_with_edge_effects_2006\",\n",
        "  # \"disturbance_with_edge_effects_2007\",\n",
        "  # \"disturbance_with_edge_effects_2008\",\n",
        "  # \"disturbance_with_edge_effects_2009\",\n",
        "  # \"disturbance_with_edge_effects_2010\",\n",
        "  # \"disturbance_with_edge_effects_2011\",\n",
        "  # \"disturbance_with_edge_effects_2012\",\n",
        "  # \"disturbance_with_edge_effects_2013\",\n",
        "  # \"disturbance_with_edge_effects_2014\",\n",
        "  # \"disturbance_with_edge_effects_2015\",\n",
        "  # \"disturbance_with_edge_effects_2016\",\n",
        "  # \"disturbance_with_edge_effects_2017\",\n",
        "  # \"disturbance_with_edge_effects_2018\",\n",
        "  # \"disturbance_with_edge_effects_2019\",\n",
        "  # \"disturbance_with_edge_effects_2020\",\n",
        "  # \"disturbance_with_edge_effects_2021\",\n",
        "  # \"forest_with_edge_effects_2005\",\n",
        "  # \"forest_with_edge_effects_2006\",\n",
        "  # \"forest_with_edge_effects_2007\",\n",
        "  # \"forest_with_edge_effects_2008\",\n",
        "  # \"forest_with_edge_effects_2009\",\n",
        "  # \"forest_with_edge_effects_2010\",\n",
        "  # \"forest_with_edge_effects_2011\",\n",
        "  # \"forest_with_edge_effects_2012\",\n",
        "  # \"forest_with_edge_effects_2013\",\n",
        "  # \"forest_with_edge_effects_2014\",\n",
        "  # \"forest_with_edge_effects_2015\",\n",
        "  # \"forest_with_edge_effects_2016\",\n",
        "  # \"forest_with_edge_effects_2017\",\n",
        "  # \"forest_with_edge_effects_2018\",\n",
        "  # \"forest_with_edge_effects_2019\",\n",
        "  # \"forest_with_edge_effects_2020\",\n",
        "  # \"forest_with_edge_effects_2021\",\n",
        "  \"latitude\",\n",
        "  \"longitude\",\n",
        "  # \"pa_taman_negara_ais\",\n",
        "  \"topo_cor_smooth_aspect_cosine\",\n",
        "  \"topo_cor_smooth_aspect_sine\",\n",
        "  \"topo_cor_smooth_circular_variance_aspect_03\",\n",
        "  \"topo_cor_smooth_circular_variance_aspect_07\",\n",
        "  \"topo_cor_smooth_circular_variance_aspect_11\",\n",
        "  \"topo_cor_smooth_deviation_mean_elevation_03\",\n",
        "  \"topo_cor_smooth_deviation_mean_elevation_07\",\n",
        "  \"topo_cor_smooth_deviation_mean_elevation_11\",\n",
        "  \"topo_cor_smooth_eastness\",\n",
        "  \"topo_cor_smooth_elevation\",\n",
        "  \"topo_cor_smooth_northness\",\n",
        "  \"topo_cor_smooth_profile_curvature\",\n",
        "  \"topo_cor_smooth_roughness_03\",\n",
        "  \"topo_cor_smooth_roughness_07\",\n",
        "  \"topo_cor_smooth_roughness_11\",\n",
        "  \"topo_cor_smooth_slope\",\n",
        "  \"topo_cor_smooth_stream_power_index_log10\",\n",
        "  \"topo_cor_smooth_surface_area_ratio\",\n",
        "  \"topo_cor_smooth_tangential_curvature\",\n",
        "  \"topo_cor_smooth_topographic_position_index_03\",\n",
        "  \"topo_cor_smooth_topographic_position_index_07\",\n",
        "  \"topo_cor_smooth_topographic_position_index_11\",\n",
        "  \"topo_cor_smooth_topographic_ruggedness_index\",\n",
        "  \"topo_cor_smooth_topographic_wetness_index\",\n",
        "  \"topo_cor_unsmooth_aspect_cosine\",\n",
        "  \"topo_cor_unsmooth_aspect_sine\",\n",
        "  \"topo_cor_unsmooth_circular_variance_aspect_03\",\n",
        "  \"topo_cor_unsmooth_circular_variance_aspect_07\",\n",
        "  \"topo_cor_unsmooth_circular_variance_aspect_11\",\n",
        "  \"topo_cor_unsmooth_deviation_mean_elevation_03\",\n",
        "  \"topo_cor_unsmooth_deviation_mean_elevation_07\",\n",
        "  \"topo_cor_unsmooth_deviation_mean_elevation_11\",\n",
        "  \"topo_cor_unsmooth_eastness\",\n",
        "  \"topo_cor_unsmooth_elevation\",\n",
        "  \"topo_cor_unsmooth_northness\",\n",
        "  \"topo_cor_unsmooth_profile_curvature\",\n",
        "  \"topo_cor_unsmooth_roughness_03\",\n",
        "  \"topo_cor_unsmooth_roughness_07\",\n",
        "  \"topo_cor_unsmooth_roughness_11\",\n",
        "  \"topo_cor_unsmooth_slope\",\n",
        "  \"topo_cor_unsmooth_stream_power_index_log10\",\n",
        "  \"topo_cor_unsmooth_surface_area_ratio\",\n",
        "  \"topo_cor_unsmooth_tangential_curvature\",\n",
        "  \"topo_cor_unsmooth_topographic_position_index_03\",\n",
        "  \"topo_cor_unsmooth_topographic_position_index_07\",\n",
        "  \"topo_cor_unsmooth_topographic_position_index_11\",\n",
        "  \"topo_cor_unsmooth_topographic_ruggedness_index\",\n",
        "  \"topo_cor_unsmooth_topographic_wetness_index\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg4XvxT5foVR"
      },
      "outputs": [],
      "source": [
        "# Historic scenarios\n",
        "\n",
        "# Set the minimum year that all yearly predictors are available for:\n",
        "first_historic_predictor_year = 1990\n",
        "# Set the maximum year that all yearly predictors are available for:\n",
        "last_historic_predictor_year = 2022\n",
        "\n",
        "# Calculate the range of scenario years and minimum historic scenario year\n",
        "model_scenario_year_range = max(model_predictor_years) - (min(model_predictor_years) -1)\n",
        "minimum_historic_scenario = first_historic_predictor_year + model_scenario_year_range\n",
        "print(f\"The minimum historic scenario year that can be predicted is {minimum_historic_scenario}.\")\n",
        "print(f\"The maximum historic scenario year that can be predicted is {last_historic_predictor_year}.\")\n",
        "\n",
        "# Set scenario predictors as all non-constant predictors\n",
        "scenario_predictors = sorted(list(set(model_predictors) - set(constant_predictors)))\n",
        "\n",
        "# Create predictor lists for all historic scenario years\n",
        "for historic_scenario in range(minimum_historic_scenario, last_historic_predictor_year+1):\n",
        "  year_difference = model_scenario - historic_scenario\n",
        "  historic_scenario_predictors = []\n",
        "  for scenario_predictor in scenario_predictors:\n",
        "    try:\n",
        "      year_change = int(scenario_predictor[-4:]) - year_difference\n",
        "      historic_scenario_predictor = scenario_predictor[:-4] + str(year_change)\n",
        "      historic_scenario_predictors.append(historic_scenario_predictor)\n",
        "    except:\n",
        "      historic_scenario_predictors.append(scenario_predictor)\n",
        "  # Compile historic predictors and save as a .csv\n",
        "  historic_predictors = sorted(historic_scenario_predictors + constant_predictors)\n",
        "  historic_scenario_filename = f\"{historic_scenario}.csv\"\n",
        "  historic_scenario_dir = join(scenarios_model_dir,historic_scenario_filename)\n",
        "  pd.DataFrame(historic_predictors).to_csv(historic_scenario_dir, index=False)\n",
        "\n",
        "print(f\"Lists of predictors for all historic scenarios have been exported to {scenarios_model_dir}/.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyQN-rk8IPgc"
      },
      "source": [
        "# Further scenarios (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KF0BAXAe0Uu"
      },
      "outputs": [],
      "source": [
        "# Alternate 'no deforestation' (nodef) and 'no deforestation and no disturbance' (nodist) scenarios.\n",
        "# These require 'forest_xxxx' and 'disturbance_xxxx' yearly predictors\n",
        "\n",
        "define_alternate = True\n",
        "\n",
        "# Years to predict for the 'no deforestation' scenario, i.e. no further loss in the 'forest' predictors\n",
        "# and the 'no disturbance' scenario, additionally with no further disturbance. Both assume no reforestation.\n",
        "alternate_prediction_years = [\n",
        "                    2027,\n",
        "                    2032,\n",
        "                    2037\n",
        "                    ]\n",
        "\n",
        "if define_alternate:\n",
        "  alternate_predictor_range = range(max(min(alternate_prediction_years)-model_scenario_year_range,last_historic_predictor_year), max(alternate_prediction_years)+1)\n",
        "\n",
        "  # Disturbance cycle years for the 'no disturbance' scenario, e.g. expected interval between selective logging of forest.\n",
        "  disturbance_cycle = 30\n",
        "\n",
        "  # Set the forest extents based on the last historic predictor year\n",
        "  alternate_forest_base = join(predictors_dir, f\"forest_with_edge_effects_{last_historic_predictor_year}.tif\")\n",
        "  print(f\"The starting year for the alternate scenarios is {last_historic_predictor_year}.\")\n",
        "\n",
        "  # Determine the minimum disturbance value, assuming it is present in the first historic scenario year\n",
        "  example_disturbance = join(predictors_dir, f\"disturbance_with_edge_effects_{first_historic_predictor_year}.tif\")\n",
        "  example_disturbance_array = gdal.Open(example_disturbance).ReadAsArray()\n",
        "  minimum_disturbance_value = example_disturbance_array.min()\n",
        "  print(f\"The minimum disturbance value is {minimum_disturbance_value}, which will be used to create the 'no disturbance' predictors.\")\n",
        "  # Create a minimum disturbance array to export for the 'no disturbance' predictor years\n",
        "  minimum_disturbance_array = np.where(example_disturbance_array, minimum_disturbance_value, minimum_disturbance_value)\n",
        "\n",
        "  for prediction_year in alternate_predictor_range:\n",
        "    # Create alternate scenario predictors for every year until the maximum prediction year\n",
        "    # Copy the forest predictor for the last historic predictor year\n",
        "    forest_nodef = join(predictors_dir, f\"forest_with_edge_effects_{prediction_year}_nodef.tif\")\n",
        "    forest_nodist = join(predictors_dir, f\"forest_with_edge_effects_{prediction_year}_nodist.tif\")\n",
        "    if not exists(forest_nodef): shutil.copyfile(alternate_forest_base, forest_nodef)\n",
        "    if not exists(forest_nodist): shutil.copyfile(alternate_forest_base, forest_nodist)\n",
        "    # Determine the 'no deforestation' disturbance predictor to copy based on the disturbance cycle\n",
        "    disturbance_year_to_copy = prediction_year - disturbance_cycle\n",
        "    disturbance_base = join(predictors_dir, f\"disturbance_with_edge_effects_{disturbance_year_to_copy}.tif\")\n",
        "    disturbance_nodef = join(predictors_dir, f\"disturbance_with_edge_effects_{prediction_year}_nodef.tif\")\n",
        "    if not exists(disturbance_nodef): shutil.copyfile(disturbance_base, disturbance_nodef)\n",
        "    # Export a 'no disturbance' predictor for every year up to the maximum prediction year\n",
        "    disturbance_nodist = join(predictors_dir, f\"disturbance_with_edge_effects_{prediction_year}_nodist.tif\")\n",
        "    if not exists(disturbance_nodist): export_array_as_tif(minimum_disturbance_array, disturbance_nodist, template = example_disturbance)\n",
        "\n",
        "  # Create predictor lists for all alternate scenario years\n",
        "  for prediction_year in alternate_prediction_years:\n",
        "    year_difference = model_scenario - prediction_year\n",
        "    nodef_scenario_predictors = []\n",
        "    nodist_scenario_predictors = []\n",
        "    for scenario_predictor in scenario_predictors:\n",
        "      try:\n",
        "        year_change = int(scenario_predictor[-4:]) - year_difference\n",
        "        if year_change > last_historic_predictor_year:\n",
        "          nodef_scenario_predictor = scenario_predictor[:-4] + str(year_change) + \"_nodef\"\n",
        "          nodist_scenario_predictor = scenario_predictor[:-4] + str(year_change) + \"_nodist\"\n",
        "        else:\n",
        "          nodef_scenario_predictor = scenario_predictor[:-4] + str(year_change)\n",
        "          nodist_scenario_predictor = scenario_predictor[:-4] + str(year_change)\n",
        "        nodef_scenario_predictors.append(nodef_scenario_predictor)\n",
        "        nodist_scenario_predictors.append(nodist_scenario_predictor)\n",
        "      except:\n",
        "        nodef_scenario_predictors.append(scenario_predictor)\n",
        "        nodist_scenario_predictors.append(scenario_predictor)\n",
        "    # Compile 'no deforestation' predictors and save as a .csv\n",
        "    nodef_predictors = sorted(nodef_scenario_predictors + constant_predictors)\n",
        "    nodef_scenario_filename = f\"{prediction_year}_nodef.csv\"\n",
        "    nodef_scenario_dir = join(scenarios_model_dir, nodef_scenario_filename)\n",
        "    pd.DataFrame(nodef_predictors).to_csv(nodef_scenario_dir, index=False)\n",
        "    # Compile 'no deforestation and no disturbance' predictors and save as a .csv\n",
        "    nodist_predictors = sorted(nodist_scenario_predictors + constant_predictors)\n",
        "    nodist_scenario_filename = f\"{prediction_year}_nodist.csv\"\n",
        "    nodist_scenario_dir = join(scenarios_model_dir, nodist_scenario_filename)\n",
        "    pd.DataFrame(nodist_predictors).to_csv(nodist_scenario_dir, index=False)\n",
        "\n",
        "  print(f\"Predictor lists for all alternate scenarios have been exported to {scenarios_model_dir}/.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwwIij7sL4Q_"
      },
      "outputs": [],
      "source": [
        "# Alternate 'no historic degradation' and 'no historic deforestation and degradation' scenarios,\n",
        "# i.e. none between first and last predictor years\n",
        "define_historic_alternate = True\n",
        "\n",
        "if define_historic_alternate:\n",
        "  # Determine the minimum disturbance value, assuming it is present in the first historic scenario year\n",
        "  example_disturbance = join(predictors_dir, f\"disturbance_with_edge_effects_{first_historic_predictor_year}.tif\")\n",
        "  example_disturbance_array = gdal.Open(example_disturbance).ReadAsArray()\n",
        "  minimum_disturbance_value = example_disturbance_array.min()\n",
        "  print(f\"The minimum disturbance value is {minimum_disturbance_value}, which will be used to create the 'no disturbance' predictors.\")\n",
        "  # Create a minimum disturbance array to export for the 'no disturbance' predictor years\n",
        "  minimum_disturbance_array = np.where(example_disturbance_array, minimum_disturbance_value, minimum_disturbance_value)\n",
        "  # Export a 'no disturbance' predictor for both historic alternate scenarios\n",
        "  disturbance_nodist_historic_name = f\"disturbance_with_edge_effects_{last_historic_predictor_year}_nodist_historic\"\n",
        "  disturbance_nodist_historic_path = join(predictors_dir, f\"{disturbance_nodist_historic_name}.tif\")\n",
        "  if not exists(disturbance_nodist_historic_path): export_array_as_tif(minimum_disturbance_array, disturbance_nodist_historic_path, template = example_disturbance)\n",
        "\n",
        "  # Set the forest extents based on the first historic predictor year for 'no historic deforestation'\n",
        "  forest_nodef_base = join(predictors_dir, f\"forest_with_edge_effects_{first_historic_predictor_year}.tif\")\n",
        "  forest_nodef_array = gdal.Open(forest_nodef_base).ReadAsArray()\n",
        "  print(f\"The baseline year for 'no historic deforestation or degradation' is {first_historic_predictor_year}.\")\n",
        "  # Export a forest predictor for the 'no deforestation and no degradation' scenario\n",
        "  forest_nodef_historic_name = f\"forest_with_edge_effects_{last_historic_predictor_year}_nodef_historic\"\n",
        "  forest_nodef_historic_path = join(predictors_dir, f\"{forest_nodef_historic_name}.tif\")\n",
        "  if not exists(forest_nodef_historic_path): export_array_as_tif(forest_nodef_array, forest_nodef_historic_path, template = forest_nodef_base)\n",
        "\n",
        "  # Create a predictor list for 'no historic degradation'\n",
        "  nodeg_historic_scenario_predictors = []\n",
        "  for scenario_predictor in scenario_predictors:\n",
        "    if \"disturbance_with_edge_effects\" in scenario_predictor:\n",
        "      nodeg_historic_scenario_predictors.append(disturbance_nodist_historic_name)\n",
        "    else: nodeg_historic_scenario_predictors.append(scenario_predictor)\n",
        "  # Compile 'no historic disturbance' predictors and save as a .csv\n",
        "  nodeg_historic_scenario_predictors = sorted(nodeg_historic_scenario_predictors + constant_predictors)\n",
        "  nodeg_historic_scenario_filename = f\"{last_historic_predictor_year}_nodeg_historic.csv\"\n",
        "  nodeg_historic_scenario_path = join(scenarios_model_dir, nodeg_historic_scenario_filename)\n",
        "  pd.DataFrame(nodeg_historic_scenario_predictors).to_csv(nodeg_historic_scenario_path, index=False)\n",
        "\n",
        "  # Create a predictor list for 'no historic deforestation or degradation'\n",
        "  nodef_historic_scenario_predictors = []\n",
        "  for scenario_predictor in scenario_predictors:\n",
        "    if \"disturbance_with_edge_effects\" in scenario_predictor:\n",
        "      nodef_historic_scenario_predictors.append(disturbance_nodist_historic_name)\n",
        "    elif \"forest_with_edge_effects\" in scenario_predictor:\n",
        "      nodef_historic_scenario_predictors.append(forest_nodef_historic_name)\n",
        "    else: nodef_historic_scenario_predictors.append(scenario_predictor)\n",
        "  # Compile 'no deforestation or degradation' predictors and save as a .csv\n",
        "  nodef_historic_scenario_predictors = sorted(nodef_historic_scenario_predictors + constant_predictors)\n",
        "  nodef_historic_scenario_filename = f\"{last_historic_predictor_year}_nodef_historic.csv\"\n",
        "  nodef_historic_scenario_path = join(scenarios_model_dir, nodef_historic_scenario_filename)\n",
        "  pd.DataFrame(nodef_historic_scenario_predictors).to_csv(nodef_historic_scenario_path, index=False)\n",
        "\n",
        "  print(f\"Predictor list for the 'no historic degradation' and 'no historic deforestation or degradation' scenarios exported to {scenarios_model_dir}/.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6u03s8VHfN8"
      },
      "outputs": [],
      "source": [
        "# Complete restoration (with current forest extent) and complete recovery (maximum forest extent) scenarios\n",
        "# Complete recovery requires the 'forest_9999_comrec.tif' predictor, which assumes all land recovers to forest.\n",
        "\n",
        "define_restoration_recovery = True\n",
        "\n",
        "# 'Complete recovery' management proxy, usually a protected area, which will be expanded to the scenario area\n",
        "comrest_comrec_proxy = 'pa_taman_negara_ais'\n",
        "\n",
        "# 'Complete recovery' redundant management areas, which will be removed from the scenario area\n",
        "comrest_comrec_redundant_areas = [\n",
        "  # \"pa_with_edge_effects_endau_rompin_johor\",\n",
        "  # \"pa_with_edge_effects_endau_rompin_pahang\",\n",
        "  # \"pa_with_edge_effects_krau\",\n",
        "]\n",
        "\n",
        "if define_restoration_recovery:\n",
        "\n",
        "  # Expand the comrec proxy management predictor to the entire scenario area\n",
        "  comrest_comrec_proxy_dir = join(predictors_dir, f\"{comrest_comrec_proxy}.tif\")\n",
        "  comrest_comrec_proxy_array = gdal.Open(comrest_comrec_proxy_dir).ReadAsArray()\n",
        "  comrest_comrec_proxy_max_value = comrest_comrec_proxy_array.max()\n",
        "  print(f\"The maximum 'complete recovory' management area proxy value is {comrest_comrec_proxy_max_value}.\")\n",
        "  comrest_comrec_proxy_max_array = np.where(comrest_comrec_proxy_array, comrest_comrec_proxy_max_value, comrest_comrec_proxy_max_value)\n",
        "  comrest_comrec_proxy_max_dir = join(predictors_dir, f\"{comrest_comrec_proxy}_9999_comrec.tif\")\n",
        "  if not exists(comrest_comrec_proxy_max_dir): export_array_as_tif(comrest_comrec_proxy_max_array, comrest_comrec_proxy_max_dir, template = comrest_comrec_proxy_dir)\n",
        "\n",
        "  # Remove the comrec redundant management predictors from the entire study area\n",
        "  for redundant_area in comrest_comrec_redundant_areas:\n",
        "    redundant_area_dir = join(predictors_dir, f\"{redundant_area}.tif\")\n",
        "    redundant_area_array = gdal.Open(redundant_area_dir).ReadAsArray()\n",
        "    redundant_area_min_value = redundant_area_array.min()\n",
        "    print(f\"The minimum 'complete recovory' redundant management area ({redundant_area}) value is {redundant_area_min_value}.\")\n",
        "    redundant_area_min_array = np.where(redundant_area_array, redundant_area_min_value, redundant_area_min_value)\n",
        "    redundant_area_min_dir = join(predictors_dir, f\"{redundant_area}_9999_comrec.tif\")\n",
        "    if not exists(redundant_area_min_dir): export_array_as_tif(redundant_area_min_array, redundant_area_min_dir, redundant_area_dir)\n",
        "\n",
        "  # Determine the minimum disturbance value, assuming it is present in the first historic scenario year\n",
        "  example_disturbance = join(predictors_dir, f\"disturbance_with_edge_effects_{first_historic_predictor_year}.tif\")\n",
        "  example_disturbance_array = gdal.Open(example_disturbance).ReadAsArray()\n",
        "  minimum_disturbance_value = example_disturbance_array.min()\n",
        "  print(f\"The minimum disturbance value is {minimum_disturbance_value}, which will be used to create the 'no disturbance' predictors.\")\n",
        "  # Create a minimum disturbance array to export for the 'no disturbance' predictor years\n",
        "  minimum_disturbance_array = np.where(example_disturbance_array, minimum_disturbance_value, minimum_disturbance_value)\n",
        "  # Generate 'no disturbance' predictor for restoration and recovery\n",
        "  disturbance_comrest_comrec_name = \"disturbance_with_edge_effects_9999_comrest_comrec\"\n",
        "  disturbance_comrest_comrec_path = join(predictors_dir, f\"{disturbance_comrest_comrec_name}.tif\")\n",
        "  if not exists(disturbance_comrest_comrec_path): export_array_as_tif(minimum_disturbance_array, disturbance_comrest_comrec_path, template = example_disturbance)\n",
        "\n",
        "  # Generate forest predictor for restoration (unchanged from most recent historic year)\n",
        "  forest_most_recent_path = join(predictors_dir, f\"forest_with_edge_effects_{last_historic_predictor_year}.tif\")\n",
        "  forest_most_recent_array = gdal.Open(forest_most_recent_path).ReadAsArray()\n",
        "  forest_comrest_name = \"forest_with_edge_effects_9999_comrest\"\n",
        "  forest_comrest_path = join(predictors_dir, f\"{forest_comrest_name}.tif\")\n",
        "  if not exists(forest_comrest_path): export_array_as_tif(forest_most_recent_array, forest_comrest_path, template = forest_most_recent_path)\n",
        "\n",
        "  # Generate 'complete restoration' scenario csv\n",
        "  comrest_scenario_predictors = []\n",
        "  forest_comrest = \"forest_with_edge_effects_9999_comrest\"\n",
        "  for scenario_predictor in scenario_predictors:\n",
        "    if \"forest_with_edge_effects\" in scenario_predictor:\n",
        "      comrest_scenario_predictors.append(forest_comrest_name)\n",
        "    if \"disturbance_with_edge_effects\" in scenario_predictor:\n",
        "      comrest_scenario_predictors.append(disturbance_comrest_comrec_name)\n",
        "    if scenario_predictor == comrest_comrec_proxy:\n",
        "      comrest_scenario_predictors.append(f\"{comrest_comrec_proxy}_9999_comrec\")\n",
        "    for redundant_area in comrest_comrec_redundant_areas:\n",
        "      if scenario_predictor == redundant_area:\n",
        "        comrest_scenario_predictors.append(f\"{redundant_area}_9999_comrec\")\n",
        "    # Compile 'complete restoration' predictors and save as a .csv\n",
        "    comrest_predictors = sorted(comrest_scenario_predictors + constant_predictors)\n",
        "    comrest_scenario_filename = \"9999_comrest.csv\"\n",
        "    comrest_scenario_dir = join(scenarios_model_dir, comrest_scenario_filename)\n",
        "    pd.DataFrame(comrest_predictors).to_csv(comrest_scenario_dir, index=False)\n",
        "\n",
        "\n",
        "  # Create 'complete recovery' forest with edge effects (requires a resampled TMF Transition Map Subtypes)\n",
        "  tmf_subtypes_filename = \"tmf_TransitionMap_Subtypes_b1.tif\"\n",
        "  tmf_subtypes_path = join(predictor_resampled_dir, tmf_subtypes_filename)\n",
        "  if exists(tmf_subtypes_path):\n",
        "    forest_comrec_name = \"forest_with_edge_effects_9999_comrec\"\n",
        "    forest_comrec_path = join(predictors_dir, f\"{forest_comrec_name}.tif\")\n",
        "    if not exists(forest_comrec_path):\n",
        "      tmf_subtypes_array = gdal.Open(tmf_subtypes_path).ReadAsArray()\n",
        "      # Convert all water values to 'nodata' and non-water values to '1'\n",
        "      comrec_forest_array = np.where((tmf_subtypes_array >= 70) & (tmf_subtypes_array <= 79), 0, 1)\n",
        "      # Set smoothing kernel\n",
        "      kernel = Gaussian2DKernel(x_stddev=1, y_stddev=1)\n",
        "      # Set precision\n",
        "      precision = 2\n",
        "      # Reclassify for binary differentiation after proximity conversion\n",
        "      differentiator_array = comrec_forest_array.copy()\n",
        "      differentiator_array[differentiator_array == 1] = 10\n",
        "      # Positive proximity\n",
        "      positive_distances = ndimage.distance_transform_edt(comrec_forest_array == 0) # target pixels\n",
        "      positive_proximity_array = np.where(positive_distances > 2, 0, positive_distances) # max distance 2\n",
        "      # Negative proximity\n",
        "      negative_distances = ndimage.distance_transform_edt(comrec_forest_array == 1) # target pixels\n",
        "      negative_proximity_array = np.where(negative_distances > 2, 0, negative_distances) # max distance 2\n",
        "      # Sum proximities and differentiator\n",
        "      pixel_prox_summed =  differentiator_array + positive_proximity_array + negative_proximity_array\n",
        "      # Reclassify for better semantic understanding of pixel proximity\n",
        "      pixel_prox_reclassed = pixel_prox_summed.copy()\n",
        "      pixel_prox_reclass_table = [(0, 0, -4), (1, 1, -1), (1.4, 1.5, -2), (2, 2, -3), (10, 10, 3), (11, 11, 0), (11.4, 11.5, 1), (12, 12, 2)]\n",
        "      for min_value, max_value, new_value in pixel_prox_reclass_table:\n",
        "        pixel_prox_reclassed[(pixel_prox_reclassed >= min_value) & (pixel_prox_reclassed <= max_value)] = new_value\n",
        "      # Smooth binary array using 2D convolution\n",
        "      binary_smoothed = convolve(comrec_forest_array, kernel, boundary='extend')\n",
        "      # Sum pixel proximity and smoothed binary array\n",
        "      edge_effects_array = np.round(pixel_prox_reclassed + binary_smoothed, precision)\n",
        "      # Export edge effects predictor\n",
        "      export_array_as_tif(edge_effects_array, forest_comrec_path)\n",
        "\n",
        "  # Generate 'complete recovery' scenario csv\n",
        "  comrec_scenario_predictors = []\n",
        "  for scenario_predictor in scenario_predictors:\n",
        "    if \"forest_with_edge_effects\" in scenario_predictor:\n",
        "      comrec_scenario_predictors.append(forest_comrec_name)\n",
        "    if \"disturbance_with_edge_effects\" in scenario_predictor:\n",
        "      comrec_scenario_predictors.append(disturbance_comrest_comrec_name)\n",
        "    if scenario_predictor == comrest_comrec_proxy:\n",
        "      comrec_scenario_predictors.append(f\"{comrest_comrec_proxy}_9999_comrec\")\n",
        "    for redundant_area in comrest_comrec_redundant_areas:\n",
        "      if scenario_predictor == redundant_area:\n",
        "        comrec_scenario_predictors.append(f\"{redundant_area}_9999_comrec\")\n",
        "    # Compile 'complete recovery' predictors and save as a .csv\n",
        "    comrec_predictors = sorted(comrec_scenario_predictors + constant_predictors)\n",
        "    comrec_scenario_filename = \"9999_comrec.csv\"\n",
        "    comrec_scenario_dir = join(scenarios_model_dir, comrec_scenario_filename)\n",
        "    pd.DataFrame(comrec_predictors).to_csv(comrec_scenario_dir, index=False)\n",
        "\n",
        "  print(f\"Predictor lists for all the complete recovery scenario has been exported to {scenarios_model_dir}/.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UooxZSvPlkb"
      },
      "outputs": [],
      "source": [
        "# Force prediction of years for which earlier historic predictors used in the model do not exist.\n",
        "# This will use comrec forest for yearly predictors <1990, assuming that forest edge was\n",
        "# > 120 m beyond the edge of the year being predicted, and also assume minimal disturbance.\n",
        "# Predictions will be increasingly over estimated the further back they are forced, but a\n",
        "# cut-off of 2007 (15 years ago) should minimise a significant effect on the trend.\n",
        "\n",
        "force_historic = False\n",
        "force_to_historic_scenario = 2008\n",
        "\n",
        "if force_historic:\n",
        "  forced_scenario_range = range(force_to_historic_scenario, minimum_historic_scenario)\n",
        "  forced_historic_scenarios = list(forced_scenario_range)\n",
        "  forced_predictor_range = range(force_to_historic_scenario-model_scenario_year_range, first_historic_predictor_year)\n",
        "  forced_predictor_years = list(forced_predictor_range)\n",
        "\n",
        "  forced_forest_base = join(predictors_dir, f\"forest_with_edge_effects_9999_comrec.tif\")\n",
        "\n",
        "  # Determine the minimum disturbance value, assuming it is present in the first historic scenario year\n",
        "  example_disturbance = join(predictors_dir, f\"disturbance_with_edge_effects_{first_historic_predictor_year}.tif\")\n",
        "  example_disturbance_array = gdal.Open(example_disturbance).ReadAsArray()\n",
        "  minimum_disturbance_value = example_disturbance_array.min()\n",
        "  print(f\"The minimum disturbance value is {minimum_disturbance_value}, which will be used to create the 'no disturbance' predictors.\")\n",
        "  # Create a minimum disturbance array to export for the 'no disturbance' predictor years\n",
        "  minimum_disturbance_array = np.where(example_disturbance_array, minimum_disturbance_value, minimum_disturbance_value)\n",
        "\n",
        "  # Create proxy predictors for years out of data range\n",
        "  for forced_predictor_year in forced_predictor_years:\n",
        "    # Create forced scenario predictors for every year until the minimum predictor year\n",
        "    # Copy the forest predictor for comrec\n",
        "    forest_forced = join(predictors_dir, f\"forest_with_edge_effects_{forced_predictor_year}.tif\")\n",
        "    if not exists(forest_forced): shutil.copyfile(forced_forest_base, forest_forced)\n",
        "    # Export a 'no disturbance' predictor for every year to the minimum prediction year\n",
        "    disturbance_forced = join(predictors_dir, f\"disturbance_with_edge_effects_{forced_predictor_year}.tif\")\n",
        "    if not exists(disturbance_forced): export_array_as_tif(minimum_disturbance_array, disturbance_forced, template = example_disturbance)\n",
        "    print(f\"Proxy predictors have been generated or already exist for {forced_predictor_year}.\")\n",
        "\n",
        "  for forced_historic_scenario in forced_historic_scenarios:\n",
        "    forced_year_difference = model_scenario - forced_historic_scenario\n",
        "    forced_historic_scenario_predictors = []\n",
        "    for scenario_predictor in scenario_predictors:\n",
        "      try:\n",
        "        forced_year_change = int(scenario_predictor[-4:]) - forced_year_difference\n",
        "        forced_historic_scenario_predictor = scenario_predictor[:-4] + str(forced_year_change)\n",
        "        forced_historic_scenario_predictors.append(forced_historic_scenario_predictor)\n",
        "      except:\n",
        "        forced_historic_scenario_predictors.append(scenario_predictor)\n",
        "    # Compile forced historic predictors and save as a .csv\n",
        "    forced_historic_predictors = sorted(forced_historic_scenario_predictors + constant_predictors)\n",
        "    forced_historic_scenario_filename = f\"{forced_historic_scenario}.csv\"\n",
        "    forced_historic_scenario_dir = join(scenarios_model_dir,forced_historic_scenario_filename)\n",
        "    pd.DataFrame(forced_historic_predictors).to_csv(forced_historic_scenario_dir, index=False)\n",
        "\n",
        "  print(f\"Lists of predictors for all forced historic scenarios have been exported to {scenarios_model_dir}/.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ9ZCpBs1T5G"
      },
      "source": [
        "# Scenario masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAj6aGaX1YdP"
      },
      "outputs": [],
      "source": [
        "mask_type = 'forest'\n",
        "complete_recovery_mask = True\n",
        "use_gedi_area = False # Instead of project_area.gpkg\n",
        "minimum_historic_year = 2008\n",
        "\n",
        "# Create an inverse project area polygon for masking\n",
        "project_area_path = join(polygons_dir, \"project_area.gpkg\")\n",
        "inverse_project_area_path = join(polygons_dir, \"project_area_inverse.gpkg\")\n",
        "if not exists(inverse_project_area_path):\n",
        "  template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "  template_polygon = gpd.read_file(template_polygon_path)\n",
        "  project_area_polygon = gpd.read_file(project_area_path)\n",
        "  inverse_project_area_polygon = template_polygon['geometry'].difference(project_area_polygon['geometry']).iloc[0]\n",
        "  inverse_project_area_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_project_area_polygon]})\n",
        "  inverse_project_area_polygon_gdf.to_file(inverse_project_area_path, driver=\"GPKG\")\n",
        "\n",
        "gedi_area_path = join(polygons_dir, \"gedi_area.gpkg\")\n",
        "inverse_gedi_area_path = join(polygons_dir, \"gedi_area_inverse.gpkg\")\n",
        "if not exists(inverse_gedi_area_path):\n",
        "  template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "  template_polygon = gpd.read_file(template_polygon_path)\n",
        "  gedi_area_polygon = gpd.read_file(gedi_area_path)\n",
        "  inverse_gedi_area_polygon = template_polygon['geometry'].difference(gedi_area_polygon['geometry']).iloc[0]\n",
        "  inverse_gedi_area_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_gedi_area_polygon]})\n",
        "  inverse_gedi_area_polygon_gdf.to_file(inverse_gedi_area_path, driver=\"GPKG\")\n",
        "\n",
        "# Loop through binary rasters of the mask type\n",
        "for binary in os.listdir(predictor_binary_dir):\n",
        "  binary_path = join(predictor_binary_dir, binary)\n",
        "  if mask_type in binary:\n",
        "    year = binary[-8:-4]\n",
        "    if int(year) >= minimum_historic_year:\n",
        "      mask_path = join(masks_dir, f\"scenario_mask_{year}.tif\")\n",
        "      if not exists(mask_path):\n",
        "        binary_array = gdal.Open(binary_path).ReadAsArray()\n",
        "        mask_array = np.where(binary_array == 0, nodatavalue, 1)\n",
        "        export_array_as_tif(mask_array, mask_path)\n",
        "        # Mask pixels outside the project area\n",
        "        if use_gedi_area: burn_polygon_to_raster(mask_path, inverse_gedi_area_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        else: burn_polygon_to_raster(mask_path, inverse_project_area_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        print(f\"A mask for {year} has been created.\")\n",
        "      else: print(f\"A mask for {year} already exists.\")\n",
        "\n",
        "# Create 'complete recovery' mask, which assumes all land is forest (requires a resampled TMF Transition Map Subtypes)\n",
        "if complete_recovery_mask:\n",
        "  tmf_subtypes_filename = \"tmf_TransitionMap_Subtypes_b1.tif\"\n",
        "  tmf_subtypes_path = join(predictor_resampled_dir, tmf_subtypes_filename)\n",
        "  if exists(tmf_subtypes_path):\n",
        "    comrec_mask_path = join(masks_dir, f\"scenario_mask_comrec_9999.tif\")\n",
        "    if not exists(comrec_mask_path):\n",
        "      tmf_subtypes_array = gdal.Open(tmf_subtypes_path).ReadAsArray()\n",
        "      # Convert all water values to 'nodata' and non-water values to '1'\n",
        "      comrec_mask_array = np.where((tmf_subtypes_array >= 70) & (tmf_subtypes_array <= 79), nodatavalue, 1)\n",
        "      export_array_as_tif(comrec_mask_array, comrec_mask_path)\n",
        "      # Mask pixels outside the project area\n",
        "      if use_gedi_area: burn_polygon_to_raster(comrec_mask_path, inverse_gedi_area_path, fixed_value=nodatavalue, all_touched=False)\n",
        "      else: burn_polygon_to_raster(comrec_mask_path, inverse_project_area_path, fixed_value=nodatavalue, all_touched=False)\n",
        "  else: print(f\"The resampled TMF Transition Map Subtypes raster is not in the indicated directory: {tmf_subtypes_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy2nfHshozVQ"
      },
      "source": [
        "# Predictor verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chCSZTV4XB8k"
      },
      "outputs": [],
      "source": [
        "# Check that all predictors in all scenario csvs exist\n",
        "scenario_csv_list = []\n",
        "all_predictors_exist = True # Changes to false if predictor missing\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    csv_dir = join(scenarios_model_dir, csv)\n",
        "    csv_predictor_list = pd.Series.tolist(pd.read_csv(csv_dir).iloc[:,0])\n",
        "    csv_predictor_dir_list = []\n",
        "    for csv_predictor in csv_predictor_list:\n",
        "      csv_predictor_dir_list.append(f\"{predictors_dir}/{csv_predictor}.tif\")\n",
        "    for predictor in csv_predictor_dir_list:\n",
        "      if not exists(predictor):\n",
        "        all_predictors_exist = False\n",
        "        print(f\"The following predictor is missing:\\n{predictor}\\n and is required for the scenario '{csv[:-4]}'\")\n",
        "\n",
        "if all_predictors_exist: print(\"All required predictors are present.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAqAaDfYkY7T"
      },
      "outputs": [],
      "source": [
        "# Is the scenario area equal to the original template area?\n",
        "original_template_area = True\n",
        "\n",
        "# If not, create a new template for the scenario area and upload to:\n",
        "# '6_scenarios/[model]/[scenario_area]/template.tif'\n",
        "if original_template_area: scenario_template_dir = join(areas_dir, \"template.tif\")\n",
        "else: scenario_template_dir = join(scenario_area_dir, \"template.tif\")\n",
        "print(f\"The following is being used as a template to verify scenario predictor dimensions and projections:\\n{scenario_template_dir}\")\n",
        "\n",
        "\n",
        "scenario_template = gdal.Open(scenario_template_dir)\n",
        "scenario_template_dimensions, scenario_template_projection = scenario_template.GetGeoTransform(), scenario_template.GetProjection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBb-Cws3mMUy"
      },
      "outputs": [],
      "source": [
        "predictor_issue = False\n",
        "for predictor in os.listdir(predictors_dir):\n",
        "  if predictor.endswith('.tif'):\n",
        "    predictor_dir = join(predictors_dir, predictor)\n",
        "    predictor_open = gdal.Open(predictor_dir)\n",
        "    predictor_dimensions, predictor_projection = predictor_open.GetGeoTransform(), predictor_open.GetProjection()\n",
        "    if predictor_dimensions != scenario_template_dimensions:\n",
        "      print(f\"{predictor} dimensions:\\n{predictor_dimensions}\\ndo not match the scenario template dimensions:\\n{scenario_template_dimensions}\\n\")\n",
        "      predictor_issue = True\n",
        "    if predictor_projection != scenario_template_projection:\n",
        "      print(f\"{predictor} projection:\\n{predictor_projection}\\ndoes not match the scenario template projection:\\n{scenario_template_projection}\\n\\n\")\n",
        "      predictor_issue = True\n",
        "\n",
        "if not predictor_issue: print(f\"All predictors in the following directory have the correct dimensions and projection:\\n{predictors_dir}\")\n",
        "else: print(\"Correct and / or resample the predictor(s) in GIS software.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T9UqJrzWECr"
      },
      "source": [
        "# Template tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zizOofP5dgW"
      },
      "outputs": [],
      "source": [
        "# Load the model scenario predictors for tile template creation\n",
        "model_scenario_predictors = pd.Series.tolist(pd.read_csv(model_scenario_dir).iloc[:,0])\n",
        "model_scenario_predictors_dirs = [predictors_dir + '/' + predictor + '.tif' for predictor in model_predictors]\n",
        "# Create a template predictor array from the first predictor\n",
        "template_base = gdal.Open(model_scenario_predictors_dirs[0])\n",
        "template_base_array = template_base.ReadAsArray()\n",
        "template_base_xsize, template_base_ysize = template_base.GetRasterBand(1).XSize, template_base.GetRasterBand(1).YSize\n",
        "print(f\"The template predictor is {template_base_xsize} x {template_base_ysize} pixels.\")\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles_exist = len(template_tile_list)\n",
        "\n",
        "if n_tiles_exist < 1: print(\"There are currently no template tiles. Run the next section.\")\n",
        "if n_tiles_exist >= 1:\n",
        "  tile_size_y_rounded_exist = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).YSize\n",
        "  tile_size_y_remainder_exist = gdal.Open(join(tile_templates_dir,f'template_tile_{n_tiles_exist}.tif')).GetRasterBand(1).YSize\n",
        "  if n_tiles_exist == 1: print(f\"There is a single 'tile' with a height of {tile_size_y_rounded_exist}.\"); tile_size_y_remainder_exist = 0\n",
        "  else: print(f\"There are {n_tiles_exist} template tiles, the first {n_tiles_exist-1} having a height of {tile_size_y_rounded_exist} pixels, the last {tile_size_y_remainder_exist} pixels.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njhqSH8e5cPu"
      },
      "outputs": [],
      "source": [
        "# Large scenario areas and / or numbers of predictors may be too much for the available memory.\n",
        "# This section defines how to split predictions into tiles that can then be merged.\n",
        "override_n_tiles = True # Useful if the tile number has already been tested.\n",
        "n_tiles_override = 1\n",
        "tile_size_y_rounded_override = int(7092/n_tiles_override)\n",
        "\n",
        "memory_utilisation = 0.5 # Ideally set to 0.5 to avoid crashes\n",
        "number_of_processes = 1\n",
        "\n",
        "# Google Colab TPU is (at present) best cost / price ratio, and faster than the standard GPU.\n",
        "# Premium GPU is about 2.5x faster than TPU, but disproportionately expensive.\n",
        "gpu_premium = True\n",
        "\n",
        "assert memory_utilisation > 0 and memory_utilisation <= 1, \"Set memory_utilisation to a value between 0 and 1\"\n",
        "print(f'There are {number_of_processes} CPUs available for parallel processing.')\n",
        "\n",
        "# Check for GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0': print('GPU device not found')\n",
        "else: print(f\"Found GPU at: {device_name}\")\n",
        "memory_utilisation = memory_utilisation / 4\n",
        "if gpu_premium:\n",
        "  memory_utilisation = memory_utilisation * 8 # More memory available if premium\n",
        "  print(\"Using premium GPU settings. Make sure runtime is actually set to GPU Premium, otherwise set gpu_premium to False.\")\n",
        "\n",
        "# Calculate total size of predictor stack\n",
        "predictor_stack_size = template_base_array.size * len(model_scenario_predictors_dirs)\n",
        "\n",
        "# Calculate memory and the number of tiles required\n",
        "total_memory_needed = 64 / 8 * predictor_stack_size\n",
        "print(f'RAM required for each prediction: ~{total_memory_needed/(1024**3):.3f} GB')\n",
        "print(f'RAM currently available: {psutil.virtual_memory().free / (1024**3):.3f} GB')\n",
        "n_tiles_temp = int(np.ceil(total_memory_needed / (psutil.virtual_memory().free * memory_utilisation / number_of_processes)))\n",
        "\n",
        "# Calculate template tile size (split on the y axis only)\n",
        "tile_size_y_rounded = int(np.ceil(template_base_ysize/n_tiles_temp)) # Round the number of y pixels in each tile\n",
        "tile_size_y_remainder = template_base_ysize%tile_size_y_rounded # Calculate the remainder for the last tile\n",
        "n_tiles = len(range(0, template_base_ysize, tile_size_y_rounded)) # Update the number of tiles to include the remainder\n",
        "\n",
        "if override_n_tiles:\n",
        "  tile_size_y_rounded = tile_size_y_rounded_override\n",
        "  tile_size_y_remainder = template_base_ysize%tile_size_y_rounded\n",
        "  n_tiles = n_tiles_override\n",
        "  print(\"n_tiles has been overridden.\")\n",
        "\n",
        "# Check if tiles need to be changed\n",
        "if n_tiles == n_tiles_exist and tile_size_y_rounded == tile_size_y_rounded_exist and tile_size_y_remainder == tile_size_y_remainder_exist:\n",
        "  print(\"No changes to tiles are required.\")\n",
        "else:\n",
        "  # Clear all tile directories\n",
        "  for tile in Path(tile_templates_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for tile in Path(tile_predictors_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "  for scenario_stack_dir in Path(tile_predictor_stacks_dir).glob(\"**/*\"):\n",
        "    shutil.rmtree(scenario_stack_dir)\n",
        "  for tile in Path(tile_prediction_cache_dir).glob(\"**/*\"):\n",
        "    if tile.is_file(): tile.unlink()\n",
        "\n",
        "  print(f'The prediction template will be divided into {n_tiles} tiles to avoid crashing.')\n",
        "\n",
        "  # Generate new tile templates based on available memory\n",
        "  tile_number = 1\n",
        "  for y_start in range(0, template_base_ysize, tile_size_y_rounded):\n",
        "    if tile_size_y_remainder != 0 and tile_number == n_tiles: tile_size_y = tile_size_y_remainder\n",
        "    else: tile_size_y = tile_size_y_rounded\n",
        "    tiling_string = \"gdal_translate -of GTIFF -srcwin \" + str(0)+ \", \" + str(y_start) + \", \" + str(template_base_xsize) + \", \" + str(tile_size_y) + \" \" + str(model_scenario_predictors_dirs[0]) + \" \" + str(tile_templates_dir) + \"/template_tile_\" + str(tile_number) + \".tif\"\n",
        "    os.system(tiling_string)\n",
        "    tile_number += 1\n",
        "\n",
        "  print(\"Template tile creation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkNV_N7UKcMd"
      },
      "source": [
        "# Predictor tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1d1ELggV4jF"
      },
      "outputs": [],
      "source": [
        "# Create predictor tiles.\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "tile_size_y_rounded = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).YSize\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "if n_tiles == 1: print(\"Predictor tile creation skipped. Predictor stack creation will use the original predictors.\")\n",
        "else:\n",
        "  # Progress\n",
        "  n_predictors = len(os.listdir(predictors_dir))\n",
        "  predictor_progress_index, predictor_progress_label = 0, widgets.Label(value=f\"Predictor progress: 0 / {n_predictors}\")\n",
        "  display(predictor_progress_label)\n",
        "  tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "  display(tile_progress_label)\n",
        "\n",
        "  # Loop through each predictor in the 6_scenarios predictors directory\n",
        "  for predictor in os.listdir(predictors_dir):\n",
        "    # Create list of tile directories\n",
        "    predictor_dir = join(predictors_dir, predictor)\n",
        "    predictor_array = gdal.Open(predictor_dir).ReadAsArray()\n",
        "    # Split the predictor array into chunks based on tile size\n",
        "    predictor_chunks = np.array_split(predictor_array, np.arange(tile_size_y_rounded, len(predictor_array), tile_size_y_rounded))\n",
        "    tile_count = 1\n",
        "    # Loop through tiles and export as .tif\n",
        "    for tile in range(n_tiles):\n",
        "      predictor_tile_filename = f\"{predictor[:-4]}_{tile_count}.tif\"\n",
        "      # Check if tile already exists\n",
        "      predictor_tile_exists = False\n",
        "      for predictor_tile in os.listdir(tile_predictors_dir):\n",
        "        if predictor_tile == predictor_tile_filename: predictor_tile_exists=True\n",
        "        # If predictor tile does not exist:\n",
        "      if predictor_tile_exists == False:\n",
        "        template_tile_dir = join(tile_templates_dir, f\"template_tile_{tile_count}.tif\")\n",
        "        export_array_as_tif(predictor_chunks[tile_count-1], join(tile_predictors_dir,predictor_tile_filename), template_tile_dir, compress = False)\n",
        "        tile_count += 1\n",
        "      # Update tile progress\n",
        "      tile_progress_index += 1\n",
        "      tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_tiles}\"\n",
        "    tile_progress_index = 0\n",
        "    # Update predictor progress\n",
        "    predictor_progress_index += 1\n",
        "    predictor_progress_label.value = f\"Predictor progress: {predictor_progress_index} / {n_predictors}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0r89JLYW_xU"
      },
      "source": [
        "# Predictor stacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaHFv1nuW5CG"
      },
      "outputs": [],
      "source": [
        "# Create predictor stack arrays for each scenario\n",
        "\n",
        "# Collect scenarios with .csv predictor lists\n",
        "scenarios_list = []\n",
        "for csv in os.listdir(scenarios_model_dir):\n",
        "  if csv.endswith('.csv'):\n",
        "    scenarios_list.append(csv[:-4])\n",
        "\n",
        "# Select scenarios to generate tiled predictor stacks\n",
        "print(\"scenarios_to_stack = [\")\n",
        "for scenario in sorted(scenarios_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-uJ-EkoxIym"
      },
      "outputs": [],
      "source": [
        "scenarios_to_stack = [\n",
        "  \"2008\",\n",
        "  \"2009\",\n",
        "  \"2010\",\n",
        "  \"2011\",\n",
        "  \"2012\",\n",
        "  \"2013\",\n",
        "  \"2014\",\n",
        "  \"2015\",\n",
        "  \"2016\",\n",
        "  \"2017\",\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2022\",\n",
        "  \"2027_nodef\",\n",
        "  \"2027_nodist\",\n",
        "  \"2032_nodef\",\n",
        "  \"2032_nodist\",\n",
        "  \"2037_nodef\",\n",
        "  \"2037_nodist\",\n",
        "  \"9999_comrec\",\n",
        "  \"9999_comrest\",\n",
        "]\n",
        "\n",
        "assert set(scenarios_to_stack).issubset(scenarios_list), \"Not all selected scenarios exist.\"\n",
        "\n",
        "use_tmf_data = False\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "# Progress\n",
        "scenario_progress_index, scenario_progress_label = 0, widgets.Label(value=f\"Scenario progress: 0 / {len(scenarios_to_stack)}\")\n",
        "display(scenario_progress_label)\n",
        "stack_progress_index, stack_progress_label = 0, widgets.Label(value=f\"Tiled predictor stack progress: 0 / {n_tiles}\")\n",
        "display(stack_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_stack:\n",
        "    # Define directory and predictors\n",
        "    scenario_predictor_stacks_dir = join(tile_predictor_stacks_dir, scenario)\n",
        "    makedirs(scenario_predictor_stacks_dir, exist_ok=True)\n",
        "    scenario_predictors_csv = join(scenarios_model_dir,f\"{scenario}.csv\")\n",
        "    scenario_predictors = pd.Series.tolist(pd.read_csv(scenario_predictors_csv).iloc[:,0])\n",
        "    # Set the number of stacks to the number of tiles\n",
        "    if n_tiles == 0: n_stacks = 1\n",
        "    else: n_stacks = n_tiles\n",
        "    # Create a tile count to match the predictor stack\n",
        "    tile_count = 1\n",
        "    for tile in range(n_stacks):\n",
        "      scenario_stack_filename = f\"predictor_stack_{scenario}_{tile_count}.npy\"\n",
        "      # Check if predictor stack already exists\n",
        "      predictor_stack_exists = False\n",
        "      for predictor_stack in os.listdir(scenario_predictor_stacks_dir):\n",
        "        if predictor_stack == scenario_stack_filename: predictor_stack_exists=True\n",
        "      # If scenario prediction tile does not exist:\n",
        "      if predictor_stack_exists == False:\n",
        "        scenario_tile_stack_dir = join(scenario_predictor_stacks_dir, scenario_stack_filename)\n",
        "        # Create predictor chunks (arrays) from tiles\n",
        "        if n_stacks == 1: predictor_tiles_dirs = [f\"{predictors_dir}/{predictor}.tif\" for predictor in scenario_predictors]\n",
        "        else: predictor_tiles_dirs = [f\"{tile_predictors_dir}/{predictor}_{tile_count}.tif\" for predictor in scenario_predictors]\n",
        "        predictor_array_chunks = []\n",
        "        for predictor in predictor_tiles_dirs:\n",
        "          predictor_array_chunk = gdal.Open(predictor).ReadAsArray()\n",
        "          # Add current year's disturbance / edge effects (which was excluded from training) to the previous yearly predictor as a proxy\n",
        "          if use_tmf_data:\n",
        "            if 'disturbance' in predictor or 'forest' in predictor:\n",
        "              predictor_year = int(re.search(r'(\\d{4})', predictor.split('/')[-1]).group())\n",
        "              current_year = int(scenario[:4])\n",
        "              previous_scenario_year = current_year - 1\n",
        "              if predictor_year == previous_scenario_year and predictor_year != 9999:\n",
        "                if n_stacks == 1: current_predictor_array_chunk = gdal.Open(f\"{predictor[:-8]}{str(current_year)}.tif\").ReadAsArray()\n",
        "                else: current_predictor_array_chunk = gdal.Open(f\"{predictor[:-10]}{str(current_year)}_{tile_count}.tif\").ReadAsArray()\n",
        "                # Higher value denotes disturbance, lower value denotes forest edge effects\n",
        "                if 'disturbance' in predictor: predictor_array_chunk = np.maximum(predictor_array_chunk, current_predictor_array_chunk)\n",
        "                if 'forest' in predictor: predictor_array_chunk = np.minimum(predictor_array_chunk, current_predictor_array_chunk)\n",
        "          predictor_array_chunks.append(predictor_array_chunk)\n",
        "        # Create a predictor stack from chunks\n",
        "        predictor_stack = np.dstack(predictor_array_chunks)\n",
        "        predictor_array_chunks = None # Flush chunks\n",
        "        stack_height, stack_width, stack_n_predictors = predictor_stack.shape\n",
        "        # Convert predictor stack to 2D numpy array with predictors as columns\n",
        "        predictor_stack_reshaped = predictor_stack.reshape(stack_height * stack_width, stack_n_predictors)\n",
        "        predictor_stack = None # Flush stack\n",
        "        # Save as a numpy file\n",
        "        np.save(scenario_tile_stack_dir, predictor_stack_reshaped)\n",
        "        predictor_stack_reshaped = None # Flush reshaped stack\n",
        "      # Update progress\n",
        "      tile_count += 1\n",
        "      stack_progress_index += 1\n",
        "      stack_progress_label.value = f\"Tiled predictor stack progress: {stack_progress_index} / {n_stacks}\"\n",
        "    # Reset tile progress\n",
        "    stack_progress_index = 0\n",
        "    scenario_progress_index += 1\n",
        "    scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index} / {len(scenarios_to_stack)}\"\n",
        "print(\"\\nPredictor stacks complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oysAS9DI7yRg"
      },
      "source": [
        "# Predict scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y92V4nKGcyE"
      },
      "outputs": [],
      "source": [
        "# Collect available scenarios from the predictor stack tiles directory\n",
        "scenario_stacks_list = []\n",
        "for scenario in os.listdir(tile_predictor_stacks_dir):\n",
        "    scenario_stacks_list.append(scenario)\n",
        "\n",
        "# Select scenarios to predict\n",
        "print(\"scenarios_to_predict = [\")\n",
        "for scenario in sorted(scenario_stacks_list):\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4lEC--FG2lq"
      },
      "outputs": [],
      "source": [
        "scenarios_to_predict = [\n",
        "  \"2008\",\n",
        "  \"2009\",\n",
        "  \"2010\",\n",
        "  \"2011\",\n",
        "  \"2012\",\n",
        "  \"2013\",\n",
        "  \"2014\",\n",
        "  \"2015\",\n",
        "  \"2016\",\n",
        "  \"2017\",\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2022\",\n",
        "  \"2027_nodef\",\n",
        "  \"2027_nodist\",\n",
        "  \"2032_nodef\",\n",
        "  \"2032_nodist\",\n",
        "  \"2037_nodef\",\n",
        "  \"2037_nodist\",\n",
        "  \"9999_comrec\",\n",
        "  \"9999_comrest\",\n",
        "]\n",
        "\n",
        "assert set(scenarios_to_predict).issubset(scenario_stacks_list), \"Not all selected scenarios exist.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TibU8xTZfcuL"
      },
      "outputs": [],
      "source": [
        "# Change this and the code within the block accordingly.\n",
        "add_covariates = True # Adds a selected covariate value as the predictor\n",
        "sensitivity_value = 0.99\n",
        "beam_value = 5\n",
        "# 5 is the first of the full beams, with the least bias on AGBD.\n",
        "# Cover beams 1 - 4 underestimate. Full beams 7 - 8 overestimate. 5 - 6 average.\n",
        "\n",
        "# Check for GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0': print('GPU device not found')\n",
        "else: print(f\"Found GPU at: {device_name}\")\n",
        "\n",
        "# Load model\n",
        "if categorise_variate: XGBPredictor = xgb.XGBClassifier(**final_hyperparameters)\n",
        "else: XGBPredictor = xgb.XGBRegressor(**final_hyperparameters)\n",
        "XGBPredictor.load_model(fname=selected_model_json)\n",
        "\n",
        "# Check existing tile parameters\n",
        "template_tile_list = []\n",
        "for file in os.listdir(tile_templates_dir):\n",
        "  if file.endswith('.tif') and file[:13] == 'template_tile':\n",
        "    template_tile_list.append(file)\n",
        "n_tiles = len(template_tile_list)\n",
        "\n",
        "assert n_tiles > 0, \"There are no template tiles. Run the template tiles section, even if only one is created.\"\n",
        "template_tile_x = gdal.Open(join(tile_templates_dir,'template_tile_1.tif')).GetRasterBand(1).XSize\n",
        "print(f\"There are {n_tiles} template tiles.\")\n",
        "\n",
        "# Scenario progress\n",
        "scenario_progress_index = 0\n",
        "scenario_progress_label = widgets.Label(f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\")\n",
        "display(scenario_progress_label)\n",
        "\n",
        "# Tile progress\n",
        "tile_progress_index, tile_progress_label = 0, widgets.Label(value=f\"Tile progress: 0 / {n_tiles}\")\n",
        "display(tile_progress_label)\n",
        "\n",
        "# Loop through each scenario\n",
        "for scenario in scenarios_to_predict:\n",
        "  # Define scenario filename and check if exists\n",
        "  scenario_prediction_unmasked_filename = f\"{scenario}__{selected_scenario_area}_{selected_model}_unmasked.tif\"\n",
        "  scenario_prediction_unmasked_exists=False\n",
        "  for scenario_prediction in os.listdir(scenario_predictions_unmasked_dir):\n",
        "    if scenario_prediction == scenario_prediction_unmasked_filename:\n",
        "      scenario_prediction_unmasked_exists=True\n",
        "  # If scenario prediction does not exist:\n",
        "  if scenario_prediction_unmasked_exists == False:\n",
        "    # Get number of stacks\n",
        "    scenario_predictor_stack_dir = join(tile_predictor_stacks_dir, scenario)\n",
        "    n_stacks = len(os.listdir(scenario_predictor_stack_dir))\n",
        "    # Create a tile cache directory for the prediction\n",
        "    tile_cache_scenario_dir = join(tile_prediction_cache_dir, scenario_prediction_unmasked_filename[:-4])\n",
        "    makedirs(tile_cache_scenario_dir, exist_ok=True)\n",
        "    # Create a tile count to match the predictor stack chunk\n",
        "    for stack in range(1, n_stacks+1):\n",
        "      scenario_tile_filename = f\"scenario_tile_{stack}.tif\"\n",
        "      # Check if tile already exists\n",
        "      scenario_tile_exists = False\n",
        "      for scenario_tile in os.listdir(tile_cache_scenario_dir):\n",
        "        if scenario_tile == scenario_tile_filename: scenario_tile_exists=True\n",
        "      # If scenario prediction tile does not exist:\n",
        "      if scenario_tile_exists == False:\n",
        "        # Load template tile parameters\n",
        "        template_tile_dir = join(tile_templates_dir, f\"template_tile_{stack}.tif\")\n",
        "        template_tile_y = gdal.Open(template_tile_dir).GetRasterBand(1).YSize\n",
        "        template_tile_x = gdal.Open(template_tile_dir).GetRasterBand(1).XSize\n",
        "        # Load predictor tile stack\n",
        "        stack_filename = f\"predictor_stack_{scenario}_{stack}.npy\"\n",
        "        predictor_stack = np.load(join(scenario_predictor_stack_dir, stack_filename))\n",
        "        # Add covariates (sensitivity and BEAM)\n",
        "        if add_covariates: predictor_stack = np.hstack((predictor_stack,\n",
        "                           np.full((predictor_stack.shape[0], 1), beam_value, dtype=int),\n",
        "                           np.full((predictor_stack.shape[0], 1), sensitivity_value, dtype=float)\n",
        "                           ))\n",
        "        # Define prediction array and reshape\n",
        "        prediction = XGBPredictor.predict(predictor_stack)\n",
        "        predictor_stack = None # Flush predictor stack\n",
        "        prediction_tile = prediction.reshape((template_tile_y, template_tile_x))\n",
        "        prediction = None # Flush prediction\n",
        "        # Export prediction array as .tif\n",
        "        export_array_as_tif(prediction_tile, join(tile_cache_scenario_dir, scenario_tile_filename), template = template_tile_dir, compress = False)\n",
        "        prediction_tile = None # Flush prediction tile\n",
        "        # Update progress\n",
        "      tile_progress_index += 1\n",
        "      tile_progress_label.value = f\"Tile progress: {tile_progress_index} / {n_stacks}\"\n",
        "    # Prepare empty array for merging tiles\n",
        "    prediction_array = np.empty((0,template_tile_x))\n",
        "    # Read each tile .tif as an array, stack, then export as a .tif\n",
        "    for subdir in os.listdir(tile_cache_scenario_dir):\n",
        "      if subdir.endswith('.tif'):\n",
        "        tile_dir = join(tile_cache_scenario_dir, subdir)\n",
        "        prediction_array = np.vstack((prediction_array, gdal.Open(tile_dir).ReadAsArray()))\n",
        "    # Define scenario template\n",
        "    scenario_template = join(predictors_dir, os.listdir(predictors_dir)[0])\n",
        "    scenario_prediction_unmasked_dir = join(scenario_predictions_unmasked_dir, f\"{scenario_prediction_unmasked_filename}\")\n",
        "    export_array_as_tif(prediction_array, scenario_prediction_unmasked_dir, template = scenario_template, compress = True)\n",
        "    # Delete scenario tile cache directory\n",
        "    shutil.rmtree(tile_cache_scenario_dir)\n",
        "  # Reset tile progress\n",
        "  tile_progress_index = 0\n",
        "  # Update scenario progress\n",
        "  scenario_progress_index += 1\n",
        "  scenario_progress_label.value = f\"Scenario progress: {scenario_progress_index}/{len(scenarios_to_predict)}\"\n",
        "print(\"\\nScenario predictions complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diwt1peRdXpj"
      },
      "source": [
        "# Mask scenario predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_historic_predictor_year = '2022'\n",
        "first_historic_predictor_year = '1990'\n",
        "\n",
        "# Mask scenarios with the relevatant mask in 1_areas/masks.\n",
        "for scenario_prediction in os.listdir(scenario_predictions_unmasked_dir): # Loop through each unmasked prediction\n",
        "  scenario_masked_filename = f\"{scenario_prediction[:-13]}.tif\"\n",
        "  scenario_masked_dir = join(scenario_predictions_dir, scenario_masked_filename)\n",
        "  if not exists(scenario_masked_dir):\n",
        "    # Match the year of the scenario (first four characters) to a mask which includes the year in the filename.\n",
        "    mask_exists = False\n",
        "    for mask in os.listdir(masks_dir):\n",
        "      # Match historic scenarios and 9999_comrec (complete recovery) with the respective mask\n",
        "      if scenario_prediction[:4] in mask and 'comrest' not in scenario_prediction and 'nodef_historic' not in scenario_prediction:\n",
        "        selected_mask_filename = mask\n",
        "        selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "        mask_exists = True\n",
        "      # Match 9999_comrest with last historic predictor year mask (e.g. forest 2022)\n",
        "      if 'comrest' in scenario_prediction and last_historic_predictor_year in mask:\n",
        "        selected_mask_filename = mask\n",
        "        selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "        mask_exists = True\n",
        "      # Match 2022_nodef_historic with first historic predictor year mask (e.g. forest 1990)\n",
        "      if 'nodef_historic' in scenario_prediction and first_historic_predictor_year in mask:\n",
        "        selected_mask_filename = mask\n",
        "        selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "        mask_exists = True\n",
        "      # Match other alternate scenarios with the last historic predictor year mask (e.g. forest 2022)\n",
        "      if int(scenario_prediction[:4]) > int(last_historic_predictor_year) and int(scenario_prediction[:4]) < 9999 and last_historic_predictor_year in mask:\n",
        "        selected_mask_filename = mask\n",
        "        selected_mask_dir = join(masks_dir, selected_mask_filename)\n",
        "        mask_exists = True\n",
        "    if mask_exists == False: print(f\"A suitable mask for {scenario_prediction} does not exist.\\n\")\n",
        "    else: # Mask the scenario prediction\n",
        "      print(f\"Masking {scenario_prediction} with {selected_mask_filename}...\")\n",
        "      mask_array = gdal.Open(selected_mask_dir).ReadAsArray()\n",
        "      scenario_prediction_unmasked_dir = join(scenario_predictions_unmasked_dir, scenario_prediction)\n",
        "      scenario_prediction_array = gdal.Open(scenario_prediction_unmasked_dir).ReadAsArray()\n",
        "      # Mask where the mask array is not 1\n",
        "      scenario_masked_array = np.where(mask_array != 1, nodatavalue, scenario_prediction_array)\n",
        "      scenario_masked_filename = f\"{scenario_prediction[:-13]}.tif\"\n",
        "      scenario_masked_dir = join(scenario_predictions_dir, scenario_masked_filename)\n",
        "      export_array_as_tif(scenario_masked_array, scenario_masked_dir, template = selected_mask_dir, compress = True)\n",
        "      print(f\"{scenario_masked_filename} exported.\")"
      ],
      "metadata": {
        "id": "daKAe6nkJ2bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-7ctfgdoF2"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcuArY00N9Mc"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "y_OkHaUkTCyB",
        "6EktN7Mqok75",
        "xy2nfHshozVQ",
        "1T9UqJrzWECr"
      ],
      "gpuType": "V28",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}