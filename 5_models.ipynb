{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/5_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhx977-WIaWJ"
      },
      "source": [
        "# Imports and directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRnNty96IThZ"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "\n",
        "# Mount Google Drive and set base directory\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVU7xbXx8wdq"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs\n",
        "!pip install kaleido\n",
        "!pip install shap\n",
        "!pip install scikit-optimize\n",
        "!pip install tabulate\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpmgsb8KKMGN"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "import json\n",
        "import pickle\n",
        "from datetime import datetime, timedelta\n",
        "from os import makedirs\n",
        "from os.path import exists, join\n",
        "from pathlib import Path\n",
        "from google.colab import runtime\n",
        "import ipywidgets as widgets\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import random\n",
        "import shap\n",
        "import xgboost as xgb\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.stats import randint, uniform\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    mean_squared_error,\n",
        "    r2_score,\n",
        "    accuracy_score\n",
        ")\n",
        "from sklearn.model_selection import KFold\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Define GPU\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found')\n",
        "else:\n",
        "  print(f\"Found GPU at: {device_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQwNO9T0_HPH"
      },
      "outputs": [],
      "source": [
        "# 4_datasets directories\n",
        "datasets_dir = join(base_dir, \"4_datasets/final\")\n",
        "\n",
        "# 5_models directories\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(models_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHzd1jjLJLT1"
      },
      "source": [
        "# Compile new model dataset (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfHM2IdhAeO4"
      },
      "source": [
        "## Select a dataset to import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxF-5sQerT1F"
      },
      "outputs": [],
      "source": [
        "print(\"Select the dataset variates .pkl to add predictors...\\n\")\n",
        "for file in os.listdir(datasets_dir):\n",
        "  if file.endswith(\".pkl\"):\n",
        "    print(f'selected_dataset = \"{file}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dUC5Ysr_zIJ"
      },
      "outputs": [],
      "source": [
        "selected_dataset = \"agbd.pkl\"\n",
        "\n",
        "# Load dataset\n",
        "print(f\"Importing '{selected_dataset}'\")\n",
        "dataset_read = pd.read_pickle(join(datasets_dir, selected_dataset))\n",
        "dataset_imported = dataset_read.copy().reset_index(drop=True)\n",
        "\n",
        "# Print dataset summary\n",
        "print(f\"'{selected_dataset}' with {len(dataset_imported)} rows and {len(dataset_imported.columns)} columns imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EB9V1T2AOb2"
      },
      "source": [
        "## Select ID column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Faht6XP_ANNV"
      },
      "outputs": [],
      "source": [
        "# Check remaining columns and select ID column\n",
        "for col in dataset_imported.columns:\n",
        "  if col[:4] == \"var_\":\n",
        "    print(f'id_column = \"{col}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQAo72JdcPDX"
      },
      "outputs": [],
      "source": [
        "id_column = \"var_shot_number\"\n",
        "\n",
        "dataset_col_list = [id_column]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8GB8AlpAWjH"
      },
      "source": [
        "## Select a variate to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0IhfEcGTDGI"
      },
      "outputs": [],
      "source": [
        "for col in dataset_imported.columns:\n",
        "  if col[:4] == \"var_\" and col not in dataset_col_list:\n",
        "    print(f'selected_variate = \"{col}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEHAsW7QAR41"
      },
      "outputs": [],
      "source": [
        "selected_variate = \"var_agbd\"\n",
        "\n",
        "dataset_col_list = dataset_col_list + [selected_variate]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBjcNYodUWaM"
      },
      "source": [
        "## Select uncertainty metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOVqFj6YUklm"
      },
      "outputs": [],
      "source": [
        "# The uncertainty metrics should allow estimation of one standard deviation of the selected variate.\n",
        "\n",
        "# Check remaining variate columns and select the uncertainty columns, if any.\n",
        "\n",
        "for col in dataset_imported.columns:\n",
        "  if col[:4] == \"var_\" and col not in dataset_col_list:\n",
        "    print(f'uncertainty = \"{col}\"')\n",
        "print(f'uncertainty = None')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYvnR5fsUpRl"
      },
      "outputs": [],
      "source": [
        "uncertainty = \"var_agbd_se\"\n",
        "\n",
        "if uncertainty:\n",
        "  dataset_col_list = dataset_col_list + [uncertainty]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqrQnDhtjvCq"
      },
      "source": [
        "## Add covariates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N19vBF98NRd"
      },
      "outputs": [],
      "source": [
        "# Covariates are used as a predictor in training, but have an 'NA' value for\n",
        "# validation and testing. These should still be included in scenarios as an\n",
        "# entirely 'NA' raster. They may / may not improve accuracy.\n",
        "\n",
        "# Check remaining columns and select covariates column\n",
        "print(\"covariates  = [\")\n",
        "for col in dataset_imported.columns:\n",
        "  if col[:4] == \"var_\" and col not in dataset_col_list:\n",
        "    print(f'\"{col}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amoXKT3PgCNa"
      },
      "outputs": [],
      "source": [
        "covariates = [\n",
        "\"var_beam\",\n",
        "\"var_sensitivity\",\n",
        "]\n",
        "\n",
        "\n",
        "dataset_imported_covar = dataset_imported.copy()\n",
        "\n",
        "# Rename var_ to pre_\n",
        "covariates_renamed = []\n",
        "if len(covariates)>0:\n",
        "  covariates_renamed = [covariate.replace('var','pre') for covariate in covariates]\n",
        "  dataset_imported_covar.rename(\n",
        "      columns={i:j for i,j in zip(covariates, covariates_renamed)}, inplace=True\n",
        "  )\n",
        "\n",
        "# If covariates are object or string, convert to integer for model training\n",
        "categorised_covariates = {}\n",
        "for col in covariates_renamed:\n",
        "    if dataset_imported_covar[col].dtype in ['object', 'string']:\n",
        "        unique_values = sorted(dataset_imported_covar[col].unique())\n",
        "        value_to_int = {val: i+1 for i, val in enumerate(unique_values)}\n",
        "        dataset_imported_covar[col] = dataset_imported_covar[col].map(value_to_int).astype('category')\n",
        "        categorised_covariates[col] = value_to_int\n",
        "print(categorised_covariates)\n",
        "\n",
        "\n",
        "dataset_col_list = dataset_col_list + covariates_renamed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0edRH3sOATkP"
      },
      "source": [
        "## Select predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_pOoS1vacx4"
      },
      "outputs": [],
      "source": [
        "print(\"selected_predictors = [\")\n",
        "for col in sorted(dataset_imported_covar.columns):\n",
        "  if col[:4] == \"pre_\" and col not in dataset_col_list:\n",
        "    print(f'  \"{col}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WvQMvxqF0ny"
      },
      "outputs": [],
      "source": [
        "selected_predictors = [\n",
        "  \"pre_coast_proximity_km\",\n",
        "  \"pre_disturbance_with_edge_effects_1994\",\n",
        "  \"pre_disturbance_with_edge_effects_1995\",\n",
        "  \"pre_disturbance_with_edge_effects_1996\",\n",
        "  \"pre_disturbance_with_edge_effects_1997\",\n",
        "  \"pre_disturbance_with_edge_effects_1998\",\n",
        "  \"pre_disturbance_with_edge_effects_1999\",\n",
        "  \"pre_disturbance_with_edge_effects_2000\",\n",
        "  \"pre_disturbance_with_edge_effects_2001\",\n",
        "  \"pre_disturbance_with_edge_effects_2002\",\n",
        "  \"pre_disturbance_with_edge_effects_2003\",\n",
        "  \"pre_disturbance_with_edge_effects_2004\",\n",
        "  \"pre_disturbance_with_edge_effects_2005\",\n",
        "  \"pre_disturbance_with_edge_effects_2006\",\n",
        "  \"pre_disturbance_with_edge_effects_2007\",\n",
        "  \"pre_disturbance_with_edge_effects_2008\",\n",
        "  \"pre_disturbance_with_edge_effects_2009\",\n",
        "  \"pre_disturbance_with_edge_effects_2010\",\n",
        "  \"pre_disturbance_with_edge_effects_2011\",\n",
        "  \"pre_disturbance_with_edge_effects_2012\",\n",
        "  \"pre_disturbance_with_edge_effects_2013\",\n",
        "  \"pre_disturbance_with_edge_effects_2014\",\n",
        "  \"pre_disturbance_with_edge_effects_2015\",\n",
        "  \"pre_disturbance_with_edge_effects_2016\",\n",
        "  \"pre_disturbance_with_edge_effects_2017\",\n",
        "  \"pre_disturbance_with_edge_effects_2018\",\n",
        "  \"pre_disturbance_with_edge_effects_2019\",\n",
        "  \"pre_disturbance_with_edge_effects_2020\",\n",
        "  \"pre_disturbance_with_edge_effects_2021\",\n",
        "  \"pre_disturbance_with_edge_effects_2022\",\n",
        "  \"pre_forest_with_edge_effects_1994\",\n",
        "  # \"pre_forest_with_edge_effects_1995\",\n",
        "  # \"pre_forest_with_edge_effects_1996\",\n",
        "  # \"pre_forest_with_edge_effects_1997\",\n",
        "  # \"pre_forest_with_edge_effects_1998\",\n",
        "  # \"pre_forest_with_edge_effects_1999\",\n",
        "  # \"pre_forest_with_edge_effects_2000\",\n",
        "  # \"pre_forest_with_edge_effects_2001\",\n",
        "  # \"pre_forest_with_edge_effects_2002\",\n",
        "  # \"pre_forest_with_edge_effects_2003\",\n",
        "  # \"pre_forest_with_edge_effects_2004\",\n",
        "  # \"pre_forest_with_edge_effects_2005\",\n",
        "  \"pre_forest_with_edge_effects_2006\",\n",
        "  # \"pre_forest_with_edge_effects_2007\",\n",
        "  # \"pre_forest_with_edge_effects_2008\",\n",
        "  # \"pre_forest_with_edge_effects_2009\",\n",
        "  # \"pre_forest_with_edge_effects_2010\",\n",
        "  # \"pre_forest_with_edge_effects_2011\",\n",
        "  # \"pre_forest_with_edge_effects_2012\",\n",
        "  # \"pre_forest_with_edge_effects_2013\",\n",
        "  # \"pre_forest_with_edge_effects_2014\",\n",
        "  # \"pre_forest_with_edge_effects_2015\",\n",
        "  # \"pre_forest_with_edge_effects_2016\",\n",
        "  # \"pre_forest_with_edge_effects_2017\",\n",
        "  \"pre_forest_with_edge_effects_2018\",\n",
        "  \"pre_forest_with_edge_effects_2019\",\n",
        "  \"pre_forest_with_edge_effects_2020\",\n",
        "  \"pre_forest_with_edge_effects_2021\",\n",
        "  \"pre_forest_with_edge_effects_2022\",\n",
        "  \"pre_latitude\",\n",
        "  \"pre_longitude\",\n",
        "  # \"pre_pa_ais_with_edge_effects\",\n",
        "  # \"pre_pa_taman_krau_ais_with_edge_effects\",\n",
        "  \"pre_pa_taman_krau_with_edge_effects\",\n",
        "  \"pre_topo_cor_smooth_aspect_cosine\",\n",
        "  \"pre_topo_cor_smooth_aspect_sine\",\n",
        "  \"pre_topo_cor_smooth_circular_variance_aspect_03\",\n",
        "  \"pre_topo_cor_smooth_circular_variance_aspect_07\",\n",
        "  \"pre_topo_cor_smooth_circular_variance_aspect_11\",\n",
        "  \"pre_topo_cor_smooth_deviation_mean_elevation_03\",\n",
        "  \"pre_topo_cor_smooth_deviation_mean_elevation_07\",\n",
        "  \"pre_topo_cor_smooth_deviation_mean_elevation_11\",\n",
        "  \"pre_topo_cor_smooth_eastness\",\n",
        "  \"pre_topo_cor_smooth_elevation\",\n",
        "  \"pre_topo_cor_smooth_northness\",\n",
        "  \"pre_topo_cor_smooth_profile_curvature\",\n",
        "  \"pre_topo_cor_smooth_roughness_03\",\n",
        "  \"pre_topo_cor_smooth_roughness_07\",\n",
        "  \"pre_topo_cor_smooth_roughness_11\",\n",
        "  \"pre_topo_cor_smooth_slope\",\n",
        "  \"pre_topo_cor_smooth_stream_power_index_log10\",\n",
        "  \"pre_topo_cor_smooth_surface_area_ratio\",\n",
        "  \"pre_topo_cor_smooth_tangential_curvature\",\n",
        "  \"pre_topo_cor_smooth_topographic_position_index_03\",\n",
        "  \"pre_topo_cor_smooth_topographic_position_index_07\",\n",
        "  \"pre_topo_cor_smooth_topographic_position_index_11\",\n",
        "  \"pre_topo_cor_smooth_topographic_ruggedness_index\",\n",
        "  \"pre_topo_cor_smooth_topographic_wetness_index\",\n",
        "  \"pre_topo_cor_unsmooth_aspect_cosine\",\n",
        "  \"pre_topo_cor_unsmooth_aspect_sine\",\n",
        "  \"pre_topo_cor_unsmooth_circular_variance_aspect_03\",\n",
        "  \"pre_topo_cor_unsmooth_circular_variance_aspect_07\",\n",
        "  \"pre_topo_cor_unsmooth_circular_variance_aspect_11\",\n",
        "  \"pre_topo_cor_unsmooth_deviation_mean_elevation_03\",\n",
        "  \"pre_topo_cor_unsmooth_deviation_mean_elevation_07\",\n",
        "  \"pre_topo_cor_unsmooth_deviation_mean_elevation_11\",\n",
        "  \"pre_topo_cor_unsmooth_eastness\",\n",
        "  \"pre_topo_cor_unsmooth_elevation\",\n",
        "  \"pre_topo_cor_unsmooth_northness\",\n",
        "  \"pre_topo_cor_unsmooth_profile_curvature\",\n",
        "  \"pre_topo_cor_unsmooth_roughness_03\",\n",
        "  \"pre_topo_cor_unsmooth_roughness_07\",\n",
        "  \"pre_topo_cor_unsmooth_roughness_11\",\n",
        "  \"pre_topo_cor_unsmooth_slope\",\n",
        "  \"pre_topo_cor_unsmooth_stream_power_index_log10\",\n",
        "  \"pre_topo_cor_unsmooth_surface_area_ratio\",\n",
        "  \"pre_topo_cor_unsmooth_tangential_curvature\",\n",
        "  \"pre_topo_cor_unsmooth_topographic_position_index_03\",\n",
        "  \"pre_topo_cor_unsmooth_topographic_position_index_07\",\n",
        "  \"pre_topo_cor_unsmooth_topographic_position_index_11\",\n",
        "  \"pre_topo_cor_unsmooth_topographic_ruggedness_index\",\n",
        "  \"pre_topo_cor_unsmooth_topographic_wetness_index\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwjw9MYRbKaC"
      },
      "outputs": [],
      "source": [
        "# Combine with covariates and sort for predictions\n",
        "selected_predictors = sorted(selected_predictors)\n",
        "dataset_col_list = dataset_col_list + sorted(selected_predictors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twsBfrX4A1Ce"
      },
      "source": [
        "## Define categorical predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pbzhADthSMm"
      },
      "outputs": [],
      "source": [
        "# Define categorical data for correct model interpretation\n",
        "print(\"categorical_columns = [\")\n",
        "for col in dataset_col_list:\n",
        "  if col.startswith('pre_'):\n",
        "    print(f'  \"{col}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qwR7mUphT-T"
      },
      "outputs": [],
      "source": [
        "categorical_columns = [\n",
        "  \"pre_beam\",\n",
        "]\n",
        "\n",
        "dataset_imported_cat = dataset_imported_covar.copy()\n",
        "if len(categorical_columns) > 0:\n",
        "  dataset_imported_cat[categorical_columns].astype(\"category\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlS-5A-Sbjhx"
      },
      "source": [
        "## Descriptive parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5C8gPptbosb"
      },
      "outputs": [],
      "source": [
        "# Check remaining variate columns and select parameters that may be useful for descriptive statistics.\n",
        "print(\"descriptive_parameters = [\")\n",
        "for col in dataset_imported_cat.columns:\n",
        "  if col not in dataset_col_list:\n",
        "    print(f'\"{col}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh2viv9Jc86m"
      },
      "outputs": [],
      "source": [
        "descriptive_parameters = [\n",
        "]\n",
        "\n",
        "dataset_col_list = dataset_col_list + descriptive_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1kODwUq_ZMY"
      },
      "source": [
        "## Filter dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgCrC_Kq_XjK"
      },
      "outputs": [],
      "source": [
        "filter = False\n",
        "\n",
        "dataset_imported_filtered = dataset_imported_cat.copy()\n",
        "\n",
        "if filter:\n",
        "  filter_parameter = \"var_sensitivity\" # Example with sensitivity\n",
        "  threshold = 0.98 # Greater than or above, change below if needed\n",
        "  # Filter\n",
        "  filter_values_to_include = f\">={threshold}\" # Descriptive for metadata. Need to change the filter manually if a different conditional.\n",
        "  for col in dataset_imported_filtered.columns:\n",
        "    if col == filter_parameter:\n",
        "      dataset_imported_filtered = dataset_imported_filtered.loc[dataset_imported_filtered[filter_parameter]>=threshold].reset_index().drop(columns=['index'])\n",
        "else: filter_parameter, threshold, filter_values_to_include = None, None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx6XO_2tAp0p"
      },
      "source": [
        "## Sample size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zc3e82LPjK9b"
      },
      "outputs": [],
      "source": [
        "# Print current number of rows (data points)\n",
        "print(f'Number of rows (data points): {len(dataset_imported_filtered)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_7Kn103TEbl"
      },
      "outputs": [],
      "source": [
        "# Randomly sampling the rows of a dataset to generate a smaller subset can\n",
        "# make testing different parameters faster\n",
        "\n",
        "sample_imported_dataset = False\n",
        "sample_imported_dataset_by_percent = False # If False then by number\n",
        "sample_imported_dataset_value = 500000 # Set to a percentage or number, or 'None' if not sampling\n",
        "\n",
        "# Global\n",
        "dataset_imported_sampled = dataset_imported_filtered.copy()\n",
        "\n",
        "# Sample dataset for testing or HPO\n",
        "if sample_imported_dataset:\n",
        "  if sample_imported_dataset_by_percent:\n",
        "    dataset_imported_sampled = dataset_imported_sampled.sample(frac=sample_imported_dataset_value/100, random_state=1).reset_index().drop(columns=['index'])\n",
        "  else:\n",
        "    dataset_imported_sampled = dataset_imported_sampled.sample(n=sample_imported_dataset_value, random_state=1).reset_index().drop(columns=['index'])\n",
        "\n",
        "print(f'Number of rows (data points) after sampling: {len(dataset_imported_sampled)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXkO3lviA8QD"
      },
      "source": [
        "## Name and compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMev-lVjucJG"
      },
      "outputs": [],
      "source": [
        "# Set model dataset name (date and time suffix will be added) and sample percent\n",
        "dataset_name = \"agbd\"\n",
        "\n",
        "# Rename ID and descriptive columns\n",
        "dataset_col_list = [item for item in dataset_col_list if item is not None]\n",
        "dataset_final = dataset_imported_sampled.copy()[dataset_col_list]\n",
        "dataset_final.columns = [col.replace('pre_', '').replace('var_', '') if col in descriptive_parameters else col for col in dataset_final.columns]\n",
        "dataset_final.columns = [col.replace('pre_', '').replace('var_', '') if col == id_column else col for col in dataset_final.columns]\n",
        "\n",
        "# Model dataset file and directory\n",
        "model_dataset_name = f\"{dataset_name}_{datetime.utcnow().strftime('%y%m%d_%H%M%S')}\"\n",
        "model_dataset_dir = join(models_dir, model_dataset_name)\n",
        "model_dataset_path = join(model_dataset_dir, model_dataset_name)\n",
        "model_dataset_description_dir = join(model_dataset_dir, \"model_dataset_description.json\")\n",
        "\n",
        "# Check if identical filtered model dataset exists\n",
        "dataset_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file.endswith('.pkl'):\n",
        "      dataset_existing = pd.read_pickle(join(subdir,file))\n",
        "      dataset_equals = pd.DataFrame.equals(dataset_existing, dataset_final)\n",
        "      if dataset_equals:\n",
        "        dataset_exists = True\n",
        "        model_dataset = dataset_existing\n",
        "        model_dataset_dir = subdir\n",
        "        print(f'An identical model dataset already exists in: {subdir}')\n",
        "        break\n",
        "  if dataset_exists:\n",
        "    break\n",
        "\n",
        "# Create a model dataset filtered to selected variate and predictors, if it does not exist\n",
        "if dataset_exists == False:\n",
        "  makedirs(model_dataset_dir, exist_ok=True)\n",
        "  dataset_final.to_pickle(f\"{model_dataset_path}.pkl\")\n",
        "  model_dataset = pd.read_pickle(f\"{model_dataset_path}.pkl\")\n",
        "  model_dataset_description = {\n",
        "      \"model_dataset_name\": model_dataset_name,\n",
        "      \"number_of_columns\": len(model_dataset.columns),\n",
        "      \"number_of_rows\": len(model_dataset),\n",
        "      \"id_column\": id_column,\n",
        "      \"selected_variate\": selected_variate,\n",
        "      \"uncertainty\": uncertainty,\n",
        "      \"covariates_renamed\": covariates_renamed,\n",
        "      \"covariates_categorised\": categorised_covariates,\n",
        "      \"selected_predictors\": selected_predictors,\n",
        "      \"categorical_columns\": categorical_columns,\n",
        "      \"descriptive_parameters\": descriptive_parameters,\n",
        "      \"filter_parameter\": filter_parameter,\n",
        "      \"filter_values_to_include\": filter_values_to_include,\n",
        "      \"sample_imported_dataset\": sample_imported_dataset,\n",
        "      \"sample_imported_dataset_by_percent\": sample_imported_dataset_by_percent,\n",
        "      \"sample_imported_dataset_value\": sample_imported_dataset_value,\n",
        "  }\n",
        "  with open(model_dataset_description_dir, \"w\") as f:\n",
        "    f.write(json.dumps(model_dataset_description))\n",
        "  print(f\"Model dataset compiled and exported to {model_dataset_dir}\")\n",
        "\n",
        "# Check dataset\n",
        "model_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lqjsLKZaQXo"
      },
      "source": [
        "# Select model dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SqkKHKnaV-8"
      },
      "outputs": [],
      "source": [
        "# Select an existing model dataset\n",
        "for subdir in os.listdir(models_dir):\n",
        "    print(f'selected_model_dataset = \"{subdir}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnerzp-gbapN"
      },
      "outputs": [],
      "source": [
        "selected_model_dataset = \"agbd_240923_083124\"\n",
        "\n",
        "# Select model dataset\n",
        "model_dataset_dir = join(models_dir,selected_model_dataset)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(model_dataset_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_variate = model_dataset_description[\"selected_variate\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "covariates_categorised = model_dataset_description[\"covariates_categorised\"]\n",
        "selected_predictors = model_dataset_description[\"selected_predictors\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "filter_parameter = model_dataset_description[\"filter_parameter\"]\n",
        "filter_values_to_include = model_dataset_description[\"filter_values_to_include\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "# Define model dataset description and .json directories\n",
        "model_description_dir = join(model_dataset_dir, \"model_description.json\")\n",
        "final_model_dir = join(model_dataset_dir, \"model.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u_wV7NAp1h-"
      },
      "source": [
        "# Model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnge5_H6FR3f"
      },
      "outputs": [],
      "source": [
        "# WIP categorise_variate\n",
        "categorise_variate = False\n",
        "\n",
        "# Load model dataset\n",
        "model_dataset = pd.read_pickle(join(f\"{model_dataset_dir}/{selected_model_dataset}.pkl\"))\n",
        "\n",
        "if categorise_variate:\n",
        "  # Create a copy to protect original\n",
        "  model_dataset_cat = model_dataset\n",
        "\n",
        "  # Define categories (must be in order and start from 0)\n",
        "\n",
        "  # # Categorise per 50 Mg/ha\n",
        "  # multiclass = True # More than two classes\n",
        "  # model_dataset_cat['var_agbd'] = np.where(model_dataset_cat.var_agbd < 50, 0, model_dataset_cat.var_agbd)\n",
        "  # model_dataset_cat['var_agbd'] = np.where((model_dataset_cat.var_agbd >= 50) & (model_dataset_cat.var_agbd < 100), 1, model_dataset_cat.var_agbd)\n",
        "  # model_dataset_cat['var_agbd'] = np.where((model_dataset_cat.var_agbd >= 100) & (model_dataset_cat.var_agbd < 150), 2, model_dataset_cat.var_agbd)\n",
        "  # model_dataset_cat['var_agbd'] = np.where((model_dataset_cat.var_agbd >= 150) & (model_dataset_cat.var_agbd < 200), 3, model_dataset_cat.var_agbd)\n",
        "  # model_dataset_cat['var_agbd'] = np.where((model_dataset_cat.var_agbd >= 200) & (model_dataset_cat.var_agbd < 250), 4, model_dataset_cat.var_agbd)\n",
        "  # model_dataset_cat['var_agbd'] = np.where((model_dataset_cat.var_agbd >= 250) & (model_dataset_cat.var_agbd < 300), 5, model_dataset_cat.var_agbd)\n",
        "  # model_dataset_cat['var_agbd'] = np.where((model_dataset_cat.var_agbd >= 300) & (model_dataset_cat.var_agbd < 350), 6, model_dataset_cat.var_agbd)\n",
        "  # model_dataset_cat['var_agbd'] = np.where((model_dataset_cat.var_agbd >= 350) & (model_dataset_cat.var_agbd < 400), 7, model_dataset_cat.var_agbd)\n",
        "  # model_dataset_cat['var_agbd'] = np.where(model_dataset_cat.var_agbd >= 400, 8, model_dataset_cat.var_agbd)\n",
        "\n",
        "  # # Categorise per 100 Mg/ha\n",
        "  # multiclass = True # More than two classes\n",
        "  # model_dataset_cat['var_agbd'] = np.where(model_dataset_cat['var_agbd'] < 100, 0, model_dataset_cat['var_agbd'])\n",
        "  # model_dataset_cat['var_agbd'] = np.where((model_dataset_cat['var_agbd'] >= 100) & (model_dataset_cat['var_agbd'] < 200), 1, model_dataset_cat['var_agbd'])\n",
        "  # model_dataset_cat['var_agbd'] = np.where((model_dataset_cat['var_agbd'] >= 200) & (model_dataset_cat['var_agbd'] < 300), 2, model_dataset_cat['var_agbd'])\n",
        "  # model_dataset_cat['var_agbd'] = np.where((model_dataset_cat['var_agbd'] >= 300) & (model_dataset_cat['var_agbd'] < 400), 3, model_dataset_cat['var_agbd'])\n",
        "  # model_dataset_cat['var_agbd'] = np.where(model_dataset_cat['var_agbd'] >= 400, 4, model_dataset_cat['var_agbd'])\n",
        "\n",
        "  # Categorise above and below 150 Mg/ha (High Carbon Stock)\n",
        "  multiclass = False\n",
        "  model_dataset_cat['var_agbd'] = np.where(model_dataset_cat.var_agbd < 150, 0, model_dataset_cat.var_agbd)\n",
        "  model_dataset_cat['var_agbd'] = np.where(model_dataset_cat.var_agbd >= 150, 1, model_dataset_cat.var_agbd)\n",
        "\n",
        "  # Set parameters\n",
        "  XGBPredictor = xgb.XGBClassifier\n",
        "  if multiclass:\n",
        "      objective = 'multi:softprob'\n",
        "      eval_metric = 'mlogloss'\n",
        "  else:\n",
        "      objective = 'binary:logistic'\n",
        "      eval_metric = 'logloss'\n",
        "  metric = 'accuracy'\n",
        "  optimal_value = 'max'\n",
        "  model_dataset_cat['var_agbd'] = model_dataset_cat['var_agbd'].astype(\"category\")\n",
        "  model_dataset = model_dataset_cat\n",
        "\n",
        "else:\n",
        "  XGBPredictor = xgb.XGBRegressor\n",
        "  objective = 'reg:squarederror'\n",
        "  eval_metric = 'rmse'\n",
        "  metric = 'rmse'\n",
        "  optimal_value = 'min'\n",
        "\n",
        "# Define x and y axes for training\n",
        "model_dataset_x = model_dataset[selected_predictors]\n",
        "model_dataset_y = model_dataset[selected_variate]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NPpDq7B4Zkc"
      },
      "outputs": [],
      "source": [
        "# Select whether to use the weighted validation score for optimisation, mitigating overfitting.\n",
        "use_score_weighted = True\n",
        "\n",
        "# Create dictionary for optimal value and define the weighted score\n",
        "dict_opt = {\"min\": min, \"max\": max}\n",
        "def def_score_weighted(mean_validation, mean_training):\n",
        "  if optimal_value == \"max\":\n",
        "    score_weighted_calc = mean_validation - 0.5*max(0, mean_training - mean_validation)\n",
        "  elif optimal_value == \"min\":\n",
        "    score_weighted_calc = mean_validation + 0.5*max(0, mean_validation - mean_training)\n",
        "  return score_weighted_calc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1lV8U3eHhWG"
      },
      "outputs": [],
      "source": [
        "# Exclude specified columns\n",
        "columns_to_exclude = [\n",
        "                      'pre_sensitivity',\n",
        "                      # 'pre_latitude',\n",
        "                      # 'pre_longitude'\n",
        "                      ]\n",
        "columns_to_consider = model_dataset_x.columns.drop(columns_to_exclude)\n",
        "\n",
        "# Calculate max_bin hyperparameter based on predictor with maximum unique values\n",
        "max_unique_col = model_dataset_x[columns_to_consider].nunique().idxmax()\n",
        "max_bin = model_dataset_x[columns_to_consider].nunique().max()\n",
        "print(f\"{max_unique_col} had the highest number of unique values, max_bin set to {max_bin}\")\n",
        "\n",
        "# Alternatively, set manually by uncommenting:\n",
        "# max_bin = 1600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfGcGNwHLJF2"
      },
      "outputs": [],
      "source": [
        "use_default_hp = False\n",
        "use_random_seed = True\n",
        "\n",
        "# Define baseline hyperparameters\n",
        "\n",
        "baseline_hyperparameters = {\n",
        " 'tree_method': 'hist',\n",
        " 'device': 'cuda',\n",
        " 'enable_categorical': True,\n",
        " 'max_bin': max_bin,\n",
        " 'n_estimators': 100000, # Will be limited by early stopping\n",
        " 'learning_rate': 0.01,\n",
        " 'early_stopping_rounds': 10,\n",
        " 'min_child_weight': 39,\n",
        " 'gamma': 0.12,\n",
        " 'alpha': 9800,\n",
        " 'lambda': 2000,\n",
        " 'colsample_bytree': 0.82,\n",
        " 'colsample_bylevel': 0.938,\n",
        " 'colsample_bynode': 0.85\n",
        "}\n",
        "\n",
        "\n",
        "if use_default_hp:\n",
        "  baseline_hyperparameters = {\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"device\": 'cuda',\n",
        "    'enable_categorical': True,\n",
        "    \"objective\": objective,\n",
        "    \"eval_metric\": eval_metric,\n",
        "    \"max_bin\": max_bin,\n",
        "    \"n_estimators\": 100_000, # Will be limited by early stopping\n",
        "    \"early_stopping_rounds\": 10\n",
        "\n",
        "  }\n",
        "\n",
        "if use_random_seed: baseline_hyperparameters[\"random_state\"] = 1\n",
        "\n",
        "# Avoids issues using dataframe from CPU\n",
        "xgb.set_config(verbosity=0, use_rmm=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3gwpt7LfJGj"
      },
      "source": [
        "# KFold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2BaRo0o83TC"
      },
      "outputs": [],
      "source": [
        "# The number of k-folds affects the proportion of data used for training,\n",
        "# with the complementary proportion used for validation / testing.\n",
        "# 10 splits = 90 % training; 5 = 80 %; 4 = 75 %; 3 = 67 %; 2 = 50 %.\n",
        "\n",
        "# Input the number of k-folds to generate train/valid splits.\n",
        "n_splits = 10\n",
        "\n",
        "# Number of folds to be used for HPO validation.\n",
        "# Remaining folds will be used for final testing.\n",
        "n_hpo_splits = 2\n",
        "\n",
        "assert n_hpo_splits < n_splits, \"n_hpo_splits must be less than n_splits\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wir622aUtK5R"
      },
      "outputs": [],
      "source": [
        "kf_split = list(KFold(n_splits=n_splits, shuffle=True, random_state=1).split(model_dataset_x, model_dataset_y))\n",
        "\n",
        "# Return a dictionary full of metrics for calculating accuracy\n",
        "def calculate_metrics(validation, prediction):\n",
        "    # Convert all data to a unified type for metric calculation\n",
        "    if not categorise_variate:\n",
        "        validation = validation.astype(\"float32\")\n",
        "        prediction = prediction.astype(\"float32\")\n",
        "    if categorise_variate:\n",
        "        metrics = {\n",
        "            \"r2\": r2_score(validation, prediction),\n",
        "            \"me\": np.mean(np.array(validation).squeeze() - np.array(prediction).squeeze()),\n",
        "            \"rmse\": mean_squared_error(validation, prediction, squared=False),\n",
        "            \"rrmse\": np.sqrt(np.sum(np.square(validation - prediction), axis=0) / np.sum(np.square(prediction), axis=0)) * 100,\n",
        "            \"accuracy\": accuracy_score(validation, prediction)\n",
        "        }\n",
        "    else:\n",
        "        metrics = {\n",
        "            \"r2\": r2_score(validation, prediction),\n",
        "            \"me\": np.mean(np.array(validation).squeeze() - np.array(prediction).squeeze()),\n",
        "            \"rmse\": mean_squared_error(validation, prediction, squared=False),\n",
        "            \"rrmse\": np.sqrt(np.sum(np.square(validation - prediction), axis=0) / np.sum(np.square(prediction), axis=0)) * 100,\n",
        "        }\n",
        "    return metrics\n",
        "\n",
        "def run_kfold(model_dataset, kf_split, predictor, verbose=False, export_test_model=False, test_model_dir=None):\n",
        "    metrics = []\n",
        "    cols = []\n",
        "    junk_data = pd.Series([1, 1, 1, 1, 1, 1])\n",
        "    for m in list(calculate_metrics(junk_data, junk_data).keys()):\n",
        "        cols += [f\"score_training ({m})\", f\"score_validation ({m})\", f\"score_difference ({m})\", f\"score_weighted ({m})\"]\n",
        "    cols.append(\"n_estimators\")\n",
        "\n",
        "    # Set fold index\n",
        "    fold_index = 1\n",
        "\n",
        "    for train_index, valid_index in kf_split:\n",
        "        # Compile datasets\n",
        "        x_train = model_dataset.loc[train_index][selected_predictors]\n",
        "        x_valid = model_dataset.loc[valid_index][selected_predictors]\n",
        "        y_train = model_dataset.loc[train_index][selected_variate]\n",
        "        y_valid = model_dataset.loc[valid_index][selected_variate]\n",
        "\n",
        "        # Create DMatrix objects\n",
        "        dtrain = xgb.DMatrix(x_train, y_train, enable_categorical=True)\n",
        "        dvalid = xgb.DMatrix(x_valid, y_valid, enable_categorical=True)\n",
        "\n",
        "        # Set up parameters\n",
        "        params = predictor.get_params()\n",
        "        params['eval_metric'] = eval_metric\n",
        "\n",
        "        # Set up early stopping\n",
        "        early_stopping = xgb.callback.EarlyStopping(\n",
        "            rounds=predictor.early_stopping_rounds,\n",
        "            metric_name=eval_metric,\n",
        "            data_name='validation_1'\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        evals_result = {}\n",
        "        model = xgb.train(params,\n",
        "                          dtrain,\n",
        "                          num_boost_round=predictor.n_estimators,\n",
        "                          evals=[(dtrain, 'training'), (dvalid, 'validation_1')],\n",
        "                          callbacks=[early_stopping],\n",
        "                          evals_result=evals_result,\n",
        "                          verbose_eval=False)\n",
        "\n",
        "        # Collect metrics\n",
        "        metrics_row = []\n",
        "        training_prediction = model.predict(dtrain)\n",
        "        validation_prediction = model.predict(dvalid)\n",
        "        training_metrics = calculate_metrics(y_train, training_prediction)\n",
        "        validation_metrics = calculate_metrics(y_valid, validation_prediction)\n",
        "        for m in list(training_metrics.keys()):\n",
        "            score_training = training_metrics[m]\n",
        "            score_validation = validation_metrics[m]\n",
        "            score_difference = score_training - score_validation\n",
        "            score_weighted = def_score_weighted(score_validation, score_training)\n",
        "\n",
        "            metrics_row += [score_training, score_validation, score_difference, score_weighted]\n",
        "\n",
        "        metrics_row.append(len(evals_result['validation_1'][eval_metric]))  # n_estimators\n",
        "        metrics.append(metrics_row)\n",
        "\n",
        "        # Export if enabled\n",
        "        if export_test_model:\n",
        "            makedirs(test_model_dir, exist_ok=True)\n",
        "            model_name = f\"model_test_fold_{fold_index}\"\n",
        "            metrics_dir = join(test_model_dir, f\"{model_name}_metrics.csv\")\n",
        "            model.save_model(join(test_model_dir, f\"{model_name}.json\"))\n",
        "            df_fold_metrics = pd.DataFrame(metrics, columns=cols)\n",
        "            df_fold_metrics.to_csv(metrics_dir)\n",
        "            print(f\"Model export complete for test fold {fold_index}.\")\n",
        "\n",
        "        # Reset scores to selected metric\n",
        "        score_training = training_metrics[metric]\n",
        "        score_validation = validation_metrics[metric]\n",
        "        score_difference = score_training - score_validation\n",
        "        score_weighted = def_score_weighted(score_validation, score_training)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Validation {metric}: {score_validation}\")\n",
        "            print(f\"Training {metric}: {score_training}\")\n",
        "            print(f\"{metric} difference: {score_difference}\")\n",
        "            print(f\"Weighted {metric}: {score_weighted}\")\n",
        "            print(f\"--------------------\")\n",
        "\n",
        "        fold_index += 1\n",
        "\n",
        "    # Place metrics into a DataFrame to make it more readable\n",
        "    df_kfold_metrics = pd.DataFrame(metrics, columns=cols)\n",
        "\n",
        "    return df_kfold_metrics\n",
        "\n",
        "def generate_statistics(df_kfold_metrics):\n",
        "    mean_training = df_kfold_metrics[f\"score_training ({metric})\"].mean()\n",
        "    std_training = df_kfold_metrics[f\"score_training ({metric})\"].std()\n",
        "    mean_validation = df_kfold_metrics[f\"score_validation ({metric})\"].mean()\n",
        "    std_validation = df_kfold_metrics[f\"score_validation ({metric})\"].std()\n",
        "    mean_difference = df_kfold_metrics[f\"score_difference ({metric})\"].mean()\n",
        "    std_difference = df_kfold_metrics[f\"score_difference ({metric})\"].std()\n",
        "    score_weighted = def_score_weighted(mean_validation, mean_training)\n",
        "    mean_weighted = df_kfold_metrics[f\"score_weighted ({metric})\"].mean()\n",
        "    std_weighted = df_kfold_metrics[f\"score_weighted ({metric})\"].std()\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"----------Results Summary----------\")\n",
        "    print(f\"Training {metric} mean: {mean_training}\")\n",
        "    print(f\"Training {metric} std: {std_training}\")\n",
        "    print(f\"Validation {metric} mean: {mean_validation}\")\n",
        "    print(f\"Validation {metric} std: {std_validation}\")\n",
        "    print(f\"{metric} difference mean: {mean_difference}\")\n",
        "    print(f\"{metric} difference std: {std_difference}\")\n",
        "    print(f\"Weighted {metric} score: {score_weighted}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDMPTLWi9d7X"
      },
      "source": [
        "# Baseline model accuracy (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnmXF0CaA7Rx"
      },
      "outputs": [],
      "source": [
        "# Baseline for HPO folds\n",
        "baseline_hpo_predictor = XGBPredictor(**baseline_hyperparameters)\n",
        "baseline_hpo = run_kfold(model_dataset, kf_split[:n_hpo_splits], baseline_hpo_predictor, verbose=True)\n",
        "generate_statistics(baseline_hpo)\n",
        "\n",
        "# Export baseline_hpo_descr.json\n",
        "baseline_hpo_descr_dir = join(model_dataset_dir, \"baseline_hpo_descr.json\")\n",
        "baseline_hpo_descr = {\n",
        "  \"selected_variate\": selected_variate,\n",
        "  \"hyperparameters\": str(baseline_hyperparameters),\n",
        "  \"metric_used_for_training\": metric\n",
        "}\n",
        "\n",
        "for col in baseline_hpo.columns:\n",
        "  baseline_hpo_descr[f\"avg_{col}\"] = float(baseline_hpo[col].mean())\n",
        "  baseline_hpo_descr[f\"std_{col}\"] = float(baseline_hpo[col].std())\n",
        "with open(baseline_hpo_descr_dir, \"w\") as file:\n",
        "  file.write(json.dumps(baseline_hpo_descr))\n",
        "print(\"baseline_hpo_descr.json generation and export complete\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub7FlaEDM-lC"
      },
      "outputs": [],
      "source": [
        "# Baseline for final testing folds\n",
        "baseline_test_predictor = XGBPredictor(**baseline_hyperparameters)\n",
        "baseline_test = run_kfold(model_dataset, kf_split[n_hpo_splits:], baseline_test_predictor, verbose=True)\n",
        "generate_statistics(baseline_test)\n",
        "\n",
        "# Export baseline_test_descr.json\n",
        "baseline_test_descr_dir = join(model_dataset_dir, \"baseline_test_descr.json\")\n",
        "baseline_test_descr = {\n",
        "  \"selected_variate\": selected_variate,\n",
        "  \"hyperparameters\": str(baseline_hyperparameters),\n",
        "  \"metric_used_for_training\": metric\n",
        "}\n",
        "\n",
        "for col in baseline_test.columns:\n",
        "  baseline_test_descr[f\"avg_{col}\"] = float(baseline_test[col].mean())\n",
        "  baseline_test_descr[f\"std_{col}\"] = float(baseline_test[col].std())\n",
        "with open(baseline_test_descr_dir, \"w\") as file:\n",
        "  file.write(json.dumps(baseline_test_descr))\n",
        "print(\"baseline_test_descr.json generation and export complete\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_n6YOnRQNa1"
      },
      "source": [
        "# Hyperparameter optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP1fzhFk7kVl"
      },
      "source": [
        "## Random search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJwPFIfH7jkU"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters and ranges to randomly sample.\n",
        "# If uniform or loguniform, subtract the lower range from the upper range.\n",
        "\n",
        "hp_distribution = {\n",
        "    \"early_stopping_rounds\": randint(10, 100),\n",
        "    \"learning_rate\": uniform(0.01, 0.3 - 0.01),\n",
        "    \"min_child_weight\": randint(1, 10),\n",
        "    \"gamma\": uniform(0, 0.5 - 0),\n",
        "    \"reg_alpha\": uniform(0, 500 - 0), # If score does not change with HPs, try reducing reg_alpha\n",
        "    \"reg_lambda\": uniform(1, 500 - 1),\n",
        "    \"colsample_bytree\": uniform(0.95, 1.0 - 0.95),\n",
        "    \"colsample_bylevel\": uniform(0.95, 1.0 - 0.95),\n",
        "    \"colsample_bynode\": uniform(0.95, 1.0 - 0.95),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua8gUmEWlW9_"
      },
      "outputs": [],
      "source": [
        "# Select how many iterations\n",
        "iterations = 1000\n",
        "\n",
        "hpo_random_search_dir = join(model_dataset_dir, 'hpo_random_search')\n",
        "Path(hpo_random_search_dir).mkdir(parents=True, exist_ok=True)\n",
        "hpo_random_search_results_filename = join(hpo_random_search_dir, 'iteration_results')\n",
        "hpo_random_search_results = pd.DataFrame()\n",
        "score_col = \"score_weighted\" if use_score_weighted else \"mean_validation\"\n",
        "hpo_random_search_best_hyperparameters_dir = join(hpo_random_search_dir,\"best_hyperparameters.csv\")\n",
        "\n",
        "# If an existing hyper parameter file exists, load it\n",
        "if exists(f\"{hpo_random_search_results_filename}.pkl\"):\n",
        "  print(\"Reading results of the existing trials records...\")\n",
        "  hpo_random_search_results = pd.read_pickle(f\"{hpo_random_search_results_filename}.pkl\")\n",
        "  print(f\"{len(hpo_random_search_results)} records read.\")\n",
        "\n",
        "# Iteratively train models with sampled hyperparameters, saving the values and accuracy scores\n",
        "while len(hpo_random_search_results) < iterations:\n",
        "\n",
        "  # Randomly sample hyperparameter distribution\n",
        "  hp_sample_random_state = max(1,len(hpo_random_search_results)+1)\n",
        "  hp_sample = {k: v.rvs(random_state=hp_sample_random_state*(i+1000000)) for i, (k, v) in enumerate(hp_distribution.items())}\n",
        "\n",
        "  # Start regression\n",
        "  t_start = datetime.now()\n",
        "  print(f\"Trying parameters: {hp_sample}\")\n",
        "\n",
        "  rs_hyperparameters = dict(baseline_hyperparameters.copy())\n",
        "  rs_hyperparameters.update(hp_sample)\n",
        "  rs_predictor = XGBPredictor(**rs_hyperparameters)\n",
        "\n",
        "  df_kfold_metrics = run_kfold(model_dataset, kf_split[:n_hpo_splits], rs_predictor, verbose=False)\n",
        "\n",
        "  # Generate statistics\n",
        "  mean_training = df_kfold_metrics[f\"score_training ({metric})\"].mean()\n",
        "  mean_validation = df_kfold_metrics[f\"score_validation ({metric})\"].mean()\n",
        "  mean_difference = df_kfold_metrics[f\"score_difference ({metric})\"].mean()\n",
        "  score_weighted = def_score_weighted(mean_validation, mean_training)\n",
        "\n",
        "  interim_rs_results = pd.Series(rs_hyperparameters)\n",
        "  interim_rs_results.loc[\"mean_validation\"] = mean_validation\n",
        "  interim_rs_results.loc[\"mean_training\"] = mean_training\n",
        "  interim_rs_results.loc[\"mean_difference\"] = mean_difference\n",
        "  interim_rs_results.loc[\"score_weighted\"] = score_weighted\n",
        "  interim_rs_results.loc[\"hyperparameters\"] = rs_hyperparameters\n",
        "  print(f\"Trial completed in {datetime.now() - t_start}.\")\n",
        "  print(f\"Mean validation {metric}: {mean_validation:.4f}, Mean training {metric}: {mean_training:.4f}, Mean {metric} difference: {mean_difference:.4f}, weighted score: {score_weighted:.4f}\")\n",
        "\n",
        "  #  Compile results\n",
        "  if len(hpo_random_search_results) == 0:\n",
        "    hpo_random_search_results = pd.DataFrame([interim_rs_results])\n",
        "  else:\n",
        "    hpo_random_search_results = pd.concat([hpo_random_search_results, pd.DataFrame([interim_rs_results])], ignore_index=True)\n",
        "  hpo_random_search_results.to_pickle(f\"{hpo_random_search_results_filename}.pkl\")\n",
        "  hpo_random_search_results.to_csv(f\"{hpo_random_search_results_filename}.csv\")\n",
        "\n",
        "  #  Export best result\n",
        "  if optimal_value == \"min\":\n",
        "    best_result_id = hpo_random_search_results[score_col].idxmin()\n",
        "  else:\n",
        "    best_result_id = hpo_random_search_results[score_col].idxmax()\n",
        "  best_rs_hyperparameters = hpo_random_search_results.loc[best_result_id][\"hyperparameters\"].copy()\n",
        "  export_rs_hyperparameters = best_rs_hyperparameters\n",
        "  export_rs_hyperparameters[score_col] = hpo_random_search_results.loc[best_result_id][score_col]\n",
        "  pd.DataFrame(export_rs_hyperparameters, index=[0]).to_csv(hpo_random_search_best_hyperparameters_dir)\n",
        "\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfcXwAf1VwW4"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter SHAP plots\n",
        "hpo_x = hpo_random_search_results[list(hp_distribution.keys())]\n",
        "hpo_x = hpo_x.apply(pd.to_numeric)\n",
        "hpo_y = hpo_random_search_results[score_col]\n",
        "hpo_y = hpo_y.apply(pd.to_numeric)\n",
        "\n",
        "hp_predictor = XGBPredictor()\n",
        "hp_predictor.fit(hpo_x, hpo_y)\n",
        "\n",
        "explainer = shap.Explainer(hp_predictor)\n",
        "shap_values = explainer(hpo_x)\n",
        "shap.plots.beeswarm(shap_values, plot_size=(20,8))\n",
        "\n",
        "shap.plots.scatter(shap_values[:,'early_stopping_rounds'], color=shap_values[:,'early_stopping_rounds'].data)\n",
        "shap.plots.scatter(shap_values[:,'learning_rate'], color=shap_values[:,'learning_rate'].data)\n",
        "shap.plots.scatter(shap_values[:,'min_child_weight'], color=shap_values[:,'min_child_weight'].data)\n",
        "shap.plots.scatter(shap_values[:,'gamma'], color=shap_values[:,'gamma'].data)\n",
        "shap.plots.scatter(shap_values[:,'reg_lambda'], color=shap_values[:,'reg_lambda'].data)\n",
        "shap.plots.scatter(shap_values[:,'reg_alpha'], color=shap_values[:,'reg_alpha'].data)\n",
        "shap.plots.scatter(shap_values[:,'colsample_bytree'], color=shap_values[:,'colsample_bytree'].data)\n",
        "shap.plots.scatter(shap_values[:,'colsample_bylevel'], color=shap_values[:,'colsample_bylevel'].data)\n",
        "shap.plots.scatter(shap_values[:,'colsample_bynode'], color=shap_values[:,'colsample_bynode'].data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFL7wRxTOIww"
      },
      "source": [
        "## Automated Random Search Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RbhiObaJpml"
      },
      "outputs": [],
      "source": [
        "# Define directories and paths\n",
        "hpo_arse_dir = join(model_dataset_dir, \"hpo_arse\")\n",
        "hpo_arse_optimisation_plots_dir = join(hpo_arse_dir, \"optimisation_plots\")\n",
        "hpo_arse_optimisation_results_dir = join(hpo_arse_dir, \"optimisation_results.pkl\")\n",
        "hpo_arse_iteration_results_dir = join(hpo_arse_dir, \"iteration_results.pkl\")\n",
        "hpo_arse_best_hyperparameters_dir = join(hpo_arse_dir, \"best_hyperparameters.csv\")\n",
        "hpo_arse_cache_shap_dir = join(hpo_arse_dir, \"cache_shap.pkl\")\n",
        "hpo_arse_line_graph_dir = join(hpo_arse_dir, \"optimisation_results.png\")\n",
        "Path(hpo_arse_dir).mkdir(parents=True, exist_ok=True)\n",
        "Path(hpo_arse_optimisation_plots_dir).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebcysq5wKeNE"
      },
      "outputs": [],
      "source": [
        "# Each optimisation an XGBoost regression model and SHAP values are used to determine\n",
        "# the hyperparameter with the largest effect on accuracy. This is then narrowed to a\n",
        "# more optimal distribution based on SHAP effect direction. If the new distribution\n",
        "# touches the upper or lower bounds of that tested then those bounds are shifted.\n",
        "# If the mean score of an optimisation is less than the previous, the 'conservative'\n",
        "# option will retry the former HP range rather than moving to the next round.\n",
        "\n",
        "# hp_distribution = {\n",
        "#     \"early_stopping_rounds\": randint(15, 21 + 1),\n",
        "#     # \"learning_rate\": uniform(0.008, 0.012 - 0.008),\n",
        "#     \"min_child_weight\": randint(26, 39 + 1),\n",
        "#     # \"gamma\": uniform(0.125, 0.175 - 0.125),\n",
        "#     \"reg_alpha\": uniform(5000, 15000 - 5000),\n",
        "#     \"reg_lambda\": uniform(22000, 24000 - 22000),\n",
        "#     \"colsample_bytree\": uniform(0.96, 1 - 0.96),\n",
        "#     \"colsample_bylevel\": uniform(0.85, 0.92 - 0.85),\n",
        "#     \"colsample_bynode\": uniform(0.85, 0.92 - 0.85),\n",
        "# }\n",
        "\n",
        "hp_distribution = {\n",
        "    # \"max_bin\": randint(1000, 2000 + 1),\n",
        "    # \"early_stopping_rounds\": randint(8, 12 + 1),\n",
        "    # \"learning_rate\": uniform(0.01, 0.3 - 0.01),\n",
        "    \"min_child_weight\": randint(49, 52 + 1),\n",
        "    \"gamma\": uniform(0.1, 0.15 - 0.1),\n",
        "    \"reg_alpha\": uniform(19000, 22000 - 19000),\n",
        "    \"reg_lambda\": uniform(1, 500 - 1),\n",
        "    \"colsample_bytree\": uniform(0.65, 0.72 - 0.7),\n",
        "    \"colsample_bylevel\": uniform(0.74, 0.87 - 0.7),\n",
        "    \"colsample_bynode\": uniform(0.8, 0.9 - 0.8),\n",
        "}\n",
        "\n",
        "\n",
        "df_hp_types = pd.DataFrame(columns=[\"type\", \"min\", \"max\"])\n",
        "df_hp_types.index.name = \"hyperparameter\"\n",
        "df_hp_types.loc[\"max_bin\"]                = [\"randint\", 1, np.inf]\n",
        "df_hp_types.loc[\"early_stopping_rounds\"]  = [\"randint\", 1, np.inf]\n",
        "df_hp_types.loc[\"learning_rate\"]          = [\"uniform\", 0.005, 1.0]\n",
        "df_hp_types.loc[\"min_child_weight\"]       = [\"randint\", 0, np.inf]\n",
        "df_hp_types.loc[\"gamma\"]                  = [\"uniform\", 0.0, np.inf]\n",
        "df_hp_types.loc[\"reg_alpha\"]              = [\"uniform\", 0.0, np.inf]\n",
        "df_hp_types.loc[\"reg_lambda\"]             = [\"uniform\", 1.0, np.inf]\n",
        "df_hp_types.loc[\"colsample_bytree\"]       = [\"uniform\", 0.5, 1.0]\n",
        "df_hp_types.loc[\"colsample_bylevel\"]      = [\"uniform\", 0.5, 1.0]\n",
        "df_hp_types.loc[\"colsample_bynode\"]       = [\"uniform\", 0.5, 1.0]\n",
        "\n",
        "function_map = {\n",
        "    \"uniform\": uniform,\n",
        "    \"randint\": randint,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qq0MfaGdUCY"
      },
      "outputs": [],
      "source": [
        "# (optional) redefine baseline hyperparameters for new ARSE round\n",
        "\n",
        "baseline_hyperparameters = {\n",
        " 'tree_method': 'hist',\n",
        " 'device': 'cuda',\n",
        " 'enable_categorical': True,\n",
        " 'max_bin': max_bin,\n",
        " 'n_estimators': 100000, # Will be limited by early stopping\n",
        " 'learning_rate': 0.01,\n",
        " 'early_stopping_rounds': 10,\n",
        " 'min_child_weight': 39,\n",
        " 'gamma': 0.12,\n",
        " 'alpha': 9800,\n",
        " 'lambda': 2000,\n",
        " 'colsample_bytree': 0.82,\n",
        " 'colsample_bylevel': 0.938,\n",
        " 'colsample_bynode': 0.85\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGbyj1a5RApN"
      },
      "outputs": [],
      "source": [
        "iterations_per_optimisation = 20\n",
        "total_optimisations = 5\n",
        "bounds_correction_percent = 50 # Shifts the HP sample range to this % of this sample range if it is at the upper or lower extremes\n",
        "conservative = False\n",
        "\n",
        "assert iterations_per_optimisation != len(hp_distribution), 'The number of iterations cannot be the same as the number of hyperparameters sampled (a known issue with shap)'\n",
        "\n",
        "# Filter warning \"ntree_limit is deprecated, use `iteration_range` or model slicing instead.\"\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
        "\n",
        "# Initialise variables and directories\n",
        "shap_data = []\n",
        "score_col = \"score_weighted\" if use_score_weighted else \"mean_validation\"\n",
        "\n",
        "display_cols = [f\"best score ({metric} {score_col})\", f\"mean score ({metric} {score_col})\", \"most_important_hp\", \"new_hp_min\", \"new_hp_max\", f\"time_taken (H:M:S)\"]\n",
        "cache_cols = [\"hp_distribution_old\", \"hp_distribution_new\"]\n",
        "hpo_arse_results = pd.DataFrame(columns=display_cols + cache_cols)\n",
        "hpo_arse_results.index.name = \"optimisations\"\n",
        "\n",
        "hpo_arse_iteration_results = pd.DataFrame(columns=[\"optimisations\", \"iterations\", \"hyperparameters\", \"metric_used\", \"mean_validation\", \"mean_training\", \"mean_difference\", \"score_weighted\", \"time_taken\", \"hp_distribution\"])\n",
        "\n",
        "optimisations = 1\n",
        "iterations = 1\n",
        "\n",
        "# Function get the last row where the distribution was updated (if using the conservative flag, this is important for selecting the latest valid row in hpo_arse_results)\n",
        "roll_back_message = \"Rolled back to last valid optimisation.\"\n",
        "def get_last_valid_row(df):\n",
        "  for i in range(len(df) - 1, -1, -1):\n",
        "    if df.iloc[i][\"most_important_hp\"] != roll_back_message:\n",
        "      return df.iloc[i]\n",
        "\n",
        "# Visualisation function\n",
        "def display_visualisations(hpo_arse_results, shap_data):\n",
        "  # Generate main scoring graph\n",
        "  arse_results_plot = px.line(hpo_arse_results, x=hpo_arse_results.index, y=[f\"best score ({metric} {score_col})\", f\"mean score ({metric} {score_col})\"], title=f\"{metric} score per optimisation\", markers=True, width=800, height=400)\n",
        "  arse_results_plot.update_xaxes(tick0=1, dtick=1) # It might be useful to specify the range to something like... range=[1,len(df) + 1]\n",
        "  arse_results_plot.update_layout(\n",
        "      title_x = 0.5,\n",
        "      yaxis_title = \"score\",\n",
        "      margin = {\"l\":0,\"r\":0,\"t\":50,\"b\":0,\"pad\":0}\n",
        "  )\n",
        "  arse_results_plot.write_image(hpo_arse_line_graph_dir)\n",
        "\n",
        "  # Update display\n",
        "  clear_output(wait=True)\n",
        "  print(tabulate(hpo_arse_results[display_cols], headers=\"keys\", tablefmt=\"psql\"))\n",
        "  arse_results_plot.show()\n",
        "\n",
        "  # Update iterative SHAP plots\n",
        "  i = len(shap_data)\n",
        "  most_important_hp, shap_values = shap_data[-1]\n",
        "\n",
        "  importances = []\n",
        "  for j in range(shap_values.values.shape[1]):\n",
        "    importances.append(np.mean(np.abs(shap_values.values[:, j])))\n",
        "  feature_importances = {fea: imp for imp, fea in zip(importances, shap_values.feature_names)}\n",
        "  feature_importances = {k: v for k, v in sorted(feature_importances.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "  # Setup matplotlib subplots for side-by-side display\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "  fig.suptitle(f\"Optimisation #{i}\", fontsize=16)\n",
        "\n",
        "  # First plot\n",
        "  shap.plots.scatter(shap_values[:,most_important_hp], color=shap_values[:,most_important_hp].data, ax=ax1, show=False)\n",
        "\n",
        "  # Second plot (not using the default SHAP plot because they don't support passing an axis)\n",
        "  y_pos = np.arange(len(feature_importances))\n",
        "  width = [width for width in feature_importances.values()]\n",
        "\n",
        "  ax2.barh(y_pos, width, color=\"#FF0051\")\n",
        "  ax2.set_yticks(y_pos)\n",
        "  ax2.set_yticklabels(list(feature_importances.keys()))\n",
        "  ax2.invert_yaxis()\n",
        "  ax2.set_xlabel(\"mean(|SHAP value|)\")\n",
        "  ax2.spines[\"top\"].set_visible(False)\n",
        "  ax2.spines[\"right\"].set_visible(False)\n",
        "\n",
        "  for j in range(len(feature_importances.keys())):\n",
        "    ax2.axhline(j+1, color=\"#888888\", lw=0.5, dashes=(1, 5), zorder=-1)\n",
        "  for j, v in enumerate(width):\n",
        "    # All values are assumed to be positive as the optimisation only selects positive SHAP values\n",
        "    ax2.text(v + 0.001, j, f\"+{v:.2f}\", color=\"#FF0051\", horizontalalignment=\"left\", verticalalignment=\"center\", fontsize=12)\n",
        "\n",
        "  # Save and display plots\n",
        "  fig.tight_layout()\n",
        "  fig.savefig(join(hpo_arse_optimisation_plots_dir, f\"{i}_shap.png\"))\n",
        "\n",
        "  print() # Add a small gap\n",
        "  plt.show()\n",
        "\n",
        "# Load cache\n",
        "if exists(hpo_arse_iteration_results_dir):\n",
        "  # Load iteration results, set the hyperparameter distribution, and set current process indicies\n",
        "  hpo_arse_iteration_results = pd.read_pickle(hpo_arse_iteration_results_dir)\n",
        "  hp_distribution_serialized = hpo_arse_iteration_results.iloc[-1][\"hp_distribution\"]\n",
        "  hp_distribution = {k:function_map[df_hp_types.loc[k][\"type\"]](v[0], v[1]) for k, v in hp_distribution_serialized.items()}\n",
        "\n",
        "  optimisations = hpo_arse_iteration_results.iloc[-1][\"optimisations\"]\n",
        "  iterations = hpo_arse_iteration_results.iloc[-1][\"iterations\"] + 1\n",
        "  if iterations > iterations_per_optimisation:\n",
        "    iterations = 1\n",
        "    optimisations += 1\n",
        "\n",
        "  # Load optimisation results, display visualisations, and update the hyperparameter distribution\n",
        "  if exists(hpo_arse_optimisation_results_dir) and exists(hpo_arse_cache_shap_dir): # May not exit if process stops before the first optimisation finishes\n",
        "    # Load data\n",
        "    hpo_arse_results = pd.read_pickle(hpo_arse_optimisation_results_dir)\n",
        "    with open(hpo_arse_cache_shap_dir, \"rb\") as data:\n",
        "      shap_data = pickle.load(data)\n",
        "\n",
        "    # Update hyperparameter distribution\n",
        "    hp_distribution_serialized = hpo_arse_results.iloc[-1][\"hp_distribution_new\"]\n",
        "    hp_distribution = {k:function_map[df_hp_types.loc[k][\"type\"]](v[0], v[1]) for k, v in hp_distribution_serialized.items()}\n",
        "\n",
        "    # Display visualisations\n",
        "    display_visualisations(hpo_arse_results, shap_data)\n",
        "\n",
        "# Iteratively train models with sampled hyper parameters, saving the values and metric score\n",
        "while optimisations <= total_optimisations:\n",
        "  iterations_progress_label = widgets.Label(value=f\"Iteration of current optimisation: {iterations}/{iterations_per_optimisation}\")\n",
        "  display(iterations_progress_label)\n",
        "  hp_distribution_old = hp_distribution.copy()\n",
        "  hp_distribution_new = hp_distribution.copy() # Copy old distribution for now, update this value later\n",
        "  hp_distribution_old_serialized = {k:v.args for k, v in hp_distribution_old.items()}\n",
        "\n",
        "  #interim_results = pd.DataFrame()\n",
        "  while iterations <= iterations_per_optimisation:\n",
        "    t_start = datetime.now()\n",
        "\n",
        "    # Sample HP values\n",
        "    hp_sample_random_state = len(hpo_arse_iteration_results)+1\n",
        "    hp_sample = {k: v.rvs(random_state=hp_sample_random_state*(i+random.randrange(10000))) for i, (k, v) in enumerate(hp_distribution.items())}\n",
        "    sampled_parameters = hp_sample\n",
        "    arse_hyperparameters = dict(baseline_hyperparameters.copy())\n",
        "    arse_hyperparameters.update(sampled_parameters)\n",
        "    iterations_progress_label.value = f\"Iteration of current optimisation: {iterations}/{iterations_per_optimisation}\"\n",
        "\n",
        "    arse_predictor = XGBPredictor(**arse_hyperparameters)\n",
        "    df_kfold_metrics = run_kfold(model_dataset, kf_split[:n_hpo_splits], arse_predictor)\n",
        "\n",
        "    # Generate statistics\n",
        "    mean_training = df_kfold_metrics[f\"score_training ({metric})\"].mean()\n",
        "    mean_validation = df_kfold_metrics[f\"score_validation ({metric})\"].mean()\n",
        "    mean_difference = df_kfold_metrics[f\"score_difference ({metric})\"].mean()\n",
        "    score_weighted = def_score_weighted(mean_validation, mean_training)\n",
        "\n",
        "    # Save results\n",
        "    time_taken = str(datetime.now() - t_start)\n",
        "    i = len(hpo_arse_iteration_results)\n",
        "    hpo_arse_iteration_results.loc[i + 1] = [optimisations, iterations, arse_hyperparameters, metric, mean_validation, mean_training, mean_difference, score_weighted, time_taken, hp_distribution_old_serialized]\n",
        "    hpo_arse_iteration_results.to_pickle(hpo_arse_iteration_results_dir)\n",
        "    hpo_arse_iteration_results.to_csv(f\"{hpo_arse_iteration_results_dir[:-4]}.csv\")\n",
        "\n",
        "    # Export best hyperparameters\n",
        "    if optimal_value == \"min\":\n",
        "      best_result_id = hpo_arse_iteration_results[score_col].idxmin()\n",
        "    else:\n",
        "      best_result_id = hpo_arse_iteration_results[score_col].idxmax()\n",
        "\n",
        "    best_arse_hyperparameters = hpo_arse_iteration_results.loc[best_result_id][\"hyperparameters\"].copy()\n",
        "\n",
        "    export_arse_hyperparameters = best_arse_hyperparameters\n",
        "    export_arse_hyperparameters[score_col] = hpo_arse_iteration_results.loc[best_result_id][score_col]\n",
        "    pd.DataFrame(export_arse_hyperparameters, index=[0]).to_csv(hpo_arse_best_hyperparameters_dir)\n",
        "\n",
        "    # Update iteration index\n",
        "    iterations += 1\n",
        "\n",
        "  iterations = 1\n",
        "\n",
        "  # Collect results of only this optimisation round\n",
        "  interim_results = hpo_arse_iteration_results.loc[hpo_arse_iteration_results[\"optimisations\"] == optimisations]\n",
        "  interim_results = pd.concat([interim_results.drop([\"hyperparameters\"], axis=1), interim_results[\"hyperparameters\"].apply(pd.Series)], axis=1)\n",
        "  interim_results = interim_results[[\"time_taken\", score_col] + list(hp_distribution.keys())] # Make sure we only collect hyperparameters being optimised\n",
        "\n",
        "  # Generate SHAP values\n",
        "  hpo_x = interim_results[list(hp_distribution.keys())]\n",
        "  hpo_x = hpo_x.apply(pd.to_numeric)\n",
        "  hpo_y = interim_results[score_col]\n",
        "  hpo_y = hpo_y.apply(pd.to_numeric)\n",
        "  hp_predictor = XGBPredictor()\n",
        "  hp_predictor.fit(hpo_x, hpo_y)\n",
        "  explainer = shap.TreeExplainer(hp_predictor)\n",
        "  shap_values = explainer(hpo_x)\n",
        "\n",
        "  # Calculate the feature importance (mean absolute shap value) for each feature, then select the most important\n",
        "  importances = []\n",
        "  for i in range(shap_values.values.shape[1]):\n",
        "    importances.append(np.mean(np.abs(shap_values.values[:, i])))\n",
        "  feature_importances = {fea: imp for imp, fea in zip(importances, hpo_x.columns)}\n",
        "  feature_importances = {k: v for k, v in sorted(feature_importances.items(), key=lambda item: item[1], reverse = True)}\n",
        "\n",
        "  most_important_hp = max(feature_importances, key=feature_importances.get)\n",
        "  try:\n",
        "    most_important_hp_shap = shap_values[:,most_important_hp]\n",
        "  except:\n",
        "    most_important_hp_shap = shap_values[most_important_hp,:]\n",
        "\n",
        "  # Break loop if most important feature has no effect (optimisation complete)\n",
        "  if len((most_important_hp_shap.values!=0).nonzero()[0]) == 0:\n",
        "    print(\"Optimisation complete\")\n",
        "    break\n",
        "\n",
        "  #  Calculate 25th, 50th and 75th percentiles for the sampled range of the most important HP\n",
        "  hp_p25, hp_p50, hp_p75 = np.percentile(hpo_x[most_important_hp], [25,50,75])\n",
        "\n",
        "  # Create inclusive masks for quartiles\n",
        "  hp_quartile1_mask = most_important_hp_shap.data <= hp_p25\n",
        "  hp_quartile2_mask = (most_important_hp_shap.data <= hp_p50) & (most_important_hp_shap.data >= hp_p25)\n",
        "  hp_quartile3_mask = (most_important_hp_shap.data <= hp_p75) & (most_important_hp_shap.data >= hp_p50)\n",
        "  hp_quartile4_mask = most_important_hp_shap.data >= hp_p75\n",
        "\n",
        "  #  Calculate mean SHAP values for each quartile\n",
        "  shap_quartile1_mean = np.average(most_important_hp_shap.values[hp_quartile1_mask])\n",
        "  shap_quartile2_mean = np.average(most_important_hp_shap.values[hp_quartile2_mask])\n",
        "  shap_quartile3_mean = np.average(most_important_hp_shap.values[hp_quartile3_mask])\n",
        "  shap_quartile4_mean = np.average(most_important_hp_shap.values[hp_quartile4_mask])\n",
        "\n",
        "  shap_quartile_means = np.array([shap_quartile1_mean,shap_quartile2_mean,shap_quartile3_mean,shap_quartile4_mean])\n",
        "\n",
        "  # Globals - check if any optimal HP value is the max or min of tested values\n",
        "  upper_limit_optimal = False\n",
        "  lower_limit_optimal = False\n",
        "\n",
        "  # Calculate the 'min' and 'max' mean SHAP values for each quartile\n",
        "  # Optimise based on 'min' or 'max' optimal value\n",
        "  if optimal_value == \"min\":\n",
        "    min_shap_quartile_mean = np.min(shap_quartile_means)\n",
        "    if min_shap_quartile_mean == shap_quartile1_mean:\n",
        "      optimal_hp_values = most_important_hp_shap.data[hp_quartile1_mask]\n",
        "      lower_limit_optimal = True\n",
        "    if min_shap_quartile_mean == shap_quartile2_mean:\n",
        "      optimal_hp_values = most_important_hp_shap.data[hp_quartile2_mask]\n",
        "    if min_shap_quartile_mean == shap_quartile3_mean:\n",
        "      optimal_hp_values = most_important_hp_shap.data[hp_quartile3_mask]\n",
        "    if min_shap_quartile_mean == shap_quartile4_mean:\n",
        "      optimal_hp_values = most_important_hp_shap.data[hp_quartile4_mask]\n",
        "      upper_limit_optimal = True\n",
        "  else:\n",
        "    max_shap_quartile_mean = np.max(shap_quartile_means)\n",
        "    if max_shap_quartile_mean == shap_quartile1_mean:\n",
        "      optimal_hp_values = most_important_hp_shap.data[hp_quartile1_mask]\n",
        "      lower_limit_optimal = True\n",
        "    if max_shap_quartile_mean == shap_quartile2_mean:\n",
        "      optimal_hp_values = most_important_hp_shap.data[hp_quartile2_mask]\n",
        "    if max_shap_quartile_mean == shap_quartile3_mean:\n",
        "      optimal_hp_values = most_important_hp_shap.data[hp_quartile3_mask]\n",
        "    if max_shap_quartile_mean == shap_quartile4_mean:\n",
        "      optimal_hp_values = most_important_hp_shap.data[hp_quartile4_mask]\n",
        "      upper_limit_optimal = True\n",
        "\n",
        "  # Get new min and max optimal values\n",
        "  if df_hp_types.loc[most_important_hp]['type'] == \"randint\":\n",
        "    new_hp_max = np.ceil(np.max(optimal_hp_values))\n",
        "    new_hp_min = np.trunc(np.min(optimal_hp_values))\n",
        "  else:\n",
        "    new_hp_max = np.max(optimal_hp_values)\n",
        "    new_hp_min = np.min(optimal_hp_values)\n",
        "\n",
        "  # Calculate range_correction percentage of tested range of optimal HP\n",
        "  range_correction = bounds_correction_percent * np.ptp(most_important_hp_shap.data) / 100\n",
        "\n",
        "  # Adjust to range_correction if necessary\n",
        "  if df_hp_types.loc[most_important_hp]['type'] == \"randint\":\n",
        "    if upper_limit_optimal:\n",
        "      new_hp_max = np.ceil(min(new_hp_max + range_correction, df_hp_types.loc[most_important_hp]['max']))\n",
        "    if lower_limit_optimal:\n",
        "      new_hp_min = np.trunc(max(new_hp_min - range_correction, df_hp_types.loc[most_important_hp]['min']))\n",
        "  else:\n",
        "    if upper_limit_optimal:\n",
        "      new_hp_max = min(new_hp_max + range_correction, df_hp_types.loc[most_important_hp]['max'])\n",
        "    if lower_limit_optimal:\n",
        "      new_hp_min = max(new_hp_min - range_correction, df_hp_types.loc[most_important_hp]['min'])\n",
        "\n",
        "  # Update and export SHAP data\n",
        "  shap_data.append([most_important_hp, shap_values])\n",
        "  with open(hpo_arse_cache_shap_dir, \"wb\") as f:\n",
        "    pickle.dump(shap_data, f)\n",
        "\n",
        "  # Update and export optimisation results\n",
        "  if optimal_value == \"min\":\n",
        "    best_result_id = interim_results[score_col].idxmin()\n",
        "  else:\n",
        "    best_result_id = interim_results[score_col].idxmax()\n",
        "\n",
        "  i = len(hpo_arse_results)\n",
        "  time_taken = 0\n",
        "  for index, value in interim_results[\"time_taken\"].items():\n",
        "    pt = datetime.strptime(value, \"%H:%M:%S.%f\")\n",
        "    time_taken += pt.second + pt.minute*60 + pt.hour*3600\n",
        "  time_taken = str(timedelta(seconds=time_taken))\n",
        "\n",
        "  roll_back_hp = False\n",
        "  if conservative and optimisations > 1:\n",
        "    avg_score_col = [col for col in hpo_arse_results.columns if col.startswith(\"mean score\")]\n",
        "    current_mean = interim_results[score_col].mean()\n",
        "    previous_mean = get_last_valid_row(hpo_arse_results)[avg_score_col][0] # Compare mean to the last valid optimisation, not necessarily the previous optimisation\n",
        "    if (optimal_value == \"min\" and current_mean > previous_mean) or (optimal_value == \"max\" and current_mean < previous_mean):\n",
        "      roll_back_hp = True\n",
        "\n",
        "  if roll_back_hp:\n",
        "    hp_distribution_new_serialized = get_last_valid_row(hpo_arse_results)[\"hp_distribution_old\"]\n",
        "    hp_distribution_new = {k:function_map[df_hp_types.loc[k][\"type\"]](v[0], v[1]) for k, v in hp_distribution_new_serialized.items()}\n",
        "    most_important_hp = roll_back_message\n",
        "    new_hp_min = np.nan\n",
        "    new_hp_max = np.nan\n",
        "  else:\n",
        "    if df_hp_types.loc[most_important_hp]['type'] == \"randint\":\n",
        "      hp_distribution_new[most_important_hp] = randint(new_hp_min, new_hp_max + 1)\n",
        "    else:\n",
        "      hp_distribution_new[most_important_hp] = uniform(new_hp_min, new_hp_max - new_hp_min)\n",
        "\n",
        "  hp_distribution = hp_distribution_new.copy()\n",
        "  hp_distribution_new_serialized = {k:v.args for k, v in hp_distribution_new.items()}\n",
        "\n",
        "  hpo_arse_results.loc[i + 1] = [interim_results.loc[best_result_id][score_col],\n",
        "                                interim_results[score_col].mean(), most_important_hp, new_hp_min, new_hp_max, time_taken, hp_distribution_old_serialized, hp_distribution_new_serialized]\n",
        "  hpo_arse_results.to_pickle(hpo_arse_optimisation_results_dir)\n",
        "  hpo_arse_results.to_csv(f\"{hpo_arse_optimisation_results_dir[:-4]}.csv\") # Human readable\n",
        "\n",
        "  # Update visualisations\n",
        "  display_visualisations(hpo_arse_results, shap_data)\n",
        "\n",
        "  # Update optimisations index\n",
        "  optimisations += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQRbn2sUoiUL"
      },
      "source": [
        "### View plots for all results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8H-YgQWrb5p"
      },
      "outputs": [],
      "source": [
        "# View plots for all ARSE results\n",
        "\n",
        "# Can change to a previous ARSE round\n",
        "# hpo_arse_dir = join(model_dataset_dir, \"hpo_arse_r1\")\n",
        "\n",
        "# Results\n",
        "hpo_arse_dir = join(model_dataset_dir, \"hpo_arse\")\n",
        "hpo_arse_iteration_results_dir = join(hpo_arse_dir, \"iteration_results.pkl\")\n",
        "hpo_arse_iteration_results = pd.read_pickle(hpo_arse_iteration_results_dir)\n",
        "\n",
        "# Hyperparameters optimised in ARSE\n",
        "arse_hpo_x = pd.json_normalize(hpo_arse_iteration_results['hyperparameters']).filter([\n",
        "    'max_bin',\n",
        "    'early_stopping_rounds',\n",
        "    'learning_rate',\n",
        "    'max_depth',\n",
        "    'min_child_weight',\n",
        "    'gamma',\n",
        "    'reg_alpha',\n",
        "    'reg_lambda',\n",
        "    'colsample_bytree',\n",
        "    'colsample_bylevel',\n",
        "    'colsample_bynode'\n",
        "    ])\n",
        "\n",
        "# Score optimised in ARSE\n",
        "arse_hpo_y = hpo_arse_iteration_results.filter([\n",
        "    'score_weighted'\n",
        "])\n",
        "\n",
        "# Filter warning \"ntree_limit is deprecated, use `iteration_range` or model slicing instead.\"\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
        "\n",
        "# predictor for SHAP interpretation\n",
        "arse_hpo_predictor = XGBPredictor()\n",
        "arse_hpo_predictor.fit(arse_hpo_x, arse_hpo_y)\n",
        "\n",
        "# SHAP explainer\n",
        "arse_hpo_explainer = shap.Explainer(arse_hpo_predictor)\n",
        "arse_hpo_shap_values = arse_hpo_explainer(arse_hpo_x)\n",
        "shap.plots.beeswarm(arse_hpo_shap_values, plot_size=(20,8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAzldgcqoqvY"
      },
      "outputs": [],
      "source": [
        "# Individual plots for hyperparameters\n",
        "# shap.plots.scatter(arse_hpo_shap_values[:,'max_bin'], color=arse_hpo_shap_values[:,'max_bin'].data)\n",
        "# shap.plots.scatter(arse_hpo_shap_values[:,'early_stopping_rounds'], color=arse_hpo_shap_values[:,'early_stopping_rounds'].data)\n",
        "# shap.plots.scatter(arse_hpo_shap_values[:,'learning_rate'], color=arse_hpo_shap_values[:,'learning_rate'].data)\n",
        "shap.plots.scatter(arse_hpo_shap_values[:,'min_child_weight'], color=arse_hpo_shap_values[:,'min_child_weight'].data)\n",
        "shap.plots.scatter(arse_hpo_shap_values[:,'gamma'], color=arse_hpo_shap_values[:,'gamma'].data)\n",
        "shap.plots.scatter(arse_hpo_shap_values[:,'reg_alpha'], color=arse_hpo_shap_values[:,'reg_alpha'].data)\n",
        "shap.plots.scatter(arse_hpo_shap_values[:,'reg_lambda'], color=arse_hpo_shap_values[:,'reg_lambda'].data)\n",
        "shap.plots.scatter(arse_hpo_shap_values[:,'colsample_bytree'], color=arse_hpo_shap_values[:,'colsample_bytree'].data)\n",
        "shap.plots.scatter(arse_hpo_shap_values[:,'colsample_bylevel'], color=arse_hpo_shap_values[:,'colsample_bylevel'].data)\n",
        "shap.plots.scatter(arse_hpo_shap_values[:,'colsample_bynode'], color=arse_hpo_shap_values[:,'colsample_bynode'].data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9-hQ7G37uZi"
      },
      "source": [
        "# Final model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFaSL-AVQQ2c"
      },
      "source": [
        "## Define and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMR_E3f-B9iL"
      },
      "outputs": [],
      "source": [
        "# (optional) redefine baseline hyperparameters for final model\n",
        "\n",
        "baseline_hyperparameters = {\n",
        " 'tree_method': 'hist',\n",
        " 'device': 'cuda',\n",
        " 'enable_categorical': True,\n",
        " 'max_bin': max_bin,\n",
        " 'n_estimators': 100000, # Will be limited by early stopping\n",
        " 'learning_rate': 0.01,\n",
        " 'early_stopping_rounds': 10,\n",
        " 'min_child_weight': 52,\n",
        " 'gamma': 0.125,\n",
        " 'alpha': 22000,\n",
        " 'lambda': 10,\n",
        " 'colsample_bytree': 0.667,\n",
        " 'colsample_bylevel': 0.86,\n",
        " 'colsample_bynode': 0.9\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMJY9AtfPv8T"
      },
      "outputs": [],
      "source": [
        "use_best_hyperparameters = False # Set to false to use baseline\n",
        "\n",
        "hpo_method = \"hpo_arse\" # Options: \"hpo_arse\", \"hpo_random_search\"\n",
        "\n",
        "# Test model\n",
        "test_hyperparameters = baseline_hyperparameters\n",
        "predictor = XGBPredictor(**test_hyperparameters)\n",
        "test_model_dir = join(model_dataset_dir,\"model_test\")\n",
        "\n",
        "# If optimised hyperparameters exist, use them\n",
        "if use_best_hyperparameters:\n",
        "  test_hyperparameters = pd.read_csv(join(model_dataset_dir, hpo_method, \"best_hyperparameters.csv\"), index_col=0).to_dict(orient=\"records\")[0]\n",
        "  test_hyperparameters = {k: v for k, v in test_hyperparameters.items() if not \"score_\" in k} # Remove scoring metric\n",
        "  predictor = XGBPredictor(**test_hyperparameters)\n",
        "\n",
        "assert not exists(model_description_dir) and not exists(final_model_dir), \"Remove both \\\"model_description.json\\\" and \\\"model.json\\\", before exporting a new model\"\n",
        "\n",
        "# Run model\n",
        "df_kfold_metrics = run_kfold(model_dataset, kf_split[n_hpo_splits:], predictor, verbose=True,\n",
        "                             export_test_model=True, test_model_dir=test_model_dir)\n",
        "\n",
        "generate_statistics(df_kfold_metrics)\n",
        "\n",
        "model_description = {\n",
        "    \"metric_used_for_training\": metric,\n",
        "    \"optimal_value\": optimal_value,\n",
        "    \"use_score_weighted\": use_score_weighted,\n",
        "    \"n_splits\": n_splits, # Number of k-fold splits\n",
        "    \"splits_hpo\": n_hpo_splits, # Number of k-fold splits set aside for HPO, the remaining are used for final testing.\n",
        "    \"hyperparameters\": str(test_hyperparameters)\n",
        "}\n",
        "\n",
        "for col in df_kfold_metrics.columns:\n",
        "  model_description[f\"{col} mean\"] = float(df_kfold_metrics[col].mean())\n",
        "  model_description[f\"{col} std\"] = float(df_kfold_metrics[col].std())\n",
        "\n",
        "# Export model_description.json\n",
        "with open(model_description_dir, \"w\") as f:\n",
        "  f.write(json.dumps(model_description))\n",
        "print(\"model_description.json generation and export complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ6tXG1bQJE4"
      },
      "source": [
        "## Descriptive plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuuF8sVWHGTL"
      },
      "outputs": [],
      "source": [
        "# Descriptive plots\n",
        "\n",
        "# Assert model description exists\n",
        "assert exists(model_description_dir), \"\\\"model_description.json\\\" must exist to continue, run previous cells\"\n",
        "\n",
        "# Reload hyperparameters\n",
        "with open(model_description_dir) as model_description_json:\n",
        "  model_description = json.load(model_description_json)\n",
        "final_hyperparameters = eval(model_description[\"hyperparameters\"])\n",
        "\n",
        "# Remove early stopping and replace with mean n_estimators\n",
        "if \"early_stopping_rounds\" in final_hyperparameters:\n",
        "  final_hyperparameters = {k:v for k, v in final_hyperparameters.items() if k != \"early_stopping_rounds\"}\n",
        "  final_hyperparameters[\"n_estimators\"] = round(model_description[\"n_estimators mean\"])\n",
        "\n",
        "# Select the last k-split\n",
        "train_index_plots, valid_index_plots = kf_split[n_splits-1]\n",
        "x_train_plots = model_dataset.loc[train_index_plots][selected_predictors]\n",
        "x_valid_plots = model_dataset.loc[valid_index_plots][selected_predictors]\n",
        "y_train_plots = model_dataset.loc[train_index_plots][selected_variate]\n",
        "y_valid_plots = model_dataset.loc[valid_index_plots][selected_variate]\n",
        "\n",
        "# Create DMatrix objects\n",
        "dtrain_plots = xgb.DMatrix(x_train_plots, y_train_plots, enable_categorical=True)\n",
        "dvalid_plots = xgb.DMatrix(x_valid_plots, y_valid_plots, enable_categorical=True)\n",
        "\n",
        "# Set up parameters\n",
        "predictor = XGBPredictor(**final_hyperparameters)\n",
        "params_plots = predictor.get_params()\n",
        "params_plots['eval_metric'] = eval_metric\n",
        "# Default fix for new XGBoost version\n",
        "[model_params.pop(key, None) for key in ['n_estimators', 'enable_categorical', 'missing']]\n",
        "\n",
        "# Train model\n",
        "evals_result = {}\n",
        "model_plots = xgb.train(params_plots,\n",
        "                        dtrain_plots,\n",
        "                        num_boost_round=final_hyperparameters['n_estimators'],\n",
        "                        evals=[(dtrain_plots, 'training'), (dvalid_plots, 'validation_1')],\n",
        "                        evals_result=evals_result,\n",
        "                        verbose_eval=False)\n",
        "\n",
        "# Feature importances\n",
        "feature_importances = model_plots.get_score(importance_type='weight')\n",
        "sorted_importances = sorted(feature_importances.items(), key=lambda x: x[1], reverse=True)\n",
        "features, importances = zip(*sorted_importances)\n",
        "\n",
        "plt.figure(figsize=(10, 30))\n",
        "plt.barh(features, importances, align='center')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importances')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
        "plt.show()\n",
        "\n",
        "# Prediction versus test data plot\n",
        "validation_prediction = model_plots.predict(dvalid_plots)\n",
        "validation_prediction_series = pd.Series(validation_prediction, index=y_valid_plots.index, name=y_valid_plots.name)\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "bins = np.linspace(y_valid_plots.min(), y_valid_plots.max(), 100)\n",
        "y_valid_plots.hist(bins=bins, label=f'{y_valid_plots.name} (actual)', alpha=0.6)\n",
        "validation_prediction_series.hist(bins=bins, label=f'{y_valid_plots.name} (predicted)', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Prediction vs. Actual Data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9PEyUnwQXZI"
      },
      "source": [
        "## Train and export with full dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate final model\n",
        "\n",
        "# Assert exists\n",
        "assert exists(model_description_dir), \"\\\"model_description.json\\\" must exist to continue, run previous cells\"\n",
        "\n",
        "# Reload hyperparameters\n",
        "with open(model_description_dir) as model_description_json:\n",
        "  model_description = json.load(model_description_json)\n",
        "final_hyperparameters = eval(model_description[\"hyperparameters\"])\n",
        "\n",
        "# Remove early stopping and replace with mean n_estimators\n",
        "if \"early_stopping_rounds\" in final_hyperparameters:\n",
        "  final_hyperparameters = {k:v for k, v in final_hyperparameters.items() if k != \"early_stopping_rounds\"}\n",
        "  final_hyperparameters[\"n_estimators\"] = round(model_description[\"n_estimators mean\"])\n",
        "\n",
        "\n",
        "# Create DMatrix objects\n",
        "dtrain_final = xgb.DMatrix(model_dataset_x, model_dataset_y, enable_categorical=True)\n",
        "\n",
        "# Train model on full dataset\n",
        "predictor = XGBPredictor(**final_hyperparameters)\n",
        "params_final = predictor.get_params()\n",
        "params_final['eval_metric'] = eval_metric\n",
        "# Default fix for new XGBoost version\n",
        "[params_final.pop(key, None) for key in ['n_estimators', 'enable_categorical', 'missing']]\n",
        "model_final = xgb.train(params_final,\n",
        "                        dtrain_final,\n",
        "                        num_boost_round=final_hyperparameters['n_estimators'],\n",
        "                        verbose_eval=True)\n",
        "\n",
        "# Export model\n",
        "model_final.save_model(final_model_dir)\n",
        "print(\"Model training and 'model.json' export complete.\")"
      ],
      "metadata": {
        "id": "PelMIeTUBC8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lzQyh9qGMAL"
      },
      "source": [
        "# Model interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oUwZ49xHCoJ"
      },
      "outputs": [],
      "source": [
        "# Set sampling criteria. Set to 100 % for all data.\n",
        "sample_model_dataset_by_percent = True  # If False then by number\n",
        "sample_model_dataset_value = 100  # Set to a percentage or number, or 'None' if not sampling\n",
        "\n",
        "# Assert model exists\n",
        "assert exists(model_description_dir) and exists(final_model_dir), \"\\\"model_description.json\\\" and \\\"model.json\\\" must exist to continue, run the 'Final model' section.\"\n",
        "\n",
        "# Sample dataset for SHAP evaluation\n",
        "if sample_model_dataset_by_percent:\n",
        "    model_dataset_shap = model_dataset.sample(frac=sample_model_dataset_value/100, random_state=1).reset_index(drop=True)\n",
        "else: model_dataset_shap = model_dataset.sample(n=sample_model_dataset_value, random_state=1).reset_index(drop=True)\n",
        "\n",
        "# Split sample into x and y\n",
        "model_dataset_shap_x = model_dataset_shap[selected_predictors]\n",
        "model_dataset_shap_y = model_dataset_shap[selected_variate]\n",
        "\n",
        "# Split into training and validation for early_stopping\n",
        "kf_split_shap = list(KFold(n_splits=n_splits, shuffle=True, random_state=1).split(model_dataset_shap_x, model_dataset_shap_y))\n",
        "\n",
        "# Run on the first split\n",
        "train_index_shap, valid_index_shap = kf_split_shap[n_splits-1]\n",
        "x_train_shap = model_dataset_shap.loc[train_index_shap][selected_predictors]\n",
        "x_valid_shap = model_dataset_shap.loc[valid_index_shap][selected_predictors]\n",
        "y_train_shap = model_dataset_shap.loc[train_index_shap][selected_variate]\n",
        "y_valid_shap = model_dataset_shap.loc[valid_index_shap][selected_variate]\n",
        "\n",
        "# Create DMatrix objects\n",
        "dtrain_shap = xgb.DMatrix(x_train_shap, y_train_shap, enable_categorical=True, feature_names=selected_predictors)\n",
        "dvalid_shap = xgb.DMatrix(x_valid_shap, y_valid_shap, enable_categorical=True, feature_names=selected_predictors)\n",
        "\n",
        "# Reload hyperparameters\n",
        "with open(model_description_dir) as model_description_json:\n",
        "    model_description = json.load(model_description_json)\n",
        "final_hyperparameters = eval(model_description[\"hyperparameters\"])\n",
        "\n",
        "# Remove early stopping and replace with mean n_estimators\n",
        "if \"early_stopping_rounds\" in final_hyperparameters:\n",
        "  final_hyperparameters = {k:v for k, v in final_hyperparameters.items() if k != \"early_stopping_rounds\"}\n",
        "  final_hyperparameters[\"n_estimators\"] = round(model_description[\"n_estimators mean\"])\n",
        "\n",
        "# Define model\n",
        "predictor = XGBPredictor(**final_hyperparameters)\n",
        "params_shap = predictor.get_params()\n",
        "params_shap['eval_metric'] = eval_metric\n",
        "# Default fix for new XGBoost version\n",
        "[model_params.pop(key, None) for key in ['n_estimators', 'enable_categorical', 'missing']]\n",
        "\n",
        "# Train model\n",
        "evals_result = {}\n",
        "model_shap = xgb.train(params_shap,\n",
        "                        dtrain_shap,\n",
        "                        num_boost_round=final_hyperparameters['n_estimators'],\n",
        "                        evals=[(dtrain_shap, 'training'), (dvalid_shap, 'validation_1')],\n",
        "                        evals_result=evals_result,\n",
        "                        verbose_eval=False)\n",
        "\n",
        "# Create explainer and SHAP values\n",
        "explainer = shap.TreeExplainer(model_shap)\n",
        "\n",
        "# Prepare categorical data for SHAP analysis\n",
        "model_dataset_shap_x_for_shap = model_dataset_shap_x.copy()\n",
        "for col in model_dataset_shap_x_for_shap.select_dtypes(include=['category']).columns:\n",
        "    model_dataset_shap_x_for_shap[col] = model_dataset_shap_x_for_shap[col].cat.codes\n",
        "\n",
        "# Convert features to DMatrix\n",
        "dmatrix_for_shap = xgb.DMatrix(model_dataset_shap_x_for_shap, enable_categorical=True, feature_names=selected_predictors)\n",
        "\n",
        "# Generate SHAP values\n",
        "shap_values = explainer(dmatrix_for_shap, check_additivity=False) # Disabling additivity check due to floating point precision issues\n",
        "\n",
        "# Export SHAP values\n",
        "shap_dir = join(model_dataset_dir, \"model_shap.pkl\")\n",
        "pd.to_pickle(shap_values, shap_dir)\n",
        "print(\"SHAP generation and export complete\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeqniW-kG8Wo"
      },
      "outputs": [],
      "source": [
        "# Load SHAP values\n",
        "shap_dir = join(model_dataset_dir, \"model_shap.pkl\")\n",
        "\n",
        "with open(shap_dir, \"rb\") as shap_pkl:\n",
        "    shap_values = pickle.load(shap_pkl)\n",
        "\n",
        "# Summary SHAP plots with feature names\n",
        "shap_values.feature_names = selected_predictors\n",
        "shap.plots.beeswarm(shap_values, max_display=200)\n",
        "shap.plots.bar(shap_values[0], max_display=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFYPJZI8lh2O"
      },
      "outputs": [],
      "source": [
        "for feature in selected_predictors:\n",
        "    print(f\"shap.plots.scatter(shap_values[:, '{feature}'])\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eENdpLwWnT63"
      },
      "outputs": [],
      "source": [
        "shap.plots.scatter(shap_values[:, 'pre_topo_cor_unsmooth_elevation'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nezNm_Gz3cnz"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "T4CiSNFVBEke"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Xhx977-WIaWJ",
        "mqrQnDhtjvCq",
        "twsBfrX4A1Ce",
        "nx6XO_2tAp0p",
        "5lqjsLKZaQXo",
        "4u_wV7NAp1h-",
        "a3gwpt7LfJGj",
        "CDMPTLWi9d7X",
        "QP1fzhFk7kVl",
        "NZ6tXG1bQJE4"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}