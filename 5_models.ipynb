{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/5_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhx977-WIaWJ"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRnNty96IThZ"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wk1EBasJAB4"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Imports and upgrades\n",
        "!pip install geopandas\n",
        "!pip install shap\n",
        "!pip install tabulate\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpmgsb8KKMGN"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import ast\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from google.colab import runtime\n",
        "import ipywidgets as widgets\n",
        "import json\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from os import makedirs\n",
        "from os.path import exists, join\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "from scipy.stats import randint, uniform\n",
        "from scipy.stats.qmc import LatinHypercube\n",
        "import shap\n",
        "from sklearn.metrics import (\n",
        "    root_mean_squared_error,\n",
        "    r2_score,\n",
        "    accuracy_score,\n",
        "    log_loss,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    matthews_corrcoef,\n",
        "    balanced_accuracy_score,\n",
        "    average_precision_score,\n",
        "    cohen_kappa_score,\n",
        "    brier_score_loss,\n",
        ")\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from tabulate import tabulate\n",
        "import warnings\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQwNO9T0_HPH"
      },
      "outputs": [],
      "source": [
        "# 4_datasets directories\n",
        "datasets_dir = join(base_dir, \"4_datasets/final\")\n",
        "\n",
        "# 5_models directories\n",
        "models_dir = join(base_dir, \"5_models\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(models_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHzd1jjLJLT1"
      },
      "source": [
        "# Compile new model dataset (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfHM2IdhAeO4"
      },
      "source": [
        "## Select a dataset to import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxF-5sQerT1F"
      },
      "outputs": [],
      "source": [
        "# Select a final dataset compiled with features in 4_datasets.ipynb\n",
        "for file in os.listdir(datasets_dir):\n",
        "  if file.endswith(\".pkl\"):\n",
        "    print(f'selected_dataset = \"{file}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dUC5Ysr_zIJ"
      },
      "outputs": [],
      "source": [
        "selected_dataset = \"agbd.pkl\"\n",
        "\n",
        "# Load dataset\n",
        "print(f\"Importing '{selected_dataset}'\")\n",
        "dataset_read = pd.read_pickle(join(datasets_dir, selected_dataset))\n",
        "dataset_imported = dataset_read.copy().reset_index(drop=True)\n",
        "\n",
        "# Drop geometry to save memory\n",
        "# Can be added back later for accuracy testing using the ID column\n",
        "dataset_imported = dataset_imported.drop(columns=['tar_geometry'])\n",
        "\n",
        "# Print dataset summary\n",
        "print(f\"'{selected_dataset}' with {len(dataset_imported)} rows and {len(dataset_imported.columns)} columns imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EB9V1T2AOb2"
      },
      "source": [
        "## Select ID column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Faht6XP_ANNV"
      },
      "outputs": [],
      "source": [
        "# Check columns and select ID column\n",
        "for col in dataset_imported.columns:\n",
        "  if col[:4] == \"tar_\":\n",
        "    print(f'id_column = \"{col}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQAo72JdcPDX"
      },
      "outputs": [],
      "source": [
        "id_column = \"tar_shot_number\"\n",
        "\n",
        "dataset_col_list = [id_column]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8GB8AlpAWjH"
      },
      "source": [
        "## Select a target to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0IhfEcGTDGI"
      },
      "outputs": [],
      "source": [
        "for col in dataset_imported.columns:\n",
        "  if col[:4] == \"tar_\" and col not in dataset_col_list:\n",
        "    print(f'selected_target = \"{col}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEHAsW7QAR41"
      },
      "outputs": [],
      "source": [
        "selected_target = \"tar_agbd\"\n",
        "\n",
        "dataset_col_list = dataset_col_list + [selected_target]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBjcNYodUWaM"
      },
      "source": [
        "## Select uncertainty metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOVqFj6YUklm"
      },
      "outputs": [],
      "source": [
        "# The uncertainty metrics should allow estimation of one standard deviation of the selected target.\n",
        "\n",
        "# Check remaining target columns and select the uncertainty columns, if any.\n",
        "\n",
        "for col in dataset_imported.columns:\n",
        "  if col[:4] == \"tar_\" and col not in dataset_col_list:\n",
        "    print(f'uncertainty = \"{col}\"')\n",
        "print(f'uncertainty = None')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYvnR5fsUpRl"
      },
      "outputs": [],
      "source": [
        "uncertainty = \"tar_agbd_se\"\n",
        "\n",
        "if uncertainty: dataset_col_list = dataset_col_list + [uncertainty]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqrQnDhtjvCq"
      },
      "source": [
        "## Add covariates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N19vBF98NRd"
      },
      "outputs": [],
      "source": [
        "# Covariates are used as a features in training, but should be handled\n",
        "# with care for predictions, either selecting a single representative value,\n",
        "# several values for different scenarios, or averaging across all values for\n",
        "# a final prediction.\n",
        "\n",
        "# Check remaining columns and select covariates column\n",
        "print(\"covariates  = [\")\n",
        "for col in dataset_imported.columns:\n",
        "  if col[:4] == \"tar_\" and col not in dataset_col_list and \"geometry\" not in col:\n",
        "    print(f'\"{col}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amoXKT3PgCNa"
      },
      "outputs": [],
      "source": [
        "covariates  = [\n",
        "\"tar_beam\",\n",
        "\"tar_sensitivity\",\n",
        "]\n",
        "\n",
        "dataset_imported_covar = dataset_imported.copy()\n",
        "\n",
        "# Rename tar_ to fea_\n",
        "covariates_renamed = []\n",
        "if len(covariates)>0:\n",
        "  covariates_renamed = [covariate.replace('tar','fea') for covariate in covariates]\n",
        "  dataset_imported_covar.rename(\n",
        "      columns={i:j for i,j in zip(covariates, covariates_renamed)}, inplace=True\n",
        "  )\n",
        "\n",
        "dataset_col_list = dataset_col_list + covariates_renamed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0edRH3sOATkP"
      },
      "source": [
        "## Select features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_pOoS1vacx4"
      },
      "outputs": [],
      "source": [
        "print(\"selected_features = [\")\n",
        "for col in sorted(dataset_imported_covar.columns):\n",
        "  if col[:4] == \"fea_\" and col not in dataset_col_list:\n",
        "    print(f'  \"{col}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu42V3jFSGTE"
      },
      "outputs": [],
      "source": [
        "#AGBD\n",
        "\n",
        "selected_features = [\n",
        "  \"fea_coast_proximity_km\",\n",
        "  \"fea_disturbance_edge_distance_1995\",\n",
        "  \"fea_disturbance_edge_distance_1996\",\n",
        "  \"fea_disturbance_edge_distance_1997\",\n",
        "  \"fea_disturbance_edge_distance_1998\",\n",
        "  \"fea_disturbance_edge_distance_1999\",\n",
        "  \"fea_disturbance_edge_distance_2000\",\n",
        "  \"fea_disturbance_edge_distance_2001\",\n",
        "  \"fea_disturbance_edge_distance_2002\",\n",
        "  \"fea_disturbance_edge_distance_2003\",\n",
        "  \"fea_disturbance_edge_distance_2004\",\n",
        "  \"fea_disturbance_edge_distance_2005\",\n",
        "  \"fea_disturbance_edge_distance_2006\",\n",
        "  \"fea_disturbance_edge_distance_2007\",\n",
        "  \"fea_disturbance_edge_distance_2008\",\n",
        "  \"fea_disturbance_edge_distance_2009\",\n",
        "  \"fea_disturbance_edge_distance_2010\",\n",
        "  \"fea_disturbance_edge_distance_2011\",\n",
        "  \"fea_disturbance_edge_distance_2012\",\n",
        "  \"fea_disturbance_edge_distance_2013\",\n",
        "  \"fea_disturbance_edge_distance_2014\",\n",
        "  \"fea_disturbance_edge_distance_2015\",\n",
        "  \"fea_disturbance_edge_distance_2016\",\n",
        "  \"fea_disturbance_edge_distance_2017\",\n",
        "  \"fea_disturbance_edge_distance_2018\",\n",
        "  \"fea_disturbance_edge_distance_2019\",\n",
        "  \"fea_disturbance_edge_distance_2020\",\n",
        "  \"fea_disturbance_edge_distance_2021\",\n",
        "  \"fea_disturbance_edge_distance_2022\",\n",
        "  \"fea_disturbance_edge_distance_2023\",\n",
        "  \"fea_disturbance_local_density_1995\",\n",
        "  \"fea_disturbance_local_density_1996\",\n",
        "  \"fea_disturbance_local_density_1997\",\n",
        "  \"fea_disturbance_local_density_1998\",\n",
        "  \"fea_disturbance_local_density_1999\",\n",
        "  \"fea_disturbance_local_density_2000\",\n",
        "  \"fea_disturbance_local_density_2001\",\n",
        "  \"fea_disturbance_local_density_2002\",\n",
        "  \"fea_disturbance_local_density_2003\",\n",
        "  \"fea_disturbance_local_density_2004\",\n",
        "  \"fea_disturbance_local_density_2005\",\n",
        "  \"fea_disturbance_local_density_2006\",\n",
        "  \"fea_disturbance_local_density_2007\",\n",
        "  \"fea_disturbance_local_density_2008\",\n",
        "  \"fea_disturbance_local_density_2009\",\n",
        "  \"fea_disturbance_local_density_2010\",\n",
        "  \"fea_disturbance_local_density_2011\",\n",
        "  \"fea_disturbance_local_density_2012\",\n",
        "  \"fea_disturbance_local_density_2013\",\n",
        "  \"fea_disturbance_local_density_2014\",\n",
        "  \"fea_disturbance_local_density_2015\",\n",
        "  \"fea_disturbance_local_density_2016\",\n",
        "  \"fea_disturbance_local_density_2017\",\n",
        "  \"fea_disturbance_local_density_2018\",\n",
        "  \"fea_disturbance_local_density_2019\",\n",
        "  \"fea_disturbance_local_density_2020\",\n",
        "  \"fea_disturbance_local_density_2021\",\n",
        "  \"fea_disturbance_local_density_2022\",\n",
        "  \"fea_disturbance_local_density_2023\",\n",
        "  \"fea_forest_edge_distance_1995\",\n",
        "  # \"fea_forest_edge_distance_1996\",\n",
        "  # \"fea_forest_edge_distance_1997\",\n",
        "  # \"fea_forest_edge_distance_1998\",\n",
        "  # \"fea_forest_edge_distance_1999\",\n",
        "  # \"fea_forest_edge_distance_2000\",\n",
        "  # \"fea_forest_edge_distance_2001\",\n",
        "  # \"fea_forest_edge_distance_2002\",\n",
        "  # \"fea_forest_edge_distance_2003\",\n",
        "  # \"fea_forest_edge_distance_2004\",\n",
        "  # \"fea_forest_edge_distance_2005\",\n",
        "  # \"fea_forest_edge_distance_2006\",\n",
        "  # \"fea_forest_edge_distance_2007\",\n",
        "  \"fea_forest_edge_distance_2008\",\n",
        "  # \"fea_forest_edge_distance_2009\",\n",
        "  # \"fea_forest_edge_distance_2010\",\n",
        "  # \"fea_forest_edge_distance_2011\",\n",
        "  # \"fea_forest_edge_distance_2012\",\n",
        "  # \"fea_forest_edge_distance_2013\",\n",
        "  # \"fea_forest_edge_distance_2014\",\n",
        "  # \"fea_forest_edge_distance_2015\",\n",
        "  # \"fea_forest_edge_distance_2016\",\n",
        "  # \"fea_forest_edge_distance_2017\",\n",
        "  # \"fea_forest_edge_distance_2018\",\n",
        "  # \"fea_forest_edge_distance_2019\",\n",
        "  # \"fea_forest_edge_distance_2020\",\n",
        "  \"fea_forest_edge_distance_2021\",\n",
        "  \"fea_forest_edge_distance_2022\",\n",
        "  \"fea_forest_edge_distance_2023\",\n",
        "  \"fea_forest_local_density_1995\",\n",
        "  # \"fea_forest_local_density_1996\",\n",
        "  # \"fea_forest_local_density_1997\",\n",
        "  # \"fea_forest_local_density_1998\",\n",
        "  # \"fea_forest_local_density_1999\",\n",
        "  # \"fea_forest_local_density_2000\",\n",
        "  # \"fea_forest_local_density_2001\",\n",
        "  # \"fea_forest_local_density_2002\",\n",
        "  # \"fea_forest_local_density_2003\",\n",
        "  # \"fea_forest_local_density_2004\",\n",
        "  # \"fea_forest_local_density_2005\",\n",
        "  # \"fea_forest_local_density_2006\",\n",
        "  # \"fea_forest_local_density_2007\",\n",
        "  \"fea_forest_local_density_2008\",\n",
        "  # \"fea_forest_local_density_2009\",\n",
        "  # \"fea_forest_local_density_2010\",\n",
        "  # \"fea_forest_local_density_2011\",\n",
        "  # \"fea_forest_local_density_2012\",\n",
        "  # \"fea_forest_local_density_2013\",\n",
        "  # \"fea_forest_local_density_2014\",\n",
        "  # \"fea_forest_local_density_2015\",\n",
        "  # \"fea_forest_local_density_2016\",\n",
        "  # \"fea_forest_local_density_2017\",\n",
        "  # \"fea_forest_local_density_2018\",\n",
        "  # \"fea_forest_local_density_2019\",\n",
        "  # \"fea_forest_local_density_2020\",\n",
        "  \"fea_forest_local_density_2021\",\n",
        "  \"fea_forest_local_density_2022\",\n",
        "  \"fea_forest_local_density_2023\",\n",
        "  \"fea_latitude\",\n",
        "  \"fea_longitude\",\n",
        "  \"fea_lu_ais_edge_distance\",\n",
        "  \"fea_lu_berkelah_jerantut_edge_distance\",\n",
        "  \"fea_lu_berkelah_kuantan_edge_distance\",\n",
        "  \"fea_lu_berkelah_temerloh_edge_distance\",\n",
        "  \"fea_lu_old-growth_protected_areas_edge_distance\",\n",
        "  \"fea_lu_remen_chereh_edge_distance\",\n",
        "  \"fea_lu_tekai_tembeling_edge_distance\",\n",
        "  \"fea_lu_tekam_edge_distance\",\n",
        "  \"fea_lu_yong_edge_distance\",\n",
        "  \"fea_lu_yong_lipis_edge_distance\",\n",
        "  \"fea_topo_dtm_smooth_aspect_cosine\",\n",
        "  \"fea_topo_dtm_smooth_aspect_sine\",\n",
        "  \"fea_topo_dtm_smooth_circular_variance_aspect_03\",\n",
        "  \"fea_topo_dtm_smooth_circular_variance_aspect_07\",\n",
        "  \"fea_topo_dtm_smooth_circular_variance_aspect_11\",\n",
        "  \"fea_topo_dtm_smooth_deviation_mean_elevation_03\",\n",
        "  \"fea_topo_dtm_smooth_deviation_mean_elevation_07\",\n",
        "  \"fea_topo_dtm_smooth_deviation_mean_elevation_11\",\n",
        "  \"fea_topo_dtm_smooth_eastness\",\n",
        "  \"fea_topo_dtm_smooth_elevation\",\n",
        "  \"fea_topo_dtm_smooth_northness\",\n",
        "  \"fea_topo_dtm_smooth_profile_curvature\",\n",
        "  \"fea_topo_dtm_smooth_roughness_03\",\n",
        "  \"fea_topo_dtm_smooth_roughness_07\",\n",
        "  \"fea_topo_dtm_smooth_roughness_11\",\n",
        "  \"fea_topo_dtm_smooth_slope\",\n",
        "  \"fea_topo_dtm_smooth_stream_power_index_log10\",\n",
        "  \"fea_topo_dtm_smooth_surface_area_ratio\",\n",
        "  \"fea_topo_dtm_smooth_tangential_curvature\",\n",
        "  \"fea_topo_dtm_smooth_topographic_position_index_03\",\n",
        "  \"fea_topo_dtm_smooth_topographic_position_index_07\",\n",
        "  \"fea_topo_dtm_smooth_topographic_position_index_11\",\n",
        "  \"fea_topo_dtm_smooth_topographic_ruggedness_index\",\n",
        "  \"fea_topo_dtm_smooth_topographic_wetness_index\",\n",
        "  \"fea_topo_dtm_unsmooth_aspect_cosine\",\n",
        "  \"fea_topo_dtm_unsmooth_aspect_sine\",\n",
        "  \"fea_topo_dtm_unsmooth_circular_variance_aspect_03\",\n",
        "  \"fea_topo_dtm_unsmooth_circular_variance_aspect_07\",\n",
        "  \"fea_topo_dtm_unsmooth_circular_variance_aspect_11\",\n",
        "  \"fea_topo_dtm_unsmooth_deviation_mean_elevation_03\",\n",
        "  \"fea_topo_dtm_unsmooth_deviation_mean_elevation_07\",\n",
        "  \"fea_topo_dtm_unsmooth_deviation_mean_elevation_11\",\n",
        "  \"fea_topo_dtm_unsmooth_eastness\",\n",
        "  \"fea_topo_dtm_unsmooth_elevation\",\n",
        "  \"fea_topo_dtm_unsmooth_northness\",\n",
        "  \"fea_topo_dtm_unsmooth_profile_curvature\",\n",
        "  \"fea_topo_dtm_unsmooth_roughness_03\",\n",
        "  \"fea_topo_dtm_unsmooth_roughness_07\",\n",
        "  \"fea_topo_dtm_unsmooth_roughness_11\",\n",
        "  \"fea_topo_dtm_unsmooth_slope\",\n",
        "  \"fea_topo_dtm_unsmooth_stream_power_index_log10\",\n",
        "  \"fea_topo_dtm_unsmooth_surface_area_ratio\",\n",
        "  \"fea_topo_dtm_unsmooth_tangential_curvature\",\n",
        "  \"fea_topo_dtm_unsmooth_topographic_position_index_03\",\n",
        "  \"fea_topo_dtm_unsmooth_topographic_position_index_07\",\n",
        "  \"fea_topo_dtm_unsmooth_topographic_position_index_11\",\n",
        "  \"fea_topo_dtm_unsmooth_topographic_ruggedness_index\",\n",
        "  \"fea_topo_dtm_unsmooth_topographic_wetness_index\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WvQMvxqF0ny"
      },
      "outputs": [],
      "source": [
        "# GEDI elevation\n",
        "\n",
        "# selected_features = [\n",
        "#   \"fea_coast_proximity_km\",\n",
        "#   \"fea_disturbance_edge_distance_1990\",\n",
        "#   \"fea_disturbance_edge_distance_1991\",\n",
        "#   \"fea_disturbance_edge_distance_1992\",\n",
        "#   \"fea_disturbance_edge_distance_1993\",\n",
        "#   \"fea_disturbance_edge_distance_1994\",\n",
        "#   \"fea_disturbance_edge_distance_1995\",\n",
        "#   \"fea_disturbance_edge_distance_1996\",\n",
        "#   \"fea_disturbance_edge_distance_1997\",\n",
        "#   \"fea_disturbance_edge_distance_1998\",\n",
        "#   \"fea_disturbance_edge_distance_1999\",\n",
        "#   \"fea_disturbance_edge_distance_2000\",\n",
        "#   \"fea_disturbance_edge_distance_2001\",\n",
        "#   \"fea_disturbance_edge_distance_2002\",\n",
        "#   \"fea_disturbance_edge_distance_2003\",\n",
        "#   \"fea_disturbance_edge_distance_2004\",\n",
        "#   \"fea_disturbance_edge_distance_2005\",\n",
        "#   \"fea_disturbance_edge_distance_2006\",\n",
        "#   \"fea_disturbance_edge_distance_2007\",\n",
        "#   \"fea_disturbance_edge_distance_2008\",\n",
        "#   \"fea_disturbance_edge_distance_2009\",\n",
        "#   \"fea_disturbance_edge_distance_2010\",\n",
        "#   \"fea_disturbance_edge_distance_2011\",\n",
        "#   \"fea_disturbance_edge_distance_2012\",\n",
        "#   \"fea_disturbance_edge_distance_2013\",\n",
        "#   \"fea_disturbance_edge_distance_2014\",\n",
        "#   \"fea_disturbance_edge_distance_2015\",\n",
        "#   \"fea_disturbance_local_density_1990\",\n",
        "#   \"fea_disturbance_local_density_1991\",\n",
        "#   \"fea_disturbance_local_density_1992\",\n",
        "#   \"fea_disturbance_local_density_1993\",\n",
        "#   \"fea_disturbance_local_density_1994\",\n",
        "#   \"fea_disturbance_local_density_1995\",\n",
        "#   \"fea_disturbance_local_density_1996\",\n",
        "#   \"fea_disturbance_local_density_1997\",\n",
        "#   \"fea_disturbance_local_density_1998\",\n",
        "#   \"fea_disturbance_local_density_1999\",\n",
        "#   \"fea_disturbance_local_density_2000\",\n",
        "#   \"fea_disturbance_local_density_2001\",\n",
        "#   \"fea_disturbance_local_density_2002\",\n",
        "#   \"fea_disturbance_local_density_2003\",\n",
        "#   \"fea_disturbance_local_density_2004\",\n",
        "#   \"fea_disturbance_local_density_2005\",\n",
        "#   \"fea_disturbance_local_density_2006\",\n",
        "#   \"fea_disturbance_local_density_2007\",\n",
        "#   \"fea_disturbance_local_density_2008\",\n",
        "#   \"fea_disturbance_local_density_2009\",\n",
        "#   \"fea_disturbance_local_density_2010\",\n",
        "#   \"fea_disturbance_local_density_2011\",\n",
        "#   \"fea_disturbance_local_density_2012\",\n",
        "#   \"fea_disturbance_local_density_2013\",\n",
        "#   \"fea_disturbance_local_density_2014\",\n",
        "#   \"fea_disturbance_local_density_2015\",\n",
        "#   \"fea_forest_edge_distance_1990\",\n",
        "#   \"fea_forest_edge_distance_2000\",\n",
        "#   \"fea_forest_edge_distance_2010\",\n",
        "#   \"fea_forest_edge_distance_2011\",\n",
        "#   \"fea_forest_edge_distance_2012\",\n",
        "#   \"fea_forest_edge_distance_2013\",\n",
        "#   \"fea_forest_edge_distance_2014\",\n",
        "#   \"fea_forest_edge_distance_2015\",\n",
        "#   \"fea_forest_local_density_1990\",\n",
        "#   \"fea_forest_local_density_2000\",\n",
        "#   \"fea_forest_local_density_2010\",\n",
        "#   \"fea_forest_local_density_2011\",\n",
        "#   \"fea_forest_local_density_2012\",\n",
        "#   \"fea_forest_local_density_2013\",\n",
        "#   \"fea_forest_local_density_2014\",\n",
        "#   \"fea_forest_local_density_2015\",\n",
        "#   \"fea_latitude\",\n",
        "#   \"fea_longitude\",\n",
        "#   \"fea_lu_ais_edge_distance\",\n",
        "#   \"fea_lu_berkelah_jerantut_edge_distance\",\n",
        "#   \"fea_lu_berkelah_kuantan_edge_distance\",\n",
        "#   \"fea_lu_berkelah_temerloh_edge_distance\",\n",
        "#   \"fea_lu_old-growth_protected_areas_edge_distance\",\n",
        "#   \"fea_lu_remen_chereh_edge_distance\",\n",
        "#   \"fea_lu_tekai_tembeling_edge_distance\",\n",
        "#   \"fea_lu_tekam_edge_distance\",\n",
        "#   \"fea_lu_yong_edge_distance\",\n",
        "#   \"fea_lu_yong_lipis_edge_distance\",\n",
        "#   \"fea_topo_dsm_smooth_aspect_cosine\",\n",
        "#   \"fea_topo_dsm_smooth_aspect_sine\",\n",
        "#   \"fea_topo_dsm_smooth_circular_variance_aspect_03\",\n",
        "#   \"fea_topo_dsm_smooth_circular_variance_aspect_07\",\n",
        "#   \"fea_topo_dsm_smooth_circular_variance_aspect_11\",\n",
        "#   \"fea_topo_dsm_smooth_deviation_mean_elevation_03\",\n",
        "#   \"fea_topo_dsm_smooth_deviation_mean_elevation_07\",\n",
        "#   \"fea_topo_dsm_smooth_deviation_mean_elevation_11\",\n",
        "#   \"fea_topo_dsm_smooth_eastness\",\n",
        "#   \"fea_topo_dsm_smooth_elevation\",\n",
        "#   \"fea_topo_dsm_smooth_northness\",\n",
        "#   \"fea_topo_dsm_smooth_profile_curvature\",\n",
        "#   \"fea_topo_dsm_smooth_roughness_03\",\n",
        "#   \"fea_topo_dsm_smooth_roughness_07\",\n",
        "#   \"fea_topo_dsm_smooth_roughness_11\",\n",
        "#   \"fea_topo_dsm_smooth_slope\",\n",
        "#   \"fea_topo_dsm_smooth_stream_power_index_log10\",\n",
        "#   \"fea_topo_dsm_smooth_surface_area_ratio\",\n",
        "#   \"fea_topo_dsm_smooth_tangential_curvature\",\n",
        "#   \"fea_topo_dsm_smooth_topographic_position_index_03\",\n",
        "#   \"fea_topo_dsm_smooth_topographic_position_index_07\",\n",
        "#   \"fea_topo_dsm_smooth_topographic_position_index_11\",\n",
        "#   \"fea_topo_dsm_smooth_topographic_ruggedness_index\",\n",
        "#   \"fea_topo_dsm_smooth_topographic_wetness_index\",\n",
        "#   \"fea_topo_dsm_unsmooth_aspect_cosine\",\n",
        "#   \"fea_topo_dsm_unsmooth_aspect_sine\",\n",
        "#   \"fea_topo_dsm_unsmooth_circular_variance_aspect_03\",\n",
        "#   \"fea_topo_dsm_unsmooth_circular_variance_aspect_07\",\n",
        "#   \"fea_topo_dsm_unsmooth_circular_variance_aspect_11\",\n",
        "#   \"fea_topo_dsm_unsmooth_deviation_mean_elevation_03\",\n",
        "#   \"fea_topo_dsm_unsmooth_deviation_mean_elevation_07\",\n",
        "#   \"fea_topo_dsm_unsmooth_deviation_mean_elevation_11\",\n",
        "#   \"fea_topo_dsm_unsmooth_eastness\",\n",
        "#   \"fea_topo_dsm_unsmooth_elevation\",\n",
        "#   \"fea_topo_dsm_unsmooth_northness\",\n",
        "#   \"fea_topo_dsm_unsmooth_profile_curvature\",\n",
        "#   \"fea_topo_dsm_unsmooth_roughness_03\",\n",
        "#   \"fea_topo_dsm_unsmooth_roughness_07\",\n",
        "#   \"fea_topo_dsm_unsmooth_roughness_11\",\n",
        "#   \"fea_topo_dsm_unsmooth_slope\",\n",
        "#   \"fea_topo_dsm_unsmooth_stream_power_index_log10\",\n",
        "#   \"fea_topo_dsm_unsmooth_surface_area_ratio\",\n",
        "#   \"fea_topo_dsm_unsmooth_tangential_curvature\",\n",
        "#   \"fea_topo_dsm_unsmooth_topographic_position_index_03\",\n",
        "#   \"fea_topo_dsm_unsmooth_topographic_position_index_07\",\n",
        "#   \"fea_topo_dsm_unsmooth_topographic_position_index_11\",\n",
        "#   \"fea_topo_dsm_unsmooth_topographic_ruggedness_index\",\n",
        "#   \"fea_topo_dsm_unsmooth_topographic_wetness_index\",\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwjw9MYRbKaC"
      },
      "outputs": [],
      "source": [
        "# Sort alphabetically for predictions\n",
        "selected_features = sorted(selected_features)\n",
        "dataset_col_list = dataset_col_list + sorted(selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twsBfrX4A1Ce"
      },
      "source": [
        "## Define categorical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pbzhADthSMm"
      },
      "outputs": [],
      "source": [
        "# Define categorical data for correct model interpretation\n",
        "print(\"categorical_columns = [\")\n",
        "for col in dataset_col_list:\n",
        "  if col.startswith('fea_'):\n",
        "    print(f'  \"{col}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qwR7mUphT-T"
      },
      "outputs": [],
      "source": [
        "categorical_columns = [\n",
        "  \"fea_beam\",\n",
        "]\n",
        "\n",
        "dataset_imported_cat = dataset_imported_covar.copy()\n",
        "categorical_mappings = {}\n",
        "categorical_descriptive_params = []\n",
        "\n",
        "if len(categorical_columns) > 0:\n",
        "    for col in categorical_columns:\n",
        "        if col in dataset_imported_cat.columns:\n",
        "            # Create tar_ descriptive parameter with original values\n",
        "            tar_col = col.replace('fea_', 'tar_')\n",
        "            dataset_imported_cat[tar_col] = dataset_imported_cat[col].copy()\n",
        "            categorical_descriptive_params.append(tar_col)\n",
        "\n",
        "            # Convert all categorical features to ordered integers starting from 1\n",
        "            unique_values = sorted(dataset_imported_cat[col].unique())\n",
        "            value_to_int = {val: i+1 for i, val in enumerate(unique_values)}\n",
        "            dataset_imported_cat[col] = dataset_imported_cat[col].map(value_to_int).astype('category')\n",
        "            categorical_mappings[col] = value_to_int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlS-5A-Sbjhx"
      },
      "source": [
        "## Descriptive parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5C8gPptbosb"
      },
      "outputs": [],
      "source": [
        "# Check remaining target columns and select parameters that may be useful for descriptive statistics.\n",
        "print(\"descriptive_parameters = [\")\n",
        "for col in dataset_imported_cat.columns:\n",
        "  if col not in dataset_col_list and col not in categorical_descriptive_params:\n",
        "    print(f'\"{col}\",')\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh2viv9Jc86m"
      },
      "outputs": [],
      "source": [
        "descriptive_parameters = [\n",
        "\n",
        "]\n",
        "\n",
        "# Add categorical descriptive parameters right after target, before features\n",
        "if 'categorical_descriptive_params' in locals():\n",
        "    dataset_col_list = dataset_col_list + categorical_descriptive_params\n",
        "\n",
        "dataset_col_list = dataset_col_list + descriptive_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx6XO_2tAp0p"
      },
      "source": [
        "## Sample size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zc3e82LPjK9b"
      },
      "outputs": [],
      "source": [
        "# Print current number of rows (data points)\n",
        "print(f'Number of rows (data points): {len(dataset_imported_cat)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_7Kn103TEbl"
      },
      "outputs": [],
      "source": [
        "# Randomly sampling the rows of a dataset to generate a smaller subset can\n",
        "# for testing different parameters. Not intended for final models.\n",
        "\n",
        "sample_imported_dataset = False\n",
        "sample_imported_dataset_by_percent = False # If False then by number\n",
        "sample_imported_dataset_value = 500000 # Set to a percentage or number, or 'None' if not sampling\n",
        "\n",
        "# Global\n",
        "dataset_imported_sampled = dataset_imported_cat.copy()\n",
        "\n",
        "# Sample dataset for testing or HPO\n",
        "if sample_imported_dataset:\n",
        "  if sample_imported_dataset_by_percent:\n",
        "    dataset_imported_sampled = dataset_imported_sampled.sample(frac=sample_imported_dataset_value/100, random_state=1).reset_index().drop(columns=['index'])\n",
        "  else:\n",
        "    dataset_imported_sampled = dataset_imported_sampled.sample(n=sample_imported_dataset_value, random_state=1).reset_index().drop(columns=['index'])\n",
        "\n",
        "print(f'Number of rows (data points) after sampling: {len(dataset_imported_sampled)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXkO3lviA8QD"
      },
      "source": [
        "## Name and compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMev-lVjucJG"
      },
      "outputs": [],
      "source": [
        "# Set model dataset name (date and time suffix will be added) and sample percent\n",
        "dataset_name = \"agbd_alpha_earth\"\n",
        "# dataset_name = \"gedi_elevation\"\n",
        "\n",
        "# Rename ID and descriptive columns\n",
        "dataset_col_list = [item for item in dataset_col_list if item is not None]\n",
        "dataset_col_list.sort(key=lambda x: x.startswith('fea_'))\n",
        "dataset_final = dataset_imported_sampled.copy()[dataset_col_list]\n",
        "dataset_final.columns = [col.replace('fea_', '').replace('tar_', '') if col == id_column else col for col in dataset_final.columns]\n",
        "\n",
        "# Model dataset file and directory\n",
        "local_timezone = timezone(timedelta(hours=8))\n",
        "model_dataset_name = f\"{dataset_name}_{datetime.now(local_timezone).strftime('%y%m%d_%H%M%S')}\"\n",
        "model_dataset_dir = join(models_dir, model_dataset_name)\n",
        "model_dataset_path = join(model_dataset_dir, model_dataset_name)\n",
        "model_dataset_description_dir = join(model_dataset_dir, \"model_dataset_description.json\")\n",
        "\n",
        "# Check if identical model dataset exists\n",
        "dataset_exists = False\n",
        "for subdir, dirs, files in os.walk(models_dir):\n",
        "  for file in files:\n",
        "    if file.endswith('.pkl'):\n",
        "      dataset_existing = pd.read_pickle(join(subdir,file))\n",
        "      dataset_equals = pd.DataFrame.equals(dataset_existing, dataset_final)\n",
        "      if dataset_equals:\n",
        "        dataset_exists = True\n",
        "        model_dataset = dataset_existing\n",
        "        model_dataset_dir = subdir\n",
        "        print(f'An identical model dataset already exists in: {subdir}')\n",
        "        break\n",
        "  if dataset_exists:\n",
        "    break\n",
        "\n",
        "# Create a model dataset with selected parameters, if it does not exist\n",
        "if dataset_exists == False:\n",
        "  makedirs(model_dataset_dir, exist_ok=True)\n",
        "  dataset_final.to_pickle(f\"{model_dataset_path}.pkl\")\n",
        "  model_dataset = pd.read_pickle(f\"{model_dataset_path}.pkl\")\n",
        "\n",
        "  # Convert categorical_mappings to JSON-serializable format\n",
        "  json_categorical_mappings = {}\n",
        "  if 'categorical_mappings' in locals():\n",
        "      for col, mapping in categorical_mappings.items():\n",
        "          json_categorical_mappings[col] = {str(k): int(v) for k, v in mapping.items()}\n",
        "\n",
        "  model_dataset_description = {\n",
        "      \"model_dataset_name\": model_dataset_name,\n",
        "      \"number_of_columns\": len(model_dataset.columns),\n",
        "      \"number_of_rows\": len(model_dataset),\n",
        "      \"id_column\": id_column,\n",
        "      \"selected_target\": selected_target,\n",
        "      \"uncertainty\": uncertainty,\n",
        "      \"covariates_renamed\": covariates_renamed,\n",
        "      \"categorical_features_mappings\": json_categorical_mappings,\n",
        "      \"selected_features\": selected_features,\n",
        "      \"categorical_columns\": categorical_columns,\n",
        "      \"descriptive_parameters\": descriptive_parameters,\n",
        "      \"sample_imported_dataset\": sample_imported_dataset,\n",
        "      \"sample_imported_dataset_by_percent\": sample_imported_dataset_by_percent,\n",
        "      \"sample_imported_dataset_value\": sample_imported_dataset_value,\n",
        "  }\n",
        "  with open(model_dataset_description_dir, \"w\") as f:\n",
        "    f.write(json.dumps(model_dataset_description, indent=2))\n",
        "  print(f\"Model dataset compiled and exported to {model_dataset_dir}\")\n",
        "\n",
        "# Check dataset\n",
        "model_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lqjsLKZaQXo"
      },
      "source": [
        "# Select model dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SqkKHKnaV-8"
      },
      "outputs": [],
      "source": [
        "# Select an existing model dataset\n",
        "for subdir in os.listdir(models_dir):\n",
        "    print(f'selected_model_dataset = \"{subdir}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnerzp-gbapN"
      },
      "outputs": [],
      "source": [
        "selected_model_dataset = \"agbd_251203_161707\"\n",
        "\n",
        "# Select model dataset\n",
        "model_dataset_dir = join(models_dir,selected_model_dataset)\n",
        "\n",
        "# Read description for model dataset attributes\n",
        "with open(join(model_dataset_dir,\"model_dataset_description.json\")) as model_dataset_description_json:\n",
        "  model_dataset_description = json.load(model_dataset_description_json)\n",
        "model_dataset_name = model_dataset_description[\"model_dataset_name\"]\n",
        "number_of_columns = model_dataset_description[\"number_of_columns\"]\n",
        "number_of_rows = model_dataset_description[\"number_of_rows\"]\n",
        "id_column = model_dataset_description[\"id_column\"]\n",
        "selected_target = model_dataset_description[\"selected_target\"]\n",
        "uncertainty = model_dataset_description[\"uncertainty\"]\n",
        "covariates_renamed = model_dataset_description[\"covariates_renamed\"]\n",
        "selected_features = model_dataset_description[\"selected_features\"] + model_dataset_description[\"covariates_renamed\"]\n",
        "categorical_features_mappings = model_dataset_description[\"categorical_features_mappings\"]\n",
        "categorical_columns = model_dataset_description[\"categorical_columns\"]\n",
        "descriptive_parameters = model_dataset_description[\"descriptive_parameters\"]\n",
        "sample_imported_dataset = model_dataset_description[\"sample_imported_dataset\"]\n",
        "sample_imported_dataset_by_percent = model_dataset_description[\"sample_imported_dataset_by_percent\"]\n",
        "sample_imported_dataset_value = model_dataset_description[\"sample_imported_dataset_value\"]\n",
        "\n",
        "# Define model dataset description and .json directories\n",
        "model_description_dir = join(model_dataset_dir, \"model_description.json\")\n",
        "final_model_dir = join(model_dataset_dir, \"model.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u_wV7NAp1h-"
      },
      "source": [
        "# Model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnge5_H6FR3f"
      },
      "outputs": [],
      "source": [
        "# WIP categorise_target\n",
        "categorise_target = False\n",
        "\n",
        "# Load model dataset\n",
        "model_dataset = pd.read_pickle(join(f\"{model_dataset_dir}/{selected_model_dataset}.pkl\"))\n",
        "model_dataset = model_dataset.copy()\n",
        "\n",
        "if categorise_target:\n",
        "  # Create a copy to protect original\n",
        "  model_dataset_cat = model_dataset.copy()\n",
        "\n",
        "  # # Categorise per 50 Mg/ha\n",
        "  # model_dataset_cat[selected_target] = np.where(model_dataset_cat[selected_target] < 50, 0, model_dataset_cat[selected_target])\n",
        "  # model_dataset_cat[selected_target] = np.where((model_dataset_cat[selected_target] >= 50) & (model_dataset_cat[selected_target] < 100), 1, model_dataset_cat[selected_target])\n",
        "  # model_dataset_cat[selected_target] = np.where((model_dataset_cat[selected_target] >= 100) & (model_dataset_cat[selected_target] < 150), 2, model_dataset_cat[selected_target])\n",
        "  # model_dataset_cat[selected_target] = np.where((model_dataset_cat[selected_target] >= 150) & (model_dataset_cat[selected_target] < 200), 3, model_dataset_cat[selected_target])\n",
        "  # model_dataset_cat[selected_target] = np.where((model_dataset_cat[selected_target] >= 200) & (model_dataset_cat[selected_target] < 250), 4, model_dataset_cat[selected_target])\n",
        "  # model_dataset_cat[selected_target] = np.where((model_dataset_cat[selected_target] >= 250) & (model_dataset_cat[selected_target] < 300), 5, model_dataset_cat[selected_target])\n",
        "  # model_dataset_cat[selected_target] = np.where((model_dataset_cat[selected_target] >= 300) & (model_dataset_cat[selected_target] < 350), 6, model_dataset_cat[selected_target])\n",
        "  # model_dataset_cat[selected_target] = np.where((model_dataset_cat[selected_target] >= 350) & (model_dataset_cat[selected_target] < 400), 7, model_dataset_cat[selected_target])\n",
        "  # model_dataset_cat[selected_target] = np.where(model_dataset_cat[selected_target] >= 400, 8, model_dataset_cat[selected_target])\n",
        "\n",
        "  # # Categorise per 100 Mg/ha\n",
        "  # model_dataset_cat[selected_target] = np.where(model_dataset_cat[selected_target] < 100, 0, model_dataset_cat[selected_target])\n",
        "  # model_dataset_cat[selected_target] = np.where((model_dataset_cat[selected_target] >= 100) & (model_dataset_cat[selected_target] < 200), 1, model_dataset_cat[selected_target])\n",
        "  # model_dataset_cat[selected_target] = np.where((model_dataset_cat[selected_target] >= 200) & (model_dataset_cat[selected_target] < 300), 2, model_dataset_cat[selected_target])\n",
        "  # model_dataset_cat[selected_target] = np.where((model_dataset_cat[selected_target] >= 300) & (model_dataset_cat[selected_target] < 400), 3, model_dataset_cat[selected_target])\n",
        "  # model_dataset_cat[selected_target] = np.where(model_dataset_cat[selected_target] >= 400, 4, model_dataset_cat[selected_target])\n",
        "\n",
        "  # Categorise above and below 150 Mg/ha (High Carbon Stock)\n",
        "  model_dataset_cat[selected_target] = np.where(model_dataset_cat[selected_target] < 150, 0, model_dataset_cat[selected_target])\n",
        "  model_dataset_cat[selected_target] = np.where(model_dataset_cat[selected_target] >= 150, 1, model_dataset_cat[selected_target])\n",
        "\n",
        "  # # Categorise species presence\n",
        "  # model_dataset_cat[selected_target] = np.where(model_dataset_cat[selected_target] == 0, 0, model_dataset_cat[selected_target])\n",
        "  # model_dataset_cat[selected_target] = np.where(model_dataset_cat[selected_target] == 1, 1, model_dataset_cat[selected_target])\n",
        "\n",
        "  # Convert to integer for classification\n",
        "  model_dataset_cat[selected_target] = model_dataset_cat[selected_target].astype(int)\n",
        "\n",
        "  # Define categories (must be in order and start from 0)\n",
        "  unique_classes = model_dataset_cat[selected_target].nunique()\n",
        "  multiclass = unique_classes > 2\n",
        "\n",
        "  # Ensure multiclass labels are properly encoded from 0 to n-1\n",
        "  if multiclass:\n",
        "      unique_vals = sorted(model_dataset_cat[selected_target].unique())\n",
        "      if unique_vals != list(range(len(unique_vals))):\n",
        "          label_mapping = {val: i for i, val in enumerate(unique_vals)}\n",
        "          model_dataset_cat[selected_target] = model_dataset_cat[selected_target].map(label_mapping)\n",
        "          print(f\"Multiclass labels remapped: {dict(zip(range(len(unique_vals)), unique_vals))}\")\n",
        "\n",
        "  # Check for class imbalance in binary classification\n",
        "  if not multiclass:\n",
        "      class_counts = model_dataset_cat[selected_target].value_counts()\n",
        "      imbalance_ratio = class_counts.max() / class_counts.min()\n",
        "\n",
        "      if imbalance_ratio > 2:\n",
        "          print(f\"Class imbalance detected (ratio: {imbalance_ratio:.2f})\")\n",
        "          # Calculate scale_pos_weight for potential use\n",
        "          neg_count = class_counts[0] if 0 in class_counts else class_counts.min()\n",
        "          pos_count = class_counts[1] if 1 in class_counts else class_counts.max()\n",
        "          scale_pos_weight = neg_count / pos_count\n",
        "          print(f\"Consider scale_pos_weight: {scale_pos_weight:.2f}\")\n",
        "\n",
        "  # Set parameters\n",
        "  XGBPredictor = xgb.XGBClassifier\n",
        "  if multiclass:\n",
        "      objective = 'multi:softprob'\n",
        "      eval_metric = 'mlogloss'\n",
        "      num_class = unique_classes  # Required for multiclass in native API\n",
        "      metric = 'mlogloss'\n",
        "  else:\n",
        "      objective = 'binary:logistic'\n",
        "      eval_metric = 'logloss'\n",
        "      num_class = None\n",
        "      metric = 'logloss'\n",
        "  optimal_value = 'min'\n",
        "  model_dataset_cat[selected_target] = model_dataset_cat[selected_target].astype(\"category\")\n",
        "  model_dataset = model_dataset_cat\n",
        "\n",
        "else:\n",
        "  XGBPredictor = xgb.XGBRegressor\n",
        "  objective = 'reg:squarederror'\n",
        "  eval_metric = 'rmse'\n",
        "  metric = 'rmse'\n",
        "  optimal_value = 'min'\n",
        "\n",
        "# Define optimal values dictionary\n",
        "optimal_values = {\n",
        "    \"r2\": \"max\", \"me\": \"min\", \"rmse\": \"min\", \"rrmse\": \"min\",\n",
        "    \"accuracy\": \"max\", \"precision\": \"max\", \"recall\": \"max\", \"f1\": \"max\",\n",
        "    \"roc_auc\": \"max\", \"roc_auc_ovr\": \"max\", \"tss\": \"max\", \"mcc\": \"max\",\n",
        "    \"balanced_accuracy\": \"max\", \"average_precision\": \"max\", \"cohen_kappa\": \"max\",\n",
        "    \"specificity\": \"max\", \"logloss\": \"min\", \"mlogloss\": \"min\",\n",
        "    \"brier_score\": \"min\"\n",
        "}\n",
        "\n",
        "# Define x and y axes for training\n",
        "model_dataset_x = model_dataset[selected_features].copy()\n",
        "model_dataset_y = model_dataset[selected_target].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpwZRnAdKGeq"
      },
      "outputs": [],
      "source": [
        "# Select whether to use the weighted validation score for optimisation, mitigating overfitting.\n",
        "use_score_weighted = True\n",
        "\n",
        "# Create dictionary for optimal value and define the weighted score\n",
        "def def_score_weighted(mean_validation, mean_training, metric_name=None):\n",
        "    if metric_name is None:\n",
        "        metric_name = metric\n",
        "    opt = optimal_values[metric_name]\n",
        "    if opt == \"max\":\n",
        "        return mean_validation - 0.5*max(0, mean_training - mean_validation)\n",
        "    return mean_validation + 0.5*max(0, mean_validation - mean_training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1lV8U3eHhWG"
      },
      "outputs": [],
      "source": [
        "# Exclude specified columns\n",
        "columns_to_exclude = [\n",
        "                      # 'fea_sensitivity',\n",
        "                      # 'fea_latitude',\n",
        "                      # 'fea_longitude'\n",
        "                      ]\n",
        "columns_to_consider = model_dataset_x.columns.drop(columns_to_exclude)\n",
        "\n",
        "# Calculate max_bin hyperparameter based on the feature with the most unique values\n",
        "max_unique_col = model_dataset_x[columns_to_consider].nunique().idxmax()\n",
        "max_bin = model_dataset_x[columns_to_consider].nunique().max()\n",
        "print(f\"{max_unique_col} had the highest number of unique values, max_bin set to {max_bin}\")\n",
        "\n",
        "# Alternatively, set manually by uncommenting:\n",
        "# max_bin = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfGcGNwHLJF2"
      },
      "outputs": [],
      "source": [
        "# Default HPs are useful for quick testing. Modify the baseline HPs,\n",
        "# and / or run hyperparameter optimisation for better results.\n",
        "use_default_hp = True\n",
        "\n",
        "# Define baseline hyperparameters\n",
        "baseline_hyperparameters = {\n",
        " 'tree_method': 'hist',\n",
        " 'device': 'cuda',\n",
        " 'enable_categorical': True,\n",
        " 'max_bin': max_bin,\n",
        " 'n_estimators': 100000, # Will be limited by early stopping\n",
        " 'eta': 0.01,\n",
        " 'early_stopping_rounds': 8,\n",
        " 'min_child_weight': 71,\n",
        " 'gamma': 0.175,\n",
        " 'alpha': 25230,\n",
        " 'lambda': 37,\n",
        " 'colsample_bytree': 0.923,\n",
        " 'colsample_bylevel': 0.83,\n",
        " 'colsample_bynode': 0.965\n",
        "}\n",
        "\n",
        "default_hyperparameters = {\n",
        "  \"tree_method\": \"hist\",\n",
        "  \"device\": 'cuda',\n",
        "  'enable_categorical': True,\n",
        "  'max_bin': max_bin,\n",
        "  \"objective\": objective,\n",
        "  \"eval_metric\": eval_metric,\n",
        "  \"n_estimators\": 100_000, # Will be limited by early stopping\n",
        "  \"early_stopping_rounds\": 10,\n",
        "}\n",
        "\n",
        "if use_default_hp: baseline_hyperparameters = default_hyperparameters\n",
        "else: # Ensure objective and eval_metric are set even when using custom baseline\n",
        "    if 'objective' not in baseline_hyperparameters: baseline_hyperparameters['objective'] = objective\n",
        "    if 'eval_metric' not in baseline_hyperparameters: baseline_hyperparameters['eval_metric'] = eval_metric\n",
        "\n",
        "# Add num_class for multiclass classification\n",
        "if categorise_target and multiclass:\n",
        "    baseline_hyperparameters[\"num_class\"] = num_class\n",
        "    default_hyperparameters[\"num_class\"] = num_class\n",
        "\n",
        "# Avoids issues using dataframe from CPU\n",
        "xgb.set_config(verbosity=0, use_rmm=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3gwpt7LfJGj"
      },
      "source": [
        "# KFold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2BaRo0o83TC"
      },
      "outputs": [],
      "source": [
        "# The number of k-folds affects the proportion of data used for training,\n",
        "# with the complementary proportion used for validation / testing.\n",
        "# 10 splits = 90 % training, 5 = 80 %, 4 = 75 %, 3 = 67 %, 2 = 50 %.\n",
        "\n",
        "# Input the number of k-folds to generate train/valid splits.\n",
        "n_splits = 10\n",
        "\n",
        "# Number of folds to be used for HPO validation.\n",
        "# Remaining folds will be used for final testing.\n",
        "n_hpo_splits = 2\n",
        "\n",
        "assert n_hpo_splits < n_splits, \"n_hpo_splits must be less than n_splits\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wir622aUtK5R"
      },
      "outputs": [],
      "source": [
        "# Use stratified k-fold for classification to maintain class distribution\n",
        "if categorise_target: kf_split = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1).split(model_dataset_x, model_dataset_y))\n",
        "else: kf_split = list(KFold(n_splits=n_splits, shuffle=True, random_state=1).split(model_dataset_x, model_dataset_y))\n",
        "\n",
        "# Return a dictionary full of metrics for calculating accuracy\n",
        "def calculate_metrics(validation, prediction, prediction_proba=None):\n",
        "    if not categorise_target:\n",
        "        # Regression metrics\n",
        "        validation = validation.astype(\"float32\")\n",
        "        prediction = prediction.astype(\"float32\")\n",
        "        metrics = {\n",
        "            \"r2\": r2_score(validation, prediction),\n",
        "            \"me\": np.mean(np.array(validation).squeeze() - np.array(prediction).squeeze()),\n",
        "            \"rmse\": root_mean_squared_error(validation, prediction),\n",
        "            \"rrmse\": (root_mean_squared_error(validation, prediction) / np.mean(validation)) * 100 if np.mean(validation) != 0 else np.nan,\n",
        "            \"optimal_value\": optimal_values\n",
        "        }\n",
        "    else:\n",
        "        # Classification metrics\n",
        "        validation = validation.astype(\"int32\")\n",
        "        prediction = prediction.astype(\"int32\")\n",
        "        metrics = {\n",
        "            \"accuracy\": accuracy_score(validation, prediction),\n",
        "            \"precision\": precision_score(validation, prediction, average='weighted', zero_division=0),\n",
        "            \"recall\": recall_score(validation, prediction, average='weighted', zero_division=0),\n",
        "            \"f1\": f1_score(validation, prediction, average='weighted', zero_division=0),\n",
        "            \"balanced_accuracy\": balanced_accuracy_score(validation, prediction),\n",
        "            \"mcc\": matthews_corrcoef(validation, prediction),\n",
        "            \"cohen_kappa\": cohen_kappa_score(validation, prediction)\n",
        "        }\n",
        "        # AUC and probability-based metrics\n",
        "        if multiclass:\n",
        "            metrics[\"roc_auc\"] = roc_auc_score(validation, prediction_proba, multi_class='ovr', average='weighted')\n",
        "            metrics[\"average_precision\"] = average_precision_score(validation, prediction_proba, average='weighted')\n",
        "        else:\n",
        "            metrics[\"roc_auc\"] = roc_auc_score(validation, prediction_proba)\n",
        "            metrics[\"average_precision\"] = average_precision_score(validation, prediction_proba)\n",
        "        # Calculate specificity and TSS\n",
        "        if multiclass:\n",
        "            # Macro-averaged specificity and TSS for multiclass\n",
        "            cm = confusion_matrix(validation, prediction)\n",
        "            specificities = []\n",
        "            sensitivities = []\n",
        "            for i in range(len(cm)):\n",
        "                tn = np.sum(cm) - (np.sum(cm[i, :]) + np.sum(cm[:, i]) - cm[i, i])\n",
        "                fp = np.sum(cm[:, i]) - cm[i, i]\n",
        "                fn = np.sum(cm[i, :]) - cm[i, i]\n",
        "                tp = cm[i, i]\n",
        "                specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "                sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                specificities.append(specificity)\n",
        "                sensitivities.append(sensitivity)\n",
        "            metrics[\"specificity\"] = np.mean(specificities)\n",
        "            metrics[\"tss\"] = np.mean([sens + spec - 1 for sens, spec in zip(sensitivities, specificities)])\n",
        "        else:\n",
        "            # Binary classification\n",
        "            cm = confusion_matrix(validation, prediction)\n",
        "            tn, fp, fn, tp = cm.ravel()\n",
        "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            metrics[\"specificity\"] = specificity\n",
        "            metrics[\"tss\"] = sensitivity + specificity - 1\n",
        "            metrics[\"brier_score\"] = brier_score_loss(validation, prediction_proba)\n",
        "        # Add logloss\n",
        "        if multiclass: metrics[\"mlogloss\"] = log_loss(validation, prediction_proba)\n",
        "        else: metrics[\"logloss\"] = log_loss(validation, prediction_proba)\n",
        "        metrics[\"optimal_value\"] = optimal_values\n",
        "    return metrics\n",
        "\n",
        "def run_kfold(model_dataset, kf_split, predictor, verbose=False, export_test_model=False, test_model_dir=None):\n",
        "    metrics = []\n",
        "\n",
        "    # Junk data is used to determine column names by seeing what metrics get calculated\n",
        "    if categorise_target and multiclass:\n",
        "        # Use at least as many samples as classes to ensure all classes are represented\n",
        "        n_junk_samples = max(6, unique_classes)\n",
        "        junk_data = pd.Series([i % unique_classes for i in range(n_junk_samples)])\n",
        "        # Create junk probabilities with correct shape (n_samples, num_classes)\n",
        "        junk_proba = np.full((n_junk_samples, unique_classes), 1.0/unique_classes)\n",
        "    else: # Binary classification or regression\n",
        "        junk_data = pd.Series([0, 1, 0, 1, 0, 1])\n",
        "        junk_proba = np.array([0.5] * 6) if categorise_target else None\n",
        "    junk_metrics = calculate_metrics(junk_data, junk_data, junk_proba if categorise_target else None)\n",
        "\n",
        "    cols = []\n",
        "    for m in [k for k in junk_metrics.keys() if k != 'optimal_value']:\n",
        "        cols += [f\"score_training ({m})\", f\"score_validation ({m})\", f\"score_difference ({m})\", f\"score_weighted ({m})\"]\n",
        "    cols.append(\"n_estimators\")\n",
        "\n",
        "    fold_index = 1\n",
        "    for train_index, valid_index in kf_split:\n",
        "        x_train = model_dataset.loc[train_index][selected_features]\n",
        "        x_valid = model_dataset.loc[valid_index][selected_features]\n",
        "        y_train = model_dataset.loc[train_index][selected_target]\n",
        "        y_valid = model_dataset.loc[valid_index][selected_target]\n",
        "\n",
        "        dtrain = xgb.DMatrix(x_train, y_train, enable_categorical=True)\n",
        "        dvalid = xgb.DMatrix(x_valid, y_valid, enable_categorical=True)\n",
        "\n",
        "        params = predictor.get_params()\n",
        "        params['eval_metric'] = eval_metric\n",
        "        if categorise_target and multiclass: params['num_class'] = num_class\n",
        "\n",
        "        # Sklearn wrapper params handled separately: n_estimators via num_boost_round,\n",
        "        # Early_stopping_rounds via callback, enable_categorical via DMatrix\n",
        "        for key in ['n_estimators', 'enable_categorical', 'missing', 'early_stopping_rounds']:\n",
        "            params.pop(key, None)\n",
        "\n",
        "        early_stopping = xgb.callback.EarlyStopping(\n",
        "            rounds=predictor.early_stopping_rounds,\n",
        "            metric_name=eval_metric,\n",
        "            data_name='validation_1'\n",
        "        )\n",
        "\n",
        "        evals_result = {}\n",
        "        model = xgb.train(params,\n",
        "                          dtrain,\n",
        "                          num_boost_round=predictor.n_estimators,\n",
        "                          evals=[(dtrain, 'training'), (dvalid, 'validation_1')],\n",
        "                          callbacks=[early_stopping],\n",
        "                          evals_result=evals_result,\n",
        "                          verbose_eval=False)\n",
        "\n",
        "        # Get predictions\n",
        "        training_prediction_proba = model.predict(dtrain)\n",
        "        validation_prediction_proba = model.predict(dvalid)\n",
        "\n",
        "        if categorise_target:\n",
        "            # Convert probabilities to class predictions\n",
        "            if multiclass:\n",
        "                training_prediction = np.argmax(training_prediction_proba, axis=1)\n",
        "                validation_prediction = np.argmax(validation_prediction_proba, axis=1)\n",
        "            else:\n",
        "                training_prediction = np.round(training_prediction_proba).astype(int)\n",
        "                validation_prediction = np.round(validation_prediction_proba).astype(int)\n",
        "\n",
        "            training_metrics = calculate_metrics(y_train, training_prediction, training_prediction_proba)\n",
        "            validation_metrics = calculate_metrics(y_valid, validation_prediction, validation_prediction_proba)\n",
        "        else:\n",
        "            training_metrics = calculate_metrics(y_train, training_prediction_proba)\n",
        "            validation_metrics = calculate_metrics(y_valid, validation_prediction_proba)\n",
        "\n",
        "        # Build metrics row\n",
        "        metrics_row = []\n",
        "        for m in [k for k in training_metrics.keys() if k != 'optimal_value']:\n",
        "            score_training = training_metrics[m]\n",
        "            score_validation = validation_metrics[m]\n",
        "            score_difference = score_training - score_validation\n",
        "            score_weighted = def_score_weighted(score_validation, score_training, m)\n",
        "            metrics_row += [score_training, score_validation, score_difference, score_weighted]\n",
        "\n",
        "        metrics_row.append(len(evals_result['validation_1'][eval_metric]))\n",
        "        metrics.append(metrics_row)\n",
        "\n",
        "        if export_test_model:\n",
        "            makedirs(test_model_dir, exist_ok=True)\n",
        "            model_name = f\"model_test_fold_{fold_index}\"\n",
        "            metrics_dir = join(test_model_dir, f\"{model_name}_metrics.csv\")\n",
        "            model.save_model(join(test_model_dir, f\"{model_name}.json\"))\n",
        "            df_fold_metrics = pd.DataFrame(metrics, columns=cols)\n",
        "            df_fold_metrics.to_csv(metrics_dir)\n",
        "            print(f\"Model export complete for test fold {fold_index}.\")\n",
        "\n",
        "        if verbose:\n",
        "            score_training = training_metrics[metric]\n",
        "            score_validation = validation_metrics[metric]\n",
        "            score_difference = score_training - score_validation\n",
        "            score_weighted = def_score_weighted(score_validation, score_training)\n",
        "            print(f\"Validation {metric}: {score_validation}\")\n",
        "            print(f\"Training {metric}: {score_training}\")\n",
        "            print(f\"{metric} difference: {score_difference}\")\n",
        "            print(f\"Weighted {metric}: {score_weighted}\")\n",
        "            print(f\"--------------------\")\n",
        "\n",
        "        fold_index += 1\n",
        "\n",
        "    df_kfold_metrics = pd.DataFrame(metrics, columns=cols)\n",
        "    return df_kfold_metrics\n",
        "\n",
        "def generate_statistics(df_kfold_metrics):\n",
        "    mean_training = df_kfold_metrics[f\"score_training ({metric})\"].mean()\n",
        "    std_training = df_kfold_metrics[f\"score_training ({metric})\"].std()\n",
        "    mean_validation = df_kfold_metrics[f\"score_validation ({metric})\"].mean()\n",
        "    std_validation = df_kfold_metrics[f\"score_validation ({metric})\"].std()\n",
        "    mean_difference = df_kfold_metrics[f\"score_difference ({metric})\"].mean()\n",
        "    std_difference = df_kfold_metrics[f\"score_difference ({metric})\"].std()\n",
        "    score_weighted = def_score_weighted(mean_validation, mean_training)\n",
        "    mean_weighted = df_kfold_metrics[f\"score_weighted ({metric})\"].mean()\n",
        "    std_weighted = df_kfold_metrics[f\"score_weighted ({metric})\"].std()\n",
        "\n",
        "    print(\"----------Results Summary----------\")\n",
        "    print(f\"Training {metric} mean: {mean_training}\")\n",
        "    print(f\"Training {metric} std: {std_training}\")\n",
        "    print(f\"Validation {metric} mean: {mean_validation}\")\n",
        "    print(f\"Validation {metric} std: {std_validation}\")\n",
        "    print(f\"{metric} difference mean: {mean_difference}\")\n",
        "    print(f\"{metric} difference std: {std_difference}\")\n",
        "    print(f\"Weighted {metric} score: {score_weighted}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDMPTLWi9d7X"
      },
      "source": [
        "# Default model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnmXF0CaA7Rx"
      },
      "outputs": [],
      "source": [
        "# Baseline for HPO folds\n",
        "default_hpo_predictor = XGBPredictor(**default_hyperparameters)\n",
        "default_hpo = run_kfold(model_dataset, kf_split[:n_hpo_splits], default_hpo_predictor, verbose=True)\n",
        "generate_statistics(default_hpo)\n",
        "\n",
        "# Export default_hpo_descr.json\n",
        "default_hpo_descr_dir = join(model_dataset_dir, \"default_hpo_descr.json\")\n",
        "default_hpo_descr = {\n",
        "  \"selected_target\": selected_target,\n",
        "  \"hyperparameters\": str(default_hyperparameters),\n",
        "  \"metric_used_for_training\": metric\n",
        "}\n",
        "\n",
        "for col in default_hpo.columns:\n",
        "  default_hpo_descr[f\"avg_{col}\"] = float(default_hpo[col].mean())\n",
        "  default_hpo_descr[f\"std_{col}\"] = float(default_hpo[col].std())\n",
        "with open(default_hpo_descr_dir, \"w\") as file:\n",
        "  file.write(json.dumps(default_hpo_descr, indent=2))\n",
        "print(\"default_hpo_descr.json generation and export complete\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub7FlaEDM-lC"
      },
      "outputs": [],
      "source": [
        "# Default for final testing folds\n",
        "default_test_predictor = XGBPredictor(**default_hyperparameters)\n",
        "default_test = run_kfold(model_dataset, kf_split[n_hpo_splits:], default_test_predictor, verbose=True)\n",
        "generate_statistics(default_test)\n",
        "\n",
        "# Export default_test_descr.json\n",
        "default_test_descr_dir = join(model_dataset_dir, \"default_test_descr.json\")\n",
        "default_test_descr = {\n",
        "  \"selected_target\": selected_target,\n",
        "  \"hyperparameters\": str(default_hyperparameters),\n",
        "  \"metric_used_for_training\": metric\n",
        "}\n",
        "\n",
        "for col in default_test.columns:\n",
        "  default_test_descr[f\"avg_{col}\"] = float(default_test[col].mean())\n",
        "  default_test_descr[f\"std_{col}\"] = float(default_test[col].std())\n",
        "with open(default_test_descr_dir, \"w\") as file:\n",
        "  file.write(json.dumps(default_test_descr, indent=2))\n",
        "print(\"default_test_descr.json generation and export complete\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_n6YOnRQNa1"
      },
      "source": [
        "# Hyperparameter optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP1fzhFk7kVl"
      },
      "source": [
        "## Random search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJwPFIfH7jkU"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters and ranges to randomly sample.\n",
        "# If uniform or loguniform, subtract the lower range from the upper range.\n",
        "\n",
        "# Latin Hypercube Sampling provides more uniform coverage of the parameter space\n",
        "# than completely random sampling. Each HP range is divided into n equal strata\n",
        "# (where n = iterations), with exactly one sample per stratum. The strata are\n",
        "# shuffled independently per HP, so combinations remain random while avoiding gaps in coverage.\n",
        "use_lhs = True\n",
        "lhs_seed = 1  # Set to None for non-reproducible sampling\n",
        "\n",
        "# Using a random seed makes the results less variable\n",
        "use_random_seed = True\n",
        "\n",
        "hp_distribution = {\n",
        "    # \"max_bin\": randint(1000, 2000 + 1),\n",
        "    \"early_stopping_rounds\": randint(8, 15 + 1),\n",
        "    # \"eta\": uniform(0.01, 0.3 - 0.01),\n",
        "    \"min_child_weight\": randint(10, 50 + 1),\n",
        "    \"gamma\": uniform(0.0, 0.2 - 0.0),\n",
        "    \"alpha\": uniform(10000, 40000 - 10000),\n",
        "    \"lambda\": uniform(1, 500 - 1),\n",
        "    \"colsample_bytree\": uniform(0.6, 1 - 0.6),\n",
        "    \"colsample_bylevel\": uniform(0.9, 1 - 0.9),\n",
        "    \"colsample_bynode\": uniform(0.9, 1 - 0.9),\n",
        "}\n",
        "\n",
        "rs_hyperparameters = baseline_hyperparameters.copy()\n",
        "if use_random_seed: rs_hyperparameters[\"random_state\"] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua8gUmEWlW9_"
      },
      "outputs": [],
      "source": [
        "# Select how many iterations\n",
        "iterations = 10\n",
        "\n",
        "# Define directories and paths\n",
        "hpo_random_search_dir = join(model_dataset_dir, 'hpo_random_search')\n",
        "hpo_random_search_results_path = join(hpo_random_search_dir, 'iteration_results')\n",
        "hpo_random_search_best_hyperparameters_dir = join(hpo_random_search_dir, \"best_hyperparameters.csv\")\n",
        "Path(hpo_random_search_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Initialise variables\n",
        "hpo_random_search_results = pd.DataFrame()\n",
        "score_col = \"score_weighted\" if use_score_weighted else \"mean_validation\"\n",
        "\n",
        "# Load existing results if available\n",
        "if exists(f\"{hpo_random_search_results_path}.pkl\"):\n",
        "  print(\"Reading results of the existing trials records...\")\n",
        "  hpo_random_search_results = pd.read_pickle(f\"{hpo_random_search_results_path}.pkl\")\n",
        "  print(f\"{len(hpo_random_search_results)} records read.\")\n",
        "\n",
        "# Generate LHS samples if enabled\n",
        "if use_lhs:\n",
        "  n_completed = len(hpo_random_search_results)\n",
        "  n_remaining = iterations - n_completed\n",
        "\n",
        "  if n_remaining > 0:\n",
        "    if lhs_seed is not None:\n",
        "      # Deterministic: regenerate identical samples, skip completed ones\n",
        "      lhs_sampler = LatinHypercube(d=len(hp_distribution), seed=lhs_seed)\n",
        "      lhs_samples_all = lhs_sampler.random(n=iterations)\n",
        "      lhs_samples = lhs_samples_all[n_completed:]\n",
        "    else:\n",
        "      # Non-deterministic: generate fresh samples for remaining iterations only\n",
        "      lhs_sampler = LatinHypercube(d=len(hp_distribution), seed=None)\n",
        "      lhs_samples = lhs_sampler.random(n=n_remaining)\n",
        "\n",
        "    hp_names = list(hp_distribution.keys())\n",
        "\n",
        "# Iteratively train models with sampled hyperparameters, saving the values and accuracy scores\n",
        "current_iteration = len(hpo_random_search_results)\n",
        "while current_iteration < iterations:\n",
        "  t_start = datetime.now()\n",
        "\n",
        "  # Sample HP values using LHS or random sampling\n",
        "  if use_lhs:\n",
        "    lhs_idx = current_iteration - (iterations - len(lhs_samples))\n",
        "    hp_sample = {}\n",
        "    for i, (hp_name, dist) in enumerate(hp_distribution.items()):\n",
        "      low, scale = dist.args[0], dist.args[1]\n",
        "      scaled_value = low + lhs_samples[lhs_idx, i] * scale\n",
        "      # Check if distribution is integer type\n",
        "      if hasattr(dist.dist, 'name') and dist.dist.name == 'randint':\n",
        "        scaled_value = int(np.floor(scaled_value))\n",
        "      hp_sample[hp_name] = scaled_value\n",
        "  else:\n",
        "    hp_sample_random_state = max(1, current_iteration + 1)\n",
        "    hp_sample = {k: v.rvs(random_state=hp_sample_random_state * (i + 1000000))\n",
        "                 for i, (k, v) in enumerate(hp_distribution.items())}\n",
        "\n",
        "  print(f\"Trying parameters: {hp_sample}\")\n",
        "\n",
        "  rs_hyperparameters_current = dict(rs_hyperparameters.copy())\n",
        "  rs_hyperparameters_current.update(hp_sample)\n",
        "  # Ensure num_class is available for multiclass\n",
        "  if categorise_target and multiclass: rs_hyperparameters_current[\"num_class\"] = num_class\n",
        "  rs_predictor = XGBPredictor(**rs_hyperparameters_current)\n",
        "\n",
        "  df_kfold_metrics = run_kfold(model_dataset, kf_split[:n_hpo_splits], rs_predictor, verbose=False)\n",
        "\n",
        "  # Generate statistics\n",
        "  mean_training = df_kfold_metrics[f\"score_training ({metric})\"].mean()\n",
        "  mean_validation = df_kfold_metrics[f\"score_validation ({metric})\"].mean()\n",
        "  mean_difference = df_kfold_metrics[f\"score_difference ({metric})\"].mean()\n",
        "  score_weighted = def_score_weighted(mean_validation, mean_training)\n",
        "  time_taken = str(datetime.now() - t_start)\n",
        "\n",
        "  # Compile iteration results\n",
        "  interim_rs_results = {\n",
        "      \"mean_validation\": mean_validation,\n",
        "      \"mean_training\": mean_training,\n",
        "      \"mean_difference\": mean_difference,\n",
        "      \"score_weighted\": score_weighted,\n",
        "      \"time_taken\": time_taken,\n",
        "      \"hyperparameters\": rs_hyperparameters_current,\n",
        "  }\n",
        "  # Add individual HP values for easier analysis\n",
        "  interim_rs_results.update(hp_sample)\n",
        "\n",
        "  print(f\"Trial completed in {time_taken}.\")\n",
        "  print(f\"Mean validation {metric}: {mean_validation:.4f}, Mean training {metric}: {mean_training:.4f}, Mean {metric} difference: {mean_difference:.4f}, Weighted score: {score_weighted:.4f}\")\n",
        "\n",
        "  # Save results\n",
        "  if len(hpo_random_search_results) == 0:\n",
        "    hpo_random_search_results = pd.DataFrame([interim_rs_results])\n",
        "  else:\n",
        "    hpo_random_search_results = pd.concat([hpo_random_search_results, pd.DataFrame([interim_rs_results])], ignore_index=True)\n",
        "  hpo_random_search_results.to_pickle(f\"{hpo_random_search_results_path}.pkl\")\n",
        "  hpo_random_search_results.to_csv(f\"{hpo_random_search_results_path}.csv\")\n",
        "\n",
        "  # Export best hyperparameters\n",
        "  if optimal_value == \"min\":\n",
        "    best_result_id = hpo_random_search_results[score_col].idxmin()\n",
        "  else:\n",
        "    best_result_id = hpo_random_search_results[score_col].idxmax()\n",
        "\n",
        "  best_rs_hyperparameters = hpo_random_search_results.loc[best_result_id][\"hyperparameters\"].copy()\n",
        "  export_rs_hyperparameters = best_rs_hyperparameters\n",
        "  export_rs_hyperparameters[score_col] = hpo_random_search_results.loc[best_result_id][score_col]\n",
        "  pd.DataFrame(export_rs_hyperparameters, index=[0]).to_csv(hpo_random_search_best_hyperparameters_dir)\n",
        "\n",
        "  # Update iteration index\n",
        "  current_iteration += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYrmnF-Unw2_"
      },
      "source": [
        "### View RS plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee7900TymWbE"
      },
      "outputs": [],
      "source": [
        "# View plots for all Random Search results\n",
        "\n",
        "# Load results if not already loaded\n",
        "if 'hpo_random_search_results' not in dir() or hpo_random_search_results.empty:\n",
        "  hpo_random_search_results = pd.read_pickle(f\"{hpo_random_search_results_path}.pkl\")\n",
        "\n",
        "# Extract hyperparameters that were optimised (vary across iterations)\n",
        "hp_expanded = pd.json_normalize(hpo_random_search_results['hyperparameters'])\n",
        "optimised_hps = [col for col in hp_expanded.columns if hp_expanded[col].nunique() > 1]\n",
        "\n",
        "# Filter to only optimised hyperparameters\n",
        "hpo_x = hp_expanded[optimised_hps]\n",
        "\n",
        "# Score optimised in Random Search\n",
        "hpo_y = hpo_random_search_results[['score_weighted']]\n",
        "\n",
        "# Filter warning \"ntree_limit is deprecated, use `iteration_range` or model slicing instead.\"\n",
        "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
        "\n",
        "# Predictor for SHAP interpretation\n",
        "hpo_predictor = xgb.XGBRegressor()\n",
        "hpo_predictor.fit(hpo_x, hpo_y)\n",
        "\n",
        "# SHAP explainer\n",
        "hpo_explainer = shap.Explainer(hpo_predictor)\n",
        "hpo_shap_values = hpo_explainer(hpo_x)\n",
        "shap.plots.beeswarm(hpo_shap_values, plot_size=(20, max(8, len(optimised_hps) * 0.8)))\n",
        "\n",
        "print(f\"Optimised hyperparameters: {optimised_hps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfcXwAf1VwW4"
      },
      "outputs": [],
      "source": [
        "# Individual scatter plots for each optimised hyperparameter\n",
        "for hp_name in optimised_hps:\n",
        "  shap.plots.scatter(hpo_shap_values[:, hp_name], color=hpo_shap_values[:, hp_name].data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFL7wRxTOIww"
      },
      "source": [
        "## Automated Random Search Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RbhiObaJpml"
      },
      "outputs": [],
      "source": [
        "# Define directories and paths\n",
        "hpo_arse_dir = join(model_dataset_dir, \"hpo_arse\")\n",
        "hpo_arse_optimisation_plots_dir = join(hpo_arse_dir, \"optimisation_plots\")\n",
        "hpo_arse_optimisation_results_dir = join(hpo_arse_dir, \"optimisation_results.pkl\")\n",
        "hpo_arse_iteration_results_dir = join(hpo_arse_dir, \"iteration_results.pkl\")\n",
        "hpo_arse_best_hyperparameters_dir = join(hpo_arse_dir, \"best_hyperparameters.csv\")\n",
        "hpo_arse_cache_shap_dir = join(hpo_arse_dir, \"cache_shap.pkl\")\n",
        "hpo_arse_line_graph_dir = join(hpo_arse_dir, \"optimisation_results.png\")\n",
        "Path(hpo_arse_dir).mkdir(parents=True, exist_ok=True)\n",
        "Path(hpo_arse_optimisation_plots_dir).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebcysq5wKeNE"
      },
      "outputs": [],
      "source": [
        "# Each optimisation an XGBoost regression model and SHAP values are used to determine\n",
        "# the hyperparameter with the largest effect on accuracy. This is then narrowed to a\n",
        "# more optimal distribution based on SHAP effect direction. If the new distribution\n",
        "# touches the upper or lower bounds of that tested then those bounds are shifted.\n",
        "# If the mean score of an optimisation is less than the previous, the 'conservative'\n",
        "# option will retry the former HP range rather than moving to the next round.\n",
        "\n",
        "# Latin Hypercube Sampling provides more uniform coverage of the parameter space\n",
        "# than completely random sampling. Each HP range is divided into n equal strata\n",
        "# (where n = iterations), with exactly one sample per stratum. The strata are\n",
        "# shuffled independently per HP, so combinations remain random while avoiding gaps in coverage.\n",
        "use_lhs = True\n",
        "lhs_seed = 1 # Set to None for non-reproducible sampling\n",
        "\n",
        "# Using a random seed makes the results less variable, and HPO might not function well\n",
        "use_random_seed = False\n",
        "\n",
        "arse_hyperparameters = baseline_hyperparameters.copy()\n",
        "if use_random_seed: arse_hyperparameters[\"random_state\"] = 1\n",
        "\n",
        "# Set static ARSE hyperparameters here\n",
        "# If no range is set (next) then HPO will use these\n",
        "# arse_hyperparameters[\"max_bin\"] = max_bin\n",
        "# arse_hyperparameters[\"early_stopping_rounds\"] = 8\n",
        "# arse_hyperparameters[\"eta\"] = 0.01\n",
        "# arse_hyperparameters[\"max_depth\"] = 6\n",
        "# arse_hyperparameters[\"min_child_weight\"] = 71\n",
        "# arse_hyperparameters[\"gamma\"] = 0.175\n",
        "# arse_hyperparameters[\"alpha\"] = 25230\n",
        "# arse_hyperparameters[\"lambda\"] = 37\n",
        "# arse_hyperparameters[\"colsample_bytree\"] = 0.923\n",
        "# arse_hyperparameters[\"colsample_bylevel\"] = 0.83\n",
        "# arse_hyperparameters[\"colsample_bynode\"] = 0.965\n",
        "\n",
        "# Modify HP ranges to sample here\n",
        "# Comment out a HP to use the baseline or static HP\n",
        "hp_distribution = {\n",
        "    # \"max_bin\": randint(1500, 2500 + 1),\n",
        "    \"early_stopping_rounds\": randint(3, 20 + 1),\n",
        "    \"eta\": uniform(0.01, 0.1 - 0.01),\n",
        "    # \"max_depth\": randint( 6, 6 + 1),\n",
        "    \"min_child_weight\": randint( 1, 50 + 1),\n",
        "    \"gamma\": uniform(0.0, 0.1 - 0.0),\n",
        "    \"alpha\": uniform(0, 10000 - 0),\n",
        "    \"lambda\": uniform(1, 10000 - 1),\n",
        "    \"colsample_bytree\": uniform(0.8, 1.0 - 0.8),\n",
        "    \"colsample_bylevel\": uniform(0.8, 1.0 - 0.8),\n",
        "    \"colsample_bynode\": uniform(0.8, 1.0 - 0.8),\n",
        "}\n",
        "\n",
        "df_hp_types = pd.DataFrame(columns=[\"type\", \"min\", \"max\"])\n",
        "df_hp_types.index.name = \"hyperparameter\"\n",
        "df_hp_types.loc[\"max_bin\"]                = [\"randint\", 1, np.inf]\n",
        "df_hp_types.loc[\"early_stopping_rounds\"]  = [\"randint\", 1, np.inf]\n",
        "df_hp_types.loc[\"eta\"]          = [\"uniform\", 0.01, 1.0]\n",
        "df_hp_types.loc[\"max_depth\"]  = [\"randint\", 1, 15]\n",
        "df_hp_types.loc[\"min_child_weight\"]       = [\"randint\", 0, np.inf]\n",
        "df_hp_types.loc[\"gamma\"]                  = [\"uniform\", 0.0, np.inf]\n",
        "df_hp_types.loc[\"alpha\"]              = [\"uniform\", 0.0, np.inf]\n",
        "df_hp_types.loc[\"lambda\"]             = [\"uniform\", 1.0, np.inf]\n",
        "df_hp_types.loc[\"colsample_bytree\"]       = [\"uniform\", 0.01, 1.0]\n",
        "df_hp_types.loc[\"colsample_bylevel\"]      = [\"uniform\", 0.01, 1.0]\n",
        "df_hp_types.loc[\"colsample_bynode\"]       = [\"uniform\", 0.01, 1.0]\n",
        "\n",
        "function_map = {\n",
        "    \"uniform\": uniform,\n",
        "    \"randint\": randint,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGbyj1a5RApN"
      },
      "outputs": [],
      "source": [
        "iterations_per_optimisation = 20\n",
        "total_optimisations = 20\n",
        "bounds_correction_percent = 50 # Shifts the HP sample range to this % of this sample range if it is at the upper or lower extremes\n",
        "conservative = False\n",
        "\n",
        "assert iterations_per_optimisation != len(hp_distribution), 'The number of iterations cannot be the same as the number of hyperparameters sampled (a known issue with shap)'\n",
        "\n",
        "# Filter warning \"ntree_limit is deprecated, use `iteration_range` or model slicing instead.\"\n",
        "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
        "\n",
        "# Initialise variables and directories\n",
        "shap_data = []\n",
        "score_col = \"score_weighted\" if use_score_weighted else \"mean_validation\"\n",
        "\n",
        "display_cols = [f\"best score ({metric} {score_col})\", f\"mean score ({metric} {score_col})\", \"most_important_hp\", \"new_hp_min\", \"new_hp_max\", f\"time_taken (H:M:S)\"]\n",
        "cache_cols = [\"hp_distribution_old\", \"hp_distribution_new\"]\n",
        "hpo_arse_results = pd.DataFrame(columns=display_cols + cache_cols)\n",
        "hpo_arse_results.index.name = \"optimisations\"\n",
        "\n",
        "hpo_arse_iteration_results = pd.DataFrame(columns=[\"optimisations\", \"iterations\", \"hyperparameters\", \"metric_used\", \"mean_validation\", \"mean_training\", \"mean_difference\", \"score_weighted\", \"time_taken\", \"hp_distribution\"])\n",
        "\n",
        "optimisations = 1\n",
        "iterations = 1\n",
        "\n",
        "# Function get the last row where the distribution was updated (if using the conservative flag, this is important for selecting the latest valid row in hpo_arse_results)\n",
        "roll_back_message = \"Rolled back to last valid optimisation.\"\n",
        "def get_last_valid_row(df):\n",
        "  for i in range(len(df) - 1, -1, -1):\n",
        "    if df.iloc[i][\"most_important_hp\"] != roll_back_message:\n",
        "      return df.iloc[i]\n",
        "\n",
        "# Visualisation function\n",
        "def display_visualisations(hpo_arse_results, shap_data):\n",
        "    print(tabulate(hpo_arse_results[display_cols], headers=\"keys\", tablefmt=\"psql\"))\n",
        "\n",
        "    try:\n",
        "        # First figure - only for saving the line plot\n",
        "        fig_line, ax_line = plt.subplots(figsize=(8, 4))\n",
        "        for score_type in ['best', 'mean']:\n",
        "            ax_line.plot(hpo_arse_results.index,\n",
        "                        hpo_arse_results[f\"{score_type} score ({metric} {score_col})\"],\n",
        "                        f\"{'b' if score_type=='best' else 'r'}o-\",\n",
        "                        label=f'{score_type.capitalize()} score')\n",
        "\n",
        "        ax_line.set_title(f\"{metric} score per optimisation\")\n",
        "        ax_line.set_xlabel(\"Optimisation\")\n",
        "        ax_line.set_ylabel(\"Score\")\n",
        "        ax_line.grid(True)\n",
        "        ax_line.legend()\n",
        "        ax_line.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
        "        fig_line.tight_layout()\n",
        "        fig_line.savefig(hpo_arse_line_graph_dir)\n",
        "        plt.close(fig_line)  # Close this figure since it's only for saving\n",
        "\n",
        "        # Now create display figure with all plots\n",
        "        if shap_data and len(shap_data) > 0:\n",
        "            i = len(shap_data)\n",
        "            most_important_hp, shap_values = shap_data[-1]\n",
        "\n",
        "            # Create figure with 3 subplots for display\n",
        "            fig_display, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "            fig_display.suptitle(f\"Optimisation #{i}\", fontsize=16)\n",
        "\n",
        "            # Line plot for display\n",
        "            for score_type in ['best', 'mean']:\n",
        "                ax0.plot(hpo_arse_results.index,\n",
        "                        hpo_arse_results[f\"{score_type} score ({metric} {score_col})\"],\n",
        "                        f\"{'b' if score_type=='best' else 'r'}o-\",\n",
        "                        label=f'{score_type.capitalize()} score')\n",
        "            ax0.set_title(f\"{metric} score per optimisation\")\n",
        "            ax0.set_xlabel(\"Optimisation\")\n",
        "            ax0.set_ylabel(\"Score\")\n",
        "            ax0.grid(True)\n",
        "            ax0.legend()\n",
        "            ax0.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
        "\n",
        "            # SHAP scatter plot\n",
        "            shap.plots.scatter(shap_values[:,most_important_hp],\n",
        "                             color=shap_values[:,most_important_hp].data,\n",
        "                             ax=ax1, show=False)\n",
        "\n",
        "            # Feature importance plot\n",
        "            importances = [np.mean(np.abs(shap_values.values[:, j]))\n",
        "                         for j in range(shap_values.values.shape[1])]\n",
        "            feature_importances = dict(sorted(zip(shap_values.feature_names, importances),\n",
        "                                           key=lambda x: x[1], reverse=True))\n",
        "\n",
        "            ax2.barh(range(len(feature_importances)), feature_importances.values(), color=\"#FF0051\")\n",
        "            ax2.set_yticks(range(len(feature_importances)))\n",
        "            ax2.set_yticklabels(feature_importances.keys())\n",
        "            ax2.invert_yaxis()\n",
        "            ax2.set_xlabel(\"mean(|SHAP value|)\")\n",
        "            ax2.spines[\"top\"].set_visible(False)\n",
        "            ax2.spines[\"right\"].set_visible(False)\n",
        "\n",
        "            for j in range(len(feature_importances)):\n",
        "                ax2.axhline(j+1, color=\"#888888\", lw=0.5, dashes=(1, 5), zorder=-1)\n",
        "                ax2.text(list(feature_importances.values())[j] + 0.001, j,\n",
        "                        f\"+{list(feature_importances.values())[j]:.2f}\",\n",
        "                        color=\"#FF0051\", ha=\"left\", va=\"center\", fontsize=12)\n",
        "\n",
        "            fig_display.tight_layout()\n",
        "            fig_display.savefig(join(hpo_arse_optimisation_plots_dir, f\"{i}_shap.png\"))\n",
        "            plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Error in main visualization: {str(e)}\")\n",
        "\n",
        "# Load cache\n",
        "if exists(hpo_arse_iteration_results_dir):\n",
        "  # Load iteration results, set the hyperparameter distribution, and set current process indicies\n",
        "  hpo_arse_iteration_results = pd.read_pickle(hpo_arse_iteration_results_dir)\n",
        "  hp_distribution_serialized = hpo_arse_iteration_results.iloc[-1][\"hp_distribution\"]\n",
        "  hp_distribution = {k:function_map[df_hp_types.loc[k][\"type\"]](v[0], v[1]) for k, v in hp_distribution_serialized.items()}\n",
        "\n",
        "  optimisations = hpo_arse_iteration_results.iloc[-1][\"optimisations\"]\n",
        "  iterations = hpo_arse_iteration_results.iloc[-1][\"iterations\"] + 1\n",
        "  if iterations > iterations_per_optimisation:\n",
        "    iterations = 1\n",
        "    optimisations += 1\n",
        "\n",
        "  # Load optimisation results, display visualisations, and update the hyperparameter distribution\n",
        "  if exists(hpo_arse_optimisation_results_dir) and exists(hpo_arse_cache_shap_dir): # May not exit if process stops before the first optimisation finishes\n",
        "    # Load data\n",
        "    hpo_arse_results = pd.read_pickle(hpo_arse_optimisation_results_dir)\n",
        "    with open(hpo_arse_cache_shap_dir, \"rb\") as data:\n",
        "      shap_data = pickle.load(data)\n",
        "\n",
        "    # Update hyperparameter distribution\n",
        "    hp_distribution_serialized = hpo_arse_results.iloc[-1][\"hp_distribution_new\"]\n",
        "    hp_distribution = {k:function_map[df_hp_types.loc[k][\"type\"]](v[0], v[1]) for k, v in hp_distribution_serialized.items()}\n",
        "\n",
        "    # Display visualisations\n",
        "    display_visualisations(hpo_arse_results, shap_data)\n",
        "\n",
        "# Iteratively train models with sampled hyper parameters, saving the values and metric score\n",
        "while optimisations <= total_optimisations:\n",
        "  iterations_progress_label = widgets.Label(value=f\"Iteration of current optimisation: {iterations}/{iterations_per_optimisation}\")\n",
        "  display(iterations_progress_label)\n",
        "  hp_distribution_old = hp_distribution.copy()\n",
        "  hp_distribution_new = hp_distribution.copy() # Copy old distribution for now, update this value later\n",
        "  hp_distribution_old_serialized = {k:v.args for k, v in hp_distribution_old.items()}\n",
        "\n",
        "  # Generate LHS samples for this optimisation round if enabled\n",
        "  if use_lhs:\n",
        "    round_seed = lhs_seed * optimisations if lhs_seed else None\n",
        "    resuming_mid_optimisation = iterations > 1\n",
        "\n",
        "    if round_seed is not None:\n",
        "      # Deterministic: regenerate identical samples\n",
        "      lhs_sampler = LatinHypercube(d=len(hp_distribution), seed=round_seed)\n",
        "      lhs_samples = lhs_sampler.random(n=iterations_per_optimisation)\n",
        "    elif resuming_mid_optimisation:\n",
        "      # Non-deterministic resume: generate fresh LHS for remaining iterations only\n",
        "      completed_iterations = hpo_arse_iteration_results[\n",
        "          hpo_arse_iteration_results[\"optimisations\"] == optimisations\n",
        "      ]\n",
        "      n_completed = len(completed_iterations)\n",
        "      n_remaining = iterations_per_optimisation - n_completed\n",
        "\n",
        "      lhs_sampler = LatinHypercube(d=len(hp_distribution), seed=None)\n",
        "      lhs_samples_remaining = lhs_sampler.random(n=n_remaining)\n",
        "\n",
        "      # Create full array with NaN placeholders for completed iterations\n",
        "      lhs_samples = np.full((iterations_per_optimisation, len(hp_distribution)), np.nan)\n",
        "      lhs_samples[n_completed:] = lhs_samples_remaining\n",
        "    else:\n",
        "      # Fresh start without seed\n",
        "      lhs_sampler = LatinHypercube(d=len(hp_distribution), seed=None)\n",
        "      lhs_samples = lhs_sampler.random(n=iterations_per_optimisation)\n",
        "\n",
        "    hp_names = list(hp_distribution.keys())\n",
        "\n",
        "  while iterations <= iterations_per_optimisation:\n",
        "    t_start = datetime.now()\n",
        "\n",
        "    # Sample HP values using LHS or random sampling\n",
        "    if use_lhs:\n",
        "      hp_sample = {}\n",
        "      for i, (hp_name, dist) in enumerate(hp_distribution.items()):\n",
        "        low, scale = dist.args[0], dist.args[1]\n",
        "        scaled_value = low + lhs_samples[iterations - 1, i] * scale\n",
        "        if df_hp_types.loc[hp_name]['type'] == 'randint':\n",
        "          scaled_value = int(np.floor(scaled_value))\n",
        "        hp_sample[hp_name] = scaled_value\n",
        "    else:\n",
        "      hp_sample_random_state = len(hpo_arse_iteration_results) + 1\n",
        "      hp_sample = {k: v.rvs(random_state=hp_sample_random_state * (i + 1) + optimisations * 1000)\n",
        "                   for i, (k, v) in enumerate(hp_distribution.items())}\n",
        "\n",
        "    sampled_parameters = hp_sample\n",
        "    arse_hyperparameters = dict(arse_hyperparameters.copy())\n",
        "    arse_hyperparameters.update(sampled_parameters)\n",
        "    # Ensure num_class is available for multiclass\n",
        "    if categorise_target and multiclass: arse_hyperparameters[\"num_class\"] = num_class\n",
        "    iterations_progress_label.value = f\"Iteration of current optimisation: {iterations}/{iterations_per_optimisation}\"\n",
        "\n",
        "    arse_predictor = XGBPredictor(**arse_hyperparameters)\n",
        "    df_kfold_metrics = run_kfold(model_dataset, kf_split[:n_hpo_splits], arse_predictor)\n",
        "\n",
        "    # Generate statistics\n",
        "    mean_training = df_kfold_metrics[f\"score_training ({metric})\"].mean()\n",
        "    mean_validation = df_kfold_metrics[f\"score_validation ({metric})\"].mean()\n",
        "    mean_difference = df_kfold_metrics[f\"score_difference ({metric})\"].mean()\n",
        "    score_weighted = def_score_weighted(mean_validation, mean_training)\n",
        "\n",
        "    # Save results\n",
        "    time_taken = str(datetime.now() - t_start)\n",
        "    i = len(hpo_arse_iteration_results)\n",
        "    hpo_arse_iteration_results.loc[i + 1] = [optimisations, iterations, arse_hyperparameters, metric, mean_validation, mean_training, mean_difference, score_weighted, time_taken, hp_distribution_old_serialized]\n",
        "    hpo_arse_iteration_results.to_pickle(hpo_arse_iteration_results_dir)\n",
        "    hpo_arse_iteration_results.to_csv(f\"{hpo_arse_iteration_results_dir[:-4]}.csv\")\n",
        "\n",
        "    # Export best hyperparameters\n",
        "    if optimal_value == \"min\":\n",
        "      best_result_id = hpo_arse_iteration_results[score_col].idxmin()\n",
        "    else:\n",
        "      best_result_id = hpo_arse_iteration_results[score_col].idxmax()\n",
        "\n",
        "    best_arse_hyperparameters = hpo_arse_iteration_results.loc[best_result_id][\"hyperparameters\"].copy()\n",
        "\n",
        "    export_arse_hyperparameters = best_arse_hyperparameters\n",
        "    export_arse_hyperparameters[score_col] = hpo_arse_iteration_results.loc[best_result_id][score_col]\n",
        "    pd.DataFrame(export_arse_hyperparameters, index=[0]).to_csv(hpo_arse_best_hyperparameters_dir)\n",
        "\n",
        "    # Update iteration index\n",
        "    iterations += 1\n",
        "\n",
        "  iterations = 1\n",
        "\n",
        "  # Collect results of only this optimisation round, expanding hyperparameters\n",
        "  interim_results = hpo_arse_iteration_results.loc[hpo_arse_iteration_results[\"optimisations\"] == optimisations].copy()\n",
        "  hp_expanded = pd.DataFrame(interim_results[\"hyperparameters\"].tolist(), index=interim_results.index)\n",
        "  interim_results = pd.concat([interim_results[[\"time_taken\", score_col]], hp_expanded[list(hp_distribution.keys())]], axis=1)\n",
        "\n",
        "  # Generate SHAP values\n",
        "  hpo_x = interim_results[list(hp_distribution.keys())]\n",
        "  hpo_x = hpo_x.apply(pd.to_numeric)\n",
        "  hpo_y = interim_results[score_col]\n",
        "  hpo_y = hpo_y.apply(pd.to_numeric)\n",
        "  hp_predictor = xgb.XGBRegressor()\n",
        "  hp_predictor.fit(hpo_x, hpo_y)\n",
        "  explainer = shap.TreeExplainer(hp_predictor)\n",
        "  shap_values = explainer(hpo_x)\n",
        "\n",
        "  # Calculate the feature importance (mean absolute shap value) for each feature, then select the most important\n",
        "  importances = []\n",
        "  for i in range(shap_values.values.shape[1]):\n",
        "    importances.append(np.mean(np.abs(shap_values.values[:, i])))\n",
        "  feature_importances = {fea: imp for imp, fea in zip(importances, hpo_x.columns)}\n",
        "  feature_importances = {k: v for k, v in sorted(feature_importances.items(), key=lambda item: item[1], reverse = True)}\n",
        "\n",
        "  most_important_hp = max(feature_importances, key=feature_importances.get)\n",
        "  try:\n",
        "    most_important_hp_shap = shap_values[:,most_important_hp]\n",
        "  except:\n",
        "    most_important_hp_shap = shap_values[most_important_hp,:]\n",
        "\n",
        "  # Break loop if most important feature has no effect (optimisation complete)\n",
        "  if len((most_important_hp_shap.values!=0).nonzero()[0]) == 0:\n",
        "    print(\"Optimisation complete\")\n",
        "    break\n",
        "\n",
        "  # Calculate quartile boundaries for the most important HP\n",
        "  hp_p25, hp_p50, hp_p75 = np.percentile(hpo_x[most_important_hp], [25, 50, 75])\n",
        "\n",
        "  # Create inclusive masks for each quartile\n",
        "  quartile_masks = [\n",
        "      most_important_hp_shap.data <= hp_p25,\n",
        "      (most_important_hp_shap.data >= hp_p25) & (most_important_hp_shap.data <= hp_p50),\n",
        "      (most_important_hp_shap.data >= hp_p50) & (most_important_hp_shap.data <= hp_p75),\n",
        "      most_important_hp_shap.data >= hp_p75\n",
        "  ]\n",
        "\n",
        "  # Calculate mean SHAP values for each quartile with zero division protection\n",
        "  shap_quartile_means = np.array([\n",
        "      np.mean(most_important_hp_shap.values[mask]) if mask.sum() > 0 else np.nan\n",
        "      for mask in quartile_masks\n",
        "  ])\n",
        "\n",
        "  # Select optimal quartile based on min or max optimal value\n",
        "  if optimal_value == \"min\":\n",
        "    best_quartile_idx = np.nanargmin(shap_quartile_means)\n",
        "  else:\n",
        "    best_quartile_idx = np.nanargmax(shap_quartile_means)\n",
        "\n",
        "  optimal_hp_values = most_important_hp_shap.data[quartile_masks[best_quartile_idx]]\n",
        "  lower_limit_optimal = (best_quartile_idx == 0)\n",
        "  upper_limit_optimal = (best_quartile_idx == 3)\n",
        "\n",
        "  # Get HP type for bounds handling\n",
        "  hp_type = df_hp_types.loc[most_important_hp]['type']\n",
        "  hp_is_int = (hp_type == \"randint\")\n",
        "\n",
        "  # Get new min and max optimal values\n",
        "  new_hp_min = np.min(optimal_hp_values)\n",
        "  new_hp_max = np.max(optimal_hp_values)\n",
        "  if hp_is_int:\n",
        "    new_hp_min = np.trunc(new_hp_min)\n",
        "    new_hp_max = np.ceil(new_hp_max)\n",
        "\n",
        "  # Calculate range correction as percentage of tested range\n",
        "  range_correction = bounds_correction_percent * np.ptp(most_important_hp_shap.data) / 100\n",
        "\n",
        "  # Extend bounds if optimal values are at the edge of tested range\n",
        "  if upper_limit_optimal:\n",
        "    new_hp_max = min(new_hp_max + range_correction, df_hp_types.loc[most_important_hp]['max'])\n",
        "  if lower_limit_optimal:\n",
        "    new_hp_min = max(new_hp_min - range_correction, df_hp_types.loc[most_important_hp]['min'])\n",
        "\n",
        "  # Round after bounds correction for integer HPs\n",
        "  if hp_is_int:\n",
        "    new_hp_min = np.trunc(new_hp_min)\n",
        "    new_hp_max = np.ceil(new_hp_max)\n",
        "\n",
        "  # Update and export SHAP data\n",
        "  shap_data.append([most_important_hp, shap_values])\n",
        "  with open(hpo_arse_cache_shap_dir, \"wb\") as f:\n",
        "    pickle.dump(shap_data, f)\n",
        "\n",
        "  # Update and export optimisation results\n",
        "  if optimal_value == \"min\":\n",
        "    best_result_id = interim_results[score_col].idxmin()\n",
        "  else:\n",
        "    best_result_id = interim_results[score_col].idxmax()\n",
        "\n",
        "  i = len(hpo_arse_results)\n",
        "  time_taken = 0\n",
        "  for index, value in interim_results[\"time_taken\"].items():\n",
        "    pt = datetime.strptime(value, \"%H:%M:%S.%f\")\n",
        "    time_taken += pt.second + pt.minute*60 + pt.hour*3600\n",
        "  time_taken = str(timedelta(seconds=time_taken))\n",
        "\n",
        "  roll_back_hp = False\n",
        "  if conservative and optimisations > 1:\n",
        "    avg_score_col = [col for col in hpo_arse_results.columns if col.startswith(\"mean score\")]\n",
        "    current_mean = interim_results[score_col].mean()\n",
        "    previous_mean = get_last_valid_row(hpo_arse_results)[avg_score_col][0] # Compare mean to the last valid optimisation, not necessarily the previous optimisation\n",
        "    if (optimal_value == \"min\" and current_mean > previous_mean) or (optimal_value == \"max\" and current_mean < previous_mean):\n",
        "      roll_back_hp = True\n",
        "\n",
        "  if roll_back_hp:\n",
        "    hp_distribution_new_serialized = get_last_valid_row(hpo_arse_results)[\"hp_distribution_old\"]\n",
        "    hp_distribution_new = {k:function_map[df_hp_types.loc[k][\"type\"]](v[0], v[1]) for k, v in hp_distribution_new_serialized.items()}\n",
        "    most_important_hp = roll_back_message\n",
        "    new_hp_min = np.nan\n",
        "    new_hp_max = np.nan\n",
        "  else:\n",
        "    if hp_is_int:\n",
        "      hp_distribution_new[most_important_hp] = randint(int(new_hp_min), int(new_hp_max) + 1)\n",
        "    else:\n",
        "      hp_distribution_new[most_important_hp] = uniform(new_hp_min, new_hp_max - new_hp_min)\n",
        "\n",
        "  hp_distribution = hp_distribution_new.copy()\n",
        "  hp_distribution_new_serialized = {k:v.args for k, v in hp_distribution_new.items()}\n",
        "\n",
        "  hpo_arse_results.loc[i + 1] = {\n",
        "      f\"best score ({metric} {score_col})\": interim_results.loc[best_result_id][score_col],\n",
        "      f\"mean score ({metric} {score_col})\": interim_results[score_col].mean(),\n",
        "      \"most_important_hp\": most_important_hp,\n",
        "      \"new_hp_min\": new_hp_min,\n",
        "      \"new_hp_max\": new_hp_max,\n",
        "      f\"time_taken (H:M:S)\": time_taken,\n",
        "      \"hp_distribution_old\": hp_distribution_old_serialized,\n",
        "      \"hp_distribution_new\": hp_distribution_new_serialized,\n",
        "  }\n",
        "  hpo_arse_results.to_pickle(hpo_arse_optimisation_results_dir)\n",
        "  hpo_arse_results.to_csv(f\"{hpo_arse_optimisation_results_dir[:-4]}.csv\") # Human readable\n",
        "\n",
        "  # Update visualisations\n",
        "  display_visualisations(hpo_arse_results, shap_data)\n",
        "\n",
        "  # Update optimisations index\n",
        "  optimisations += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQRbn2sUoiUL"
      },
      "source": [
        "### View ARSE plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJaNeuE3LmRD"
      },
      "outputs": [],
      "source": [
        "# View plots for all ARSE results\n",
        "\n",
        "# Load results if not already loaded\n",
        "if 'hpo_arse_iteration_results' not in dir() or hpo_arse_iteration_results.empty:\n",
        "  hpo_arse_dir = join(model_dataset_dir, \"hpo_arse\")\n",
        "  hpo_arse_iteration_results_dir = join(hpo_arse_dir, \"iteration_results.pkl\")\n",
        "  hpo_arse_iteration_results = pd.read_pickle(hpo_arse_iteration_results_dir)\n",
        "\n",
        "# Extract hyperparameters that were optimised (vary across iterations)\n",
        "hp_expanded = pd.json_normalize(hpo_arse_iteration_results['hyperparameters'])\n",
        "optimised_hps = [col for col in hp_expanded.columns if hp_expanded[col].nunique() > 1]\n",
        "\n",
        "# Filter to only optimised hyperparameters\n",
        "arse_hpo_x = hp_expanded[optimised_hps]\n",
        "\n",
        "# Score optimised in ARSE\n",
        "arse_hpo_y = hpo_arse_iteration_results[['score_weighted']]\n",
        "\n",
        "# Predictor for SHAP interpretation\n",
        "arse_hpo_predictor = xgb.XGBRegressor()\n",
        "arse_hpo_predictor.fit(arse_hpo_x, arse_hpo_y)\n",
        "\n",
        "# SHAP explainer\n",
        "arse_hpo_explainer = shap.Explainer(arse_hpo_predictor)\n",
        "arse_hpo_shap_values = arse_hpo_explainer(arse_hpo_x)\n",
        "shap.plots.beeswarm(arse_hpo_shap_values, plot_size=(20, max(8, len(optimised_hps) * 0.8)))\n",
        "\n",
        "print(f\"Optimised hyperparameters: {optimised_hps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N13dNEFRLt94"
      },
      "outputs": [],
      "source": [
        "# Individual scatter plots for each optimised hyperparameter\n",
        "for hp_name in optimised_hps:\n",
        "  shap.plots.scatter(arse_hpo_shap_values[:, hp_name], color=arse_hpo_shap_values[:, hp_name].data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9-hQ7G37uZi"
      },
      "source": [
        "# Final model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFaSL-AVQQ2c"
      },
      "source": [
        "## Define and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMR_E3f-B9iL"
      },
      "outputs": [],
      "source": [
        "# Using a random seed makes the results replicable\n",
        "use_random_seed = True\n",
        "\n",
        "# Set to True to use the 'best hyperparameters' from HPO\n",
        "# Set to false to use hyperparameters in the code block above\n",
        "use_best_hyperparameters = False\n",
        "hpo_method = \"hpo_arse\" # Options: \"hpo_arse\", \"hpo_random_search\"\n",
        "\n",
        "# Use default hyperparameters\n",
        "test_hyperparameters = default_hyperparameters.copy()\n",
        "\n",
        "# Manually define final hyperparameters\n",
        "# GEDI elevation\n",
        "# test_hyperparameters = {\n",
        "#  'tree_method': 'hist',\n",
        "#  'device': 'cuda',\n",
        "#  'enable_categorical': True,\n",
        "#  'max_bin': max_bin,\n",
        "#  'n_estimators': 100000, # Will be limited by early stopping\n",
        "#  'eta': 0.01,\n",
        "#  'early_stopping_rounds': 18,\n",
        "#  'min_child_weight': 29,\n",
        "#  'gamma': 0.05,\n",
        "#  'alpha': 1480,\n",
        "#  'lambda': 2150,\n",
        "#  'colsample_bytree': 0.98,\n",
        "#  'colsample_bylevel': 1,\n",
        "#  'colsample_bynode': 1\n",
        "# }\n",
        "\n",
        "# # AGBD Alpha Earth\n",
        "# test_hyperparameters = {\n",
        "#  'tree_method': 'hist',\n",
        "#  'device': 'cuda',\n",
        "#  'enable_categorical': True,\n",
        "#  'max_bin': 256,\n",
        "#  'n_estimators': 100000, # Will be limited by early stopping\n",
        "#  'eta': 0.05,\n",
        "#  'early_stopping_rounds': 3,\n",
        "#  'min_child_weight': 55,\n",
        "#  'gamma': 0.018,\n",
        "#  'alpha': 83,\n",
        "#  'lambda': 10500,\n",
        "#  'colsample_bytree': 0.57,\n",
        "#  'colsample_bylevel': 0.53,\n",
        "#  'colsample_bynode': 0.5\n",
        "# }\n",
        "\n",
        "if use_random_seed: test_hyperparameters[\"random_state\"] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w0PaV3u6wVx"
      },
      "outputs": [],
      "source": [
        "# Test model\n",
        "test_model_dir = join(model_dataset_dir,\"model_test\")\n",
        "\n",
        "# If optimised hyperparameters exist, use them\n",
        "if use_best_hyperparameters:\n",
        "  test_hyperparameters = pd.read_csv(join(model_dataset_dir, hpo_method, \"best_hyperparameters.csv\"), index_col=0).to_dict(orient=\"records\")[0]\n",
        "  test_hyperparameters = {k: v for k, v in test_hyperparameters.items() if \"score_\" not in k}\n",
        "\n",
        "predictor = XGBPredictor(**test_hyperparameters)\n",
        "\n",
        "assert not exists(model_description_dir) and not exists(final_model_dir), \"Remove both \\\"model_description.json\\\" and \\\"model.json\\\", before exporting a new model\"\n",
        "\n",
        "# Run model\n",
        "df_kfold_metrics = run_kfold(model_dataset, kf_split[n_hpo_splits:], predictor, verbose=True,\n",
        "                             export_test_model=True, test_model_dir=test_model_dir)\n",
        "\n",
        "generate_statistics(df_kfold_metrics)\n",
        "\n",
        "model_description = {\n",
        "    \"metric_used_for_training\": metric,\n",
        "    \"optimal_value\": optimal_value,\n",
        "    \"use_score_weighted\": use_score_weighted,\n",
        "    \"n_splits\": n_splits, # Number of k-fold splits\n",
        "    \"splits_hpo\": n_hpo_splits, # Number of k-fold splits set aside for HPO, the remaining are used for final testing.\n",
        "    \"hyperparameters\": str(test_hyperparameters)\n",
        "}\n",
        "\n",
        "for col in df_kfold_metrics.columns:\n",
        "  model_description[f\"{col} mean\"] = float(df_kfold_metrics[col].mean())\n",
        "  model_description[f\"{col} std\"] = float(df_kfold_metrics[col].std())\n",
        "\n",
        "# Export model_description.json\n",
        "with open(model_description_dir, \"w\") as f:\n",
        "  f.write(json.dumps(model_description, indent=2))\n",
        "print(\"model_description.json generation and export complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ6tXG1bQJE4"
      },
      "source": [
        "## Descriptive plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuuF8sVWHGTL"
      },
      "outputs": [],
      "source": [
        "# Descriptive plots\n",
        "\n",
        "# Assert model description exists\n",
        "assert exists(model_description_dir), \"\\\"model_description.json\\\" must exist to continue, run previous cells\"\n",
        "\n",
        "# Reload hyperparameters\n",
        "with open(model_description_dir) as model_description_json:\n",
        "  model_description = json.load(model_description_json)\n",
        "final_hyperparameters = ast.literal_eval(model_description[\"hyperparameters\"])\n",
        "\n",
        "# Remove early stopping and replace with mean n_estimators\n",
        "if \"early_stopping_rounds\" in final_hyperparameters:\n",
        "  final_hyperparameters = {k:v for k, v in final_hyperparameters.items() if k != \"early_stopping_rounds\"}\n",
        "  final_hyperparameters[\"n_estimators\"] = round(model_description[\"n_estimators mean\"])\n",
        "\n",
        "# Select the last k-split\n",
        "train_index_plots, valid_index_plots = kf_split[n_splits-1]\n",
        "x_train_plots = model_dataset.loc[train_index_plots][selected_features]\n",
        "x_valid_plots = model_dataset.loc[valid_index_plots][selected_features]\n",
        "y_train_plots = model_dataset.loc[train_index_plots][selected_target]\n",
        "y_valid_plots = model_dataset.loc[valid_index_plots][selected_target]\n",
        "\n",
        "# Create DMatrix objects\n",
        "dtrain_plots = xgb.DMatrix(x_train_plots, y_train_plots, enable_categorical=True)\n",
        "dvalid_plots = xgb.DMatrix(x_valid_plots, y_valid_plots, enable_categorical=True)\n",
        "\n",
        "# Set up parameters\n",
        "predictor = XGBPredictor(**final_hyperparameters)\n",
        "params_plots = predictor.get_params()\n",
        "params_plots['eval_metric'] = eval_metric\n",
        "# Default fix for new XGBoost version\n",
        "for key in ['n_estimators', 'enable_categorical', 'missing']: params_plots.pop(key, None)\n",
        "\n",
        "# Train model\n",
        "evals_result = {}\n",
        "model_plots = xgb.train(params_plots,\n",
        "                        dtrain_plots,\n",
        "                        num_boost_round=final_hyperparameters['n_estimators'],\n",
        "                        evals=[(dtrain_plots, 'training'), (dvalid_plots, 'validation_1')],\n",
        "                        evals_result=evals_result,\n",
        "                        verbose_eval=False)\n",
        "\n",
        "# Feature importances\n",
        "feature_importances = model_plots.get_score(importance_type='weight')\n",
        "sorted_importances = sorted(feature_importances.items(), key=lambda x: x[1], reverse=True)\n",
        "features, importances = zip(*sorted_importances)\n",
        "\n",
        "plt.figure(figsize=(10, 30))\n",
        "plt.barh(features, importances, align='center')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importances')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
        "plt.show()\n",
        "\n",
        "# Prediction versus test data plot\n",
        "validation_prediction = model_plots.predict(dvalid_plots)\n",
        "# Convert predictions to class labels for classification\n",
        "if categorise_target:\n",
        "    # For multiclass, use argmax to get predicted class from probabilities\n",
        "    if multiclass: validation_prediction = np.argmax(validation_prediction, axis=1)\n",
        "    # For binary, round probabilities to get class labels\n",
        "    else: validation_prediction = np.round(validation_prediction).astype(int)\n",
        "validation_prediction_series = pd.Series(validation_prediction, index=y_valid_plots.index, name=y_valid_plots.name)\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "if categorise_target:\n",
        "    # For classification, use discrete bins for categories\n",
        "    if hasattr(y_valid_plots, 'cat'): actual_values = y_valid_plots.cat.codes\n",
        "    else: actual_values = y_valid_plots.astype(int)\n",
        "    # For multiclass, validation_prediction already contains class labels from argmax\n",
        "    # For binary, it contains probabilities that need rounding\n",
        "    if multiclass: predicted_values = validation_prediction.astype(int)\n",
        "    else: predicted_values = np.round(validation_prediction).astype(int)\n",
        "\n",
        "    # Get unique classes for bins\n",
        "    all_classes = sorted(set(actual_values) | set(predicted_values))\n",
        "    bins = np.arange(min(all_classes), max(all_classes) + 2) - 0.5\n",
        "\n",
        "    plt.hist(actual_values, bins=bins, label=f'{y_valid_plots.name} (actual)', alpha=0.6, align='mid')\n",
        "    plt.hist(predicted_values, bins=bins, label=f'{y_valid_plots.name} (predicted)', alpha=0.6, align='mid')\n",
        "    plt.xticks(all_classes)\n",
        "    plt.xlabel('Class')\n",
        "else:\n",
        "    # For regression, use continuous bins\n",
        "    bins = np.linspace(y_valid_plots.min(), y_valid_plots.max(), 100)\n",
        "    y_valid_plots.hist(bins=bins, label=f'{y_valid_plots.name} (actual)', alpha=0.6)\n",
        "    validation_prediction_series.hist(bins=bins, label=f'{y_valid_plots.name} (predicted)', alpha=0.6)\n",
        "    plt.xlabel('Value')\n",
        "\n",
        "plt.legend()\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Prediction vs. Actual Data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9PEyUnwQXZI"
      },
      "source": [
        "## Train and export with full dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PelMIeTUBC8u"
      },
      "outputs": [],
      "source": [
        "# Generate final model\n",
        "\n",
        "# Assert exists\n",
        "assert exists(model_description_dir), \"\\\"model_description.json\\\" must exist to continue, run previous cells\"\n",
        "\n",
        "# Reload hyperparameters\n",
        "with open(model_description_dir) as model_description_json:\n",
        "  model_description = json.load(model_description_json)\n",
        "final_hyperparameters = ast.literal_eval(model_description[\"hyperparameters\"])\n",
        "\n",
        "# Remove early stopping and replace with mean n_estimators\n",
        "if \"early_stopping_rounds\" in final_hyperparameters:\n",
        "  final_hyperparameters = {k:v for k, v in final_hyperparameters.items() if k != \"early_stopping_rounds\"}\n",
        "  final_hyperparameters[\"n_estimators\"] = round(model_description[\"n_estimators mean\"])\n",
        "\n",
        "\n",
        "# Create DMatrix objects\n",
        "dtrain_final = xgb.DMatrix(model_dataset_x, model_dataset_y, enable_categorical=True)\n",
        "\n",
        "# Train model on full dataset\n",
        "predictor = XGBPredictor(**final_hyperparameters)\n",
        "params_final = predictor.get_params()\n",
        "params_final['eval_metric'] = eval_metric\n",
        "# sklearn wrapper params handled separately via num_boost_round and DMatrix\n",
        "for key in ['n_estimators', 'enable_categorical', 'missing']: params_final.pop(key, None)\n",
        "model_final = xgb.train(params_final,\n",
        "                        dtrain_final,\n",
        "                        num_boost_round=final_hyperparameters['n_estimators'],\n",
        "                        verbose_eval=True)\n",
        "\n",
        "# Export model\n",
        "model_final.save_model(final_model_dir)\n",
        "print(\"Model training and 'model.json' export complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_HRe0_WLFt8"
      },
      "source": [
        "# Model interpretation (SHAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSO349uTLHDl"
      },
      "outputs": [],
      "source": [
        "# Define and create directories\n",
        "shap_dir = join(model_dataset_dir, \"shap\")\n",
        "shap_feature_effect_plots_dir = join(shap_dir, 'feature_effect_plots')\n",
        "shap_cache_dir = join(shap_dir, 'shap_cache')\n",
        "\n",
        "os.makedirs(shap_dir, exist_ok=True)\n",
        "os.makedirs(shap_feature_effect_plots_dir, exist_ok=True)\n",
        "os.makedirs(shap_cache_dir, exist_ok=True)\n",
        "\n",
        "# Print the number of features\n",
        "num_features = len(selected_features)\n",
        "print(f\"# Number of features: {num_features}\\n\")\n",
        "\n",
        "def modify_feature_name(feature):\n",
        "    # First capture if it's smoothed/unsmoothed\n",
        "    is_unsmoothed = 'unsmooth_' in feature\n",
        "    is_smoothed = 'smooth_' in feature and not is_unsmoothed\n",
        "    # Remove smoothing indicators (we'll add them back at the end)\n",
        "    if is_unsmoothed:\n",
        "      feature = feature.replace('unsmooth_', '')\n",
        "      feature = feature.replace('un', '', 1)\n",
        "    if is_smoothed:\n",
        "      feature = feature.replace('smooth_', '')\n",
        "      feature = feature.replace('un', '', 1)\n",
        "    # Remove specified prefixes from the feature name\n",
        "    prefixes = ['fea_', 'topo_', 'dtm_', 'dsm_', 'lu_', 'pa_']\n",
        "    for prefix in prefixes: feature = feature.replace(prefix, '')\n",
        "    # Replace specific patterns\n",
        "    feature = feature.replace('beam', 'GEDI beam')\n",
        "    feature = feature.replace('sensitivity', 'GEDI sensitivity')\n",
        "    feature = feature.replace('disturbance_edge_distance', 'Disturbance with edge distance')\n",
        "    feature = feature.replace('_km', ' (km)')\n",
        "    feature = feature.replace('_03', ' (3 pixel)')\n",
        "    feature = feature.replace('_07', ' (7 pixel)')\n",
        "    feature = feature.replace('_11', ' (11 pixel)')\n",
        "    # Specific to Tekai landscape, Malaysia\n",
        "    feature = feature.replace('ais_edge_distance', 'Ais forest reserves with edge distance')\n",
        "    feature = feature.replace('berkelah_jerantut_edge_distance', 'Berkelah Jerantut Forest Reserve with edge distance')\n",
        "    feature = feature.replace('berkelah_kuantan_edge_distance', 'Berkelah Kuantan forest reserves with edge distance')\n",
        "    feature = feature.replace('berkelah_temerloh_edge_distance', 'Berkelah Temerloh Forest Reserve with edge distance')\n",
        "    feature = feature.replace('old-growth_protected_areas_edge_distance', 'Old-growth protected areas with edge distance')\n",
        "    feature = feature.replace('remen_chereh_edge_distance', 'Remen Chereh forest reserves ewith edge distance')\n",
        "    feature = feature.replace('tekai_tembeling_edge_distance', 'Tekai Tembeling forest reserves with edge distance')\n",
        "    feature = feature.replace('tekam_edge_distance', 'Tekam Forest Reserve edge distance')\n",
        "    feature = feature.replace('yong_edge_distance', 'Yong Lipis forest reserves with edge distance')\n",
        "    feature = feature.replace('yong_lipis_edge_distance', 'Yong forest reserves with edge distance')\n",
        "    # Replace remaining underscores with spaces\n",
        "    feature = feature.replace('_', ' ')\n",
        "    # Add smoothing term at the end\n",
        "    if is_unsmoothed: feature = feature + ' unsmoothed'\n",
        "    elif is_smoothed: feature = feature + ' smoothed'\n",
        "    # Capitalize and clean up\n",
        "    feature = feature.strip()\n",
        "    feature = feature[0].upper() + feature[1:]\n",
        "    return feature\n",
        "\n",
        "# Generate a dictionary of original feature names mapped to modified names\n",
        "# Copy and paste the following 'feature_name_mapping' dictionary and modify it as needed\n",
        "print(\"feature_name_mapping = {\")\n",
        "for feature in selected_features:\n",
        "    modified_name = modify_feature_name(feature)\n",
        "    print(f\"    '{feature}': '{modified_name}',\")\n",
        "print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xt6ji2OPSHa5"
      },
      "outputs": [],
      "source": [
        "## GEDI elevation\n",
        "\n",
        "# # Number of features: 131\n",
        "\n",
        "# feature_name_mapping = {\n",
        "#     'fea_coast_proximity_km': 'Coast proximity (km)',\n",
        "#     'fea_disturbance_edge_distance_1990': 'Disturbance with edge distance 1990',\n",
        "#     'fea_disturbance_edge_distance_1991': 'Disturbance with edge distance 1991',\n",
        "#     'fea_disturbance_edge_distance_1992': 'Disturbance with edge distance 1992',\n",
        "#     'fea_disturbance_edge_distance_1993': 'Disturbance with edge distance 1993',\n",
        "#     'fea_disturbance_edge_distance_1994': 'Disturbance with edge distance 1994',\n",
        "#     'fea_disturbance_edge_distance_1995': 'Disturbance with edge distance 1995',\n",
        "#     'fea_disturbance_edge_distance_1996': 'Disturbance with edge distance 1996',\n",
        "#     'fea_disturbance_edge_distance_1997': 'Disturbance with edge distance 1997',\n",
        "#     'fea_disturbance_edge_distance_1998': 'Disturbance with edge distance 1998',\n",
        "#     'fea_disturbance_edge_distance_1999': 'Disturbance with edge distance 1999',\n",
        "#     'fea_disturbance_edge_distance_2000': 'Disturbance with edge distance 2000',\n",
        "#     'fea_disturbance_edge_distance_2001': 'Disturbance with edge distance 2001',\n",
        "#     'fea_disturbance_edge_distance_2002': 'Disturbance with edge distance 2002',\n",
        "#     'fea_disturbance_edge_distance_2003': 'Disturbance with edge distance 2003',\n",
        "#     'fea_disturbance_edge_distance_2004': 'Disturbance with edge distance 2004',\n",
        "#     'fea_disturbance_edge_distance_2005': 'Disturbance with edge distance 2005',\n",
        "#     'fea_disturbance_edge_distance_2006': 'Disturbance with edge distance 2006',\n",
        "#     'fea_disturbance_edge_distance_2007': 'Disturbance with edge distance 2007',\n",
        "#     'fea_disturbance_edge_distance_2008': 'Disturbance with edge distance 2008',\n",
        "#     'fea_disturbance_edge_distance_2009': 'Disturbance with edge distance 2009',\n",
        "#     'fea_disturbance_edge_distance_2010': 'Disturbance with edge distance 2010',\n",
        "#     'fea_disturbance_edge_distance_2011': 'Disturbance with edge distance 2011',\n",
        "#     'fea_disturbance_edge_distance_2012': 'Disturbance with edge distance 2012',\n",
        "#     'fea_disturbance_edge_distance_2013': 'Disturbance with edge distance 2013',\n",
        "#     'fea_disturbance_edge_distance_2014': 'Disturbance with edge distance 2014',\n",
        "#     'fea_disturbance_edge_distance_2015': 'Disturbance with edge distance 2015',\n",
        "#     'fea_disturbance_local_density_1990': 'Disturbance local density 1990',\n",
        "#     'fea_disturbance_local_density_1991': 'Disturbance local density 1991',\n",
        "#     'fea_disturbance_local_density_1992': 'Disturbance local density 1992',\n",
        "#     'fea_disturbance_local_density_1993': 'Disturbance local density 1993',\n",
        "#     'fea_disturbance_local_density_1994': 'Disturbance local density 1994',\n",
        "#     'fea_disturbance_local_density_1995': 'Disturbance local density 1995',\n",
        "#     'fea_disturbance_local_density_1996': 'Disturbance local density 1996',\n",
        "#     'fea_disturbance_local_density_1997': 'Disturbance local density 1997',\n",
        "#     'fea_disturbance_local_density_1998': 'Disturbance local density 1998',\n",
        "#     'fea_disturbance_local_density_1999': 'Disturbance local density 1999',\n",
        "#     'fea_disturbance_local_density_2000': 'Disturbance local density 2000',\n",
        "#     'fea_disturbance_local_density_2001': 'Disturbance local density 2001',\n",
        "#     'fea_disturbance_local_density_2002': 'Disturbance local density 2002',\n",
        "#     'fea_disturbance_local_density_2003': 'Disturbance local density 2003',\n",
        "#     'fea_disturbance_local_density_2004': 'Disturbance local density 2004',\n",
        "#     'fea_disturbance_local_density_2005': 'Disturbance local density 2005',\n",
        "#     'fea_disturbance_local_density_2006': 'Disturbance local density 2006',\n",
        "#     'fea_disturbance_local_density_2007': 'Disturbance local density 2007',\n",
        "#     'fea_disturbance_local_density_2008': 'Disturbance local density 2008',\n",
        "#     'fea_disturbance_local_density_2009': 'Disturbance local density 2009',\n",
        "#     'fea_disturbance_local_density_2010': 'Disturbance local density 2010',\n",
        "#     'fea_disturbance_local_density_2011': 'Disturbance local density 2011',\n",
        "#     'fea_disturbance_local_density_2012': 'Disturbance local density 2012',\n",
        "#     'fea_disturbance_local_density_2013': 'Disturbance local density 2013',\n",
        "#     'fea_disturbance_local_density_2014': 'Disturbance local density 2014',\n",
        "#     'fea_disturbance_local_density_2015': 'Disturbance local density 2015',\n",
        "#     'fea_forest_edge_distance_1990': 'Forest edge distance 1990',\n",
        "#     'fea_forest_edge_distance_2000': 'Forest edge distance 2000',\n",
        "#     'fea_forest_edge_distance_2010': 'Forest edge distance 2010',\n",
        "#     'fea_forest_edge_distance_2011': 'Forest edge distance 2011',\n",
        "#     'fea_forest_edge_distance_2012': 'Forest edge distance 2012',\n",
        "#     'fea_forest_edge_distance_2013': 'Forest edge distance 2013',\n",
        "#     'fea_forest_edge_distance_2014': 'Forest edge distance 2014',\n",
        "#     'fea_forest_edge_distance_2015': 'Forest edge distance 2015',\n",
        "#     'fea_forest_local_density_1990': 'Forest local density 1990',\n",
        "#     'fea_forest_local_density_2000': 'Forest local density 2000',\n",
        "#     'fea_forest_local_density_2010': 'Forest local density 2010',\n",
        "#     'fea_forest_local_density_2011': 'Forest local density 2011',\n",
        "#     'fea_forest_local_density_2012': 'Forest local density 2012',\n",
        "#     'fea_forest_local_density_2013': 'Forest local density 2013',\n",
        "#     'fea_forest_local_density_2014': 'Forest local density 2014',\n",
        "#     'fea_forest_local_density_2015': 'Forest local density 2015',\n",
        "#     'fea_latitude': 'Latitude',\n",
        "#     'fea_longitude': 'Longitude',\n",
        "#     'fea_lu_ais_edge_distance': 'Ais forest reserves with edge distance',\n",
        "#     'fea_lu_berkelah_jerantut_edge_distance': 'Berkelah Jerantut Forest Reserve with edge distance',\n",
        "#     'fea_lu_berkelah_kuantan_edge_distance': 'Berkelah Kuantan forest reserves with edge distance',\n",
        "#     'fea_lu_berkelah_temerloh_edge_distance': 'Berkelah Temerloh Forest Reserve with edge distance',\n",
        "#     'fea_lu_old-growth_protected_areas_edge_distance': 'Old-growth protected areas with edge distance',\n",
        "#     'fea_lu_remen_chereh_edge_distance': 'Remen Chereh forest reserves ewith edge distance',\n",
        "#     'fea_lu_tekai_tembeling_edge_distance': 'Tekai Tembeling forest reserves with edge distance',\n",
        "#     'fea_lu_tekam_edge_distance': 'Tekam Forest Reserve edge distance',\n",
        "#     'fea_lu_yong_edge_distance': 'Yong Lipis forest reserves with edge distance',\n",
        "#     'fea_lu_yong_lipis_edge_distance': 'Yong forest reserves with edge distance',\n",
        "#     'fea_topo_dsm_smooth_aspect_cosine': 'Aspect cosine smoothed',\n",
        "#     'fea_topo_dsm_smooth_aspect_sine': 'Aspect sine smoothed',\n",
        "#     'fea_topo_dsm_smooth_circular_variance_aspect_03': 'Circular variance aspect (3 pixel) smoothed',\n",
        "#     'fea_topo_dsm_smooth_circular_variance_aspect_07': 'Circular variance aspect (7 pixel) smoothed',\n",
        "#     'fea_topo_dsm_smooth_circular_variance_aspect_11': 'Circular variance aspect (11 pixel) smoothed',\n",
        "#     'fea_topo_dsm_smooth_deviation_mean_elevation_03': 'Deviation mean elevation (3 pixel) smoothed',\n",
        "#     'fea_topo_dsm_smooth_deviation_mean_elevation_07': 'Deviation mean elevation (7 pixel) smoothed',\n",
        "#     'fea_topo_dsm_smooth_deviation_mean_elevation_11': 'Deviation mean elevation (11 pixel) smoothed',\n",
        "#     'fea_topo_dsm_smooth_eastness': 'Eastness smoothed',\n",
        "#     'fea_topo_dsm_smooth_elevation': 'Elevation smoothed',\n",
        "#     'fea_topo_dsm_smooth_northness': 'Northness smoothed',\n",
        "#     'fea_topo_dsm_smooth_profile_curvature': 'Profile curvature smoothed',\n",
        "#     'fea_topo_dsm_smooth_roughness_03': 'Roughness (3 pixel) smoothed',\n",
        "#     'fea_topo_dsm_smooth_roughness_07': 'Roughness (7 pixel) smoothed',\n",
        "#     'fea_topo_dsm_smooth_roughness_11': 'Roughness (11 pixel) smoothed',\n",
        "#     'fea_topo_dsm_smooth_slope': 'Slope smoothed',\n",
        "#     'fea_topo_dsm_smooth_stream_power_index_log10': 'Stream power index log10 smoothed',\n",
        "#     'fea_topo_dsm_smooth_surface_area_ratio': 'Surface area ratio smoothed',\n",
        "#     'fea_topo_dsm_smooth_tangential_curvature': 'Tangential curvature smoothed',\n",
        "#     'fea_topo_dsm_smooth_topographic_position_index_03': 'Topographic position index (3 pixel) smoothed',\n",
        "#     'fea_topo_dsm_smooth_topographic_position_index_07': 'Topographic position index (7 pixel) smoothed',\n",
        "#     'fea_topo_dsm_smooth_topographic_position_index_11': 'Topographic position index (11 pixel) smoothed',\n",
        "#     'fea_topo_dsm_smooth_topographic_ruggedness_index': 'Topographic ruggedness index smoothed',\n",
        "#     'fea_topo_dsm_smooth_topographic_wetness_index': 'Topographic wetness index smoothed',\n",
        "#     'fea_topo_dsm_unsmooth_aspect_cosine': 'Aspect cosine unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_aspect_sine': 'Aspect sine unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_circular_variance_aspect_03': 'Circular variance aspect (3 pixel) unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_circular_variance_aspect_07': 'Circular variance aspect (7 pixel) unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_circular_variance_aspect_11': 'Circular variance aspect (11 pixel) unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_deviation_mean_elevation_03': 'Deviation mean elevation (3 pixel) unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_deviation_mean_elevation_07': 'Deviation mean elevation (7 pixel) unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_deviation_mean_elevation_11': 'Deviation mean elevation (11 pixel) unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_eastness': 'Eastness unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_elevation': 'Elevation unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_northness': 'Northness unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_profile_curvature': 'Profile curvature unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_roughness_03': 'Roughness (3 pixel) unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_roughness_07': 'Roughness (7 pixel) unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_roughness_11': 'Roughness (11 pixel) unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_slope': 'Slope unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_stream_power_index_log10': 'Stream power index log10 unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_surface_area_ratio': 'Surface area ratio unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_tangential_curvature': 'Tangential curvature unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_topographic_position_index_03': 'Topographic position index (3 pixel) unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_topographic_position_index_07': 'Topographic position index (7 pixel) unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_topographic_position_index_11': 'Topographic position index (11 pixel) unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_topographic_ruggedness_index': 'Topographic ruggedness index unsmoothed',\n",
        "#     'fea_topo_dsm_unsmooth_topographic_wetness_index': 'Topographic wetness index unsmoothed',\n",
        "#     'fea_beam': 'GEDI beam',\n",
        "#     'fea_sensitivity': 'GEDI sensitivity',\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3X-sEtcrcN0"
      },
      "outputs": [],
      "source": [
        "# AGBD\n",
        "\n",
        "# Number of features: 131\n",
        "\n",
        "feature_name_mapping = {\n",
        "    'fea_coast_proximity_km': 'Coast proximity (km)',\n",
        "    'fea_disturbance_edge_distance_1995': 'Disturbance with edge distance 1995',\n",
        "    'fea_disturbance_edge_distance_1996': 'Disturbance with edge distance 1996',\n",
        "    'fea_disturbance_edge_distance_1997': 'Disturbance with edge distance 1997',\n",
        "    'fea_disturbance_edge_distance_1998': 'Disturbance with edge distance 1998',\n",
        "    'fea_disturbance_edge_distance_1999': 'Disturbance with edge distance 1999',\n",
        "    'fea_disturbance_edge_distance_2000': 'Disturbance with edge distance 2000',\n",
        "    'fea_disturbance_edge_distance_2001': 'Disturbance with edge distance 2001',\n",
        "    'fea_disturbance_edge_distance_2002': 'Disturbance with edge distance 2002',\n",
        "    'fea_disturbance_edge_distance_2003': 'Disturbance with edge distance 2003',\n",
        "    'fea_disturbance_edge_distance_2004': 'Disturbance with edge distance 2004',\n",
        "    'fea_disturbance_edge_distance_2005': 'Disturbance with edge distance 2005',\n",
        "    'fea_disturbance_edge_distance_2006': 'Disturbance with edge distance 2006',\n",
        "    'fea_disturbance_edge_distance_2007': 'Disturbance with edge distance 2007',\n",
        "    'fea_disturbance_edge_distance_2008': 'Disturbance with edge distance 2008',\n",
        "    'fea_disturbance_edge_distance_2009': 'Disturbance with edge distance 2009',\n",
        "    'fea_disturbance_edge_distance_2010': 'Disturbance with edge distance 2010',\n",
        "    'fea_disturbance_edge_distance_2011': 'Disturbance with edge distance 2011',\n",
        "    'fea_disturbance_edge_distance_2012': 'Disturbance with edge distance 2012',\n",
        "    'fea_disturbance_edge_distance_2013': 'Disturbance with edge distance 2013',\n",
        "    'fea_disturbance_edge_distance_2014': 'Disturbance with edge distance 2014',\n",
        "    'fea_disturbance_edge_distance_2015': 'Disturbance with edge distance 2015',\n",
        "    'fea_disturbance_edge_distance_2016': 'Disturbance with edge distance 2016',\n",
        "    'fea_disturbance_edge_distance_2017': 'Disturbance with edge distance 2017',\n",
        "    'fea_disturbance_edge_distance_2018': 'Disturbance with edge distance 2018',\n",
        "    'fea_disturbance_edge_distance_2019': 'Disturbance with edge distance 2019',\n",
        "    'fea_disturbance_edge_distance_2020': 'Disturbance with edge distance 2020',\n",
        "    'fea_disturbance_edge_distance_2021': 'Disturbance with edge distance 2021',\n",
        "    'fea_disturbance_edge_distance_2022': 'Disturbance with edge distance 2022',\n",
        "    'fea_disturbance_edge_distance_2023': 'Disturbance with edge distance 2023',\n",
        "    'fea_disturbance_local_density_1995': 'Disturbance local density 1995',\n",
        "    'fea_disturbance_local_density_1996': 'Disturbance local density 1996',\n",
        "    'fea_disturbance_local_density_1997': 'Disturbance local density 1997',\n",
        "    'fea_disturbance_local_density_1998': 'Disturbance local density 1998',\n",
        "    'fea_disturbance_local_density_1999': 'Disturbance local density 1999',\n",
        "    'fea_disturbance_local_density_2000': 'Disturbance local density 2000',\n",
        "    'fea_disturbance_local_density_2001': 'Disturbance local density 2001',\n",
        "    'fea_disturbance_local_density_2002': 'Disturbance local density 2002',\n",
        "    'fea_disturbance_local_density_2003': 'Disturbance local density 2003',\n",
        "    'fea_disturbance_local_density_2004': 'Disturbance local density 2004',\n",
        "    'fea_disturbance_local_density_2005': 'Disturbance local density 2005',\n",
        "    'fea_disturbance_local_density_2006': 'Disturbance local density 2006',\n",
        "    'fea_disturbance_local_density_2007': 'Disturbance local density 2007',\n",
        "    'fea_disturbance_local_density_2008': 'Disturbance local density 2008',\n",
        "    'fea_disturbance_local_density_2009': 'Disturbance local density 2009',\n",
        "    'fea_disturbance_local_density_2010': 'Disturbance local density 2010',\n",
        "    'fea_disturbance_local_density_2011': 'Disturbance local density 2011',\n",
        "    'fea_disturbance_local_density_2012': 'Disturbance local density 2012',\n",
        "    'fea_disturbance_local_density_2013': 'Disturbance local density 2013',\n",
        "    'fea_disturbance_local_density_2014': 'Disturbance local density 2014',\n",
        "    'fea_disturbance_local_density_2015': 'Disturbance local density 2015',\n",
        "    'fea_disturbance_local_density_2016': 'Disturbance local density 2016',\n",
        "    'fea_disturbance_local_density_2017': 'Disturbance local density 2017',\n",
        "    'fea_disturbance_local_density_2018': 'Disturbance local density 2018',\n",
        "    'fea_disturbance_local_density_2019': 'Disturbance local density 2019',\n",
        "    'fea_disturbance_local_density_2020': 'Disturbance local density 2020',\n",
        "    'fea_disturbance_local_density_2021': 'Disturbance local density 2021',\n",
        "    'fea_disturbance_local_density_2022': 'Disturbance local density 2022',\n",
        "    'fea_disturbance_local_density_2023': 'Disturbance local density 2023',\n",
        "    'fea_forest_edge_distance_1995': 'Forest edge distance 1995',\n",
        "    'fea_forest_edge_distance_2008': 'Forest edge distance 2008',\n",
        "    'fea_forest_edge_distance_2021': 'Forest edge distance 2021',\n",
        "    'fea_forest_edge_distance_2022': 'Forest edge distance 2022',\n",
        "    'fea_forest_edge_distance_2023': 'Forest edge distance 2023',\n",
        "    'fea_forest_local_density_1995': 'Forest local density 1995',\n",
        "    'fea_forest_local_density_2008': 'Forest local density 2008',\n",
        "    'fea_forest_local_density_2021': 'Forest local density 2021',\n",
        "    'fea_forest_local_density_2022': 'Forest local density 2022',\n",
        "    'fea_forest_local_density_2023': 'Forest local density 2023',\n",
        "    'fea_latitude': 'Latitude',\n",
        "    'fea_longitude': 'Longitude',\n",
        "    'fea_lu_ais_edge_distance': 'Ais forest reserves with edge distance',\n",
        "    'fea_lu_berkelah_jerantut_edge_distance': 'Berkelah Jerantut Forest Reserve with edge distance',\n",
        "    'fea_lu_berkelah_kuantan_edge_distance': 'Berkelah Kuantan forest reserves with edge distance',\n",
        "    'fea_lu_berkelah_temerloh_edge_distance': 'Berkelah Temerloh Forest Reserve with edge distance',\n",
        "    'fea_lu_old-growth_protected_areas_edge_distance': 'Old-growth protected areas with edge distance',\n",
        "    'fea_lu_remen_chereh_edge_distance': 'Remen Chereh forest reserves ewith edge distance',\n",
        "    'fea_lu_tekai_tembeling_edge_distance': 'Tekai Tembeling forest reserves with edge distance',\n",
        "    'fea_lu_tekam_edge_distance': 'Tekam Forest Reserve edge distance',\n",
        "    'fea_lu_yong_edge_distance': 'Yong Lipis forest reserves with edge distance',\n",
        "    'fea_lu_yong_lipis_edge_distance': 'Yong forest reserves with edge distance',\n",
        "    'fea_topo_dtm_smooth_aspect_cosine': 'Aspect cosine smoothed',\n",
        "    'fea_topo_dtm_smooth_aspect_sine': 'Aspect sine smoothed',\n",
        "    'fea_topo_dtm_smooth_circular_variance_aspect_03': 'Circular variance aspect (3 pixel) smoothed',\n",
        "    'fea_topo_dtm_smooth_circular_variance_aspect_07': 'Circular variance aspect (7 pixel) smoothed',\n",
        "    'fea_topo_dtm_smooth_circular_variance_aspect_11': 'Circular variance aspect (11 pixel) smoothed',\n",
        "    'fea_topo_dtm_smooth_deviation_mean_elevation_03': 'Deviation mean elevation (3 pixel) smoothed',\n",
        "    'fea_topo_dtm_smooth_deviation_mean_elevation_07': 'Deviation mean elevation (7 pixel) smoothed',\n",
        "    'fea_topo_dtm_smooth_deviation_mean_elevation_11': 'Deviation mean elevation (11 pixel) smoothed',\n",
        "    'fea_topo_dtm_smooth_eastness': 'Eastness smoothed',\n",
        "    'fea_topo_dtm_smooth_elevation': 'Elevation smoothed',\n",
        "    'fea_topo_dtm_smooth_northness': 'Northness smoothed',\n",
        "    'fea_topo_dtm_smooth_profile_curvature': 'Profile curvature smoothed',\n",
        "    'fea_topo_dtm_smooth_roughness_03': 'Roughness (3 pixel) smoothed',\n",
        "    'fea_topo_dtm_smooth_roughness_07': 'Roughness (7 pixel) smoothed',\n",
        "    'fea_topo_dtm_smooth_roughness_11': 'Roughness (11 pixel) smoothed',\n",
        "    'fea_topo_dtm_smooth_slope': 'Slope smoothed',\n",
        "    'fea_topo_dtm_smooth_stream_power_index_log10': 'Stream power index log10 smoothed',\n",
        "    'fea_topo_dtm_smooth_surface_area_ratio': 'Surface area ratio smoothed',\n",
        "    'fea_topo_dtm_smooth_tangential_curvature': 'Tangential curvature smoothed',\n",
        "    'fea_topo_dtm_smooth_topographic_position_index_03': 'Topographic position index (3 pixel) smoothed',\n",
        "    'fea_topo_dtm_smooth_topographic_position_index_07': 'Topographic position index (7 pixel) smoothed',\n",
        "    'fea_topo_dtm_smooth_topographic_position_index_11': 'Topographic position index (11 pixel) smoothed',\n",
        "    'fea_topo_dtm_smooth_topographic_ruggedness_index': 'Topographic ruggedness index smoothed',\n",
        "    'fea_topo_dtm_smooth_topographic_wetness_index': 'Topographic wetness index smoothed',\n",
        "    'fea_topo_dtm_unsmooth_aspect_cosine': 'Aspect cosine unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_aspect_sine': 'Aspect sine unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_circular_variance_aspect_03': 'Circular variance aspect (3 pixel) unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_circular_variance_aspect_07': 'Circular variance aspect (7 pixel) unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_circular_variance_aspect_11': 'Circular variance aspect (11 pixel) unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_deviation_mean_elevation_03': 'Deviation mean elevation (3 pixel) unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_deviation_mean_elevation_07': 'Deviation mean elevation (7 pixel) unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_deviation_mean_elevation_11': 'Deviation mean elevation (11 pixel) unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_eastness': 'Eastness unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_elevation': 'Elevation unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_northness': 'Northness unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_profile_curvature': 'Profile curvature unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_roughness_03': 'Roughness (3 pixel) unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_roughness_07': 'Roughness (7 pixel) unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_roughness_11': 'Roughness (11 pixel) unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_slope': 'Slope unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_stream_power_index_log10': 'Stream power index log10 unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_surface_area_ratio': 'Surface area ratio unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_tangential_curvature': 'Tangential curvature unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_topographic_position_index_03': 'Topographic position index (3 pixel) unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_topographic_position_index_07': 'Topographic position index (7 pixel) unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_topographic_position_index_11': 'Topographic position index (11 pixel) unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_topographic_ruggedness_index': 'Topographic ruggedness index unsmoothed',\n",
        "    'fea_topo_dtm_unsmooth_topographic_wetness_index': 'Topographic wetness index unsmoothed',\n",
        "    'fea_beam': 'GEDI beam',\n",
        "    'fea_sensitivity': 'GEDI sensitivity',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QC47CltpP5XR"
      },
      "outputs": [],
      "source": [
        "# Sample model dataset to reduce computation time\n",
        "# If False, sample by number instead of percent\n",
        "sample_model_dataset_by_percent, sample_model_dataset_value = True, 100\n",
        "\n",
        "# Show how each feature's value affects its impact on predictions (with trend lines)\n",
        "generate_feature_effects = True\n",
        "\n",
        "# Polynomial degree for feature effect trendline\n",
        "poly_degree = 4\n",
        "\n",
        "# Generate full summary plot (time consuming, will generate only no_top_features if False)\n",
        "generate_summary_full_plot = False\n",
        "\n",
        "# Remove feature value outliers for plotting visualisation only (does not affect calculated curves)\n",
        "remove_outliers = True\n",
        "plot_feature_percentile_range = 99.9  # Removes 0.05% from each tail for visualisation\n",
        "\n",
        "# Number of top (most important) features to plot\n",
        "no_top_features = 20\n",
        "\n",
        "# Set non-interactive backend for better performance\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "# Verify model and create explainer\n",
        "assert os.path.exists(model_description_dir) and os.path.exists(final_model_dir), \"Model files must exist to continue.\"\n",
        "\n",
        "# Load model appropriately for SHAP\n",
        "if categorise_target:\n",
        "    # For classification, recreate the sklearn model for better SHAP compatibility\n",
        "    with open(model_description_dir) as f: model_desc = json.load(f)\n",
        "    final_params = ast.literal_eval(model_desc[\"hyperparameters\"])\n",
        "    # Ensure num_class is available for multiclass SHAP\n",
        "    if categorise_target and multiclass: final_params[\"num_class\"] = num_class\n",
        "    model = XGBPredictor(**final_params)\n",
        "    model.load_model(final_model_dir)\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "else:\n",
        "    # For regression, use booster directly\n",
        "    model = xgb.Booster()\n",
        "    model.load_model(final_model_dir)\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "print(\"Model loaded and SHAP explainer created.\")\n",
        "\n",
        "# Set up feature naming\n",
        "try: feature_name_mapping\n",
        "except NameError:\n",
        "    feature_name_mapping = {feature: feature for feature in selected_features}\n",
        "    print(\"Using default feature names.\")\n",
        "\n",
        "# Load or prepare dataset\n",
        "processed_data_path = join(shap_cache_dir, 'shap_processed_data.pkl')\n",
        "if exists(processed_data_path):\n",
        "    with open(processed_data_path, 'rb') as f:\n",
        "        X, y = pickle.load(f)\n",
        "    print(\"Processed data loaded from cache.\")\n",
        "else:\n",
        "    # Sample and prepare dataset\n",
        "    model_dataset_shap = (model_dataset.sample(frac=sample_model_dataset_value/100, random_state=1) if sample_model_dataset_by_percent\n",
        "                        else model_dataset.sample(n=sample_model_dataset_value, random_state=1))\n",
        "    X = model_dataset_shap[selected_features].copy()\n",
        "    y = model_dataset_shap[selected_target]\n",
        "\n",
        "    # Handle categorical features for SHAP\n",
        "    for col in X.select_dtypes(include=['category']).columns: X[col] = X[col].cat.codes\n",
        "    # Ensure data types are compatible with SHAP\n",
        "    if categorise_target: X = X.astype('float32')\n",
        "\n",
        "    # Clean data\n",
        "    mask = ~X.isin([np.nan, np.inf, -np.inf]).any(axis=1) & ~y.isin([np.nan, np.inf, -np.inf])\n",
        "    X, y = X[mask], y[mask]\n",
        "\n",
        "    with open(processed_data_path, 'wb') as f:\n",
        "        pickle.dump((X, y), f)\n",
        "    print(\"Data processed and cached.\")\n",
        "\n",
        "# Compute or load SHAP values\n",
        "shap_values_path = join(shap_cache_dir, 'shap_values.pkl')\n",
        "if exists(shap_values_path):\n",
        "    with open(shap_values_path, 'rb') as f:\n",
        "        shap_values = pickle.load(f)\n",
        "    print(\"SHAP values loaded from cache.\")\n",
        "else:\n",
        "    # For classification, use the explainer method that works with sklearn models\n",
        "    if categorise_target:\n",
        "        shap_values = explainer(X, check_additivity=False)\n",
        "        # Extract values if it's an Explanation object\n",
        "        if hasattr(shap_values, 'values'): shap_values = shap_values.values\n",
        "    # For regression, use the traditional method\n",
        "    else: shap_values = explainer.shap_values(X, check_additivity=False)\n",
        "    with open(shap_values_path, 'wb') as f: pickle.dump(shap_values, f)\n",
        "    print(\"SHAP values computed and cached.\")\n",
        "\n",
        "# Initial data prep\n",
        "X_renamed = X.rename(columns=feature_name_mapping).reset_index(drop=True)\n",
        "\n",
        "# Handle multiclass SHAP values (3D array: samples  features  classes)\n",
        "if shap_values.ndim == 3:\n",
        "    # For multiclass, take mean across classes to get overall feature importance\n",
        "    shap_values_2d = np.mean(shap_values, axis=2)\n",
        "    print(f\"Multiclass SHAP values aggregated across {shap_values.shape[2]} classes\")\n",
        "# For binary classification or regression, use as-is\n",
        "else: shap_values_2d = shap_values\n",
        "\n",
        "shap_values_df = pd.DataFrame(shap_values_2d, columns=X_renamed.columns)\n",
        "y = y.reset_index(drop=True)\n",
        "\n",
        "# Update shap_values for later plotting functions\n",
        "shap_values = shap_values_2d\n",
        "\n",
        "# Remove zero variance features\n",
        "zero_var_features = X_renamed.columns[X_renamed.nunique() <= 1]\n",
        "if not zero_var_features.empty:\n",
        "    print(f\"Excluding zero variance features: {zero_var_features.tolist()}\")\n",
        "    X_renamed = X_renamed.drop(columns=zero_var_features)\n",
        "    shap_values_df = shap_values_df.drop(columns=zero_var_features)\n",
        "    shap_values = shap_values_df.values\n",
        "\n",
        "# Clean data for plotting\n",
        "mask = ~X_renamed.replace([np.inf, -np.inf], np.nan).isna().any(axis=1)\n",
        "X_renamed = X_renamed[mask]\n",
        "shap_values_df = shap_values_df[mask]\n",
        "y = y[mask]\n",
        "shap_values = shap_values_df.values\n",
        "\n",
        "if X_renamed.empty or shap_values_df.empty:\n",
        "    print(\"No data available for plotting after cleaning.\")\n",
        "    exit()\n",
        "\n",
        "# Define output paths\n",
        "shap_settings_path = join(shap_dir, 'shap_settings.csv')\n",
        "feature_analysis_path = join(shap_dir, 'shap_feature_analysis.csv')\n",
        "feature_importance_path = join(shap_dir, 'shap_feature_importance.csv')\n",
        "summary_plot_top_path = join(shap_dir, 'shap_summary_plot_top')\n",
        "summary_plot_full_path = join(shap_dir, 'shap_summary_plot_full')\n",
        "feature_importance_top_path = join(shap_dir, 'shap_feature_importance_top')\n",
        "feature_importance_full_path = join(shap_dir, 'shap_feature_importance_full')\n",
        "\n",
        "# Save SHAP analysis settings\n",
        "if not exists(shap_settings_path):\n",
        "    settings_df = pd.DataFrame({\n",
        "        'Setting': [\n",
        "            'polynomial_degree',\n",
        "            'remove_outliers',\n",
        "            'plot_feature_percentile_range',\n",
        "            'sample_model_dataset_by_percent',\n",
        "            'sample_model_dataset_value',\n",
        "            'no_top_features'\n",
        "        ],\n",
        "        'Value': [\n",
        "            poly_degree,\n",
        "            remove_outliers,\n",
        "            plot_feature_percentile_range,\n",
        "            sample_model_dataset_by_percent,\n",
        "            sample_model_dataset_value,\n",
        "            no_top_features\n",
        "        ]\n",
        "    })\n",
        "    settings_df.to_csv(shap_settings_path, index=False)\n",
        "    print(\"SHAP settings saved.\")\n",
        "\n",
        "# Calculate feature importance\n",
        "importance_df = pd.DataFrame({'Feature': X_renamed.columns, 'Mean Absolute SHAP Value': np.abs(shap_values_df).mean(axis=0)})\n",
        "importance_df.sort_values('Mean Absolute SHAP Value', ascending=False, inplace=True)\n",
        "importance_df.to_csv(feature_importance_path, index=False)\n",
        "\n",
        "# Calculate feature statistics\n",
        "reverse_name_mapping = {v: k for k, v in feature_name_mapping.items()}\n",
        "if not exists(feature_analysis_path):\n",
        "    warnings.filterwarnings('ignore', message='Polyfit may be poorly conditioned')\n",
        "\n",
        "    # Pre-extract data\n",
        "    X_values = X_renamed.values\n",
        "    shap_values_array = shap_values_df.values\n",
        "\n",
        "    # Calculate relationship statistics and neutral effect values for each feature\n",
        "    feature_stats = []\n",
        "    for feature in X_renamed.columns:\n",
        "        feature_idx = X_renamed.columns.get_loc(feature)\n",
        "        x = X_values[:, feature_idx]\n",
        "        y_shap = shap_values_array[:, feature_idx]\n",
        "\n",
        "        x_mean, x_scale = np.mean(x), np.std(x)\n",
        "\n",
        "        # Normalize for stable fitting\n",
        "        if x_scale != 0:\n",
        "            x_norm = (x - x_mean) / x_scale\n",
        "\n",
        "            # Linear fit\n",
        "            lin_coef = np.polyfit(x_norm, y_shap, 1)\n",
        "            lin_pred = np.polyval(lin_coef, x_norm)\n",
        "            ss_tot = np.sum((y_shap - np.mean(y_shap))**2)\n",
        "            lin_r2 = 1 - (np.sum((y_shap - lin_pred)**2) / ss_tot) if ss_tot > 0 else 0.0\n",
        "\n",
        "            # Polynomial fit\n",
        "            poly_coef = np.polyfit(x_norm, y_shap, poly_degree)\n",
        "            poly_pred = np.polyval(poly_coef, x_norm)\n",
        "            poly_r2 = 1 - (np.sum((y_shap - poly_pred)**2) / ss_tot) if ss_tot > 0 else 0.0\n",
        "\n",
        "            # Determine predominant direction based on percentage of positive effects\n",
        "            pct_positive = (y_shap > 0).sum() / len(y_shap) * 100\n",
        "            direction = \"Positive\" if pct_positive > 70 else \"Negative\" if pct_positive < 30 else \"Mixed\"\n",
        "\n",
        "            # Calculate neutral effect value: feature value where mean SHAP impact is minimal\n",
        "            # Used for setting covariate features to constant values during spatial predictions\n",
        "            # e.g. when creating raster outputs where certain features must be held constant\n",
        "            # Binning averages SHAP values locally to find robust neutral point\n",
        "            # Single-sample approach unreliable due to outliers and feature interactions\n",
        "            n_unique = len(np.unique(x))\n",
        "            if n_unique <= 20:\n",
        "                # Categorical or low-cardinality: bin by unique value directly\n",
        "                unique_vals = np.unique(x)\n",
        "                bin_means = unique_vals.astype(float)\n",
        "                bin_shaps = [y_shap[x == v].mean() for v in unique_vals]\n",
        "                n_bins = n_unique\n",
        "            else:\n",
        "                # Continuous: quantile-based binning with adaptive bin count (sqrt of n)\n",
        "                # Equal sample distribution per bin prevents unreliable estimates in tails\n",
        "                n_bins = int(np.sqrt(len(x)))\n",
        "                bin_edges = np.percentile(x, np.linspace(0, 100, n_bins + 1))\n",
        "                bin_indices = np.clip(np.digitize(x, bin_edges), 1, n_bins)\n",
        "\n",
        "                bin_means, bin_shaps = [], []\n",
        "                for i in range(1, n_bins + 1):\n",
        "                    mask = bin_indices == i\n",
        "                    if mask.sum() > 0:\n",
        "                        bin_means.append(x[mask].mean())\n",
        "                        bin_shaps.append(y_shap[mask].mean())\n",
        "\n",
        "            # Find bin where mean SHAP closest to zero\n",
        "            if len(bin_shaps) > 0:\n",
        "                neutral_idx = np.argmin(np.abs(bin_shaps))\n",
        "                neutral_value = bin_means[neutral_idx]\n",
        "                neutral_shap = bin_shaps[neutral_idx]\n",
        "            else:\n",
        "                neutral_value, neutral_shap = np.median(x), 0.0\n",
        "        else:\n",
        "            # Zero variance feature\n",
        "            lin_coef, poly_coef = np.array([0, 0]), np.zeros(poly_degree + 1)\n",
        "            lin_r2, poly_r2 = 0.0, 0.0\n",
        "            direction = \"Mixed\"\n",
        "            neutral_value, neutral_shap = x_mean, 0.0\n",
        "            n_bins = 0\n",
        "\n",
        "        feature_stats.append({\n",
        "            'Feature': feature,\n",
        "            'Dataset name': reverse_name_mapping.get(feature, feature),\n",
        "            'Feature_Mean': x_mean,\n",
        "            'Feature_Std': x_scale,\n",
        "            'Linear_Coefficient': lin_coef[0],\n",
        "            'Linear_R2': lin_r2,\n",
        "            'Polynomial_Coefficients': ','.join(map(str, poly_coef)),\n",
        "            'Polynomial_R2': poly_r2,\n",
        "            'Predominant_Direction': direction,\n",
        "            'Neutral_Effect_Value': neutral_value,\n",
        "            'SHAP_at_Neutral': neutral_shap,\n",
        "            'Neutral_Effect_Bins': n_bins\n",
        "        })\n",
        "\n",
        "    # Save feature statistics sorted alphabetically by feature name\n",
        "    stats_df = pd.DataFrame(feature_stats)\n",
        "    stats_df.sort_values('Feature', inplace=True)\n",
        "    stats_df.to_csv(feature_analysis_path, index=False)\n",
        "    print(\"Feature relationship statistics and neutral effect values generated.\")\n",
        "else:\n",
        "    print(\"Feature statistics already exist.\")\n",
        "\n",
        "# Generate top feature importance plot\n",
        "if not exists(f\"{feature_importance_top_path}.png\") or not exists(f\"{feature_importance_top_path}.svg\"):\n",
        "   shap.summary_plot(shap_values, X_renamed, plot_type=\"bar\", show=False, max_display=no_top_features)\n",
        "   plt.gcf().set_size_inches(12, 0.4 * no_top_features)\n",
        "   plt.tight_layout()\n",
        "   if not exists(f\"{feature_importance_top_path}.png\"):\n",
        "       plt.savefig(f\"{feature_importance_top_path}.png\", bbox_inches='tight', dpi=150, pad_inches=0.1)\n",
        "   if not exists(f\"{feature_importance_top_path}.svg\"):\n",
        "       plt.savefig(f\"{feature_importance_top_path}.svg\", bbox_inches='tight', dpi=150, pad_inches=0.1, format='svg')\n",
        "   plt.close()\n",
        "   print(\"Top feature importance plot created.\")\n",
        "else: print(\"Top feature importance plot already exists.\")\n",
        "\n",
        "# Generate full feature importance plot\n",
        "if not exists(f\"{feature_importance_full_path}.png\") or not exists(f\"{feature_importance_full_path}.svg\"):\n",
        "   shap.summary_plot(shap_values, X_renamed, plot_type=\"bar\", show=False, max_display=len(X_renamed.columns))\n",
        "   plt.gcf().set_size_inches(12, 0.4 * len(X_renamed.columns))\n",
        "   plt.tight_layout()\n",
        "   if not exists(f\"{feature_importance_full_path}.png\"):\n",
        "       plt.savefig(f\"{feature_importance_full_path}.png\", bbox_inches='tight', dpi=150, pad_inches=0.1)\n",
        "   if not exists(f\"{feature_importance_full_path}.svg\"):\n",
        "       plt.savefig(f\"{feature_importance_full_path}.svg\", bbox_inches='tight', dpi=150, pad_inches=0.1, format='svg')\n",
        "   plt.close()\n",
        "   print(\"Full feature importance plot created.\")\n",
        "else: print(\"Full feature importance plot already exists.\")\n",
        "\n",
        "# Generate top summary plot\n",
        "if not exists(f\"{summary_plot_top_path}.png\") or not exists(f\"{summary_plot_top_path}.svg\"):\n",
        "   shap.summary_plot(shap_values, X_renamed, show=False, max_display=no_top_features)\n",
        "   plt.gcf().set_size_inches(12, 0.4 * no_top_features)\n",
        "   plt.tight_layout()\n",
        "   if not exists(f\"{summary_plot_top_path}.png\"):\n",
        "       plt.savefig(f\"{summary_plot_top_path}.png\", bbox_inches='tight', dpi=150, pad_inches=0.1)\n",
        "   if not exists(f\"{summary_plot_top_path}.svg\"):\n",
        "       plt.savefig(f\"{summary_plot_top_path}.svg\", bbox_inches='tight', dpi=150, pad_inches=0.1, format='svg')\n",
        "   plt.close()\n",
        "   print(\"Top summary plot created.\")\n",
        "else: print(\"Top summary plot already exists.\")\n",
        "\n",
        "# Generate full summary plot\n",
        "if generate_summary_full_plot:\n",
        "  if not exists(f\"{summary_plot_full_path}.png\") or not exists(f\"{summary_plot_full_path}.svg\"):\n",
        "    shap.summary_plot(shap_values, X_renamed, show=False, max_display=len(X_renamed.columns))\n",
        "    plt.gcf().set_size_inches(12, 0.4 * len(X_renamed.columns))\n",
        "    plt.tight_layout()\n",
        "    if not exists(f\"{summary_plot_full_path}.png\"):\n",
        "        plt.savefig(f\"{summary_plot_full_path}.png\", bbox_inches='tight', dpi=150, pad_inches=0.1)\n",
        "    if not exists(f\"{summary_plot_full_path}.svg\"):\n",
        "        plt.savefig(f\"{summary_plot_full_path}.svg\", bbox_inches='tight', dpi=150, pad_inches=0.1, format='svg')\n",
        "    plt.close()\n",
        "    print(\"Full summary plot created.\")\n",
        "  else: print(\"Full summary plot already exists.\")\n",
        "\n",
        "# Generate feature effect plots using statistics from feature analysis CSV\n",
        "if generate_feature_effects:\n",
        "   # Load feature statistics containing coefficients and neutral effect values\n",
        "   feature_stats_df = pd.read_csv(feature_analysis_path)\n",
        "\n",
        "   # Pre-extract data for plotting\n",
        "   X_values = X_renamed.values\n",
        "   shap_values_array = shap_values_df.values\n",
        "   feature_names = importance_df['Feature'].tolist()\n",
        "\n",
        "   for feature in feature_names:\n",
        "       feature_effect_png_path = join(shap_feature_effect_plots_dir, f'feature_effect_{re.sub(r'[<>:\"/\\\\|?*]', '_', feature)}.png')\n",
        "       feature_effect_svg_path = join(shap_feature_effect_plots_dir, f'feature_effect_{re.sub(r'[<>:\"/\\\\|?*]', '_', feature)}.svg')\n",
        "       if not exists(feature_effect_png_path) or not exists(feature_effect_svg_path):\n",
        "          # Get feature data\n",
        "          feature_idx = X_renamed.columns.get_loc(feature)\n",
        "          feature_values = X_values[:, feature_idx]\n",
        "          shap_values_feature = shap_values_array[:, feature_idx]\n",
        "\n",
        "          # Get feature statistics for trend lines and neutral effect value from alphabetically sorted CSV\n",
        "          feature_row = feature_stats_df[feature_stats_df['Feature'] == feature].iloc[0]\n",
        "          x_mean = feature_row['Feature_Mean']\n",
        "          x_scale = feature_row['Feature_Std']\n",
        "          neutral_value = feature_row['Neutral_Effect_Value']\n",
        "\n",
        "          # Parse polynomial coefficients from CSV\n",
        "          poly_coef = np.array([float(c) for c in feature_row['Polynomial_Coefficients'].split(',')])\n",
        "          lin_coef = np.array([feature_row['Linear_Coefficient'], 0])\n",
        "\n",
        "          # Determine x range for plotting with optional outlier removal for visualisation\n",
        "          if remove_outliers:\n",
        "              lower = np.percentile(feature_values, (100 - plot_feature_percentile_range) / 2)\n",
        "              upper = np.percentile(feature_values, 100 - (100 - plot_feature_percentile_range) / 2)\n",
        "              mask = (feature_values >= lower) & (feature_values <= upper)\n",
        "              feature_values_plot = feature_values[mask]\n",
        "              shap_values_plot = shap_values_feature[mask]\n",
        "              x_range = np.linspace(lower, upper, 200)\n",
        "          else:\n",
        "              feature_values_plot = feature_values\n",
        "              shap_values_plot = shap_values_feature\n",
        "              x_range = np.linspace(feature_values.min(), feature_values.max(), 200)\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(10, 6))\n",
        "          ax.scatter(feature_values_plot, shap_values_plot, alpha=0.5, rasterized=True)\n",
        "\n",
        "          # Draw trend lines using coefficients from statistics CSV\n",
        "          if x_scale != 0:\n",
        "              x_range_norm = (x_range - x_mean) / x_scale\n",
        "\n",
        "              # Linear trend\n",
        "              lin_pred = np.polyval(lin_coef, x_range_norm)\n",
        "              ax.plot(x_range, lin_pred, \"--\", color='black', alpha=0.8, label='Linear trend')\n",
        "\n",
        "              # Polynomial trend\n",
        "              poly_pred = np.polyval(poly_coef, x_range_norm)\n",
        "              ax.plot(x_range, poly_pred, \"--\", color='red', alpha=0.8, label=f'Polynomial trend (degree {poly_degree})')\n",
        "\n",
        "          # Add neutral effect line from statistics CSV\n",
        "          ax.axvline(x=neutral_value, color='blue', linestyle=':', linewidth=2, alpha=0.8, label='Neutral effect value')\n",
        "\n",
        "          ax.set_title(f'SHAP Feature Effect: {feature}')\n",
        "          ax.set_xlabel(f'{feature} value')\n",
        "          ax.set_ylabel('SHAP value (impact on prediction)')\n",
        "          ax.legend(loc='upper right')\n",
        "          if not exists(feature_effect_png_path):\n",
        "            plt.savefig(feature_effect_png_path, bbox_inches='tight', dpi=150, pad_inches=0.1)\n",
        "          if not exists(feature_effect_svg_path):\n",
        "            plt.savefig(feature_effect_svg_path, bbox_inches='tight', dpi=150, pad_inches=0.1, format='svg')\n",
        "          plt.close()\n",
        "   print(\"Feature effect plots generated.\")\n",
        "\n",
        "print(\"Analysis complete. All plots and statistics have been generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coGZAB3Rl9Yd"
      },
      "source": [
        "## Combine features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wO0NFCwbl_Az"
      },
      "outputs": [],
      "source": [
        "# Modify these rules to control feature combination behavior\n",
        "# Run after the main SHAP calculations (uses the cache)\n",
        "\n",
        "# Automatic smoothed/unsmoothed detection\n",
        "# Detects and combines features with both smoothed and unsmoothed versions\n",
        "shap_smoothed_combined_dir = join(model_dataset_dir, 'shap_smoothed_combined')\n",
        "combine_smoothed_unsmoothed = True\n",
        "generate_smoothed_importance_plots = True  # bar plots showing feature importance\n",
        "generate_smoothed_summary_plots = True     # scatter plots showing feature effects\n",
        "generate_smoothed_summary_full_plot = False # Time consuming, will generate only no_top_features if False\n",
        "generate_smoothed_effect_plots = True      # individual feature effect plots with trend lines\n",
        "\n",
        "# Conceptual combinations\n",
        "# Sums SHAP values for features containing specified patterns\n",
        "generate_conceptual_combinations = True\n",
        "\n",
        "# Each combination creates a numbered directory with saved grouping rules\n",
        "conceptual_combinations_list = [\n",
        "    {\n",
        "        'Topographic elevation': ['elevation'],\n",
        "        'Topographic slope': ['slope'],\n",
        "        'Topographic surface area ratio': ['surface_area_ratio'],\n",
        "        'Topographic orientation': ['aspect_cosine', 'aspect_sine', 'northness', 'eastness'],\n",
        "        'Topographic shape': ['profile_curvature', 'tangential_curvature', 'topographic_position_index'],\n",
        "        'Topographic complexity': ['roughness', 'topographic_ruggedness_index', 'circular_variance_aspect', 'deviation_mean_elevation'],\n",
        "        'Hydrography': ['stream_power_index', 'topographic_wetness_index'],\n",
        "        'Disturbance with edge effects': ['disturbance_edge_distance', 'disturbance_local_density'],\n",
        "        'Forest edge effects': ['forest_edge_distance', 'forest_local_density'],\n",
        "        'Old-growth': ['old-growth'],\n",
        "        'Forest reserve management': ['yong', 'tekai', 'tekam', 'remen', 'berkelah', 'ais'],\n",
        "        'Geographic location': ['coast', 'latitude', 'longitude'],\n",
        "        'GEDI parameters': ['beam', 'sensitivity'],\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "F7Vhfm9J-SFZ"
      },
      "outputs": [],
      "source": [
        "# Validate conceptual combinations\n",
        "for combo_idx, conceptual_combinations in enumerate(conceptual_combinations_list, 1):\n",
        "    print(f\"Validating combination set {combo_idx}:\")\n",
        "\n",
        "    all_original_features = list(feature_name_mapping.keys())\n",
        "    available_features = list(feature_name_mapping.values())\n",
        "\n",
        "    covered_features = set()\n",
        "    unmatched_patterns = []\n",
        "\n",
        "    for concept_name, patterns in conceptual_combinations.items():\n",
        "        concept_matches = set()\n",
        "        for pattern in patterns:\n",
        "            pattern_matches = [orig_name for orig_name in all_original_features if pattern in orig_name]\n",
        "            if not pattern_matches:\n",
        "                unmatched_patterns.append((concept_name, pattern))\n",
        "            else:\n",
        "                renamed_matches = [feature_name_mapping[orig] for orig in pattern_matches\n",
        "                                 if feature_name_mapping[orig] in available_features]\n",
        "                concept_matches.update(renamed_matches)\n",
        "        covered_features.update(concept_matches)\n",
        "        print(f\"  {concept_name}: {len(concept_matches)} features\")\n",
        "\n",
        "    # Report unmatched patterns\n",
        "    if unmatched_patterns:\n",
        "        print(f\"  Unmatched patterns: {len(unmatched_patterns)}\")\n",
        "        for concept_name, pattern in unmatched_patterns:\n",
        "            print(f\"    {concept_name} -> {pattern}\")\n",
        "\n",
        "    # Report uncovered features\n",
        "    uncovered_features = set(available_features) - covered_features\n",
        "    if uncovered_features:\n",
        "        print(f\"  Uncovered features: {len(uncovered_features)}\")\n",
        "        for feature in sorted(uncovered_features):\n",
        "            print(f\"    {feature}\")\n",
        "\n",
        "    # Coverage summary\n",
        "    coverage_pct = (len(covered_features) / len(available_features)) * 100 if available_features else 0\n",
        "    print(f\"  Coverage: {len(covered_features)}/{len(available_features)} features ({coverage_pct:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbncZXW5mBHR"
      },
      "outputs": [],
      "source": [
        "# Polynomial degree for feature effect trendline\n",
        "poly_degree = 4\n",
        "\n",
        "# Remove feature value outliers for plotting visualisation only (does not affect calculated curves)\n",
        "remove_outliers = True\n",
        "plot_feature_percentile_range = 99.9  # Removes 0.05% from each tail for visualisation\n",
        "\n",
        "# Number of top (most important) features to plot\n",
        "no_top_features = 20\n",
        "\n",
        "# Set non-interactive backend for better performance\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "# Define cache paths\n",
        "shap_dir = join(model_dataset_dir, 'shap')\n",
        "shap_cache_dir = join(shap_dir, 'shap_cache')\n",
        "processed_data_path = join(shap_cache_dir, 'shap_processed_data.pkl')\n",
        "shap_values_path = join(shap_cache_dir, 'shap_values.pkl')\n",
        "\n",
        "# Create directories for combined analyses\n",
        "os.makedirs(shap_smoothed_combined_dir, exist_ok=True)\n",
        "\n",
        "# Load cached data - no recomputation required\n",
        "with open(processed_data_path, 'rb') as f:\n",
        "    X_original, y_original = pickle.load(f)\n",
        "with open(shap_values_path, 'rb') as f:\n",
        "    shap_values_original = pickle.load(f)\n",
        "\n",
        "print(\"Cached data loaded for feature combination analysis.\")\n",
        "\n",
        "# Handle multiclass SHAP values for combination analysis\n",
        "if shap_values_original.ndim == 3:\n",
        "    shap_values_2d_original = np.mean(shap_values_original, axis=2)\n",
        "else:\n",
        "    shap_values_2d_original = shap_values_original\n",
        "\n",
        "# Apply feature name mapping and prepare base datasets\n",
        "X_renamed_original = X_original.rename(columns=feature_name_mapping).reset_index(drop=True)\n",
        "shap_values_df_original = pd.DataFrame(shap_values_2d_original, columns=X_renamed_original.columns)\n",
        "y_reset = y_original.reset_index(drop=True)\n",
        "\n",
        "# Remove zero variance features\n",
        "zero_var_features = X_renamed_original.columns[X_renamed_original.nunique() <= 1]\n",
        "if not zero_var_features.empty:\n",
        "    X_renamed_original = X_renamed_original.drop(columns=zero_var_features)\n",
        "    shap_values_df_original = shap_values_df_original.drop(columns=zero_var_features)\n",
        "\n",
        "# Clean data for combination analysis\n",
        "mask = ~X_renamed_original.replace([np.inf, -np.inf], np.nan).isna().any(axis=1)\n",
        "X_clean = X_renamed_original[mask].reset_index(drop=True)\n",
        "shap_clean = shap_values_df_original[mask].reset_index(drop=True)\n",
        "y_clean = y_reset[mask].reset_index(drop=True)\n",
        "\n",
        "if combine_smoothed_unsmoothed:\n",
        "    print(\"Processing smoothed/unsmoothed feature combinations...\")\n",
        "\n",
        "    # Detect smoothed/unsmoothed pairs automatically by removing smoothing indicators\n",
        "    feature_base_names = {}\n",
        "    for feature in X_clean.columns:\n",
        "        base_name = feature\n",
        "        is_smoothed = base_name.endswith(' smoothed')\n",
        "        is_unsmoothed = base_name.endswith(' unsmoothed')\n",
        "\n",
        "        if is_smoothed:\n",
        "            base_name = base_name.replace(' smoothed', '')\n",
        "        elif is_unsmoothed:\n",
        "            base_name = base_name.replace(' unsmoothed', '')\n",
        "        else:\n",
        "            continue  # Skip features without smoothing indicators\n",
        "\n",
        "        if base_name not in feature_base_names:\n",
        "            feature_base_names[base_name] = {}\n",
        "\n",
        "        if is_smoothed:\n",
        "            feature_base_names[base_name]['smoothed'] = feature\n",
        "        elif is_unsmoothed:\n",
        "            feature_base_names[base_name]['unsmoothed'] = feature\n",
        "\n",
        "    # Identify complete pairs with both smoothed and unsmoothed versions\n",
        "    smoothed_unsmoothed_pairs = {base: features for base, features in feature_base_names.items()\n",
        "                                if 'smoothed' in features and 'unsmoothed' in features}\n",
        "\n",
        "    print(f\"Found {len(smoothed_unsmoothed_pairs)} smoothed/unsmoothed pairs.\")\n",
        "\n",
        "    # Create combined datasets by averaging feature values and summing SHAP values\n",
        "    X_smoothed_combined = X_clean.copy()\n",
        "    shap_smoothed_combined = shap_clean.copy()\n",
        "\n",
        "    # Add combined features and remove individual smoothed/unsmoothed features\n",
        "    features_to_remove = []\n",
        "    for base_name, feature_pair in smoothed_unsmoothed_pairs.items():\n",
        "        smoothed_feature = feature_pair['smoothed']\n",
        "        unsmoothed_feature = feature_pair['unsmoothed']\n",
        "\n",
        "        # Average feature values - best estimate of underlying concept\n",
        "        X_smoothed_combined[base_name] = (X_clean[smoothed_feature] + X_clean[unsmoothed_feature]) / 2\n",
        "\n",
        "        # Sum SHAP values - total contribution of concept to predictions\n",
        "        shap_smoothed_combined[base_name] = shap_clean[smoothed_feature] + shap_clean[unsmoothed_feature]\n",
        "\n",
        "        features_to_remove.extend([smoothed_feature, unsmoothed_feature])\n",
        "\n",
        "    # Remove original smoothed/unsmoothed features\n",
        "    X_smoothed_combined = X_smoothed_combined.drop(columns=features_to_remove)\n",
        "    shap_smoothed_combined = shap_smoothed_combined.drop(columns=features_to_remove)\n",
        "\n",
        "    # Calculate importance for combined features\n",
        "    importance_smoothed_df = pd.DataFrame({\n",
        "        'Feature': X_smoothed_combined.columns,\n",
        "        'Mean Absolute SHAP Value': np.abs(shap_smoothed_combined).mean(axis=0)\n",
        "    })\n",
        "    importance_smoothed_df.sort_values('Mean Absolute SHAP Value', ascending=False, inplace=True)\n",
        "\n",
        "    # Define output paths and subdirectories for smoothed combinations\n",
        "    smoothed_shap_settings_path = join(shap_smoothed_combined_dir, 'shap_settings.csv')\n",
        "    smoothed_feature_analysis_path = join(shap_smoothed_combined_dir, 'shap_feature_analysis.csv')\n",
        "    smoothed_feature_importance_path = join(shap_smoothed_combined_dir, 'shap_feature_importance.csv')\n",
        "    smoothed_summary_plot_top_path = join(shap_smoothed_combined_dir, 'shap_summary_plot_top')\n",
        "    smoothed_summary_plot_full_path = join(shap_smoothed_combined_dir, 'shap_summary_plot_full')\n",
        "    smoothed_feature_importance_top_path = join(shap_smoothed_combined_dir, 'shap_feature_importance_top')\n",
        "    smoothed_feature_importance_full_path = join(shap_smoothed_combined_dir, 'shap_feature_importance_full')\n",
        "    smoothed_feature_effect_plots_dir = join(shap_smoothed_combined_dir, 'feature_effect_plots')\n",
        "\n",
        "    if generate_smoothed_effect_plots:\n",
        "        os.makedirs(smoothed_feature_effect_plots_dir, exist_ok=True)\n",
        "\n",
        "    # Save SHAP analysis settings for smoothed combinations\n",
        "    if not exists(smoothed_shap_settings_path):\n",
        "        settings_df = pd.DataFrame({\n",
        "            'Setting': [\n",
        "                'polynomial_degree',\n",
        "                'remove_outliers',\n",
        "                'plot_feature_percentile_range',\n",
        "                'no_top_features'\n",
        "            ],\n",
        "            'Value': [\n",
        "                poly_degree,\n",
        "                remove_outliers,\n",
        "                plot_feature_percentile_range,\n",
        "                no_top_features\n",
        "            ]\n",
        "        })\n",
        "        settings_df.to_csv(smoothed_shap_settings_path, index=False)\n",
        "        print(\"Smoothed combinations: SHAP settings saved.\")\n",
        "\n",
        "    # Save feature importance for smoothed combinations\n",
        "    importance_smoothed_df.to_csv(smoothed_feature_importance_path, index=False)\n",
        "\n",
        "    # Calculate comprehensive feature statistics including neutral effect values\n",
        "    if not exists(smoothed_feature_analysis_path):\n",
        "        warnings.filterwarnings('ignore', message='Polyfit may be poorly conditioned')\n",
        "\n",
        "        # Pre-extract data\n",
        "        X_smoothed_values = X_smoothed_combined.values\n",
        "        shap_smoothed_array = shap_smoothed_combined.values\n",
        "\n",
        "        # Calculate relationship statistics and neutral effect values for each combined feature\n",
        "        feature_stats = []\n",
        "        for feature in X_smoothed_combined.columns:\n",
        "            feature_idx = X_smoothed_combined.columns.get_loc(feature)\n",
        "            x = X_smoothed_values[:, feature_idx]\n",
        "            y_shap = shap_smoothed_array[:, feature_idx]\n",
        "\n",
        "            # Normalize for stable fitting\n",
        "            x_scale = np.std(x)\n",
        "            if x_scale != 0:\n",
        "                x_mean = np.mean(x)\n",
        "                x_norm = (x - x_mean) / x_scale\n",
        "\n",
        "                # Linear fit\n",
        "                lin_coef = np.polyfit(x_norm, y_shap, 1)\n",
        "                lin_pred = np.polyval(lin_coef, x_norm)\n",
        "                ss_tot = np.sum((y_shap - np.mean(y_shap))**2)\n",
        "                lin_r2 = 1 - (np.sum((y_shap - lin_pred)**2) / ss_tot) if ss_tot > 0 else 0.0\n",
        "\n",
        "                # Polynomial fit\n",
        "                poly_coef = np.polyfit(x_norm, y_shap, poly_degree)\n",
        "                poly_pred = np.polyval(poly_coef, x_norm)\n",
        "                poly_r2 = 1 - (np.sum((y_shap - poly_pred)**2) / ss_tot) if ss_tot > 0 else 0.0\n",
        "\n",
        "                # Determine predominant direction based on percentage of positive effects\n",
        "                pct_positive = (y_shap > 0).sum() / len(y_shap) * 100\n",
        "                if pct_positive > 70:\n",
        "                    direction = \"Positive\"\n",
        "                elif pct_positive < 30:\n",
        "                    direction = \"Negative\"\n",
        "                else:\n",
        "                    direction = \"Mixed\"\n",
        "\n",
        "                # Calculate neutral effect value using adaptive quantile-based binning\n",
        "                # Neutral effect value: feature value where average SHAP impact approaches zero\n",
        "                # Useful for setting covariate features to fixed neutral values during spatial predictions\n",
        "                # e.g. when creating raster outputs where certain features must be held constant\n",
        "                # Binning approach averages SHAP values within local neighbourhoods to find robust neutral point\n",
        "                # Alternative single-sample approach unreliable due to outliers and feature interactions\n",
        "                # Adaptive bin count scales with sample size (sqrt of n) to balance resolution and reliability\n",
        "                # Quantile-based bins ensure equal sample distribution across bins regardless of feature distribution\n",
        "                # Prevents sparse bins in tail regions that would produce unreliable estimates\n",
        "                n_bins = int(np.sqrt(len(x)))\n",
        "                bin_edges = np.percentile(x, np.linspace(0, 100, n_bins + 1))\n",
        "                bin_indices = np.digitize(x, bin_edges)\n",
        "\n",
        "                bin_means = []\n",
        "                bin_shaps = []\n",
        "                bin_counts = []\n",
        "                for i in range(1, len(bin_edges)):\n",
        "                    mask = bin_indices == i\n",
        "                    count = mask.sum()\n",
        "                    if count > 0:\n",
        "                        bin_means.append(x[mask].mean())\n",
        "                        bin_shaps.append(y_shap[mask].mean())\n",
        "                        bin_counts.append(count)\n",
        "\n",
        "                # Find bin where mean SHAP closest to zero\n",
        "                if len(bin_shaps) > 0:\n",
        "                    neutral_idx = np.argmin(np.abs(bin_shaps))\n",
        "                    neutral_value = bin_means[neutral_idx]\n",
        "                    neutral_shap = bin_shaps[neutral_idx]\n",
        "                else:\n",
        "                    neutral_value = np.median(x)\n",
        "                    neutral_shap = 0.0\n",
        "            else:\n",
        "                # Zero variance feature\n",
        "                x_mean = np.mean(x)\n",
        "                lin_coef = np.array([0, 0])\n",
        "                poly_coef = np.zeros(poly_degree + 1)\n",
        "                lin_r2 = 0\n",
        "                poly_r2 = 0\n",
        "                direction = \"Mixed\"\n",
        "                neutral_value = x_mean\n",
        "                neutral_shap = 0.0\n",
        "                n_bins = 0\n",
        "\n",
        "            feature_stats.append({\n",
        "                'Feature': feature,\n",
        "                'Feature_Mean': x_mean,\n",
        "                'Feature_Std': x_scale,\n",
        "                'Linear_Coefficient': lin_coef[0],\n",
        "                'Linear_R2': lin_r2,\n",
        "                'Polynomial_Coefficients': ','.join(map(str, poly_coef)),\n",
        "                'Polynomial_R2': poly_r2,\n",
        "                'Predominant_Direction': direction,\n",
        "                'Neutral_Effect_Value': neutral_value,\n",
        "                'SHAP_at_Neutral': neutral_shap,\n",
        "                'Neutral_Effect_Bins': n_bins\n",
        "            })\n",
        "\n",
        "        # Save feature statistics sorted alphabetically by feature name\n",
        "        stats_df = pd.DataFrame(feature_stats)\n",
        "        stats_df.sort_values('Feature', inplace=True)\n",
        "        stats_df.to_csv(smoothed_feature_analysis_path, index=False)\n",
        "        print(\"Smoothed combinations: feature relationship statistics and neutral effect values generated.\")\n",
        "    else:\n",
        "        print(\"Smoothed combinations: feature statistics already exist.\")\n",
        "\n",
        "    # Convert to arrays for plotting\n",
        "    shap_values_smoothed_array = shap_smoothed_combined.values\n",
        "\n",
        "    # Generate importance plots for smoothed combinations\n",
        "    if generate_smoothed_importance_plots:\n",
        "        if not exists(f\"{smoothed_feature_importance_top_path}.png\") or not exists(f\"{smoothed_feature_importance_top_path}.svg\"):\n",
        "            shap.summary_plot(shap_values_smoothed_array, X_smoothed_combined, plot_type=\"bar\", show=False, max_display=no_top_features)\n",
        "            plt.gcf().set_size_inches(12, 0.4 * no_top_features)\n",
        "            plt.tight_layout()\n",
        "            if not exists(f\"{smoothed_feature_importance_top_path}.png\"):\n",
        "                plt.savefig(f\"{smoothed_feature_importance_top_path}.png\", bbox_inches='tight', dpi=150, pad_inches=0.1)\n",
        "            if not exists(f\"{smoothed_feature_importance_top_path}.svg\"):\n",
        "                plt.savefig(f\"{smoothed_feature_importance_top_path}.svg\", bbox_inches='tight', dpi=150, pad_inches=0.1, format='svg')\n",
        "            plt.close()\n",
        "            print(\"Smoothed combinations: top feature importance plot created.\")\n",
        "\n",
        "        if not exists(f\"{smoothed_feature_importance_full_path}.png\") or not exists(f\"{smoothed_feature_importance_full_path}.svg\"):\n",
        "            shap.summary_plot(shap_values_smoothed_array, X_smoothed_combined, plot_type=\"bar\", show=False, max_display=len(X_smoothed_combined.columns))\n",
        "            plt.gcf().set_size_inches(12, 0.4 * len(X_smoothed_combined.columns))\n",
        "            plt.tight_layout()\n",
        "            if not exists(f\"{smoothed_feature_importance_full_path}.png\"):\n",
        "                plt.savefig(f\"{smoothed_feature_importance_full_path}.png\", bbox_inches='tight', dpi=150, pad_inches=0.1)\n",
        "            if not exists(f\"{smoothed_feature_importance_full_path}.svg\"):\n",
        "                plt.savefig(f\"{smoothed_feature_importance_full_path}.svg\", bbox_inches='tight', dpi=150, pad_inches=0.1, format='svg')\n",
        "            plt.close()\n",
        "            print(\"Smoothed combinations: full feature importance plot created.\")\n",
        "\n",
        "    # Generate summary plots for smoothed combinations\n",
        "    if generate_smoothed_summary_plots:\n",
        "        if not exists(f\"{smoothed_summary_plot_top_path}.png\") or not exists(f\"{smoothed_summary_plot_top_path}.svg\"):\n",
        "            shap.summary_plot(shap_values_smoothed_array, X_smoothed_combined, show=False, max_display=no_top_features)\n",
        "            plt.gcf().set_size_inches(12, 0.4 * no_top_features)\n",
        "            plt.tight_layout()\n",
        "            if not exists(f\"{smoothed_summary_plot_top_path}.png\"):\n",
        "                plt.savefig(f\"{smoothed_summary_plot_top_path}.png\", bbox_inches='tight', dpi=150, pad_inches=0.1)\n",
        "            if not exists(f\"{smoothed_summary_plot_top_path}.svg\"):\n",
        "                plt.savefig(f\"{smoothed_summary_plot_top_path}.svg\", bbox_inches='tight', dpi=150, pad_inches=0.1, format='svg')\n",
        "            plt.close()\n",
        "            print(\"Smoothed combinations: top summary plot created.\")\n",
        "        if generate_smoothed_summary_full_plot:\n",
        "            if not exists(f\"{smoothed_summary_plot_full_path}.png\") or not exists(f\"{smoothed_summary_plot_full_path}.svg\"):\n",
        "                shap.summary_plot(shap_values_smoothed_array, X_smoothed_combined, show=False, max_display=len(X_smoothed_combined.columns))\n",
        "                plt.gcf().set_size_inches(12, 0.4 * len(X_smoothed_combined.columns))\n",
        "                plt.tight_layout()\n",
        "                if not exists(f\"{smoothed_summary_plot_full_path}.png\"):\n",
        "                    plt.savefig(f\"{smoothed_summary_plot_full_path}.png\", bbox_inches='tight', dpi=150, pad_inches=0.1)\n",
        "                if not exists(f\"{smoothed_summary_plot_full_path}.svg\"):\n",
        "                    plt.savefig(f\"{smoothed_summary_plot_full_path}.svg\", bbox_inches='tight', dpi=150, pad_inches=0.1, format='svg')\n",
        "                plt.close()\n",
        "                print(\"Smoothed combinations: full summary plot created.\")\n",
        "\n",
        "    # Generate feature effect plots using statistics from feature analysis CSV\n",
        "    if generate_smoothed_effect_plots:\n",
        "        # Load feature statistics containing coefficients and neutral effect values\n",
        "        feature_stats_df = pd.read_csv(smoothed_feature_analysis_path)\n",
        "\n",
        "        # Pre-extract data for plotting\n",
        "        X_smoothed_values = X_smoothed_combined.values\n",
        "        shap_smoothed_array = shap_smoothed_combined.values\n",
        "        feature_names_smoothed = importance_smoothed_df['Feature'].tolist()\n",
        "\n",
        "        for feature in feature_names_smoothed:\n",
        "            feature_effect_png_path = join(smoothed_feature_effect_plots_dir, f'feature_effect_{re.sub(r'[<>:\"/\\\\|?*]', '_', feature)}.png')\n",
        "            feature_effect_svg_path = join(smoothed_feature_effect_plots_dir, f'feature_effect_{re.sub(r'[<>:\"/\\\\|?*]', '_', feature)}.svg')\n",
        "            if not exists(feature_effect_png_path) or not exists(feature_effect_svg_path):\n",
        "                # Get feature data\n",
        "                feature_idx = X_smoothed_combined.columns.get_loc(feature)\n",
        "                feature_values = X_smoothed_values[:, feature_idx]\n",
        "                shap_values_feature = shap_smoothed_array[:, feature_idx]\n",
        "\n",
        "                # Get feature statistics for trend lines and neutral effect value from alphabetically sorted CSV\n",
        "                feature_row = feature_stats_df[feature_stats_df['Feature'] == feature].iloc[0]\n",
        "                x_mean = feature_row['Feature_Mean']\n",
        "                x_scale = feature_row['Feature_Std']\n",
        "                neutral_value = feature_row['Neutral_Effect_Value']\n",
        "\n",
        "                # Parse polynomial coefficients from CSV\n",
        "                poly_coef = np.array([float(c) for c in feature_row['Polynomial_Coefficients'].split(',')])\n",
        "                lin_coef = np.array([feature_row['Linear_Coefficient'], 0])\n",
        "\n",
        "                # Determine x range for plotting with optional outlier removal for visualisation\n",
        "                if remove_outliers:\n",
        "                    lower = np.percentile(feature_values, (100 - plot_feature_percentile_range) / 2)\n",
        "                    upper = np.percentile(feature_values, 100 - (100 - plot_feature_percentile_range) / 2)\n",
        "                    mask = (feature_values >= lower) & (feature_values <= upper)\n",
        "                    feature_values_plot = feature_values[mask]\n",
        "                    shap_values_plot = shap_values_feature[mask]\n",
        "                    x_range = np.linspace(lower, upper, 200)\n",
        "                else:\n",
        "                    feature_values_plot = feature_values\n",
        "                    shap_values_plot = shap_values_feature\n",
        "                    x_range = np.linspace(feature_values.min(), feature_values.max(), 200)\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                ax.scatter(feature_values_plot, shap_values_plot, alpha=0.5, rasterized=True)\n",
        "\n",
        "                # Draw trend lines using coefficients from statistics CSV\n",
        "                if x_scale != 0:\n",
        "                    x_range_norm = (x_range - x_mean) / x_scale\n",
        "\n",
        "                    # Linear trend\n",
        "                    lin_pred = np.polyval(lin_coef, x_range_norm)\n",
        "                    ax.plot(x_range, lin_pred, \"--\", color='black', alpha=0.8, label='Linear trend')\n",
        "\n",
        "                    # Polynomial trend\n",
        "                    poly_pred = np.polyval(poly_coef, x_range_norm)\n",
        "                    ax.plot(x_range, poly_pred, \"--\", color='red', alpha=0.8, label=f'Polynomial trend (degree {poly_degree})')\n",
        "\n",
        "                # Add neutral effect line from statistics CSV\n",
        "                ax.axvline(x=neutral_value, color='blue', linestyle=':', linewidth=2, alpha=0.8, label='Neutral effect value')\n",
        "\n",
        "                ax.set_title(f'SHAP Feature Effect: {feature}')\n",
        "                ax.set_xlabel(f'{feature} value')\n",
        "                ax.set_ylabel('SHAP value (impact on prediction)')\n",
        "                ax.legend(loc='upper right')\n",
        "                if not exists(feature_effect_png_path):\n",
        "                    plt.savefig(feature_effect_png_path, bbox_inches='tight', dpi=150, pad_inches=0.1)\n",
        "                if not exists(feature_effect_svg_path):\n",
        "                    plt.savefig(feature_effect_svg_path, bbox_inches='tight', dpi=150, pad_inches=0.1, format='svg')\n",
        "                plt.close()\n",
        "\n",
        "        print(\"Smoothed combinations: feature effect plots generated.\")\n",
        "\n",
        "if generate_conceptual_combinations:\n",
        "    print(\"Processing conceptual feature combinations...\")\n",
        "\n",
        "    for combo_idx, conceptual_combinations in enumerate(conceptual_combinations_list, 1):\n",
        "        print(f\"\\nProcessing conceptual combination set {combo_idx}...\")\n",
        "\n",
        "        # Create directory for this combination set\n",
        "        conceptual_dir = join(model_dataset_dir, f'shap_conceptual_combination_{combo_idx}')\n",
        "        os.makedirs(conceptual_dir, exist_ok=True)\n",
        "\n",
        "        # Save grouping rules as CSV\n",
        "        grouping_rules_data = []\n",
        "        for concept_name, patterns in conceptual_combinations.items():\n",
        "            for pattern in patterns:\n",
        "                grouping_rules_data.append({'Concept': concept_name, 'Pattern': pattern})\n",
        "\n",
        "        grouping_rules_df = pd.DataFrame(grouping_rules_data)\n",
        "        grouping_rules_path = join(conceptual_dir, 'conceptual_grouping_rules.csv')\n",
        "        grouping_rules_df.to_csv(grouping_rules_path, index=False)\n",
        "\n",
        "        # Create conceptual combinations by summing SHAP values of related features\n",
        "        shap_conceptual_combined = pd.DataFrame()\n",
        "\n",
        "        for concept_name, patterns in conceptual_combinations.items():\n",
        "            # Find original features matching patterns, then map to renamed features\n",
        "            matching_original_features = []\n",
        "            for pattern in patterns:\n",
        "                matching_original_features.extend([orig_name for orig_name in feature_name_mapping.keys() if pattern in orig_name])\n",
        "\n",
        "            # Remove duplicates and convert to renamed features\n",
        "            matching_original_features = list(dict.fromkeys(matching_original_features))\n",
        "            matching_renamed_features = [feature_name_mapping[orig] for orig in matching_original_features if feature_name_mapping[orig] in X_clean.columns]\n",
        "\n",
        "            if matching_renamed_features:\n",
        "                # Sum SHAP values for all features in this conceptual group\n",
        "                concept_shap_sum = shap_clean[matching_renamed_features].sum(axis=1)\n",
        "                shap_conceptual_combined[concept_name] = concept_shap_sum\n",
        "                print(f\"  '{concept_name}': {len(matching_renamed_features)} features combined\")\n",
        "\n",
        "        # Calculate importance for conceptual combinations\n",
        "        importance_conceptual_df = pd.DataFrame({\n",
        "            'Feature': shap_conceptual_combined.columns,\n",
        "            'Mean Absolute SHAP Value': np.abs(shap_conceptual_combined).mean(axis=0)\n",
        "        })\n",
        "        importance_conceptual_df.sort_values('Mean Absolute SHAP Value', ascending=False, inplace=True)\n",
        "\n",
        "        # Define output paths for this conceptual combination\n",
        "        conceptual_feature_importance_path = join(conceptual_dir, 'shap_feature_importance.csv')\n",
        "        conceptual_feature_importance_plot_path = join(conceptual_dir, 'shap_feature_importance')\n",
        "\n",
        "        # Save feature importance\n",
        "        importance_conceptual_df.to_csv(conceptual_feature_importance_path, index=False)\n",
        "\n",
        "        # Convert to array for plotting\n",
        "        shap_values_conceptual_array = shap_conceptual_combined.values\n",
        "\n",
        "        # Create dummy feature values dataframe for plotting compatibility\n",
        "        X_conceptual_dummy = pd.DataFrame(np.zeros_like(shap_values_conceptual_array),\n",
        "                                         columns=shap_conceptual_combined.columns)\n",
        "\n",
        "        # Generate importance plot for this conceptual combination\n",
        "        if not exists(f\"{conceptual_feature_importance_plot_path}.png\") or not exists(f\"{conceptual_feature_importance_plot_path}.svg\"):\n",
        "            shap.summary_plot(shap_values_conceptual_array, X_conceptual_dummy, plot_type=\"bar\", show=False)\n",
        "            plt.gcf().set_size_inches(12, 0.4 * len(shap_conceptual_combined.columns))\n",
        "            plt.tight_layout()\n",
        "            if not exists(f\"{conceptual_feature_importance_plot_path}.png\"):\n",
        "                plt.savefig(f\"{conceptual_feature_importance_plot_path}.png\", bbox_inches='tight', dpi=150, pad_inches=0.1)\n",
        "            if not exists(f\"{conceptual_feature_importance_plot_path}.svg\"):\n",
        "                plt.savefig(f\"{conceptual_feature_importance_plot_path}.svg\", bbox_inches='tight', dpi=150, pad_inches=0.1, format='svg')\n",
        "            plt.close()\n",
        "            print(f\"  Conceptual combination {combo_idx}: feature importance plot created.\")\n",
        "\n",
        "print(\"Feature combination analysis complete. All combined plots and statistics generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare accuracy (GEDI AGBD)"
      ],
      "metadata": {
        "id": "KV8EFg1WlPtX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_lgjEC_LoRZ"
      },
      "outputs": [],
      "source": [
        "# Imports for this section\n",
        "import geopandas as gpd\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from osgeo import gdal\n",
        "\n",
        "# Directories\n",
        "gedi_raster_final_dir = join(base_dir, \"2_targets/gedi_raster_final\")\n",
        "masks_dir = join(base_dir, \"1_areas/masks\")\n",
        "polygons_dir = join(base_dir, \"1_areas/polygons\")\n",
        "accuracy_dir = join(model_dataset_dir, \"accuracy_comparison\")\n",
        "gedi_baseline_dir = join(accuracy_dir, \"gedi_baseline_data\")\n",
        "makedirs(gedi_baseline_dir, exist_ok=True)\n",
        "\n",
        "# Global function: Sample raster values\n",
        "def sample_raster_values(pd_dataframe, raster_path, geom_x, geom_y, feature=False, n_threads=1):\n",
        "    # Derive column name from filename\n",
        "    raster_name = raster_path.split('/')[-1][:-4]\n",
        "    if feature: raster_name = 'fea_' + raster_name\n",
        "    # Load raster and extract metadata\n",
        "    raster = gdal.Open(raster_path)\n",
        "    band = raster.GetRasterBand(1)\n",
        "    geotransform = raster.GetGeoTransform()\n",
        "    raster_array = band.ReadAsArray()\n",
        "    nodata = band.GetNoDataValue()\n",
        "    rows, cols = raster_array.shape\n",
        "    fill_value = nodata if nodata is not None else np.nan\n",
        "    # Initialise output array with nodata\n",
        "    sampled_values = np.full(len(geom_x), fill_value, dtype=raster_array.dtype)\n",
        "    # Worker function for threaded sampling\n",
        "    def sample_chunk(start, end):\n",
        "        x_idx = ((geom_x[start:end] - geotransform[0]) / geotransform[1]).astype(int)\n",
        "        y_idx = ((geom_y[start:end] - geotransform[3]) / geotransform[5]).astype(int)\n",
        "        valid = (x_idx >= 0) & (x_idx < cols) & (y_idx >= 0) & (y_idx < rows)\n",
        "        local_values = np.full(end - start, fill_value, dtype=raster_array.dtype)\n",
        "        local_values[valid] = raster_array[y_idx[valid], x_idx[valid]]\n",
        "        sampled_values[start:end] = local_values\n",
        "    # Split points into chunks and process in parallel\n",
        "    n_points = len(geom_x)\n",
        "    chunk_size = (n_points + n_threads - 1) // n_threads\n",
        "    chunk_ranges = [(i, min(i + chunk_size, n_points)) for i in range(0, n_points, chunk_size)]\n",
        "    with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
        "        executor.map(lambda r: sample_chunk(*r), chunk_ranges)\n",
        "    # Assign to dataframe and release resources\n",
        "    pd_dataframe[raster_name] = sampled_values\n",
        "    raster = band = None\n",
        "\n",
        "nodatavalue = -11111"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select target variate (uncomment one, must match model target)\n",
        "\n",
        "# Use L4D naming convention\n",
        "selected_variate = \"agbd\"\n",
        "# selected_variate = \"cover_z_000\"\n",
        "# selected_variate = \"rh_010\"\n",
        "# selected_variate = \"rh_020\"\n",
        "# selected_variate = \"rh_030\"\n",
        "# selected_variate = \"rh_040\"\n",
        "# selected_variate = \"rh_050\"\n",
        "# selected_variate = \"rh_060\"\n",
        "# selected_variate = \"rh_070\"\n",
        "# selected_variate = \"rh_080\"\n",
        "# selected_variate = \"rh_090\"\n",
        "# selected_variate = \"rh_095\"\n",
        "# selected_variate = \"rh_098\"\n",
        "\n",
        "l4d_gpkg_url = \"https://data.ornldaac.earthdata.nasa.gov/public/gedi/GEDI_L4D_Imputed_Waveforms/comp/GEDI_L4D_20190418_20230316_validation_metrics.gpkg\"\n",
        "\n",
        "# Template for clipping\n",
        "template_gdf = gpd.read_file(join(polygons_dir, \"template.gpkg\"))\n",
        "template_bounds = template_gdf.total_bounds"
      ],
      "metadata": {
        "id": "5baihYhWlRuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile GEDI baseline data\n",
        "\n",
        "# Template for clipping\n",
        "template_gdf = gpd.read_file(join(polygons_dir, \"template.gpkg\"))\n",
        "template_bounds = template_gdf.total_bounds\n",
        "\n",
        "# Determine if L4B is needed (AGBD only)\n",
        "need_l4b = selected_variate == \"agbd\"\n",
        "\n",
        "# Search for L4D rasters (original UTM projections)\n",
        "gedi04d_final_dir = join(gedi_raster_final_dir, \"GEDI04_D\")\n",
        "l4d_rasters = []\n",
        "if exists(gedi04d_final_dir):\n",
        "    for f in os.listdir(gedi04d_final_dir):\n",
        "        if f.startswith(\"GEDI04_D_original_\") and f.endswith(f\"_{selected_variate}.tif\"):\n",
        "            l4d_rasters.append(join(gedi04d_final_dir, f))\n",
        "    if l4d_rasters: print(f\"L4D: found {len(l4d_rasters)} raster(s) for '{selected_variate}'\")\n",
        "    else: print(f\"L4D: no rasters found for '{selected_variate}'\")\n",
        "else: print(\"L4D: directory not found\")\n",
        "\n",
        "# Search for L4B raster (AGBD only, MU variant)\n",
        "gedi04b_final_dir = join(gedi_raster_final_dir, \"GEDI04_B\")\n",
        "l4b_raster = None\n",
        "if need_l4b:\n",
        "    if exists(gedi04b_final_dir):\n",
        "        for f in os.listdir(gedi04b_final_dir):\n",
        "            if f.startswith(\"GEDI04_B_original_\") and \"MU\" in f:\n",
        "                l4b_raster = join(gedi04b_final_dir, f)\n",
        "                print(f\"L4B: found '{f}'\")\n",
        "                break\n",
        "        if not l4b_raster: print(\"L4B: no MU raster found\")\n",
        "    else: print(\"L4B: directory not found\")\n",
        "\n",
        "# Check if required data exists\n",
        "missing = []\n",
        "if not l4d_rasters: missing.append(\"L4D\")\n",
        "if need_l4b and not l4b_raster: missing.append(\"L4B\")\n",
        "if missing:\n",
        "    print(f\"\\nMissing data: {', '.join(missing)}. Run '2_variates.ipynb' to download GEDI rasters.\")\n",
        "else:\n",
        "    print(\"\\nAll required GEDI data found. Proceeding...\")\n",
        "\n",
        "    # Copy L4D rasters to baseline directory\n",
        "    print(\"\\nCopying L4D rasters\")\n",
        "    l4d_copied = []\n",
        "    for src in l4d_rasters:\n",
        "        dst = join(gedi_baseline_dir, os.path.basename(src))\n",
        "        if not exists(dst):\n",
        "            print(f\"  Copying: {os.path.basename(src)}\")\n",
        "            gdal.Translate(dst, src, options=gdal.TranslateOptions(format='GTiff', creationOptions=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1']))\n",
        "        else: print(f\"  Exists: {os.path.basename(src)}\")\n",
        "        l4d_copied.append(dst)\n",
        "\n",
        "    # Copy L4B raster if needed\n",
        "    l4b_copied = None\n",
        "    if need_l4b and l4b_raster:\n",
        "        print(\"\\nCopying L4B raster\")\n",
        "        dst = join(gedi_baseline_dir, os.path.basename(l4b_raster))\n",
        "        if not exists(dst):\n",
        "            print(f\"  Copying: {os.path.basename(l4b_raster)}\")\n",
        "            gdal.Translate(dst, l4b_raster, options=gdal.TranslateOptions(format='GTiff', creationOptions=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1']))\n",
        "        else: print(f\"  Exists: {os.path.basename(l4b_raster)}\")\n",
        "        l4b_copied = dst\n",
        "\n",
        "    # Create masked versions for presentation\n",
        "    print(\"\\nCreating masked rasters\")\n",
        "    mask_2023 = join(masks_dir, \"mask_forest_2023.tif\")\n",
        "    mask_2021 = join(masks_dir, \"mask_forest_2021.tif\")\n",
        "\n",
        "    # Mask raster to forest extent, resampling mask if needed.\n",
        "    def apply_forest_mask(raster_path, mask_path, output_path):\n",
        "        if exists(output_path):\n",
        "            print(f\"  Masked exists: {os.path.basename(output_path)}\")\n",
        "            return\n",
        "        raster_ds = gdal.Open(raster_path)\n",
        "        raster_proj = raster_ds.GetProjection()\n",
        "        raster_gt = raster_ds.GetGeoTransform()\n",
        "        raster_xsize, raster_ysize = raster_ds.RasterXSize, raster_ds.RasterYSize\n",
        "        raster_bounds = [raster_gt[0], raster_gt[3] + raster_gt[5] * raster_ysize,\n",
        "                         raster_gt[0] + raster_gt[1] * raster_xsize, raster_gt[3]]\n",
        "        mask_resampled = gdal.Warp('', mask_path, options=gdal.WarpOptions(\n",
        "            format='MEM', dstSRS=raster_proj, outputBounds=raster_bounds,\n",
        "            width=raster_xsize, height=raster_ysize, resampleAlg='near', dstNodata=nodatavalue))\n",
        "        mask_array = mask_resampled.GetRasterBand(1).ReadAsArray()\n",
        "        raster_array = raster_ds.GetRasterBand(1).ReadAsArray()\n",
        "        raster_array = np.where(mask_array == 1, raster_array, nodatavalue)\n",
        "        driver = gdal.GetDriverByName('GTiff')\n",
        "        out_ds = driver.Create(output_path, raster_xsize, raster_ysize, 1, raster_ds.GetRasterBand(1).DataType,\n",
        "                               options=['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'])\n",
        "        out_ds.SetGeoTransform(raster_gt)\n",
        "        out_ds.SetProjection(raster_proj)\n",
        "        out_band = out_ds.GetRasterBand(1)\n",
        "        out_band.WriteArray(raster_array)\n",
        "        out_band.SetNoDataValue(nodatavalue)\n",
        "        out_ds = raster_ds = mask_resampled = None\n",
        "        print(f\"  Created: {os.path.basename(output_path)}\")\n",
        "\n",
        "    # Mask L4D rasters (2023 forest)\n",
        "    for raster_path in l4d_copied:\n",
        "        masked_name = os.path.basename(raster_path).replace(\"_original_\", \"_masked_\")\n",
        "        apply_forest_mask(raster_path, mask_2023, join(gedi_baseline_dir, masked_name))\n",
        "\n",
        "    # Mask L4B raster (2021 forest)\n",
        "    if l4b_copied:\n",
        "        masked_name = os.path.basename(l4b_copied).replace(\"_original_\", \"_masked_\")\n",
        "        apply_forest_mask(l4b_copied, mask_2021, join(gedi_baseline_dir, masked_name))\n",
        "\n",
        "    # Download and clip L4D validation gpkg\n",
        "    print(\"\\nL4D validation metrics\")\n",
        "    gpkg_raw = join(gedi_baseline_dir, \"GEDI_L4D_validation_metrics_raw.gpkg\")\n",
        "    gpkg_clipped = join(gedi_baseline_dir, \"GEDI_L4D_validation_metrics_clipped.gpkg\")\n",
        "\n",
        "    if not exists(gpkg_raw):\n",
        "        print(\"  Downloading L4D validation gpkg...\")\n",
        "        response = requests.get(l4d_gpkg_url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        with open(gpkg_raw, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(\"  Download complete\")\n",
        "    else: print(\"  Raw gpkg exists\")\n",
        "\n",
        "    if not exists(gpkg_clipped):\n",
        "        print(\"  Clipping gpkg to template extent...\")\n",
        "        tiles_gdf = gpd.read_file(gpkg_raw)\n",
        "        tiles_gdf = tiles_gdf.to_crs(template_gdf.crs)\n",
        "        tiles_clipped = gpd.clip(tiles_gdf, template_gdf)\n",
        "        tiles_clipped.to_file(gpkg_clipped, driver=\"GPKG\")\n",
        "        print(f\"  Clipped to {len(tiles_clipped)} tiles\")\n",
        "    else: print(\"  Clipped gpkg exists\")"
      ],
      "metadata": {
        "id": "XOINfQqdHbe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate weighted L4D accuracy metrics\n",
        "\n",
        "# Weighting logic: each tile's contribution proportional to forest pixel count within it.\n",
        "# This accounts for partial overlap and non-forest areas.\n",
        "# RMSE aggregated via MSE-weighting (statistically valid for combining RMSEs).\n",
        "# R approximated using predicted variance as proxy for observed variance (valid for k=1 NN).\n",
        "\n",
        "print(\"Calculating L4D accuracy\")\n",
        "tiles_gdf = gpd.read_file(gpkg_clipped)\n",
        "\n",
        "# Determine column names based on variate\n",
        "if selected_variate == \"agbd\": rmse_col, mean_col = \"rmse_agbd\", \"agbd_mean_valid\"\n",
        "elif selected_variate == \"cover_z_000\": rmse_col, mean_col = \"rmse_cover\", \"cover_mean_valid\"\n",
        "else:\n",
        "    rh_num = selected_variate.replace(\"rh_\", \"\").lstrip(\"0\") or \"0\"\n",
        "    rmse_col, mean_col = f\"rmse_{rh_num}\", f\"rh_mean_valid_{rh_num}\"\n",
        "\n",
        "# Verify columns exist\n",
        "if rmse_col not in tiles_gdf.columns or mean_col not in tiles_gdf.columns:\n",
        "    raise ValueError(f\"Columns '{rmse_col}' or '{mean_col}' not found in gpkg\")\n",
        "\n",
        "# Load reference L4D raster for pixel counting and variance calculation\n",
        "ref_raster = l4d_copied[0]\n",
        "ref_ds = gdal.Open(ref_raster)\n",
        "ref_proj = ref_ds.GetProjection()\n",
        "ref_gt = ref_ds.GetGeoTransform()\n",
        "ref_xsize, ref_ysize = ref_ds.RasterXSize, ref_ds.RasterYSize\n",
        "ref_bounds = [ref_gt[0], ref_gt[3] + ref_gt[5] * ref_ysize, ref_gt[0] + ref_gt[1] * ref_xsize, ref_gt[3]]\n",
        "\n",
        "# Load L4D array for variance calculation\n",
        "l4d_array = ref_ds.GetRasterBand(1).ReadAsArray().astype(float)\n",
        "l4d_nodata = ref_ds.GetRasterBand(1).GetNoDataValue()\n",
        "ref_ds = None\n",
        "l4d_array[l4d_array == l4d_nodata] = np.nan\n",
        "\n",
        "# Resample forest mask to reference grid\n",
        "mask_aligned = gdal.Warp('', mask_2023, options=gdal.WarpOptions(\n",
        "    format='MEM', dstSRS=ref_proj, outputBounds=ref_bounds,\n",
        "    width=ref_xsize, height=ref_ysize, resampleAlg='near', dstNodata=nodatavalue))\n",
        "mask_array = mask_aligned.GetRasterBand(1).ReadAsArray()\n",
        "mask_aligned = None\n",
        "\n",
        "# Reproject tiles to raster CRS\n",
        "from osgeo import osr\n",
        "raster_srs = osr.SpatialReference()\n",
        "raster_srs.ImportFromWkt(ref_proj)\n",
        "raster_epsg = raster_srs.GetAuthorityCode(None)\n",
        "tiles_reproj = tiles_gdf.to_crs(epsg=int(raster_epsg))\n",
        "\n",
        "# Calculate forest pixel counts and R approximation per tile\n",
        "forest_counts = []\n",
        "r2_approx_list = []\n",
        "for idx, tile in tiles_reproj.iterrows():\n",
        "    minx, miny, maxx, maxy = tile.geometry.bounds\n",
        "    col_start = max(0, int((minx - ref_gt[0]) / ref_gt[1]))\n",
        "    col_end = min(ref_xsize, int((maxx - ref_gt[0]) / ref_gt[1]))\n",
        "    row_start = max(0, int((ref_gt[3] - maxy) / abs(ref_gt[5])))\n",
        "    row_end = min(ref_ysize, int((ref_gt[3] - miny) / abs(ref_gt[5])))\n",
        "\n",
        "    if col_end > col_start and row_end > row_start:\n",
        "        tile_mask = mask_array[row_start:row_end, col_start:col_end]\n",
        "        forest_count = np.sum(tile_mask == 1)\n",
        "        # Variance from forest pixels only\n",
        "        tile_pixels = l4d_array[row_start:row_end, col_start:col_end]\n",
        "        forest_pixels = tile_pixels[(tile_mask == 1) & (~np.isnan(tile_pixels))]\n",
        "        if len(forest_pixels) > 1:\n",
        "            var_pred = np.var(forest_pixels)\n",
        "            rmse_val = tiles_gdf.loc[idx, rmse_col]\n",
        "            r2 = 1 - (rmse_val**2 / var_pred) if var_pred > 0 else np.nan\n",
        "        else: r2 = np.nan\n",
        "    else:\n",
        "        forest_count = 0\n",
        "        r2 = np.nan\n",
        "\n",
        "    forest_counts.append(forest_count)\n",
        "    r2_approx_list.append(r2)\n",
        "\n",
        "tiles_gdf['forest_pixels'] = forest_counts\n",
        "tiles_gdf['r2_approx'] = r2_approx_list\n",
        "tiles_gdf['rrmse'] = (tiles_gdf[rmse_col] / tiles_gdf[mean_col]) * 100\n",
        "\n",
        "# Filter to tiles with forest data and valid metrics\n",
        "tiles_valid = tiles_gdf[(tiles_gdf['forest_pixels'] > 0)].dropna(subset=['rrmse', 'r2_approx'])\n",
        "\n",
        "# Weighted aggregation\n",
        "weights = tiles_valid['forest_pixels'].values\n",
        "weighted_rmse = np.sqrt(np.average(tiles_valid[rmse_col].values**2, weights=weights))\n",
        "weighted_rrmse = np.average(tiles_valid['rrmse'].values, weights=weights)\n",
        "weighted_r2 = np.average(tiles_valid['r2_approx'].values, weights=weights)\n",
        "\n",
        "print(f\"  Tiles with data: {len(tiles_valid)}\")\n",
        "print(f\"  Total forest pixels: {weights.sum():,}\")\n",
        "print(f\"  Weighted RMSE: {weighted_rmse:.4f}\")\n",
        "print(f\"  Weighted rRMSE: {weighted_rrmse:.4f}%\")\n",
        "print(f\"  Weighted R (approx): {weighted_r2:.4f}\")\n",
        "\n",
        "# Save L4D tile-weighted accuracy\n",
        "l4d_accuracy = {\n",
        "    'source': 'L4D (tile-weighted)',\n",
        "    'variate': selected_variate,\n",
        "    'rmse': weighted_rmse,\n",
        "    'rrmse': weighted_rrmse,\n",
        "    'r2': weighted_r2,\n",
        "    'n_tiles': len(tiles_valid),\n",
        "    'n_forest_pixels': int(weights.sum())\n",
        "}\n",
        "l4d_accuracy_df = pd.DataFrame([l4d_accuracy])\n",
        "l4d_accuracy_path = join(accuracy_dir, \"l4d_tile_accuracy.csv\")\n",
        "l4d_accuracy_df.to_csv(l4d_accuracy_path, index=False)\n",
        "print(f\"  Saved: {l4d_accuracy_path}\")"
      ],
      "metadata": {
        "id": "sI4day1vHela"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print available final datasets\n",
        "\n",
        "print(\"Available datasets\")\n",
        "for f in os.listdir(datasets_dir):\n",
        "    if f.endswith(\".pkl\"):\n",
        "        print(f'original_dataset = \"{f}\"')"
      ],
      "metadata": {
        "id": "uxtXAuVwHnP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Point-based accuracy comparison\n",
        "\n",
        "# Paste the correct dataset from above\n",
        "original_dataset = \"agbd.pkl\"\n",
        "\n",
        "# Load original dataset to recover geometry and year\n",
        "print(\"Loading datasets\")\n",
        "original_df = pd.read_pickle(join(datasets_dir, original_dataset))\n",
        "model_df = model_dataset.copy()\n",
        "\n",
        "# Join on shot number (model_df uses 'shot_number', original uses 'tar_shot_number')\n",
        "print(f\"  Model dataset: {len(model_df)} points\")\n",
        "original_subset = original_df[['tar_shot_number', 'tar_geometry', 'tar_year']].copy()\n",
        "accuracy_df = model_df.merge(original_subset, left_on='shot_number', right_on='tar_shot_number', how='left')\n",
        "accuracy_df = accuracy_df.dropna(subset=['tar_geometry'])\n",
        "print(f\"  After geometry join: {len(accuracy_df)} points\")\n",
        "\n",
        "# Split by year for sampling\n",
        "l4d_years = [2024] # Year after\n",
        "l4b_years = [2020, 2021, 2022]\n",
        "accuracy_l4d = accuracy_df[accuracy_df['tar_year'].isin(l4d_years)].copy()\n",
        "accuracy_l4b = accuracy_df[accuracy_df['tar_year'].isin(l4b_years)].copy() if need_l4b else None\n",
        "\n",
        "print(f\"  L4D-eligible points (2023): {len(accuracy_l4d)}\")\n",
        "if need_l4b: print(f\"  L4B-eligible points (2019-2021): {len(accuracy_l4b)}\")\n",
        "\n",
        "# Sample L4D rasters (may be multiple UTM zones)\n",
        "print(\"\\nSampling L4D rasters\")\n",
        "l4d_sampled_col = f\"l4d_{selected_variate}\"\n",
        "if len(accuracy_l4d) > 0:\n",
        "    # Sample each zone raster, merge results (first non-nodata wins)\n",
        "    l4d_samples = np.full(len(accuracy_l4d), nodatavalue, dtype=float)\n",
        "    for raster_path in l4d_copied:\n",
        "        print(f\"  Sampling: {os.path.basename(raster_path)}\")\n",
        "        # Get raster CRS\n",
        "        raster_ds = gdal.Open(raster_path)\n",
        "        raster_proj = raster_ds.GetProjection()\n",
        "        raster_ds = None\n",
        "        srs = osr.SpatialReference()\n",
        "        srs.ImportFromWkt(raster_proj)\n",
        "        raster_epsg = int(srs.GetAuthorityCode(None))\n",
        "        # Reproject points and extract coordinates\n",
        "        gdf = gpd.GeoDataFrame(accuracy_l4d, geometry='tar_geometry', crs='EPSG:4326')\n",
        "        gdf_reproj = gdf.to_crs(epsg=raster_epsg)\n",
        "        geom_x = gdf_reproj.geometry.x.values\n",
        "        geom_y = gdf_reproj.geometry.y.values\n",
        "        # Sample raster\n",
        "        temp_df = pd.DataFrame(index=accuracy_l4d.index)\n",
        "        sample_raster_values(temp_df, raster_path, geom_x, geom_y)\n",
        "        values = temp_df.iloc[:, 0].values\n",
        "        # Fill where we don't have data yet\n",
        "        mask = (l4d_samples == nodatavalue) & (values != nodatavalue)\n",
        "        l4d_samples[mask] = values[mask]\n",
        "    accuracy_l4d[l4d_sampled_col] = l4d_samples\n",
        "    # Filter out nodata\n",
        "    accuracy_l4d_valid = accuracy_l4d[accuracy_l4d[l4d_sampled_col] != nodatavalue].copy()\n",
        "    print(f\"  Valid samples: {len(accuracy_l4d_valid)} / {len(accuracy_l4d)}\")\n",
        "else:\n",
        "    accuracy_l4d_valid = accuracy_l4d\n",
        "\n",
        "# Sample L4B raster (AGBD only)\n",
        "l4b_sampled_col = \"l4b_agbd\"\n",
        "accuracy_l4b_valid = None\n",
        "if need_l4b and l4b_copied and accuracy_l4b is not None and len(accuracy_l4b) > 0:\n",
        "    print(\"\\nSampling L4B raster\")\n",
        "    print(f\"  Sampling: {os.path.basename(l4b_copied)}\")\n",
        "    # Get raster CRS\n",
        "    raster_ds = gdal.Open(l4b_copied)\n",
        "    raster_proj = raster_ds.GetProjection()\n",
        "    raster_ds = None\n",
        "    srs = osr.SpatialReference()\n",
        "    srs.ImportFromWkt(raster_proj)\n",
        "    raster_epsg = int(srs.GetAuthorityCode(None))\n",
        "    # Reproject points and extract coordinates\n",
        "    gdf = gpd.GeoDataFrame(accuracy_l4b, geometry='tar_geometry', crs='EPSG:4326')\n",
        "    gdf_reproj = gdf.to_crs(epsg=raster_epsg)\n",
        "    geom_x = gdf_reproj.geometry.x.values\n",
        "    geom_y = gdf_reproj.geometry.y.values\n",
        "    # Sample raster\n",
        "    sample_raster_values(accuracy_l4b, l4b_copied, geom_x, geom_y)\n",
        "    accuracy_l4b.rename(columns={accuracy_l4b.columns[-1]: l4b_sampled_col}, inplace=True)\n",
        "    # Filter out nodata\n",
        "    accuracy_l4b_valid = accuracy_l4b[accuracy_l4b[l4b_sampled_col] != nodatavalue].copy()\n",
        "    print(f\"  Valid samples: {len(accuracy_l4b_valid)} / {len(accuracy_l4b)}\")\n",
        "\n",
        "# Calculate accuracy metrics\n",
        "print(\"\\nCalculating point-based accuracy\")\n",
        "results = []\n",
        "\n",
        "# L4D accuracy\n",
        "if len(accuracy_l4d_valid) > 0:\n",
        "    l4d_metrics = calculate_metrics(\n",
        "        accuracy_l4d_valid[selected_target].values,\n",
        "        accuracy_l4d_valid[l4d_sampled_col].values\n",
        "    )\n",
        "    l4d_metrics = {k: v for k, v in l4d_metrics.items() if k != 'optimal_value'}\n",
        "    l4d_metrics['source'] = 'L4D (point)'\n",
        "    l4d_metrics['n_points'] = len(accuracy_l4d_valid)\n",
        "    results.append(l4d_metrics)\n",
        "    print(f\"  L4D: R={l4d_metrics['r2']:.4f}, RMSE={l4d_metrics['rmse']:.4f}, rRMSE={l4d_metrics['rrmse']:.4f}%\")\n",
        "\n",
        "# L4B accuracy (AGBD only)\n",
        "if need_l4b and accuracy_l4b_valid is not None and len(accuracy_l4b_valid) > 0:\n",
        "    l4b_metrics = calculate_metrics(\n",
        "        accuracy_l4b_valid[selected_target].values,\n",
        "        accuracy_l4b_valid[l4b_sampled_col].values\n",
        "    )\n",
        "    l4b_metrics = {k: v for k, v in l4b_metrics.items() if k != 'optimal_value'}\n",
        "    l4b_metrics['source'] = 'L4B (point)'\n",
        "    l4b_metrics['n_points'] = len(accuracy_l4b_valid)\n",
        "    results.append(l4b_metrics)\n",
        "    print(f\"  L4B: R={l4b_metrics['r2']:.4f}, RMSE={l4b_metrics['rmse']:.4f}, rRMSE={l4b_metrics['rrmse']:.4f}%\")\n",
        "\n",
        "# Load model validation metrics from model_description\n",
        "print(\"\\nLoading model validation metrics\")\n",
        "with open(model_description_dir) as f:\n",
        "    model_desc = json.load(f)\n",
        "\n",
        "model_metrics = {\n",
        "    'source': 'Model (CV)',\n",
        "    'r2': model_desc.get('score_validation (r2) mean', np.nan),\n",
        "    'me': model_desc.get('score_validation (me) mean', np.nan),\n",
        "    'rmse': model_desc.get('score_validation (rmse) mean', np.nan),\n",
        "    'rrmse': model_desc.get('score_validation (rrmse) mean', np.nan),\n",
        "    'n_points': f\"{n_splits - n_hpo_splits} folds\"\n",
        "}\n",
        "results.append(model_metrics)\n",
        "print(f\"  Model: R={model_metrics['r2']:.4f}, RMSE={model_metrics['rmse']:.4f}, rRMSE={model_metrics['rrmse']:.4f}%\")\n",
        "\n",
        "# Build comparison dataframe\n",
        "comparison_df = pd.DataFrame(results)\n",
        "comparison_df = comparison_df.set_index('source').T\n",
        "\n",
        "# Determine best performer per metric\n",
        "def best_source(row, metric_name):\n",
        "    opt = optimal_values.get(metric_name, 'min')\n",
        "    numeric_vals = {k: v for k, v in row.items() if isinstance(v, (int, float)) and not np.isnan(v)}\n",
        "    if not numeric_vals: return '-'\n",
        "    if opt == 'max': return max(numeric_vals, key=numeric_vals.get)\n",
        "    return min(numeric_vals, key=numeric_vals.get)\n",
        "\n",
        "comparison_df['best'] = comparison_df.apply(\n",
        "    lambda row: best_source(row, row.name) if row.name in optimal_values else '-', axis=1\n",
        ")\n",
        "\n",
        "# Save comparison\n",
        "comparison_path = join(accuracy_dir, \"accuracy_comparison.csv\")\n",
        "comparison_df.to_csv(comparison_path)\n",
        "print(f\"\\nComparison saved: {comparison_path}\")\n",
        "print(comparison_df.to_string())"
      ],
      "metadata": {
        "id": "W9f8aHSuEj8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nezNm_Gz3cnz"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4CiSNFVBEke"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "0edRH3sOATkP",
        "twsBfrX4A1Ce"
      ],
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}