{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/dev/8_statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j0th2jhtnAR"
      },
      "source": [
        "# Imports & Subdirectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9drq4euvG0MS"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi_asartr\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi_asartr'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGWgfUqto0F"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Imports and upgrades\n",
        "!pip install geopandas\n",
        "!pip install kaleido\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MTERlyxuPd4"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# Imports\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import math\n",
        "import numpy as np\n",
        "from os.path import exists, join\n",
        "from os import makedirs\n",
        "from osgeo import gdal\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import rasterio\n",
        "from rasterio import mask as msk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwar9sxbts-3"
      },
      "outputs": [],
      "source": [
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "mask_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "predictions_dir = join(base_dir, \"7_predictions\")\n",
        "statistics_dir = join(base_dir, \"8_statistics\")\n",
        "sample_polygons_dir = join(statistics_dir, \"sample_polygons\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(statistics_dir, exist_ok=True)\n",
        "makedirs(sample_polygons_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atvGRSPYgwk9"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjy-T1TqScbE"
      },
      "source": [
        "# Select model, area and sample polygons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj5RbVSuEp-B"
      },
      "outputs": [],
      "source": [
        "# Select if to source predictions from scenarios_dir or predictions_dir\n",
        "source_dir = predictions_dir\n",
        "# source_dir = scenarios_dir\n",
        "\n",
        "source_dir_name = f\"{source_dir.split('_')[-1]}_dir\"\n",
        "\n",
        "# Select the model\n",
        "for subdir in os.listdir(source_dir):\n",
        "  if 'scenario_masks' not in subdir:\n",
        "    print(f\"selected_model = '{subdir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7UWN6aIL-dX"
      },
      "outputs": [],
      "source": [
        "selected_model = 'agbd_historic_250429_223033'\n",
        "\n",
        "selected_model_dir = join(source_dir, selected_model)\n",
        "# Select the prediction area\n",
        "for subdir in os.listdir(selected_model_dir):\n",
        "  if source_dir == scenarios_dir and not subdir.endswith('.csv') and not subdir.endswith('.json'):\n",
        "    print(f\"prediction_area = '{subdir}'\")\n",
        "  if source_dir == predictions_dir and subdir != 'model_iterations':\n",
        "    print(f\"prediction_area = '{subdir[10:]}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txTXPYFLpH6v"
      },
      "outputs": [],
      "source": [
        "prediction_area = 'asartr'\n",
        "\n",
        "# Model-area stats directory\n",
        "model_area_statistics_dir = join(statistics_dir, f\"{selected_model}_{source_dir_name}_{prediction_area}\")\n",
        "makedirs(model_area_statistics_dir, exist_ok=True)\n",
        "\n",
        "# Select sample area polygons. This should be a single .gpkg with the field 'name' differentiating polygons.\n",
        "sample_polygons = []\n",
        "for geopackage in os.listdir(sample_polygons_dir):\n",
        "  sample_polygons.append(geopackage)\n",
        "\n",
        "if len(sample_polygons) == 0:\n",
        "  print(f\"No sample areas found. Upload .gpkg polygons to {sample_polygons_dir}\")\n",
        "else:\n",
        "  for sample_polygon in sample_polygons: print(f\"selected_sample_polygons = '{sample_polygon}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwWvaX5qDANc"
      },
      "outputs": [],
      "source": [
        "selected_sample_polygons = 'asartr_phase_1.gpkg'\n",
        "\n",
        "# Set whether to adjust area calculations to match rasters (True) or to match polygon areas (False).\n",
        "# The raster approach (True) uses the precise pixel area calculated in 1_areas.ipynb.\n",
        "adjust_polygon_to_raster = True\n",
        "\n",
        "# Load cell area raster for accurate pixel-by-pixel area calculations\n",
        "cell_area_path = join(areas_dir, \"cell_area.tif\")\n",
        "\n",
        "selected_sample_polygons_dir = join(sample_polygons_dir, selected_sample_polygons)\n",
        "selected_sample_polygons_gpkg = gpd.read_file(selected_sample_polygons_dir)\n",
        "sample_polygons_statistics_dir = join(model_area_statistics_dir, selected_sample_polygons[:-5])\n",
        "makedirs(sample_polygons_statistics_dir, exist_ok=True)\n",
        "detailed_stats_by_area_dir = join(sample_polygons_statistics_dir, 'detailed_stats_by_area')\n",
        "makedirs(detailed_stats_by_area_dir, exist_ok=True)\n",
        "detailed_stats_by_scenario_dir = join(sample_polygons_statistics_dir, 'detailed_stats_by_scenario')\n",
        "makedirs(detailed_stats_by_scenario_dir, exist_ok=True)\n",
        "forecast_input_dir = join(sample_polygons_statistics_dir, 'forecast_input')\n",
        "makedirs(forecast_input_dir, exist_ok=True)\n",
        "detailed_dist_stats_by_area_dir = join(sample_polygons_statistics_dir, 'detailed_dist_stats_by_area')\n",
        "makedirs(detailed_dist_stats_by_area_dir, exist_ok=True)\n",
        "detailed_dist_stats_by_scenario_dir = join(sample_polygons_statistics_dir, 'detailed_dist_stats_by_scenario')\n",
        "makedirs(detailed_dist_stats_by_scenario_dir, exist_ok=True)\n",
        "intactness_stats_dir = join(sample_polygons_statistics_dir, 'intactness')\n",
        "makedirs(intactness_stats_dir, exist_ok=True)\n",
        "report_statistics_dir = join(sample_polygons_statistics_dir, 'report_statistics')\n",
        "makedirs(report_statistics_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlQ2iLAiwQoB"
      },
      "source": [
        "# Scenario statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgC9laZXPB9Y"
      },
      "outputs": [],
      "source": [
        "# Create list of available prediction rasters and scenarios. Rasters must already be masked (e.g. to forest).\n",
        "if source_dir == scenarios_dir: prediction_raster_dir = join(selected_model_dir, prediction_area, 'scenario_predictions')\n",
        "if source_dir == predictions_dir: prediction_raster_dir = join(selected_model_dir, f'scenarios_{prediction_area}', 'statistics_masked')\n",
        "prediction_raster_dirs = []\n",
        "scenarios = set()\n",
        "for prediction_raster in os.listdir(prediction_raster_dir):\n",
        "  prediction_raster_dirs.append(join(prediction_raster_dir, prediction_raster))\n",
        "  if source_dir == predictions_dir: scenarios.add(prediction_raster.split(\"__\")[1])\n",
        "  if source_dir == scenarios_dir: scenarios.add(prediction_raster.split(\"__\")[0])\n",
        "scenarios = sorted(list(scenarios))\n",
        "\n",
        "# Select scenario predictions to calculate statistics\n",
        "print('selected_scenarios = [')\n",
        "for scenario in scenarios:\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(']\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_scenarios = [\n",
        "  # \"1990_oldgrowth\",\n",
        "  # \"2014\",\n",
        "  # \"2014_no_degradation_since_1991\",\n",
        "  # \"2014_oldgrowth\",\n",
        "  # \"2015\",\n",
        "  # \"2016\",\n",
        "  # \"2017\",\n",
        "  # \"2018\",\n",
        "  # \"2019\",\n",
        "  # \"2020\",\n",
        "  # \"2021\",\n",
        "  # \"2021_oldgrowth\",\n",
        "  # \"2022\",\n",
        "  # \"2022_alternate_degradation_2021\",\n",
        "  # \"2022_no_degradation_since_2022\",\n",
        "  # \"2022_oldgrowth\",\n",
        "  # \"2023\",\n",
        "  # \"2023_alternate_degradation_2022\",\n",
        "  # \"2023_no_degradation_since_2023\",\n",
        "  # \"2023_oldgrowth\",\n",
        "  \"2024\",\n",
        "  # \"2024_alternate_degradation_2014\",\n",
        "  # \"2024_alternate_degradation_2021\",\n",
        "  # \"2024_alternate_degradation_2022\",\n",
        "  # \"2024_alternate_degradation_2023\",\n",
        "  # \"2024_no_degradation_since_2000\",\n",
        "  # \"2024_no_degradation_since_2015\",\n",
        "  # \"2024_no_degradation_since_2022\",\n",
        "  # \"2024_no_degradation_since_2023\",\n",
        "  # \"2024_no_degradation_since_2024\",\n",
        "  # \"2024_oldgrowth\",\n",
        "  # \"2024_oldgrowth_1\",\n",
        "  \"2024_road\",\n",
        "  # \"all_oldgrowth\",\n",
        "]\n",
        "\n",
        "# Filter to selected scenarios, and separate prediction and uncertainty rasters\n",
        "prediction_rasters = []\n",
        "uncertainty_rasters = []\n",
        "for prediction_raster in prediction_raster_dirs:\n",
        "  for scenario in selected_scenarios:\n",
        "    if source_dir == predictions_dir:\n",
        "      if scenario == prediction_raster.split('/')[-1].split('__')[1] and 'mean__' in prediction_raster:\n",
        "        prediction_rasters.append(prediction_raster)\n",
        "      if scenario == prediction_raster.split('/')[-1].split('__')[1] and 'uncertainty__' in prediction_raster:\n",
        "        uncertainty_rasters.append(prediction_raster)\n",
        "    else: # If the source directory is scenarios_dir (without uncertainty values)\n",
        "      if scenario == prediction_raster.split('/')[-1].split('__')[0]: prediction_rasters.append(prediction_raster)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "prediction_rasters = sorted(prediction_rasters)\n",
        "uncertainty_rasters = sorted(uncertainty_rasters)\n",
        "\n",
        "# Create lookup dictionary for faster uncertainty matching\n",
        "uncertainty_lookup = {}\n",
        "for uncertainty_raster in uncertainty_rasters:\n",
        "  base_name = os.path.basename(uncertainty_raster).replace('uncertainty__', 'mean__')\n",
        "  uncertainty_lookup[base_name] = uncertainty_raster\n",
        "\n",
        "# Toggle whether to predict uncertainty stats\n",
        "generate_uncertainty_stats = bool(uncertainty_rasters)\n",
        "\n",
        "# Generate empty dataframes for statistics\n",
        "df_base = pd.DataFrame(index = selected_scenarios)\n",
        "df_base.rename_axis('scenario', inplace=True)\n",
        "df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg = df_base.copy(), df_base.copy(), df_base.copy(), df_base.copy()\n",
        "# If uncertainty rasters are present, generate empty dataframes\n",
        "if generate_uncertainty_stats:\n",
        "  df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agb_total_tg_ci95 = df_base.copy(), df_base.copy(), df_base.copy()\n",
        "\n",
        "# Initialise polygon area dataframe\n",
        "df_polygon_area_km2 = pd.DataFrame(columns = [\"Name\", \"Area (km^2)\"])\n",
        "\n",
        "# Loop through each polygon stored in GPKG to generate statistics\n",
        "for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "\n",
        "  # Define the polygon\n",
        "  sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"]\n",
        "  polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "  # Calculate sample_polygon_geometry area (ellipsoidal as opposed to planimetric)\n",
        "  sample_polygons_crs = selected_sample_polygons_gpkg.crs\n",
        "  temp_gdf = gpd.GeoDataFrame({'name': [sample_polygon_name], 'geometry': sample_polygon_geometry}, crs=sample_polygons_crs)\n",
        "  temp_gdf_utm = temp_gdf.estimate_utm_crs()\n",
        "  polygon_area_ha = np.divide(temp_gdf.to_crs(temp_gdf_utm).area[0], 10000, dtype='float64')\n",
        "\n",
        "  # Mask the cell area raster to the polygon once for efficiency\n",
        "  with rasterio.open(cell_area_path) as cell_area:\n",
        "    cell_area_masked, transform_1 = msk.mask(cell_area, polygons, crop=True, filled=False)\n",
        "\n",
        "  # Calculate total area of all pixels within polygon in hectares\n",
        "  pixel_area_sum_m2 = np.ma.sum(cell_area_masked, dtype='float64')\n",
        "  pixel_area_sum_ha = np.divide(pixel_area_sum_m2, 10000, dtype='float64')\n",
        "\n",
        "  # Calculate ratio between polygon and raster areas\n",
        "  area_ratio = np.divide(polygon_area_ha, pixel_area_sum_ha, dtype='float64')\n",
        "\n",
        "  # Apply area adjustment based on setting\n",
        "  if adjust_polygon_to_raster:\n",
        "    # Adjust polygon area to match raster area\n",
        "    adjusted_polygon_area_ha = pixel_area_sum_ha\n",
        "    # No adjustment needed for pixel values - keep original cell areas\n",
        "    adjusted_cell_area_masked = cell_area_masked\n",
        "  else:\n",
        "    # Keep original polygon area and adjust individual pixel areas\n",
        "    adjusted_polygon_area_ha = polygon_area_ha\n",
        "    # Adjust each pixel area by the area ratio to maintain total equal to polygon area\n",
        "    adjusted_cell_area_masked = np.multiply(cell_area_masked, area_ratio, dtype='float64')\n",
        "\n",
        "  # Convert adjusted cell areas from m² to ha\n",
        "  adjusted_cell_area_masked_ha = np.divide(adjusted_cell_area_masked, 10000, dtype='float64')\n",
        "\n",
        "  # Add polygon area to dataframe\n",
        "  new_row = pd.DataFrame([{'Name': sample_polygon_name, 'Area (km^2)': adjusted_polygon_area_ha / 100}], dtype=object)\n",
        "  df_polygon_area_km2 = pd.concat([df_polygon_area_km2, new_row], ignore_index=True, sort=False)\n",
        "\n",
        "  # Create empty lists for each prediction raster statistic\n",
        "  values_forest_cover_ha, values_agbd_mean_mg_ha, values_agbd_stdev_mg_ha, values_agb_total_tg = [], [], [], []\n",
        "\n",
        "  # If uncertainty rasters are present, create empty lists for uncertainty statistics\n",
        "  if generate_uncertainty_stats:\n",
        "    values_agbd_mean_mg_ha_ci95, values_agbd_mean_mg_ha_uncertainty, values_agb_total_tg_ci95 = [], [], []\n",
        "\n",
        "  # Loop through prediction rasters\n",
        "  for prediction_raster in prediction_rasters:\n",
        "\n",
        "    # Mask feature to sample_polygon_geometry\n",
        "    with rasterio.open(prediction_raster) as prediction:\n",
        "      nodatavalue = int(prediction.nodatavals[0])\n",
        "      prediction_array_masked, transform_2 = msk.mask(prediction, polygons, crop=True, filled=False)\n",
        "\n",
        "    # Extract forest pixels (non-masked pixels in prediction array)\n",
        "    forest_pixels_mask = ~np.ma.getmaskarray(prediction_array_masked)\n",
        "\n",
        "    # Calculate forest area by summing adjusted cell areas of forest pixels\n",
        "    forest_cell_areas_ha = np.ma.array(adjusted_cell_area_masked_ha.data, mask=~forest_pixels_mask)\n",
        "    forest_cover_ha = np.ma.sum(forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "    # Calculate total biomass by multiplying AGBD by pixel area for each pixel, then summing\n",
        "    pixel_biomass_mg = np.multiply(prediction_array_masked, forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "    # Sum to get total biomass in Mg\n",
        "    agb_total_mg = np.ma.sum(pixel_biomass_mg, dtype='float64')\n",
        "\n",
        "    # Calculate mean AGBD (Mg/ha) by dividing total biomass by forest area\n",
        "    agbd_mean_mg_ha = np.divide(agb_total_mg, forest_cover_ha, dtype='float64')\n",
        "\n",
        "    # Calculate standard deviation - using unweighted for now\n",
        "    agbd_mean_stdev_ha = np.ma.std(prediction_array_masked, dtype='float64')\n",
        "\n",
        "    # Convert total AGB from Mg to Tg\n",
        "    agb_total_tg = np.divide(agb_total_mg, 1000000, dtype='float64')\n",
        "\n",
        "    # Append results to statistics lists\n",
        "    values_forest_cover_ha.append(forest_cover_ha)\n",
        "    values_agbd_mean_mg_ha.append(agbd_mean_mg_ha)\n",
        "    values_agbd_stdev_mg_ha.append(agbd_mean_stdev_ha)\n",
        "    values_agb_total_tg.append(agb_total_tg)\n",
        "\n",
        "    if generate_uncertainty_stats:\n",
        "      # Use lookup dictionary to quickly find matching uncertainty raster\n",
        "      prediction_basename = os.path.basename(prediction_raster)\n",
        "      uncertainty_raster_present = prediction_basename in uncertainty_lookup\n",
        "\n",
        "      if not uncertainty_raster_present:\n",
        "        print(f\"There is no uncertainty raster for {prediction_basename}\")\n",
        "      else:\n",
        "        matching_uncertainty_raster = uncertainty_lookup[prediction_basename]\n",
        "\n",
        "        # Open and mask uncertainty raster to polygon\n",
        "        with rasterio.open(matching_uncertainty_raster) as uncertainty:\n",
        "          nodatavalue = int(uncertainty.nodatavals[0])\n",
        "          uncertainty_array_masked, transform_2 = msk.mask(uncertainty, polygons, crop=True, filled=False)\n",
        "\n",
        "        # Calculate uncertainty with area-weighted approach\n",
        "        uncertainty_ratios = np.divide(uncertainty_array_masked, 100, dtype='float64')\n",
        "        # Multiply uncertainty % by AGBD by pixel area\n",
        "        pixel_ci95_mg = np.multiply(np.multiply(prediction_array_masked, uncertainty_ratios, dtype='float64'),\n",
        "                                  forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "        # Sum to get total CI95 for the area (in Mg)\n",
        "        agb_total_ci95_mg = np.ma.sum(pixel_ci95_mg, dtype='float64')\n",
        "\n",
        "        # Calculate the mean CI95 as percentage of mean AGBD\n",
        "        if abs(agb_total_mg) > 0:  # Use absolute value for division check\n",
        "            # Uncertainty percentage is relative to absolute value\n",
        "            agb_total_ci95_percent = np.divide(abs(agb_total_ci95_mg), abs(agb_total_mg), dtype='float64') * 100\n",
        "            # Preserve sign of original measurement\n",
        "            sign = np.sign(agb_total_mg)\n",
        "            agbd_mean_mg_ha_ci95 = sign * np.multiply(abs(agbd_mean_mg_ha), np.divide(agb_total_ci95_percent, 100, dtype='float64'), dtype='float64')\n",
        "        else:\n",
        "            agb_total_ci95_percent = 0\n",
        "            agbd_mean_mg_ha_ci95 = 0\n",
        "\n",
        "        # Store uncertainty as percentage\n",
        "        agbd_mean_mg_ha_uncertainty = agb_total_ci95_percent\n",
        "\n",
        "        # Calculate total AGB CI95 in Tg - preserve sign\n",
        "        agb_total_tg_ci95 = np.divide(agb_total_ci95_mg, 1000000, dtype='float64')\n",
        "\n",
        "        # Append results to statistics list\n",
        "        values_agbd_mean_mg_ha_ci95.append(agbd_mean_mg_ha_ci95)\n",
        "        values_agbd_mean_mg_ha_uncertainty.append(agbd_mean_mg_ha_uncertainty)\n",
        "        values_agb_total_tg_ci95.append(agb_total_tg_ci95)\n",
        "\n",
        "  # Concatenate new columns to the main DataFrames for each statistic\n",
        "  df_forest_cover_ha = pd.concat([df_forest_cover_ha, pd.DataFrame({sample_polygon_name: values_forest_cover_ha}, index=df_forest_cover_ha.index)], axis=1)\n",
        "  df_agbd_mean_mg_ha = pd.concat([df_agbd_mean_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha}, index=df_agbd_mean_mg_ha.index)], axis=1)\n",
        "  df_agbd_stdev_mg_ha = pd.concat([df_agbd_stdev_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_stdev_mg_ha}, index=df_agbd_stdev_mg_ha.index)], axis=1)\n",
        "  df_agb_total_tg = pd.concat([df_agb_total_tg, pd.DataFrame({sample_polygon_name: values_agb_total_tg}, index=df_agb_total_tg.index)], axis=1)\n",
        "\n",
        "  if generate_uncertainty_stats:\n",
        "      df_agbd_mean_mg_ha_ci95 = pd.concat([df_agbd_mean_mg_ha_ci95, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_ci95}, index=df_agbd_mean_mg_ha_ci95.index)], axis=1)\n",
        "      df_agbd_mean_mg_ha_uncertainty = pd.concat([df_agbd_mean_mg_ha_uncertainty, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_uncertainty}, index=df_agbd_mean_mg_ha_uncertainty.index)], axis=1)\n",
        "      df_agb_total_tg_ci95 = pd.concat([df_agb_total_tg_ci95, pd.DataFrame({sample_polygon_name: values_agb_total_tg_ci95}, index=df_agb_total_tg_ci95.index)], axis=1)\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "  df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_mean_mg_ha_ci95,\n",
        "                 df_agbd_mean_mg_ha_uncertainty, df_agbd_stdev_mg_ha, df_agb_total_tg, df_agb_total_tg_ci95]\n",
        "else: df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Set index of the polygon area km2 dataframe to 'Name' of the polygon\n",
        "df_polygon_area_km2 = df_polygon_area_km2.set_index('Name')\n",
        "\n",
        "# Export statistics for forecast input\n",
        "df_forecast_list = [df_forest_cover_ha, df_agb_total_tg]\n",
        "for df_forecast in df_forecast_list:\n",
        "  df_noalts = df_forecast[~df_forecast.index.str.contains(\"_\")]  # More efficient filtering\n",
        "  if df_forecast.equals(df_forest_cover_ha): df_filename = \"forest_cover_ha\"\n",
        "  if df_forecast.equals(df_agb_total_tg): df_filename = \"agb_total_tg\"\n",
        "  df_noalts.to_csv(join(forecast_input_dir, f'{df_filename}.csv'))\n",
        "\n",
        "# Generate summary stats\n",
        "df_forest_cover_ha_t = df_forest_cover_ha.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest cover (ha)\")\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB (Tg)\")\n",
        "\n",
        "# Use list for more efficient concatenation\n",
        "summary_components = [df_polygon_area_km2, df_forest_cover_ha_t, df_agb_total_tg_t]\n",
        "if generate_uncertainty_stats:\n",
        "  df_agb_total_tg_ci95_t = df_agb_total_tg_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB CI95 (Tg)\")\n",
        "  summary_components.append(df_agb_total_tg_ci95_t)\n",
        "\n",
        "summary_stats = pd.concat(summary_components, axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_stats.to_csv(join(sample_polygons_statistics_dir, 'summary_stats.csv'))\n",
        "\n",
        "# Generate detailed stats by area\n",
        "for polygon_area in df_stats_list[0]:\n",
        "  polygon_area_km2 = df_polygon_area_km2.loc[polygon_area][\"Area (km^2)\"]\n",
        "  df_detailed_stats = df_base.copy()\n",
        "  df_detailed_stats[\"Area (km^2)\"] = polygon_area_km2\n",
        "  for df_stats in df_stats_list:\n",
        "    if df_stats.equals(df_forest_cover_ha): stat_col = \"Forest cover (ha)\"\n",
        "    if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"Forest AGBD mean (Mg / ha)\"\n",
        "    if df_stats.equals(df_agbd_stdev_mg_ha): stat_col = \"Forest AGBD stdev (Mg / ha)\"\n",
        "    if df_stats.equals(df_agb_total_tg): stat_col = \"Forest AGB total (Tg)\"\n",
        "    if generate_uncertainty_stats:\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_ci95): stat_col = \"Forest AGBD CI95 (Mg / ha)\"\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_uncertainty): stat_col = \"Forest AGBD uncertainty (%)\"\n",
        "      if df_stats.equals(df_agb_total_tg_ci95): stat_col = \"Forest AGB total CI95 (Tg)\"\n",
        "    for stats_polygon_area in df_stats:\n",
        "      if stats_polygon_area == polygon_area:\n",
        "        df_stats_renamed = df_stats.rename(columns={stats_polygon_area:stat_col})\n",
        "        df_detailed_stats = pd.concat([df_detailed_stats, df_stats_renamed[stat_col]], axis=1)\n",
        "  df_detailed_stats.to_csv(join(detailed_stats_by_area_dir, f'{polygon_area}.csv'))\n",
        "\n",
        "# Generate detailed stats by scenario - build dictionary once then process\n",
        "scenarios = {}\n",
        "for stats_csv in os.listdir(detailed_stats_by_area_dir):\n",
        "    polygon_name = f\"{stats_csv[:-4]}\"\n",
        "    stats_csv_path = join(detailed_stats_by_area_dir, stats_csv)\n",
        "    stats_csv_df = pd.read_csv(stats_csv_path)\n",
        "    # Process all scenarios for this polygon in one pass\n",
        "    for scenario in stats_csv_df['scenario'].unique():\n",
        "        scenario_df = stats_csv_df[stats_csv_df['scenario'] == scenario].copy()\n",
        "        scenario_df.drop('scenario', axis=1, inplace=True)\n",
        "        scenario_df.insert(0, 'Name', polygon_name)\n",
        "        if scenario in scenarios:\n",
        "            scenarios[scenario] = pd.concat([scenarios[scenario], scenario_df], ignore_index=True)\n",
        "        else:\n",
        "            scenarios[scenario] = scenario_df\n",
        "\n",
        "# Write all scenario CSVs at once\n",
        "for scenario, scenario_df in scenarios.items():\n",
        "    output_file_path = join(detailed_stats_by_scenario_dir,f'{scenario}.csv')\n",
        "    scenario_df.to_csv(output_file_path, index=False)"
      ],
      "metadata": {
        "id": "exDLp760d8tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGzdNKRbo0H5"
      },
      "source": [
        "# Disturbance statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTpXfmfAo7hY"
      },
      "outputs": [],
      "source": [
        "# Create list of available disturbance rasters and scenarios\n",
        "if source_dir == scenarios_dir: dist_raster_dir = join(selected_model_dir, prediction_area, 'scenario_disturbance')\n",
        "if source_dir == predictions_dir: dist_raster_dir = join(selected_model_dir, f'scenarios_{prediction_area}', 'scenario_disturbance')\n",
        "\n",
        "dist_raster_dirs = []\n",
        "dists = set()\n",
        "for dist_raster in os.listdir(dist_raster_dir):\n",
        "  dist_raster_dirs.append(join(dist_raster_dir, dist_raster))\n",
        "  if source_dir == predictions_dir: dists.add(dist_raster.split(\"__\")[1])\n",
        "  if source_dir == scenarios_dir: dists.add(dist_raster.split(\"__\")[0])\n",
        "dists = sorted(list(dists))\n",
        "\n",
        "# Select disturbance rasters to calculate statistics\n",
        "print('selected_dists = [')\n",
        "for dist in dists:\n",
        "  print(f'  \"{dist}\",')\n",
        "print(']')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfyIlrVKwd6S"
      },
      "outputs": [],
      "source": [
        "selected_dists = [\n",
        "  # \"2014_deforestation_total\",\n",
        "  # \"2014_degradation_before_1991\",\n",
        "  # \"2014_degradation_deforestation_total\",\n",
        "  # \"2014_degradation_since_1991\",\n",
        "  # \"2014_degradation_total\",\n",
        "  # \"2015_change_2014\",\n",
        "  # \"2016_change_2015\",\n",
        "  # \"2017_change_2016\",\n",
        "  # \"2018_change_2017\",\n",
        "  # \"2019_change_2018\",\n",
        "  # \"2020_change_2019\",\n",
        "  # \"2021_change_2020\",\n",
        "  # \"2021_deforestation_total\",\n",
        "  # \"2021_degradation_deforestation_total\",\n",
        "  # \"2021_degradation_total\",\n",
        "  # \"2022_change_2021\",\n",
        "  # \"2022_deforestation_before_2022\",\n",
        "  # \"2022_deforestation_since_2022\",\n",
        "  # \"2022_deforestation_total\",\n",
        "  # \"2022_degradation_before_2022\",\n",
        "  # \"2022_degradation_deforestation_before_2022\",\n",
        "  # \"2022_degradation_deforestation_since_2022\",\n",
        "  # \"2022_degradation_deforestation_total\",\n",
        "  # \"2022_degradation_since_2022\",\n",
        "  # \"2022_degradation_total\",\n",
        "  # \"2023_change_2022\",\n",
        "  # \"2023_deforestation_before_2023\",\n",
        "  # \"2023_deforestation_since_2023\",\n",
        "  # \"2023_deforestation_total\",\n",
        "  # \"2023_degradation_before_2023\",\n",
        "  # \"2023_degradation_deforestation_before_2023\",\n",
        "  # \"2023_degradation_deforestation_since_2023\",\n",
        "  # \"2023_degradation_deforestation_total\",\n",
        "  # \"2023_degradation_since_2023\",\n",
        "  # \"2023_degradation_total\",\n",
        "  # \"2024_change_2014\",\n",
        "  # \"2024_change_2023\",\n",
        "  # \"2024_deforestation_before_2015\",\n",
        "  # \"2024_deforestation_before_2022\",\n",
        "  # \"2024_deforestation_before_2023\",\n",
        "  # \"2024_deforestation_before_2024\",\n",
        "  # \"2024_deforestation_since_2015\",\n",
        "  # \"2024_deforestation_since_2022\",\n",
        "  # \"2024_deforestation_since_2023\",\n",
        "  # \"2024_deforestation_since_2024\",\n",
        "  # \"2024_deforestation_total\",\n",
        "  # \"2024_degradation_before_2000\",\n",
        "  # \"2024_degradation_before_2015\",\n",
        "  # \"2024_degradation_before_2022\",\n",
        "  # \"2024_degradation_before_2023\",\n",
        "  # \"2024_degradation_before_2024\",\n",
        "  # \"2024_degradation_deforestation_before_2015\",\n",
        "  # \"2024_degradation_deforestation_before_2022\",\n",
        "  # \"2024_degradation_deforestation_before_2023\",\n",
        "  # \"2024_degradation_deforestation_before_2024\",\n",
        "  # \"2024_degradation_deforestation_since_2015\",\n",
        "  # \"2024_degradation_deforestation_since_2022\",\n",
        "  # \"2024_degradation_deforestation_since_2023\",\n",
        "  # \"2024_degradation_deforestation_since_2024\",\n",
        "  # \"2024_degradation_deforestation_total\",\n",
        "  # \"2024_degradation_since_2000\",\n",
        "  # \"2024_degradation_since_2015\",\n",
        "  # \"2024_degradation_since_2022\",\n",
        "  # \"2024_degradation_since_2023\",\n",
        "  # \"2024_degradation_since_2024\",\n",
        "  # \"2024_degradation_total\",\n",
        "  \"2024_disturbance_road\",\n",
        "]\n",
        "\n",
        "# Filter to selected disturbances, and separate prediction and uncertainty rasters\n",
        "dist_rasters = []\n",
        "dist_uncertainty_rasters = []\n",
        "for dist_raster in dist_raster_dirs:\n",
        "  for dist in selected_dists:\n",
        "    if source_dir == predictions_dir:\n",
        "      if dist == dist_raster.split('/')[-1].split('__')[1] and 'mean__' in dist_raster:\n",
        "        dist_rasters.append(dist_raster)\n",
        "      if dist == dist_raster.split('/')[-1].split('__')[1] and 'uncertainty__' in dist_raster:\n",
        "        dist_uncertainty_rasters.append(dist_raster)\n",
        "    else: # If the source directory is scenarios_dir (without uncertainty values)\n",
        "      if dist == dist_raster.split('/')[-1].split('__')[0]: dist_rasters.append(dist_raster)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "dist_rasters = sorted(dist_rasters)\n",
        "dist_uncertainty_rasters = sorted(dist_uncertainty_rasters)\n",
        "\n",
        "# Create lookup dictionary for faster uncertainty matching\n",
        "uncertainty_lookup = {}\n",
        "for uncertainty_raster in dist_uncertainty_rasters:\n",
        "  base_name = os.path.basename(uncertainty_raster).replace('uncertainty__', 'mean__')\n",
        "  uncertainty_lookup[base_name] = uncertainty_raster\n",
        "\n",
        "# Toggle whether to predict uncertainty stats\n",
        "generate_uncertainty_stats = bool(dist_uncertainty_rasters)\n",
        "\n",
        "# Generate empty dataframes for statistics\n",
        "df_base = pd.DataFrame(index = selected_dists)\n",
        "df_base.rename_axis('dist', inplace=True)\n",
        "df_agbd_mean_mg_ha, df_agb_total_tg = df_base.copy(), df_base.copy()\n",
        "# If uncertainty rasters are present, generate empty dataframes\n",
        "if generate_uncertainty_stats:\n",
        "  df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agb_total_tg_ci95 = df_base.copy(), df_base.copy(), df_base.copy()\n",
        "\n",
        "# Initialise polygon area dataframe\n",
        "df_polygon_area_km2 = pd.DataFrame(columns = [\"Name\", \"Area (km^2)\"])\n",
        "\n",
        "# Load cell area raster for accurate pixel-by-pixel area calculations\n",
        "cell_area_path = join(areas_dir, \"cell_area.tif\")\n",
        "\n",
        "# Loop through each polygon stored in GPKG to generate statistics\n",
        "for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "\n",
        "  # Define the polygon\n",
        "  sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"]\n",
        "  polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "  # Calculate sample_polygon_geometry area (ellipsoidal as opposed to planimetric)\n",
        "  sample_polygons_crs = selected_sample_polygons_gpkg.crs\n",
        "  temp_gdf = gpd.GeoDataFrame({'name': [sample_polygon_name], 'geometry': sample_polygon_geometry}, crs=sample_polygons_crs)\n",
        "  temp_gdf_utm = temp_gdf.estimate_utm_crs()\n",
        "  polygon_area_ha = np.divide(temp_gdf.to_crs(temp_gdf_utm).area[0], 10000, dtype='float64')\n",
        "\n",
        "  # Mask the cell area raster to the polygon once for efficiency\n",
        "  with rasterio.open(cell_area_path) as cell_area:\n",
        "    cell_area_masked, transform_1 = msk.mask(cell_area, polygons, crop=True, filled=False)\n",
        "\n",
        "  # Calculate total area of all pixels within polygon in hectares\n",
        "  pixel_area_sum_m2 = np.ma.sum(cell_area_masked, dtype='float64')\n",
        "  pixel_area_sum_ha = np.divide(pixel_area_sum_m2, 10000, dtype='float64')\n",
        "\n",
        "  # Calculate ratio between polygon and raster areas\n",
        "  area_ratio = np.divide(polygon_area_ha, pixel_area_sum_ha, dtype='float64')\n",
        "\n",
        "  # Apply area adjustment based on setting\n",
        "  if adjust_polygon_to_raster:\n",
        "    # Adjust polygon area to match raster area\n",
        "    adjusted_polygon_area_ha = pixel_area_sum_ha\n",
        "    # No adjustment needed for pixel values - keep original cell areas\n",
        "    adjusted_cell_area_masked = cell_area_masked\n",
        "  else:\n",
        "    # Keep original polygon area and adjust individual pixel areas\n",
        "    adjusted_polygon_area_ha = polygon_area_ha\n",
        "    # Adjust each pixel area by the area ratio to maintain total equal to polygon area\n",
        "    adjusted_cell_area_masked = np.multiply(cell_area_masked, area_ratio, dtype='float64')\n",
        "\n",
        "  # Convert adjusted cell areas from m² to ha\n",
        "  adjusted_cell_area_masked_ha = np.divide(adjusted_cell_area_masked, 10000, dtype='float64')\n",
        "\n",
        "  # Add polygon area to dataframe\n",
        "  new_row = pd.DataFrame([{'Name': sample_polygon_name, 'Area (km^2)': adjusted_polygon_area_ha / 100}], dtype=object)\n",
        "  df_polygon_area_km2 = pd.concat([df_polygon_area_km2, new_row], ignore_index=True, sort=False)\n",
        "\n",
        "  # Create empty lists for each disturbance raster statistic\n",
        "  values_agbd_mean_mg_ha, values_agb_total_tg = [], []\n",
        "\n",
        "  # If uncertainty rasters are present, create empty lists for uncertainty statistics\n",
        "  if generate_uncertainty_stats:\n",
        "    values_agbd_mean_mg_ha_ci95, values_agbd_mean_mg_ha_uncertainty, values_agb_total_tg_ci95 = [], [], []\n",
        "\n",
        "  # Loop through disturbance rasters\n",
        "  for dist_raster in dist_rasters:\n",
        "\n",
        "    # Mask feature to sample_polygon_geometry\n",
        "    with rasterio.open(dist_raster) as dist:\n",
        "      nodatavalue = int(dist.nodatavals[0])\n",
        "      dist_array_masked, transform_2 = msk.mask(dist, polygons, crop=True, filled=False)\n",
        "\n",
        "    # Extract forest pixels (non-masked pixels in disturbance array)\n",
        "    forest_pixels_mask = ~np.ma.getmaskarray(dist_array_masked)\n",
        "\n",
        "    # Calculate forest area by summing adjusted cell areas of forest pixels\n",
        "    forest_cell_areas_ha = np.ma.array(adjusted_cell_area_masked_ha.data, mask=~forest_pixels_mask)\n",
        "    forest_cover_ha = np.ma.sum(forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "    # Calculate total biomass by multiplying AGBD by pixel area for each pixel, then summing\n",
        "    pixel_biomass_mg = np.multiply(dist_array_masked, forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "    # Sum to get total biomass in Mg\n",
        "    agb_total_mg = np.ma.sum(pixel_biomass_mg, dtype='float64')\n",
        "\n",
        "    # Calculate mean AGBD (Mg/ha) by dividing total biomass by forest area\n",
        "    agbd_mean_mg_ha = np.divide(agb_total_mg, forest_cover_ha, dtype='float64') if forest_cover_ha > 0 else 0\n",
        "\n",
        "    # Convert total AGB from Mg to Tg\n",
        "    agb_total_tg = np.divide(agb_total_mg, 1000000, dtype='float64')\n",
        "\n",
        "    # Append results to statistics lists\n",
        "    values_agbd_mean_mg_ha.append(agbd_mean_mg_ha)\n",
        "    values_agb_total_tg.append(agb_total_tg)\n",
        "\n",
        "    if generate_uncertainty_stats:\n",
        "      # Use lookup dictionary to quickly find matching uncertainty raster\n",
        "      dist_basename = os.path.basename(dist_raster)\n",
        "      uncertainty_raster_present = dist_basename in uncertainty_lookup\n",
        "\n",
        "      if not uncertainty_raster_present:\n",
        "        print(f\"There is no uncertainty raster for {dist_basename}\")\n",
        "      else:\n",
        "        matching_uncertainty_raster = uncertainty_lookup[dist_basename]\n",
        "\n",
        "        # Open and mask uncertainty raster to polygon\n",
        "        with rasterio.open(matching_uncertainty_raster) as uncertainty:\n",
        "          nodatavalue = int(uncertainty.nodatavals[0])\n",
        "          uncertainty_array_masked, transform_2 = msk.mask(uncertainty, polygons, crop=True, filled=False)\n",
        "\n",
        "        # Calculate uncertainty with area-weighted approach\n",
        "        uncertainty_ratios = np.divide(uncertainty_array_masked, 100, dtype='float64')\n",
        "        # Multiply uncertainty % by AGBD by pixel area\n",
        "        pixel_ci95_mg = np.multiply(np.multiply(dist_array_masked, uncertainty_ratios, dtype='float64'),\n",
        "                                  forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "        # Sum to get total CI95 for the area (in Mg)\n",
        "        agb_total_ci95_mg = np.ma.sum(pixel_ci95_mg, dtype='float64')\n",
        "\n",
        "        # Calculate the mean CI95 as percentage of mean AGBD\n",
        "        if abs(agb_total_mg) > 0:  # Use absolute value for division check\n",
        "            # Uncertainty percentage is relative to absolute value\n",
        "            agb_total_ci95_percent = np.divide(abs(agb_total_ci95_mg), abs(agb_total_mg), dtype='float64') * 100\n",
        "            # Preserve sign of original measurement\n",
        "            sign = np.sign(agb_total_mg)\n",
        "            agbd_mean_mg_ha_ci95 = sign * np.multiply(abs(agbd_mean_mg_ha), np.divide(agb_total_ci95_percent, 100, dtype='float64'), dtype='float64')\n",
        "        else:\n",
        "            agb_total_ci95_percent = 0\n",
        "            agbd_mean_mg_ha_ci95 = 0\n",
        "\n",
        "        # Store uncertainty as percentage\n",
        "        agbd_mean_mg_ha_uncertainty = agb_total_ci95_percent\n",
        "\n",
        "        # Calculate total AGB CI95 in Tg - preserve sign\n",
        "        agb_total_tg_ci95 = np.divide(agb_total_ci95_mg, 1000000, dtype='float64')\n",
        "\n",
        "        # Append results to statistics list\n",
        "        values_agbd_mean_mg_ha_ci95.append(agbd_mean_mg_ha_ci95)\n",
        "        values_agbd_mean_mg_ha_uncertainty.append(agbd_mean_mg_ha_uncertainty)\n",
        "        values_agb_total_tg_ci95.append(agb_total_tg_ci95)\n",
        "\n",
        "  # Concatenate new columns to the main DataFrames for each statistic\n",
        "  df_agbd_mean_mg_ha = pd.concat([df_agbd_mean_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha}, index=df_agbd_mean_mg_ha.index)], axis=1)\n",
        "  df_agb_total_tg = pd.concat([df_agb_total_tg, pd.DataFrame({sample_polygon_name: values_agb_total_tg}, index=df_agb_total_tg.index)], axis=1)\n",
        "\n",
        "  if generate_uncertainty_stats:\n",
        "      df_agbd_mean_mg_ha_ci95 = pd.concat([df_agbd_mean_mg_ha_ci95, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_ci95}, index=df_agbd_mean_mg_ha_ci95.index)], axis=1)\n",
        "      df_agbd_mean_mg_ha_uncertainty = pd.concat([df_agbd_mean_mg_ha_uncertainty, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_uncertainty}, index=df_agbd_mean_mg_ha_uncertainty.index)], axis=1)\n",
        "      df_agb_total_tg_ci95 = pd.concat([df_agb_total_tg_ci95, pd.DataFrame({sample_polygon_name: values_agb_total_tg_ci95}, index=df_agb_total_tg_ci95.index)], axis=1)\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "  df_stats_list = [df_agbd_mean_mg_ha, df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agb_total_tg, df_agb_total_tg_ci95]\n",
        "else: df_stats_list = [df_agbd_mean_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Set index of the polygon area km2 dataframe to 'Name' of the polygon\n",
        "df_polygon_area_km2 = df_polygon_area_km2.set_index('Name')\n",
        "\n",
        "# Generate summary stats\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB (Tg)\")\n",
        "\n",
        "# Use list for more efficient concatenation\n",
        "summary_components = [df_polygon_area_km2, df_agb_total_tg_t]\n",
        "if generate_uncertainty_stats:\n",
        "  df_agb_total_tg_ci95_t = df_agb_total_tg_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB CI95 (Tg)\")\n",
        "  summary_components.append(df_agb_total_tg_ci95_t)\n",
        "\n",
        "summary_stats = pd.concat(summary_components, axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_stats.to_csv(join(sample_polygons_statistics_dir, 'summary_dist_stats.csv'))\n",
        "\n",
        "# Generate detailed stats by polygon\n",
        "for polygon_area in df_stats_list[0]:\n",
        "  polygon_area_km2 = df_polygon_area_km2.loc[polygon_area][\"Area (km^2)\"]\n",
        "  df_detailed_dist_stats = df_base.copy()\n",
        "  df_detailed_dist_stats[\"Area (km^2)\"] = polygon_area_km2\n",
        "  for df_stats in df_stats_list:\n",
        "    if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"Forest AGBD mean (Mg / ha)\"\n",
        "    if df_stats.equals(df_agb_total_tg): stat_col = \"Forest AGB total (Tg)\"\n",
        "    if generate_uncertainty_stats:\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_ci95): stat_col = \"Forest AGBD CI95 (Mg / ha)\"\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_uncertainty): stat_col = \"Forest AGBD uncertainty (%)\"\n",
        "      if df_stats.equals(df_agb_total_tg_ci95): stat_col = \"Forest AGB total CI95 (Tg)\"\n",
        "    for stats_polygon_area in df_stats:\n",
        "      if stats_polygon_area == polygon_area:\n",
        "        df_stats_renamed = df_stats.rename(columns={stats_polygon_area:stat_col})\n",
        "        df_detailed_dist_stats = pd.concat([df_detailed_dist_stats, df_stats_renamed[stat_col]], axis=1)\n",
        "  df_detailed_dist_stats.to_csv(join(detailed_dist_stats_by_area_dir, f'{polygon_area}.csv'))\n",
        "\n",
        "# Generate detailed stats by disturbance type - build dictionary once then process\n",
        "dists = {}\n",
        "for stats_csv in os.listdir(detailed_dist_stats_by_area_dir):\n",
        "    polygon_name = f\"{stats_csv[:-4]}\"\n",
        "    stats_csv_path = join(detailed_dist_stats_by_area_dir, stats_csv)\n",
        "    stats_csv_df = pd.read_csv(stats_csv_path)\n",
        "    # Process all disturbance types for this polygon in one pass\n",
        "    for dist in stats_csv_df['dist'].unique():\n",
        "        dist_df = stats_csv_df[stats_csv_df['dist'] == dist].copy()\n",
        "        dist_df.drop('dist', axis=1, inplace=True)\n",
        "        dist_df.insert(0, 'Name', polygon_name)\n",
        "        if dist in dists:\n",
        "            dists[dist] = pd.concat([dists[dist], dist_df], ignore_index=True)\n",
        "        else:\n",
        "            dists[dist] = dist_df\n",
        "\n",
        "# Write all disturbance CSVs at once\n",
        "for dist, dist_df in dists.items():\n",
        "    output_file_path = join(detailed_dist_stats_by_scenario_dir,f'{dist}.csv')\n",
        "    dist_df.to_csv(output_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intactness statistics"
      ],
      "metadata": {
        "id": "6t45TVipaVma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of available percentage change and intactness rasters\n",
        "if source_dir == scenarios_dir:\n",
        "    intactness_dir = join(selected_model_dir, prediction_area, 'intactness')\n",
        "if source_dir == predictions_dir:\n",
        "    intactness_dir = join(selected_model_dir, f'scenarios_{prediction_area}', 'intactness')\n",
        "\n",
        "percentage_change_rasters = []\n",
        "intactness_rasters = []\n",
        "for root, dirs, files in os.walk(intactness_dir):\n",
        "    for file in files:\n",
        "        if \"intactness__\" in file and file.endswith('tif'):\n",
        "            relative_path = os.path.relpath(join(root, file), intactness_dir)\n",
        "            intactness_rasters.append(relative_path)\n",
        "\n",
        "# Select intactness rasters to calculate statistics\n",
        "print(\"# Select intactness raster to calculate statistics\")\n",
        "print(\"intactness_rasters = [\")\n",
        "for raster in intactness_rasters:\n",
        "    print(f\"'{raster}',\")\n",
        "print(\"]\")"
      ],
      "metadata": {
        "id": "a6p_6d-0adF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select intactness raster to calculate statistics\n",
        "intactness_rasters = [\n",
        "'all_oldgrowth__2024_degradation_deforestation_total/intactness__asartr_phase_2_10_quantiles__all_oldgrowth__2024_degradation_deforestation_total__agbd_historic_250429_223033.tif',\n",
        "'all_oldgrowth__2024_degradation_deforestation_total/intactness__project_area_10_quantiles__all_oldgrowth__2024_degradation_deforestation_total__agbd_historic_250429_223033.tif',\n",
        "'all_oldgrowth__2024_degradation_deforestation_total/intactness__intactness_wo_tn_10_quantiles__all_oldgrowth__2024_degradation_deforestation_total__agbd_historic_250429_223033.tif',\n",
        "'all_oldgrowth__2024_degradation_deforestation_total/intactness__10_quantiles__all_oldgrowth__2024_degradation_deforestation_total__agbd_historic_250429_223033.tif',\n",
        "'2024_no_degradation_since_2000__2024_degradation_since_2000/intactness__asartr_phase_2_10_quantiles__2024_no_degradation_since_2000__2024_degradation_since_2000__agbd_historic_250429_223033.tif',\n",
        "'2024_no_degradation_since_2000__2024_degradation_since_2000/intactness__project_area_10_quantiles__2024_no_degradation_since_2000__2024_degradation_since_2000__agbd_historic_250429_223033.tif',\n",
        "'2024_no_degradation_since_2000__2024_degradation_since_2000/intactness__intactness_wo_tn_10_quantiles__2024_no_degradation_since_2000__2024_degradation_since_2000__agbd_historic_250429_223033.tif',\n",
        "'2024_no_degradation_since_2000__2024_degradation_since_2000/intactness__10_quantiles__2024_no_degradation_since_2000__2024_degradation_since_2000__agbd_historic_250429_223033.tif',\n",
        "]"
      ],
      "metadata": {
        "id": "T8mA9ax8IjCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load cell area raster for accurate pixel-by-pixel area calculations\n",
        "# The 'mask_forest_all_oldgrowth.tif' must be present in the scenario mask dir\n",
        "cell_area_path = join(areas_dir, \"cell_area.tif\")\n",
        "all_land_mask_path = join(mask_dir, \"mask_forest_all_oldgrowth.tif\")\n",
        "\n",
        "# Match percentage change rasters\n",
        "intactness_percentage_raster_paths = {}\n",
        "for intactness_raster in intactness_rasters:\n",
        "    intactness_raster_path = join(intactness_dir, intactness_raster)\n",
        "    intactness_baseline_dist_dir = intactness_raster.split('/')[0]\n",
        "    percentage_change_filename = f\"percentage_change__{intactness_baseline_dist_dir}__{selected_model}.tif\"\n",
        "    percentage_change_path = join(intactness_dir, intactness_baseline_dist_dir, percentage_change_filename)\n",
        "    intactness_percentage_raster_paths[intactness_raster_path] = percentage_change_path\n",
        "\n",
        "# Function to calculate area-weighted statistics\n",
        "def weighted_stats(values, weights):\n",
        "    # Handle empty arrays\n",
        "    if len(values) == 0:\n",
        "        return None, None\n",
        "\n",
        "    # Calculate weighted mean\n",
        "    weighted_sum = np.sum(values * weights, dtype='float64')\n",
        "    sum_of_weights = np.sum(weights, dtype='float64')\n",
        "    weighted_mean = weighted_sum / sum_of_weights if sum_of_weights > 0 else 0\n",
        "\n",
        "    # Calculate weighted standard deviation\n",
        "    if sum_of_weights > 0:\n",
        "        variance = np.sum(weights * np.square(values - weighted_mean, dtype='float64'), dtype='float64') / sum_of_weights\n",
        "        weighted_std = np.sqrt(variance, dtype='float64')\n",
        "    else:\n",
        "        weighted_std = 0\n",
        "\n",
        "    return weighted_mean, weighted_std\n",
        "\n",
        "# Loop through each polygon stored in GPKG to generate statistics\n",
        "for intactness_raster, percentage_raster in intactness_percentage_raster_paths.items():\n",
        "\n",
        "    polygon_quantiles = intactness_raster.split('/')[-1].split('__')[1]\n",
        "    baseline = intactness_raster.split('/')[-1].split('__')[2]\n",
        "    disturbance = intactness_raster.split('/')[-1].split('__')[3]\n",
        "    current_year = disturbance.split('_')[0]\n",
        "    mask_path = join(mask_dir, f\"mask_forest_{current_year}.tif\")\n",
        "    intactness_csv_name = f\"{polygon_quantiles}__{baseline}__{disturbance}.csv\"\n",
        "    intactness_csv_path = join(intactness_stats_dir, intactness_csv_name)\n",
        "\n",
        "    total_score = int(intactness_raster.split('/')[-1].split('__')[1].split('_')[-2])  # Extracts the quantiles used for total score\n",
        "    total_stdev = int(total_score / 2)\n",
        "\n",
        "    df_intactness_stats = pd.DataFrame(columns=[\n",
        "        \"Name\",\n",
        "        \"Percentage change (remaining forest) mean\",\n",
        "        \"Percentage change (remaining forest) stdev\",\n",
        "        \"Percentage change (non-forest = -100) mean\",\n",
        "        \"Percentage change (non-forest = -100) stdev\",\n",
        "        f\"Intactness (remaining forest) mean / {total_score}\",\n",
        "        f\"Intactness (remaining forest) stdev / {total_stdev}\",\n",
        "        f\"Intactness (non-forest = 0) mean / {total_score}\",\n",
        "        f\"Intactness (non-forest = 0) stdev / {total_stdev}\"\n",
        "    ])\n",
        "\n",
        "    for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "\n",
        "        # Define the polygon\n",
        "        sample_polygon_geometry = row[\"geometry\"]\n",
        "        sample_polygon_name = row[\"name\"]\n",
        "        polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "        # Calculate sample_polygon_geometry area (ellipsoidal as opposed to planimetric)\n",
        "        sample_polygons_crs = selected_sample_polygons_gpkg.crs\n",
        "        temp_gdf = gpd.GeoDataFrame({'name': [sample_polygon_name], 'geometry': sample_polygon_geometry}, crs=sample_polygons_crs)\n",
        "        temp_gdf_utm = temp_gdf.estimate_utm_crs()\n",
        "        polygon_area_ha = np.divide(temp_gdf.to_crs(temp_gdf_utm).area[0], 10000, dtype='float64')\n",
        "\n",
        "        # First check if the intactness raster has any valid data in this polygon\n",
        "        with rasterio.open(intactness_raster) as intactness:\n",
        "            intactness_masked, transform_2 = msk.mask(intactness, polygons, crop=True, filled=False)\n",
        "\n",
        "        # Check if all values are masked (nodatavalue)\n",
        "        if np.ma.count(intactness_masked) == 0:\n",
        "            # No valid intactness data - set all stats to None and continue to next polygon\n",
        "            new_row = pd.DataFrame([{\n",
        "                'Name': sample_polygon_name,\n",
        "                'Percentage change (remaining forest) mean': None,\n",
        "                'Percentage change (remaining forest) stdev': None,\n",
        "                'Percentage change (non-forest = -100) mean': None,\n",
        "                'Percentage change (non-forest = -100) stdev': None,\n",
        "                f'Intactness (remaining forest) mean / {total_score}': None,\n",
        "                f'Intactness (remaining forest) stdev / {total_stdev}': None,\n",
        "                f'Intactness (non-forest = 0) mean / {total_score}': None,\n",
        "                f'Intactness (non-forest = 0) stdev / {total_stdev}': None,\n",
        "            }], dtype=object)\n",
        "\n",
        "            df_intactness_stats = pd.concat([df_intactness_stats, new_row], ignore_index=True)\n",
        "            continue\n",
        "\n",
        "        # Mask the cell area raster to the polygon\n",
        "        with rasterio.open(cell_area_path) as cell_area:\n",
        "            cell_area_masked, transform_ca = msk.mask(cell_area, polygons, crop=True, filled=False)\n",
        "\n",
        "        # Calculate total area of all pixels within polygon in hectares\n",
        "        pixel_area_sum_m2 = np.ma.sum(cell_area_masked, dtype='float64')\n",
        "        pixel_area_sum_ha = np.divide(pixel_area_sum_m2, 10000, dtype='float64')\n",
        "\n",
        "        # Calculate ratio between polygon and raster areas\n",
        "        area_ratio = np.divide(polygon_area_ha, pixel_area_sum_ha, dtype='float64')\n",
        "\n",
        "        # Apply area adjustment based on setting\n",
        "        if adjust_polygon_to_raster:\n",
        "            # No adjustment needed for pixel values - keep original cell areas\n",
        "            adjusted_cell_area_masked = cell_area_masked\n",
        "        else:\n",
        "            # Adjust each pixel area by the area ratio to maintain total equal to polygon area\n",
        "            adjusted_cell_area_masked = np.multiply(cell_area_masked, area_ratio, dtype='float64')\n",
        "\n",
        "        # Convert adjusted cell areas from m² to ha for easier calculations\n",
        "        adjusted_cell_area_masked_ha = np.divide(adjusted_cell_area_masked, 10000, dtype='float64')\n",
        "\n",
        "        # Get land mask from mask directory (1 = land, NoData = non-land)\n",
        "        with rasterio.open(all_land_mask_path) as all_land_mask_raster:\n",
        "            all_land_mask_data, transform_alm = msk.mask(all_land_mask_raster, polygons, crop=True, filled=False)\n",
        "\n",
        "        # Create land mask where True = land\n",
        "        all_land_mask = ~np.ma.getmaskarray(all_land_mask_data) & (all_land_mask_data == 1)\n",
        "\n",
        "        # Get the area of all land pixels\n",
        "        all_land_area_ha = np.ma.array(adjusted_cell_area_masked_ha.data, mask=~all_land_mask)\n",
        "        all_land_area_ha_sum = np.ma.sum(all_land_area_ha, dtype='float64')\n",
        "\n",
        "        # Check if we have any valid land area to process\n",
        "        if all_land_area_ha_sum <= 0:\n",
        "            # No land in this polygon - set all stats to None and continue to next polygon\n",
        "            new_row = pd.DataFrame([{\n",
        "                'Name': sample_polygon_name,\n",
        "                'Percentage change (remaining forest) mean': None,\n",
        "                'Percentage change (remaining forest) stdev': None,\n",
        "                'Percentage change (non-forest = -100) mean': None,\n",
        "                'Percentage change (non-forest = -100) stdev': None,\n",
        "                f'Intactness (remaining forest) mean / {total_score}': None,\n",
        "                f'Intactness (remaining forest) stdev / {total_stdev}': None,\n",
        "                f'Intactness (non-forest = 0) mean / {total_score}': None,\n",
        "                f'Intactness (non-forest = 0) stdev / {total_stdev}': None,\n",
        "            }], dtype=object)\n",
        "\n",
        "            df_intactness_stats = pd.concat([df_intactness_stats, new_row], ignore_index=True)\n",
        "            continue\n",
        "\n",
        "        # Get forest mask for current year from mask raster\n",
        "        with rasterio.open(mask_path) as forest_mask_raster:\n",
        "            forest_mask_data, transform_fm = msk.mask(forest_mask_raster, polygons, crop=True, filled=False)\n",
        "\n",
        "        # Create forest mask where True = forest\n",
        "        forest_mask = ~np.ma.getmaskarray(forest_mask_data) & (forest_mask_data == 1)\n",
        "\n",
        "        # Extract forest areas using the forest mask\n",
        "        forest_cell_areas_ha = np.ma.array(adjusted_cell_area_masked_ha.data, mask=~forest_mask)\n",
        "        forest_area_ha_sum = np.ma.sum(forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "        # Non-forest area (land that is not forest)\n",
        "        non_forest_area_ha = all_land_area_ha_sum - forest_area_ha_sum\n",
        "\n",
        "        # Mask percentage change raster to the polygon\n",
        "        with rasterio.open(percentage_raster) as percent_change:\n",
        "            percent_change_masked, transform_pc = msk.mask(percent_change, polygons, crop=True, filled=False)\n",
        "\n",
        "        # Apply forest mask to percentage change values (for forest-only statistics)\n",
        "        percent_change_forest_only = np.ma.array(percent_change_masked.data, mask=~forest_mask)\n",
        "\n",
        "        if forest_area_ha_sum > 0:\n",
        "            # Extract percentage change values for forest pixels\n",
        "            forest_percent_values = np.ma.compressed(percent_change_forest_only)\n",
        "            forest_percent_weights = np.ma.compressed(forest_cell_areas_ha)\n",
        "\n",
        "            # Calculate area-weighted percentage change statistics for remaining forest\n",
        "            percent_change_forest_mean, percent_change_forest_std = weighted_stats(\n",
        "                forest_percent_values, forest_percent_weights\n",
        "            )\n",
        "\n",
        "            # For all land including non-forest (treated as -100% change)\n",
        "            if non_forest_area_ha > 0:\n",
        "                # Calculate the weighted mean directly\n",
        "                all_mean_numerator = np.sum(forest_percent_values * forest_percent_weights)\n",
        "                all_mean_denominator = forest_area_ha_sum + non_forest_area_ha\n",
        "                all_mean_numerator += non_forest_area_ha * (-100.0)\n",
        "                percent_change_all_mean = all_mean_numerator / all_mean_denominator\n",
        "\n",
        "                # Calculate the weighted variance directly\n",
        "                forest_variance_contribution = np.sum(\n",
        "                    forest_percent_weights * np.square(forest_percent_values - percent_change_all_mean)\n",
        "                )\n",
        "                non_forest_variance_contribution = non_forest_area_ha * np.square((-100.0) - percent_change_all_mean)\n",
        "                all_variance = (forest_variance_contribution + non_forest_variance_contribution) / all_mean_denominator\n",
        "                percent_change_all_std = np.sqrt(all_variance)\n",
        "            else:\n",
        "                # If no non-forest area, all-land stats are the same as forest stats\n",
        "                percent_change_all_mean = percent_change_forest_mean\n",
        "                percent_change_all_std = percent_change_forest_std\n",
        "        else:\n",
        "            # If no forest, set forest stats to None and all-land stats to -100% change\n",
        "            percent_change_forest_mean = percent_change_forest_std = None\n",
        "            percent_change_all_mean = -100.0\n",
        "            percent_change_all_std = 0.0\n",
        "\n",
        "        # Read & mask intactness to polygon (keep inside-polygon nodata in .data)\n",
        "        with rasterio.open(intactness_raster) as src:\n",
        "            nodata_value = src.nodata\n",
        "            raw_int_masked, _ = msk.mask(src, polygons, crop=True, filled=False)\n",
        "\n",
        "        # Mask any pixel == nodata_value on top of \"outside-polygon\"\n",
        "        if nodata_value is not None:\n",
        "            nodata_mask = (raw_int_masked.data == nodata_value)\n",
        "            combined_mask = raw_int_masked.mask | nodata_mask\n",
        "            intactness_masked = np.ma.array(raw_int_masked.data, mask=combined_mask)\n",
        "        else:\n",
        "            intactness_masked = raw_int_masked\n",
        "\n",
        "        # Drop non-forest: combine \"not forest\" with existing mask\n",
        "        # Now mask = True for any pixel that is outside polygon, nodata, or non-forest\n",
        "        masked_intactness = np.ma.array(\n",
        "            intactness_masked.data,\n",
        "            mask=(~forest_mask) | intactness_masked.mask\n",
        "        )\n",
        "\n",
        "        # Compute intactness stats\n",
        "        if forest_area_ha_sum > 0:\n",
        "            # Build a single boolean array for pixels that are valid forest & not nodata\n",
        "            valid_forest_mask = (~intactness_masked.mask) & forest_mask\n",
        "\n",
        "            # Remaining-forest intactness\n",
        "            forest_intact_vals = intactness_masked.data[valid_forest_mask]\n",
        "            forest_intact_weights = adjusted_cell_area_masked_ha.data[valid_forest_mask]\n",
        "            intactness_remaining_mean, intactness_remaining_std = weighted_stats(\n",
        "                forest_intact_vals, forest_intact_weights\n",
        "            )\n",
        "\n",
        "            # All-land intactness (non-forest = 0)\n",
        "            total_land = forest_area_ha_sum + non_forest_area_ha\n",
        "            num = np.sum(forest_intact_vals * forest_intact_weights)\n",
        "            den = total_land\n",
        "            intactness_all_mean = num / den\n",
        "\n",
        "            # Variance: forest + non-forest contributions\n",
        "            var_forest = np.sum(forest_intact_weights * (forest_intact_vals - intactness_all_mean) ** 2)\n",
        "            var_nonforest = non_forest_area_ha * (0 - intactness_all_mean) ** 2\n",
        "            intactness_all_std = np.sqrt((var_forest + var_nonforest) / den)\n",
        "        else:\n",
        "            # No forest present\n",
        "            intactness_remaining_mean = intactness_remaining_std = None\n",
        "            intactness_all_mean = 0.0\n",
        "            intactness_all_std = 0.0\n",
        "\n",
        "        # Create new row with statistics\n",
        "        new_row = pd.DataFrame([{\n",
        "            'Name': sample_polygon_name,\n",
        "            'Percentage change (remaining forest) mean': percent_change_forest_mean,\n",
        "            'Percentage change (remaining forest) stdev': percent_change_forest_std,\n",
        "            'Percentage change (non-forest = -100) mean': percent_change_all_mean,\n",
        "            'Percentage change (non-forest = -100) stdev': percent_change_all_std,\n",
        "            f'Intactness (remaining forest) mean / {total_score}': intactness_remaining_mean,\n",
        "            f'Intactness (remaining forest) stdev / {total_stdev}': intactness_remaining_std,\n",
        "            f'Intactness (non-forest = 0) mean / {total_score}': intactness_all_mean,\n",
        "            f'Intactness (non-forest = 0) stdev / {total_stdev}': intactness_all_std,\n",
        "        }], dtype=object)\n",
        "\n",
        "        # Append to main dataframe\n",
        "        df_intactness_stats = pd.concat([df_intactness_stats, new_row], ignore_index=True)\n",
        "\n",
        "    # Set index to Name and save to CSV\n",
        "    df_intactness_stats = df_intactness_stats.set_index('Name')\n",
        "    df_intactness_stats.to_csv(intactness_csv_path)\n",
        "    print(f\"Saved statistics to {intactness_csv_path}\")"
      ],
      "metadata": {
        "id": "-59x0xOrfaPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report statistics"
      ],
      "metadata": {
        "id": "PC-rviHPi4q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define disturbances for report\n",
        "csv_files = [f[:-4] for f in os.listdir(detailed_dist_stats_by_scenario_dir) if 'change' not in f and f.endswith('.csv')]\n",
        "\n",
        "def get_disturbance_type(filename):\n",
        "    if 'degradation_deforestation' in filename:\n",
        "        return 3  # comes third\n",
        "    elif 'deforestation' in filename:\n",
        "        return 2  # comes second\n",
        "    else:\n",
        "        return 1  # comes first (degradation)\n",
        "\n",
        "# Sort by year, then disturbance type\n",
        "files_by_category = {}\n",
        "for file in csv_files:\n",
        "    year = file.split('_')[0]\n",
        "    dist_type = get_disturbance_type(file)\n",
        "    key = (year, dist_type)\n",
        "\n",
        "    if key not in files_by_category:\n",
        "        files_by_category[key] = []\n",
        "    files_by_category[key].append(file)\n",
        "\n",
        "# Process each category in order\n",
        "sorted_files = []\n",
        "for key in sorted(files_by_category.keys(), key=lambda k: (int(k[0]), k[1])):\n",
        "    files = files_by_category[key]\n",
        "\n",
        "    # First add 'total' files\n",
        "    total_files = [f for f in files if '_total' in f]\n",
        "    sorted_files.extend(total_files)\n",
        "\n",
        "    # Group remaining files by reference year\n",
        "    ref_year_files = {}\n",
        "    for file in files:\n",
        "        if '_total' in file:\n",
        "            continue\n",
        "\n",
        "        ref_year = file.split('_')[-1]\n",
        "        if ref_year not in ref_year_files:\n",
        "            ref_year_files[ref_year] = []\n",
        "        ref_year_files[ref_year].append(file)\n",
        "\n",
        "    # Process each reference year, placing 'since' before 'before'\n",
        "    for ref_year in sorted(ref_year_files.keys(), key=int, reverse=True):\n",
        "        year_files = ref_year_files[ref_year]\n",
        "        since_files = [f for f in year_files if '_since_' in f]\n",
        "        before_files = [f for f in year_files if '_before_' in f]\n",
        "        sorted_files.extend(since_files + before_files)\n",
        "\n",
        "print(\"disturbance_list = [\")\n",
        "for file in sorted_files:\n",
        "    print(f\"    '{file}',\")\n",
        "print(\"]\")"
      ],
      "metadata": {
        "id": "uMGMSKImNygU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define disturbances for report\n",
        "csv_files = [f[:-4] for f in os.listdir(detailed_dist_stats_by_scenario_dir) if 'change' not in f and f.endswith('.csv')]\n",
        "\n",
        "def get_disturbance_type(filename):\n",
        "    if 'degradation_deforestation' in filename: return 3  # comes third\n",
        "    elif 'deforestation' in filename: return 2  # comes second\n",
        "    else: return 1  # comes first (degradation)\n",
        "\n",
        "# Sort by year, then disturbance type\n",
        "files_by_category = {}\n",
        "for file in csv_files:\n",
        "    year = file.split('_')[0]\n",
        "    dist_type = get_disturbance_type(file)\n",
        "    key = (year, dist_type)\n",
        "\n",
        "    if key not in files_by_category:\n",
        "        files_by_category[key] = []\n",
        "    files_by_category[key].append(file)\n",
        "print(\"disturbance_list = [\")\n",
        "current_year = None\n",
        "\n",
        "# Process each category in order\n",
        "for key in sorted(files_by_category.keys(), key=lambda k: (int(k[0]), k[1])):\n",
        "    files = files_by_category[key]\n",
        "    # First add 'total' files\n",
        "    total_files = [f for f in files if '_total' in f]\n",
        "    for file in total_files:\n",
        "        print(f\"    '{file}',\")\n",
        "    # Group remaining files by reference year\n",
        "    ref_year_files = {}\n",
        "    for file in files:\n",
        "        if '_total' in file:\n",
        "            continue\n",
        "        ref_year = file.split('_')[-1]\n",
        "        if ref_year not in ref_year_files:\n",
        "            ref_year_files[ref_year] = []\n",
        "        ref_year_files[ref_year].append(file)\n",
        "    # Process each reference year, placing 'since' before 'before'\n",
        "    for ref_year in sorted(ref_year_files.keys(), key=int, reverse=True):\n",
        "        year_files = ref_year_files[ref_year]\n",
        "        since_files = [f for f in year_files if '_since_' in f]\n",
        "        before_files = [f for f in year_files if '_before_' in f]\n",
        "        # Add 'since' files normally\n",
        "        for file in since_files:\n",
        "            print(f\"    '{file}',\")\n",
        "        # Add 'before' files commented out\n",
        "        for file in before_files:\n",
        "            print(f\"    # '{file}',\")\n",
        "\n",
        "print(\"]\")"
      ],
      "metadata": {
        "id": "BHpAf6qg1dEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scenario_list = [\n",
        "# '1990_oldgrowth',\n",
        "# '2014',\n",
        "# '2014_no_degradation_since_1991',\n",
        "# '2014_oldgrowth',\n",
        "# '2015',\n",
        "# '2016',\n",
        "# '2017',\n",
        "# '2018',\n",
        "# '2019',\n",
        "# '2020',\n",
        "# '2021',\n",
        "# '2021_oldgrowth',\n",
        "'2022',\n",
        "# '2022_alternate_degradation_2021',\n",
        "# '2022_no_degradation_since_2022',\n",
        "# '2022_oldgrowth',\n",
        "# '2023',\n",
        "# '2023_alternate_degradation_2022',\n",
        "# '2023_no_degradation_since_2023',\n",
        "# '2023_oldgrowth',\n",
        "# '2024',\n",
        "# '2024_alternate_degradation_2014',\n",
        "# '2024_alternate_degradation_2021',\n",
        "# '2024_alternate_degradation_2022',\n",
        "# '2024_alternate_degradation_2023',\n",
        "# '2024_no_degradation_since_2015',\n",
        "# '2024_no_degradation_since_2022',\n",
        "# '2024_no_degradation_since_2023',\n",
        "# '2024_no_degradation_since_2024',\n",
        "# '2024_oldgrowth',\n",
        "'all_oldgrowth',\n",
        "]\n",
        "\n",
        "disturbance_list = [\n",
        "    # '2014_degradation_total',\n",
        "    # '2014_degradation_since_1991',\n",
        "    # # '2014_degradation_before_1991',\n",
        "    # '2014_deforestation_total',\n",
        "    # '2014_degradation_deforestation_total',\n",
        "    # '2021_degradation_total',\n",
        "    # '2021_deforestation_total',\n",
        "    # '2021_degradation_deforestation_total',\n",
        "    '2022_degradation_total',\n",
        "    '2022_degradation_since_2022',\n",
        "    # '2022_degradation_before_2022',\n",
        "    '2022_deforestation_total',\n",
        "    '2022_deforestation_since_2022',\n",
        "    # '2022_deforestation_before_2022',\n",
        "    '2022_degradation_deforestation_total',\n",
        "    '2022_degradation_deforestation_since_2022',\n",
        "    '2022_degradation_deforestation_before_2022',\n",
        "    # '2023_degradation_total',\n",
        "    # '2023_degradation_since_2023',\n",
        "    # # '2023_degradation_before_2023',\n",
        "    # '2023_deforestation_total',\n",
        "    # '2023_deforestation_since_2023',\n",
        "    # # '2023_deforestation_before_2023',\n",
        "    # '2023_degradation_deforestation_total',\n",
        "    # '2023_degradation_deforestation_since_2023',\n",
        "    # '2023_degradation_deforestation_before_2023',\n",
        "    # '2024_degradation_total',\n",
        "    # '2024_degradation_since_2024',\n",
        "    # # '2024_degradation_before_2024',\n",
        "    # '2024_degradation_since_2023',\n",
        "    # # '2024_degradation_before_2023',\n",
        "    # '2024_degradation_since_2022',\n",
        "    # # '2024_degradation_before_2022',\n",
        "    # '2024_degradation_since_2015',\n",
        "    # # '2024_degradation_before_2015',\n",
        "    # '2024_deforestation_total',\n",
        "    # '2024_deforestation_since_2024',\n",
        "    # # '2024_deforestation_before_2024',\n",
        "    # '2024_deforestation_since_2023',\n",
        "    # # '2024_deforestation_before_2023',\n",
        "    # '2024_deforestation_since_2022',\n",
        "    # # '2024_deforestation_before_2022',\n",
        "    # '2024_deforestation_since_2015',\n",
        "    # # '2024_deforestation_before_2015',\n",
        "    # '2024_degradation_deforestation_total',\n",
        "    # '2024_degradation_deforestation_since_2024',\n",
        "    # # '2024_degradation_deforestation_before_2024',\n",
        "    # '2024_degradation_deforestation_since_2023',\n",
        "    # # '2024_degradation_deforestation_before_2023',\n",
        "    # '2024_degradation_deforestation_since_2022',\n",
        "    # # '2024_degradation_deforestation_before_2022',\n",
        "    # '2024_degradation_deforestation_since_2015',\n",
        "    # # '2024_degradation_deforestation_before_2015',\n",
        "]\n",
        "\n",
        "report_year = '2022'\n",
        "\n",
        "all_land_scenario = None\n",
        "for scenario in scenario_list:\n",
        "  if 'all' in scenario:\n",
        "    all_land_scenario = scenario\n",
        "if all_land_scenario == None: print(\"No all land scenario exists in the detailed stats.\")\n",
        "\n",
        "# Read summary stats\n",
        "summary_stats_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_stats.csv'))\n",
        "summary_dist_stats_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_dist_stats.csv'))\n",
        "\n",
        "# Create attributes CSV\n",
        "attributes = pd.DataFrame()\n",
        "attributes['Name'] = summary_stats_df['Unnamed: 0']\n",
        "attributes['Area (km^2)'] = summary_stats_df['Area (km^2)']\n",
        "attributes[f'{report_year} forest cover (ha)'] = summary_stats_df[f'{report_year} forest cover (ha)']\n",
        "attributes[f'{all_land_scenario} forest cover (ha)'] = summary_stats_df[f'{all_land_scenario} forest cover (ha)']\n",
        "attributes.to_csv(join(report_statistics_dir, f'{report_year}_attributes.csv'), index=False)\n",
        "\n",
        "# Create scenarios total AGB CSV\n",
        "scenarios_total_agb = pd.DataFrame()\n",
        "scenarios_total_agb['Name'] = summary_stats_df['Unnamed: 0']\n",
        "for scenario in scenario_list:\n",
        "  scenarios_total_agb[f'{scenario} forest AGB (Tg)'] = summary_stats_df[f'{scenario} forest AGB (Tg)']\n",
        "if source_dir == predictions_dir:\n",
        "  for scenario in scenario_list:\n",
        "    scenarios_total_agb[f'{scenario} forest AGB CI95 (Tg)'] = summary_stats_df[f'{scenario} forest AGB CI95 (Tg)']\n",
        "scenarios_total_agb.to_csv(join(report_statistics_dir, f'{report_year}_scenarios_total_agb.csv'), index=False)\n",
        "\n",
        "# Create scenarios AGBD CSV\n",
        "scenarios_agbd = pd.DataFrame()\n",
        "scenarios_agbd['Name'] = summary_stats_df['Unnamed: 0']\n",
        "for scenario in scenario_list:\n",
        "  scenario_detailed_stats_df = pd.read_csv(join(detailed_stats_by_scenario_dir, f'{scenario}.csv'))\n",
        "  scenarios_agbd[f'{scenario} forest AGBD (Mg / ha)'] = scenario_detailed_stats_df['Forest AGBD mean (Mg / ha)']\n",
        "if source_dir == predictions_dir:\n",
        "  for scenario in scenario_list:\n",
        "    scenario_detailed_stats_df = pd.read_csv(join(detailed_stats_by_scenario_dir, f'{scenario}.csv'))\n",
        "    scenarios_agbd[f'{scenario} forest AGBD CI95 (Mg / ha)'] = scenario_detailed_stats_df['Forest AGBD CI95 (Mg / ha)']\n",
        "scenarios_agbd.to_csv(join(report_statistics_dir, f'{report_year}_scenarios_agbd.csv'), index=False)\n",
        "\n",
        "# Create disturbance total AGB CSV\n",
        "disturbance_total_agb = pd.DataFrame()\n",
        "disturbance_total_agb['Name'] = summary_dist_stats_df['Unnamed: 0']\n",
        "for disturbance in disturbance_list:\n",
        "  disturbance_total_agb[f'{disturbance} forest AGB (Tg)'] = summary_dist_stats_df[f'{disturbance} forest AGB (Tg)']\n",
        "if source_dir == predictions_dir:\n",
        "  for disturbance in disturbance_list:\n",
        "    disturbance_total_agb[f'{disturbance} forest AGB CI95 (Tg)'] = summary_dist_stats_df[f'{disturbance} forest AGB CI95 (Tg)']\n",
        "disturbance_total_agb.to_csv(join(report_statistics_dir, f'{report_year}_disturbance_total_agb.csv'), index=False)\n",
        "\n",
        "# Create disturbance AGBD CSV\n",
        "disturbance_agbd = pd.DataFrame()\n",
        "disturbance_agbd['Name'] = summary_stats_df['Unnamed: 0']\n",
        "for disturbance in disturbance_list:\n",
        "  disturbance_detailed_stats_df = pd.read_csv(join(detailed_dist_stats_by_scenario_dir, f'{disturbance}.csv'))\n",
        "  disturbance_agbd[f'{disturbance} forest AGBD (Mg / ha)'] = disturbance_detailed_stats_df['Forest AGBD mean (Mg / ha)']\n",
        "if source_dir == predictions_dir:\n",
        "  for disturbance in disturbance_list:\n",
        "    disturbance_detailed_stats_df = pd.read_csv(join(detailed_dist_stats_by_scenario_dir, f'{disturbance}.csv'))\n",
        "    disturbance_agbd[f'{disturbance} forest AGBD CI95 (Mg / ha)'] = disturbance_detailed_stats_df['Forest AGBD CI95 (Mg / ha)']\n",
        "disturbance_agbd.to_csv(join(report_statistics_dir, f'{report_year}_disturbance_agbd.csv'), index=False)"
      ],
      "metadata": {
        "id": "QCMvXcIa2Q0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sankey plots"
      ],
      "metadata": {
        "id": "b70oSO_VNtkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and create directories\n",
        "sankey_labelled = join(sample_polygons_statistics_dir, 'sankey_labelled')\n",
        "sankey_unlabelled = join(sample_polygons_statistics_dir, 'sankey_unlabelled')\n",
        "sankey_labelled_svg = join(sample_polygons_statistics_dir, 'sankey_labelled_svg')\n",
        "sankey_unlabelled_svg = join(sample_polygons_statistics_dir, 'sankey_unlabelled_svg')\n",
        "\n",
        "for dir in [sankey_labelled, sankey_unlabelled, sankey_labelled_svg, sankey_unlabelled_svg]:\n",
        "    makedirs(dir, exist_ok=True)\n",
        "\n",
        "# Load the CSV files\n",
        "summary_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_stats.csv'))\n",
        "summary_dist_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_dist_stats.csv'))\n",
        "\n",
        "# Check that all rows in both .csv files have the same strings (polygon areas) in column A\n",
        "polygon_areas_stats = summary_stats.iloc[:, 0]\n",
        "polygon_areas_dist_stats = summary_dist_stats.iloc[:, 0]\n",
        "\n",
        "assert all(polygon_areas_stats == polygon_areas_dist_stats), \"Polygon areas do not match between the two CSV files.\"\n",
        "\n",
        "# Print columns of summary_stats\n",
        "print(\"Columns in summary_stats:\")\n",
        "for i, col in enumerate(summary_stats.columns[1:]):\n",
        "    print(col, end=' ')\n",
        "    if (i+1) % 5 == 0:\n",
        "        print()\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print columns of summary_dist_stats\n",
        "print(\"Columns in summary_dist_stats:\")\n",
        "for i, col in enumerate(summary_dist_stats.columns[1:]):\n",
        "    print(col, end=' ')\n",
        "    if (i+1) % 5 == 0:\n",
        "        print()"
      ],
      "metadata": {
        "id": "JHEy0mBX2yNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot degradation and deforestation separately\n",
        "separate_disturbance = True\n",
        "# Plot degradation before and since a date separately\n",
        "separate_degradation = True\n",
        "# Plot deforestation before and since a date separately\n",
        "separate_deforestation = True\n",
        "\n",
        "# DPI (default is 96, output image will scale accordingly)\n",
        "dpi = 300\n",
        "# Relative width modifier (ratio, e.g. 0.5 or 2)\n",
        "width_modifier = 0.85\n",
        "\n",
        "# Title (polygon area), density and label variables (weight of 800 ~ bold, 400 ~ normal)\n",
        "show_title = True\n",
        "show_density = True\n",
        "show_labels = True\n",
        "left_axis_label = True\n",
        "svg_transparent_background = True\n",
        "title_font_size = 20\n",
        "title_font_weight = 600\n",
        "density_font_size = 17\n",
        "density_font_weight = 600\n",
        "label_font_size = 17\n",
        "label_font_weight = 600\n",
        "\n",
        "# Base columns and year (summary_stats)\n",
        "old_growth_agb_column = 'all_oldgrowth forest AGB (Tg)'\n",
        "current_agb_column = '2024 forest AGB (Tg)'\n",
        "current_year = current_agb_column.split(' ')[0] # Usually first word of current_agb_column\n",
        "\n",
        "# Disturbance columns (summary_dist_stats)\n",
        "degradation_before_column = '2024_degradation_before_2015 forest AGB (Tg)'\n",
        "degradation_since_column = '2024_degradation_since_2015 forest AGB (Tg)'\n",
        "degradation_total_column = '2024_degradation_total forest AGB (Tg)'\n",
        "deforestation_before_column = '2024_deforestation_before_2015 forest AGB (Tg)'\n",
        "deforestation_since_column = '2024_deforestation_since_2015 forest AGB (Tg)'\n",
        "deforestation_total_column = '2024_deforestation_total forest AGB (Tg)'\n",
        "disturbance_total_column = '2024_degradation_deforestation_total forest AGB (Tg)'\n",
        "\n",
        "# Node labels and colours\n",
        "remaining_name = f'Remaining in {current_year}:'\n",
        "remaining_colour = '#007fff' # Blue\n",
        "degradation_before_name = 'Degradation loss before 2015'\n",
        "degradation_before_colour = '#1a801a'  # Dark green\n",
        "degradation_since_name = 'Degradation loss since 2015'\n",
        "degradation_since_colour = '#8dc00d'  # Light green\n",
        "degradation_total_name = 'Degradation loss'\n",
        "degradation_total_colour = '#8dc00d'  # Light green\n",
        "deforestation_before_name = 'Deforestation loss before 2015'\n",
        "deforestation_before_colour = '#ffffff'  # White\n",
        "deforestation_since_name = 'Deforestation loss since 2015'\n",
        "deforestation_since_colour = '#ffff00'  # Yellow\n",
        "deforestation_total_name = 'Deforestation loss'\n",
        "deforestation_total_colour = '#ffffff'  # White\n",
        "disturbance_total_name = 'Disturbance loss'\n",
        "disturbance_total_colour = '#ffffff'  # White\n",
        "\n",
        "# Assert checking separate_disturbance is True if separate_degradation or separate_deforestation is True\n",
        "assert not separate_degradation or separate_disturbance, \"separate_disturbance must be True if separate_degradation is True.\"\n",
        "assert not separate_deforestation or separate_disturbance, \"separate_disturbance must be True if separate_deforestation is True.\"\n",
        "\n",
        "# Function to get values from statistics\n",
        "def get_value(df, idx, column_name):\n",
        "    try:\n",
        "        value = df.loc[idx, column_name]\n",
        "        if pd.isnull(value): return 0.0\n",
        "        else: return float(value)\n",
        "    except KeyError:\n",
        "        print(f\"Column '{column_name}' not found in the dataframe.\")\n",
        "        return 0.0\n",
        "\n",
        "# Loop through each row (polygon area)\n",
        "for idx in summary_stats.index:\n",
        "\n",
        "    # Get the polygon name\n",
        "    polygon_name = summary_stats.iloc[idx, 0]\n",
        "\n",
        "    # Get old-growth and current AGB values\n",
        "    old_growth_agb = get_value(summary_stats, idx, old_growth_agb_column)\n",
        "    current_agb = get_value(summary_stats, idx, current_agb_column)\n",
        "\n",
        "    # Get values from summary_dist_stats\n",
        "    degradation_before = get_value(summary_dist_stats, idx, degradation_before_column)\n",
        "    degradation_since = get_value(summary_dist_stats, idx, degradation_since_column)\n",
        "    degradation_total = get_value(summary_dist_stats, idx, degradation_total_column)\n",
        "    deforestation_before = get_value(summary_dist_stats, idx, deforestation_before_column)\n",
        "    deforestation_since = get_value(summary_dist_stats, idx, deforestation_since_column)\n",
        "    deforestation_total = get_value(summary_dist_stats, idx, deforestation_total_column)\n",
        "    disturbance_total = get_value(summary_dist_stats, idx, disturbance_total_column)\n",
        "\n",
        "    # Load detailed stats to get mean AGBD and CI95 values\n",
        "    detailed_stats_df = pd.read_csv(join(detailed_stats_by_area_dir, f\"{polygon_name}.csv\"))\n",
        "    old_growth_index = detailed_stats_df.index[detailed_stats_df['scenario'] == f\"{old_growth_agb_column.split(' ')[0]}\"].item()\n",
        "    old_growth_mean_agbd = get_value(detailed_stats_df, old_growth_index, \"Forest AGBD mean (Mg / ha)\")\n",
        "    current_index = detailed_stats_df.index[detailed_stats_df['scenario'] == f\"{current_agb_column.split(' ')[0]}\"].item()\n",
        "    current_mean_agbd = get_value(detailed_stats_df, current_index, \"Forest AGBD mean (Mg / ha)\")\n",
        "    uncertainty = False # Uncertainty may not have been calculated\n",
        "    if 'Forest AGB total CI95 (Tg)' in detailed_stats_df.columns:\n",
        "      uncertainty = True # CI95 will be divided by 2 for margin of error\n",
        "      old_growth_agb_ci95 = get_value(detailed_stats_df, old_growth_index, \"Forest AGB total CI95 (Tg)\")\n",
        "      old_growth_mean_agbd_ci95 = get_value(detailed_stats_df, old_growth_index, \"Forest AGBD CI95 (Mg / ha)\")\n",
        "      current_agb_ci95 = get_value(detailed_stats_df, current_index, \"Forest AGB total CI95 (Tg)\")\n",
        "      current_mean_agbd_ci95 = get_value(detailed_stats_df, current_index, \"Forest AGBD CI95 (Mg / ha)\")\n",
        "\n",
        "    # Title line 1 name\n",
        "    title_name = f\"{polygon_name}\"\n",
        "\n",
        "    # Subtitle line 1 name\n",
        "    if uncertainty: subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} ± {old_growth_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "    else: subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} Mg / ha\"\n",
        "\n",
        "    # Subtitle line 2 name\n",
        "    if uncertainty: subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} ± {current_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "    else: subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} Mg / ha\"\n",
        "\n",
        "    # Left axis name\n",
        "    if left_axis_label:\n",
        "      if uncertainty: left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} ± {old_growth_agb_ci95:.2f} Tg\"\n",
        "      else: left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} Tg\"\n",
        "    else: left_axis = ''\n",
        "\n",
        "    # Update remaining_name with AGB\n",
        "    if uncertainty: remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} ± {current_agb_ci95:.2f} Tg\"\n",
        "    else: remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} Tg\"\n",
        "\n",
        "    # Depending on the settings, perform assertions and plot\n",
        "    if separate_disturbance and separate_degradation and separate_deforestation:\n",
        "        assert abs(degradation_before + degradation_since - degradation_total) < 1e-9, f\"{polygon_name}: degradation_before_column + degradation_since_column != degradation_total_column\"\n",
        "        assert abs(deforestation_before + deforestation_since - deforestation_total) < 1e-9, f\"{polygon_name}: deforestation_before_column + deforestation_since_column != deforestation_total_column\"\n",
        "        assert abs(degradation_total + deforestation_total - disturbance_total) < 1e-9, f\"{polygon_name}: degradation_total_column + deforestation_total_column != disturbance_total_column\"\n",
        "        assert abs(current_agb - disturbance_total - old_growth_agb) < 1e-9, f\"{polygon_name}: current_agb_column - disturbance_total_column != old_growth_agb_column\"\n",
        "        # Define nodes\n",
        "        nodes = [left_axis, degradation_before_name, degradation_since_name, deforestation_before_name, deforestation_since_name, remaining_name_agb]\n",
        "        # Calculate 'Remaining' value (should be equal to current_agb)\n",
        "        remaining_value = current_agb\n",
        "        # Define links\n",
        "        sources = [0, 0, 0, 0, 0]\n",
        "        targets = [1, 2, 3, 4, 5]\n",
        "        values = [-degradation_before, -degradation_since, -deforestation_before, -deforestation_since, remaining_value]\n",
        "        colors = [degradation_before_colour, degradation_since_colour, deforestation_before_colour, deforestation_since_colour, remaining_colour]\n",
        "        # Define node colors\n",
        "        node_colors = [remaining_colour] + colors\n",
        "\n",
        "    elif separate_disturbance and separate_degradation and not separate_deforestation:\n",
        "        assert abs(degradation_before + degradation_since - degradation_total) < 1e-9, f\"{polygon_name}: degradation_before_column + degradation_since_column != degradation_total_column\"\n",
        "        assert abs(degradation_total + deforestation_total - disturbance_total) < 1e-9, f\"{polygon_name}: degradation_total_column + deforestation_total_column != disturbance_total_column\"\n",
        "        assert abs(current_agb - disturbance_total - old_growth_agb) < 1e-9, f\"{polygon_name}: current_agb_column - disturbance_total_column != old_growth_agb_column\"\n",
        "        # Define nodes\n",
        "        nodes = [left_axis, degradation_before_name, degradation_since_name, deforestation_total_name, remaining_name_agb]\n",
        "        # Calculate 'Remaining' value (should be equal to current_agb)\n",
        "        remaining_value = current_agb\n",
        "        # Define links\n",
        "        sources = [0, 0, 0, 0]\n",
        "        targets = [1, 2, 3, 4]\n",
        "        values = [-degradation_before, -degradation_since, -deforestation_total, remaining_value]\n",
        "        colors = [degradation_before_colour, degradation_since_colour, deforestation_total_colour, remaining_colour]\n",
        "        # Define node colors\n",
        "        node_colors = [remaining_colour] + colors\n",
        "\n",
        "    elif separate_disturbance and not separate_degradation and separate_deforestation:\n",
        "        assert abs(deforestation_before + deforestation_since - deforestation_total) < 1e-9, f\"{polygon_name}: deforestation_before_column + deforestation_since_column != deforestation_total_column\"\n",
        "        assert abs(degradation_total + deforestation_total - disturbance_total) < 1e-9, f\"{polygon_name}: degradation_total_column + deforestation_total_column != disturbance_total_column\"\n",
        "        assert abs(current_agb - disturbance_total - old_growth_agb) < 1e-9, f\"{polygon_name}: current_agb_column - disturbance_total_column != old_growth_agb_column\"\n",
        "        # Define nodes\n",
        "        nodes = [left_axis, degradation_total_name, deforestation_before_name, deforestation_since_name, remaining_name_agb]\n",
        "        # Calculate 'Remaining' value (should be equal to current_agb)\n",
        "        remaining_value = current_agb\n",
        "        # Define links\n",
        "        sources = [0, 0, 0, 0]\n",
        "        targets = [1, 2, 3, 4]\n",
        "        values = [-degradation_total, -deforestation_before, -deforestation_since, remaining_value]\n",
        "        colors = [degradation_total_colour, deforestation_before_colour, deforestation_since_colour, remaining_colour]\n",
        "        # Define node colors\n",
        "        node_colors = [remaining_colour] + colors\n",
        "\n",
        "    elif separate_disturbance and not separate_degradation and not separate_deforestation:\n",
        "        assert abs(degradation_total + deforestation_total - disturbance_total) < 1e-9, f\"{polygon_name}: degradation_total_column + deforestation_total_column != disturbance_total_column\"\n",
        "        assert abs(current_agb - disturbance_total - old_growth_agb) < 1e-9, f\"{polygon_name}: current_agb_column - disturbance_total_column != old_growth_agb_column\"\n",
        "        # Define nodes\n",
        "        nodes = [left_axis, degradation_total_name, deforestation_total_name, remaining_name_agb]\n",
        "        # Calculate 'Remaining' value (should be equal to current_agb)\n",
        "        remaining_value = current_agb\n",
        "        # Define links\n",
        "        sources = [0, 0, 0]\n",
        "        targets = [1, 2, 3]\n",
        "        values = [-degradation_total, -deforestation_total, remaining_value]\n",
        "        colors = [degradation_total_colour, deforestation_total_colour, remaining_colour]\n",
        "        # Define node colors\n",
        "        node_colors = [remaining_colour] + colors\n",
        "\n",
        "    else:\n",
        "        # Both separate_disturbance and separate_degradation are False\n",
        "        assert abs(current_agb - disturbance_total - old_growth_agb) < 1e-9, f\"{polygon_name}: current_agb_column - disturbance_total_column != old_growth_agb_column\"\n",
        "        # Define nodes\n",
        "        nodes = [left_axis, disturbance_total_name, remaining_name_agb]\n",
        "        # Calculate 'Remaining' value (should be equal to current_agb)\n",
        "        remaining_value = current_agb\n",
        "        # Define links\n",
        "        sources = [0, 0]\n",
        "        targets = [1, 2]\n",
        "        values = [-disturbance_total, remaining_value]\n",
        "        colors = [disturbance_total_colour, remaining_colour]\n",
        "        # Define node colors\n",
        "        node_colors = [remaining_colour] + colors\n",
        "\n",
        "    # Calculate percentages and update right node labels\n",
        "    percentages = [(abs(val) / old_growth_agb * 100) for val in values]\n",
        "    for i in range(1, len(nodes)):\n",
        "        if i - 1 < len(percentages):\n",
        "            nodes[i] += f\" ({percentages[i-1]:.0f}%)\"\n",
        "\n",
        "    title_and_density = [\n",
        "            dict(\n",
        "                x=0,\n",
        "                y=1.28,\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                text=title_name,\n",
        "                showarrow=False,\n",
        "                xanchor='left',\n",
        "                align='left',\n",
        "                font=dict(\n",
        "                    family=\"arial, sans serif\",\n",
        "                    size=title_font_size,\n",
        "                    color=\"black\",\n",
        "                    weight=title_font_weight\n",
        "                )\n",
        "            ),\n",
        "            dict(\n",
        "                x=0,\n",
        "                y=1.19,\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                text=subtitle_1_name,\n",
        "                showarrow=False,\n",
        "                xanchor='left',\n",
        "                align='left',\n",
        "                font=dict(\n",
        "                    family=\"arial, sans serif\",\n",
        "                    size=density_font_size,\n",
        "                    color=\"black\",\n",
        "                    weight=density_font_weight\n",
        "                )\n",
        "            ),\n",
        "            dict(\n",
        "                x=0,\n",
        "                y=1.11,\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                text=subtitle_2_name,\n",
        "                showarrow=False,\n",
        "                xanchor='left',\n",
        "                align='left',\n",
        "                font=dict(\n",
        "                    family=\"arial, sans serif\",\n",
        "                    size=density_font_size,\n",
        "                    color=\"black\",\n",
        "                    weight=density_font_weight\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    if show_title and not show_density: title_and_density = title_and_density[0:1]\n",
        "    if not show_title and show_density: title_and_density = title_and_density[1:3]\n",
        "    if not show_title and not show_density: title_and_density = []\n",
        "\n",
        "    # If labels are toggled off, replace node labels with empty strings\n",
        "    if not show_labels: nodes = [''] * len(nodes)\n",
        "\n",
        "    # Create the Sankey diagram\n",
        "    fig = go.Figure(data=[go.Sankey(\n",
        "        arrangement=\"freeform\",\n",
        "        node=dict(\n",
        "            label=nodes,\n",
        "            color=node_colors,  # Set node colors\n",
        "            pad=15,\n",
        "            thickness=20,\n",
        "            line=dict(color=\"black\", width=1)\n",
        "        ),\n",
        "        link=dict(\n",
        "            source=sources,\n",
        "            target=targets,\n",
        "            value=values,\n",
        "            color=colors,\n",
        "            line=dict(color=\"black\", width=1),  # Add border to ribbons\n",
        "        )\n",
        "    )])\n",
        "\n",
        "    fig.update_layout(\n",
        "        width=700 * width_modifier,\n",
        "        height=500,\n",
        "        font=dict(\n",
        "            family=\"arial, sans serif\",\n",
        "            size=label_font_size,\n",
        "            color=\"black\",\n",
        "            weight=label_font_weight\n",
        "        ),\n",
        "        # Adjust the margins\n",
        "        margin=dict(\n",
        "            l=25,\n",
        "            r=25,\n",
        "            t=115,  # Increased top margin to accommodate title\n",
        "            b=25\n",
        "        ),\n",
        "        annotations=title_and_density\n",
        "    )\n",
        "\n",
        "    # Save labelled version (with user settings)\n",
        "    png_path = os.path.join(sankey_labelled, f'sankey_diagram_{polygon_name}.png')\n",
        "    fig.write_image(png_path, scale=dpi / 96)\n",
        "\n",
        "    if svg_transparent_background:\n",
        "        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "    svg_path = os.path.join(sankey_labelled_svg, f'sankey_diagram_vector_{polygon_name}.svg')\n",
        "    fig.write_image(svg_path, scale=dpi / 96)\n",
        "\n",
        "    # Create and save unlabelled version\n",
        "    fig_unlabelled = go.Figure(data=[go.Sankey(\n",
        "        arrangement=\"freeform\",\n",
        "        node=dict(label=[''] * len(nodes), color=node_colors, pad=15, thickness=20,\n",
        "                 line=dict(color=\"black\", width=1)),\n",
        "        link=dict(source=sources, target=targets, value=values, color=colors,\n",
        "                 line=dict(color=\"black\", width=1))\n",
        "    )])\n",
        "\n",
        "    fig_unlabelled.update_layout(\n",
        "        width=700 * width_modifier, height=500,\n",
        "        font=dict(family=\"arial, sans serif\", size=label_font_size, color=\"black\", weight=label_font_weight),\n",
        "        margin=dict(l=25, r=25, t=115, b=25)\n",
        "    )\n",
        "\n",
        "    png_path = os.path.join(sankey_unlabelled, f'sankey_diagram_{polygon_name}.png')\n",
        "    fig_unlabelled.write_image(png_path, scale=dpi / 96)\n",
        "\n",
        "    if svg_transparent_background:\n",
        "        fig_unlabelled.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "    svg_path = os.path.join(sankey_unlabelled_svg, f'sankey_diagram_vector_{polygon_name}.svg')\n",
        "    fig_unlabelled.write_image(svg_path, scale=dpi / 96)\n",
        "\n",
        "    print(f\"Statistical assertions and sankey diagram complete for {polygon_name}.\")\n",
        "\n",
        "    # Show the figure (with white background)\n",
        "    fig.update_layout(plot_bgcolor='white', paper_bgcolor='white')\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "5ykformx2-nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXAGIvpk_KWS"
      },
      "source": [
        "# Disconnected runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3qMsENC_MP2"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "6j0th2jhtnAR",
        "PC-rviHPi4q5",
        "b70oSO_VNtkY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}