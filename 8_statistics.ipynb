{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/dev/8_statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j0th2jhtnAR"
      },
      "source": [
        "# Imports & Subdirectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9drq4euvG0MS"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGWgfUqto0F"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Imports and upgrades\n",
        "!pip install geopandas\n",
        "!pip install kaleido\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MTERlyxuPd4"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# Imports\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import math\n",
        "import numpy as np\n",
        "from os.path import exists, join\n",
        "from os import makedirs\n",
        "from osgeo import gdal\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import rasterio\n",
        "from rasterio import mask as msk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwar9sxbts-3"
      },
      "outputs": [],
      "source": [
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "predictions_dir = join(base_dir, \"7_predictions\")\n",
        "statistics_dir = join(base_dir, \"8_statistics\")\n",
        "sample_polygons_dir = join(statistics_dir, \"sample_polygons\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(statistics_dir, exist_ok=True)\n",
        "makedirs(sample_polygons_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atvGRSPYgwk9"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjy-T1TqScbE"
      },
      "source": [
        "# Select model, area and sample polygons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj5RbVSuEp-B"
      },
      "outputs": [],
      "source": [
        "# Select if to source predictions from scenarios_dir or predictions_dir\n",
        "source_dir = predictions_dir\n",
        "\n",
        "# Select the model\n",
        "for subdir in os.listdir(source_dir):\n",
        "  if 'scenario_masks' not in subdir:\n",
        "    print(f\"selected_model = '{subdir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7UWN6aIL-dX"
      },
      "outputs": [],
      "source": [
        "selected_model = 'agbd_historic_250429_223033'\n",
        "\n",
        "selected_model_dir = join(source_dir, selected_model)\n",
        "# Select the prediction area\n",
        "for subdir in os.listdir(selected_model_dir):\n",
        "  if source_dir == scenarios_dir and not subdir.endswith('.csv') and not subdir.endswith('.json'):\n",
        "    print(f\"prediction_area = '{subdir}'\")\n",
        "  if source_dir == predictions_dir and subdir != 'model_iterations':\n",
        "    print(f\"prediction_area = '{subdir[10:]}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txTXPYFLpH6v"
      },
      "outputs": [],
      "source": [
        "prediction_area = 'asartr'\n",
        "\n",
        "# Model-area stats directory\n",
        "model_area_statistics_dir = join(statistics_dir, f\"{selected_model}_{prediction_area}\")\n",
        "makedirs(model_area_statistics_dir, exist_ok=True)\n",
        "\n",
        "# Select sample area polygons. This should be a single .gpkg with the field 'name' differentiating polygons.\n",
        "sample_polygons = []\n",
        "for geopackage in os.listdir(sample_polygons_dir):\n",
        "  sample_polygons.append(geopackage)\n",
        "\n",
        "if len(sample_polygons) == 0:\n",
        "  print(f\"No sample areas found. Upload .gpkg polygons to {sample_polygons_dir}\")\n",
        "else:\n",
        "  for sample_polygon in sample_polygons: print(f\"selected_sample_polygons = '{sample_polygon}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwWvaX5qDANc"
      },
      "outputs": [],
      "source": [
        "selected_sample_polygons = 'sample_polygons_asartr.gpkg'\n",
        "\n",
        "# Set whether to adjust area calculations to match rasters (True) or to match polygon areas (False).\n",
        "# The raster approach (True) uses the precise pixel area calculated in 1_areas.ipynb.\n",
        "adjust_polygon_to_raster = True\n",
        "\n",
        "# Load cell area raster for accurate pixel-by-pixel area calculations\n",
        "cell_area_path = join(areas_dir, \"cell_area.tif\")\n",
        "\n",
        "selected_sample_polygons_dir = join(sample_polygons_dir, selected_sample_polygons)\n",
        "selected_sample_polygons_gpkg = gpd.read_file(selected_sample_polygons_dir)\n",
        "sample_polygons_statistics_dir = join(model_area_statistics_dir, selected_sample_polygons[:-5])\n",
        "makedirs(sample_polygons_statistics_dir, exist_ok=True)\n",
        "detailed_stats_by_area_dir = join(sample_polygons_statistics_dir, 'detailed_stats_by_area')\n",
        "makedirs(detailed_stats_by_area_dir, exist_ok=True)\n",
        "detailed_stats_by_scenario_dir = join(sample_polygons_statistics_dir, 'detailed_stats_by_scenario')\n",
        "makedirs(detailed_stats_by_scenario_dir, exist_ok=True)\n",
        "forecast_input_dir = join(sample_polygons_statistics_dir, 'forecast_input')\n",
        "makedirs(forecast_input_dir, exist_ok=True)\n",
        "detailed_dist_stats_by_area_dir = join(sample_polygons_statistics_dir, 'detailed_dist_stats_by_area')\n",
        "makedirs(detailed_dist_stats_by_area_dir, exist_ok=True)\n",
        "detailed_dist_stats_by_scenario_dir = join(sample_polygons_statistics_dir, 'detailed_dist_stats_by_scenario')\n",
        "makedirs(detailed_dist_stats_by_scenario_dir, exist_ok=True)\n",
        "report_statistics_dir = join(sample_polygons_statistics_dir, 'report_statistics')\n",
        "makedirs(report_statistics_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlQ2iLAiwQoB"
      },
      "source": [
        "# Scenario statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgC9laZXPB9Y"
      },
      "outputs": [],
      "source": [
        "# Create list of available prediction rasters and scenarios. Rasters must already be masked (e.g. to forest).\n",
        "if source_dir == scenarios_dir: prediction_raster_dir = join(selected_model_dir, prediction_area, 'scenario_predictions')\n",
        "if source_dir == predictions_dir: prediction_raster_dir = join(selected_model_dir, f'scenarios_{prediction_area}', 'statistics_masked')\n",
        "prediction_raster_dirs = []\n",
        "scenarios = set()\n",
        "for prediction_raster in os.listdir(prediction_raster_dir):\n",
        "  prediction_raster_dirs.append(join(prediction_raster_dir, prediction_raster))\n",
        "  if source_dir == predictions_dir: scenarios.add(prediction_raster.split(\"__\")[1])\n",
        "  if source_dir == scenarios_dir: scenarios.add(prediction_raster.split(\"__\")[0])\n",
        "scenarios = sorted(list(scenarios))\n",
        "\n",
        "# Select scenario predictions to calculate statistics\n",
        "print('selected_scenarios = [')\n",
        "for scenario in scenarios:\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(']\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_scenarios = [\n",
        "  \"2014\",\n",
        "  \"2014_no_degradation_since_1990\",\n",
        "  \"2014_oldgrowth\",\n",
        "  \"2015\",\n",
        "  \"2016\",\n",
        "  \"2017\",\n",
        "  \"2018\",\n",
        "  \"2019\",\n",
        "  \"2020\",\n",
        "  \"2021\",\n",
        "  \"2022\",\n",
        "  \"2023\",\n",
        "  \"2024\",\n",
        "  \"2024_no_degradation_since_2000\",\n",
        "  \"2024_no_degradation_since_2015\",\n",
        "  \"2024_oldgrowth\",\n",
        "  \"all_oldgrowth\",\n",
        "]\n",
        "\n",
        "# Filter to selected scenarios, and separate prediction and uncertainty rasters\n",
        "prediction_rasters = []\n",
        "uncertainty_rasters = []\n",
        "for prediction_raster in prediction_raster_dirs:\n",
        "  for scenario in selected_scenarios:\n",
        "    if source_dir == predictions_dir:\n",
        "      if scenario == prediction_raster.split('/')[-1].split('__')[1] and 'mean__' in prediction_raster:\n",
        "        prediction_rasters.append(prediction_raster)\n",
        "      if scenario == prediction_raster.split('/')[-1].split('__')[1] and 'uncertainty__' in prediction_raster:\n",
        "        uncertainty_rasters.append(prediction_raster)\n",
        "    else: # If the source directory is scenarios_dir (without uncertainty values)\n",
        "      if scenario == prediction_raster.split('/')[-1].split('__')[0]: prediction_rasters.append(prediction_raster)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "prediction_rasters = sorted(prediction_rasters)\n",
        "uncertainty_rasters = sorted(uncertainty_rasters)\n",
        "\n",
        "# Create lookup dictionary for faster uncertainty matching\n",
        "uncertainty_lookup = {}\n",
        "for uncertainty_raster in uncertainty_rasters:\n",
        "  base_name = os.path.basename(uncertainty_raster).replace('uncertainty__', 'mean__')\n",
        "  uncertainty_lookup[base_name] = uncertainty_raster\n",
        "\n",
        "# Toggle whether to predict uncertainty stats\n",
        "generate_uncertainty_stats = bool(uncertainty_rasters)\n",
        "\n",
        "# Generate empty dataframes for statistics\n",
        "df_base = pd.DataFrame(index = selected_scenarios)\n",
        "df_base.rename_axis('scenario', inplace=True)\n",
        "df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg = df_base.copy(), df_base.copy(), df_base.copy(), df_base.copy()\n",
        "# If uncertainty rasters are present, generate empty dataframes\n",
        "if generate_uncertainty_stats:\n",
        "  df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agb_total_tg_ci95 = df_base.copy(), df_base.copy(), df_base.copy()\n",
        "\n",
        "# Initialise polygon area dataframe\n",
        "df_polygon_area_km2 = pd.DataFrame(columns = [\"Name\", \"Area (km^2)\"])\n",
        "\n",
        "# Loop through each polygon stored in GPKG to generate statistics\n",
        "for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "\n",
        "  # Define the polygon\n",
        "  sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"]\n",
        "  polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "  # Calculate sample_polygon_geometry area (ellipsoidal as opposed to planimetric)\n",
        "  sample_polygons_crs = selected_sample_polygons_gpkg.crs\n",
        "  temp_gdf = gpd.GeoDataFrame({'name': [sample_polygon_name], 'geometry': sample_polygon_geometry}, crs=sample_polygons_crs)\n",
        "  temp_gdf_utm = temp_gdf.estimate_utm_crs()\n",
        "  polygon_area_ha = np.divide(temp_gdf.to_crs(temp_gdf_utm).area[0], 10000, dtype='float64')\n",
        "\n",
        "  # Mask the cell area raster to the polygon once for efficiency\n",
        "  with rasterio.open(cell_area_path) as cell_area:\n",
        "    cell_area_masked, transform_1 = msk.mask(cell_area, polygons, crop=True, filled=False)\n",
        "\n",
        "  # Calculate total area of all pixels within polygon in hectares\n",
        "  pixel_area_sum_m2 = np.ma.sum(cell_area_masked, dtype='float64')\n",
        "  pixel_area_sum_ha = np.divide(pixel_area_sum_m2, 10000, dtype='float64')\n",
        "\n",
        "  # Calculate ratio between polygon and raster areas\n",
        "  area_ratio = np.divide(polygon_area_ha, pixel_area_sum_ha, dtype='float64')\n",
        "\n",
        "  # Apply area adjustment based on setting\n",
        "  if adjust_polygon_to_raster:\n",
        "    # Adjust polygon area to match raster area\n",
        "    adjusted_polygon_area_ha = pixel_area_sum_ha\n",
        "    # No adjustment needed for pixel values - keep original cell areas\n",
        "    adjusted_cell_area_masked = cell_area_masked\n",
        "  else:\n",
        "    # Keep original polygon area and adjust individual pixel areas\n",
        "    adjusted_polygon_area_ha = polygon_area_ha\n",
        "    # Adjust each pixel area by the area ratio to maintain total equal to polygon area\n",
        "    adjusted_cell_area_masked = np.multiply(cell_area_masked, area_ratio, dtype='float64')\n",
        "\n",
        "  # Convert adjusted cell areas from m² to ha\n",
        "  adjusted_cell_area_masked_ha = np.divide(adjusted_cell_area_masked, 10000, dtype='float64')\n",
        "\n",
        "  # Add polygon area to dataframe\n",
        "  new_row = pd.DataFrame([{'Name': sample_polygon_name, 'Area (km^2)': adjusted_polygon_area_ha / 100}], dtype=object)\n",
        "  df_polygon_area_km2 = pd.concat([df_polygon_area_km2, new_row], ignore_index=True, sort=False)\n",
        "\n",
        "  # Create empty lists for each prediction raster statistic\n",
        "  values_forest_cover_ha, values_agbd_mean_mg_ha, values_agbd_stdev_mg_ha, values_agb_total_tg = [], [], [], []\n",
        "\n",
        "  # If uncertainty rasters are present, create empty lists for uncertainty statistics\n",
        "  if generate_uncertainty_stats:\n",
        "    values_agbd_mean_mg_ha_ci95, values_agbd_mean_mg_ha_uncertainty, values_agb_total_tg_ci95 = [], [], []\n",
        "\n",
        "  # Loop through prediction rasters\n",
        "  for prediction_raster in prediction_rasters:\n",
        "\n",
        "    # Mask feature to sample_polygon_geometry\n",
        "    with rasterio.open(prediction_raster) as prediction:\n",
        "      nodatavalue = int(prediction.nodatavals[0])\n",
        "      prediction_array_masked, transform_2 = msk.mask(prediction, polygons, crop=True, filled=False)\n",
        "\n",
        "    # Extract forest pixels (non-masked pixels in prediction array)\n",
        "    forest_pixels_mask = ~np.ma.getmaskarray(prediction_array_masked)\n",
        "\n",
        "    # Calculate forest area by summing adjusted cell areas of forest pixels\n",
        "    forest_cell_areas_ha = np.ma.array(adjusted_cell_area_masked_ha.data, mask=~forest_pixels_mask)\n",
        "    forest_cover_ha = np.ma.sum(forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "    # Calculate total biomass by multiplying AGBD by pixel area for each pixel, then summing\n",
        "    pixel_biomass_mg = np.multiply(prediction_array_masked, forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "    # Sum to get total biomass in Mg\n",
        "    agb_total_mg = np.ma.sum(pixel_biomass_mg, dtype='float64')\n",
        "\n",
        "    # Calculate mean AGBD (Mg/ha) by dividing total biomass by forest area\n",
        "    agbd_mean_mg_ha = np.divide(agb_total_mg, forest_cover_ha, dtype='float64')\n",
        "\n",
        "    # Calculate standard deviation - using unweighted for now\n",
        "    agbd_mean_stdev_ha = np.ma.std(prediction_array_masked, dtype='float64')\n",
        "\n",
        "    # Convert total AGB from Mg to Tg\n",
        "    agb_total_tg = np.divide(agb_total_mg, 1000000, dtype='float64')\n",
        "\n",
        "    # Append results to statistics lists\n",
        "    values_forest_cover_ha.append(forest_cover_ha)\n",
        "    values_agbd_mean_mg_ha.append(agbd_mean_mg_ha)\n",
        "    values_agbd_stdev_mg_ha.append(agbd_mean_stdev_ha)\n",
        "    values_agb_total_tg.append(agb_total_tg)\n",
        "\n",
        "    if generate_uncertainty_stats:\n",
        "      # Use lookup dictionary to quickly find matching uncertainty raster\n",
        "      prediction_basename = os.path.basename(prediction_raster)\n",
        "      uncertainty_raster_present = prediction_basename in uncertainty_lookup\n",
        "\n",
        "      if not uncertainty_raster_present:\n",
        "        print(f\"There is no uncertainty raster for {prediction_basename}\")\n",
        "      else:\n",
        "        matching_uncertainty_raster = uncertainty_lookup[prediction_basename]\n",
        "\n",
        "        # Open and mask uncertainty raster to polygon\n",
        "        with rasterio.open(matching_uncertainty_raster) as uncertainty:\n",
        "          nodatavalue = int(uncertainty.nodatavals[0])\n",
        "          uncertainty_array_masked, transform_2 = msk.mask(uncertainty, polygons, crop=True, filled=False)\n",
        "\n",
        "        # Calculate uncertainty with area-weighted approach\n",
        "        uncertainty_ratios = np.divide(uncertainty_array_masked, 100, dtype='float64')\n",
        "        # Multiply uncertainty % by AGBD by pixel area\n",
        "        pixel_ci95_mg = np.multiply(np.multiply(prediction_array_masked, uncertainty_ratios, dtype='float64'),\n",
        "                                  forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "        # Sum to get total CI95 for the area (in Mg)\n",
        "        agb_total_ci95_mg = np.ma.sum(pixel_ci95_mg, dtype='float64')\n",
        "\n",
        "        # Calculate the mean CI95 as percentage of mean AGBD\n",
        "        if abs(agb_total_mg) > 0:  # Use absolute value for division check\n",
        "            # Uncertainty percentage is relative to absolute value\n",
        "            agb_total_ci95_percent = np.divide(abs(agb_total_ci95_mg), abs(agb_total_mg), dtype='float64') * 100\n",
        "            # Preserve sign of original measurement\n",
        "            sign = np.sign(agb_total_mg)\n",
        "            agbd_mean_mg_ha_ci95 = sign * np.multiply(abs(agbd_mean_mg_ha), np.divide(agb_total_ci95_percent, 100, dtype='float64'), dtype='float64')\n",
        "        else:\n",
        "            agb_total_ci95_percent = 0\n",
        "            agbd_mean_mg_ha_ci95 = 0\n",
        "\n",
        "        # Store uncertainty as percentage\n",
        "        agbd_mean_mg_ha_uncertainty = agb_total_ci95_percent\n",
        "\n",
        "        # Calculate total AGB CI95 in Tg - preserve sign\n",
        "        agb_total_tg_ci95 = np.divide(agb_total_ci95_mg, 1000000, dtype='float64')\n",
        "\n",
        "        # Append results to statistics list\n",
        "        values_agbd_mean_mg_ha_ci95.append(agbd_mean_mg_ha_ci95)\n",
        "        values_agbd_mean_mg_ha_uncertainty.append(agbd_mean_mg_ha_uncertainty)\n",
        "        values_agb_total_tg_ci95.append(agb_total_tg_ci95)\n",
        "\n",
        "  # Concatenate new columns to the main DataFrames for each statistic\n",
        "  df_forest_cover_ha = pd.concat([df_forest_cover_ha, pd.DataFrame({sample_polygon_name: values_forest_cover_ha}, index=df_forest_cover_ha.index)], axis=1)\n",
        "  df_agbd_mean_mg_ha = pd.concat([df_agbd_mean_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha}, index=df_agbd_mean_mg_ha.index)], axis=1)\n",
        "  df_agbd_stdev_mg_ha = pd.concat([df_agbd_stdev_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_stdev_mg_ha}, index=df_agbd_stdev_mg_ha.index)], axis=1)\n",
        "  df_agb_total_tg = pd.concat([df_agb_total_tg, pd.DataFrame({sample_polygon_name: values_agb_total_tg}, index=df_agb_total_tg.index)], axis=1)\n",
        "\n",
        "  if generate_uncertainty_stats:\n",
        "      df_agbd_mean_mg_ha_ci95 = pd.concat([df_agbd_mean_mg_ha_ci95, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_ci95}, index=df_agbd_mean_mg_ha_ci95.index)], axis=1)\n",
        "      df_agbd_mean_mg_ha_uncertainty = pd.concat([df_agbd_mean_mg_ha_uncertainty, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_uncertainty}, index=df_agbd_mean_mg_ha_uncertainty.index)], axis=1)\n",
        "      df_agb_total_tg_ci95 = pd.concat([df_agb_total_tg_ci95, pd.DataFrame({sample_polygon_name: values_agb_total_tg_ci95}, index=df_agb_total_tg_ci95.index)], axis=1)\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "  df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_mean_mg_ha_ci95,\n",
        "                 df_agbd_mean_mg_ha_uncertainty, df_agbd_stdev_mg_ha, df_agb_total_tg, df_agb_total_tg_ci95]\n",
        "else: df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Set index of the polygon area km2 dataframe to 'Name' of the polygon\n",
        "df_polygon_area_km2 = df_polygon_area_km2.set_index('Name')\n",
        "\n",
        "# Export statistics for forecast input\n",
        "df_forecast_list = [df_forest_cover_ha, df_agb_total_tg]\n",
        "for df_forecast in df_forecast_list:\n",
        "  df_noalts = df_forecast[~df_forecast.index.str.contains(\"_\")]  # More efficient filtering\n",
        "  if df_forecast.equals(df_forest_cover_ha): df_filename = \"forest_cover_ha\"\n",
        "  if df_forecast.equals(df_agb_total_tg): df_filename = \"agb_total_tg\"\n",
        "  df_noalts.to_csv(join(forecast_input_dir, f'{df_filename}.csv'))\n",
        "\n",
        "# Generate summary stats\n",
        "df_forest_cover_ha_t = df_forest_cover_ha.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest cover (ha)\")\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB (Tg)\")\n",
        "\n",
        "# Use list for more efficient concatenation\n",
        "summary_components = [df_polygon_area_km2, df_forest_cover_ha_t, df_agb_total_tg_t]\n",
        "if generate_uncertainty_stats:\n",
        "  df_agb_total_tg_ci95_t = df_agb_total_tg_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB CI95 (Tg)\")\n",
        "  summary_components.append(df_agb_total_tg_ci95_t)\n",
        "\n",
        "summary_stats = pd.concat(summary_components, axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_stats.to_csv(join(sample_polygons_statistics_dir, 'summary_stats.csv'))\n",
        "\n",
        "# Generate detailed stats by area\n",
        "for polygon_area in df_stats_list[0]:\n",
        "  polygon_area_km2 = df_polygon_area_km2.loc[polygon_area][\"Area (km^2)\"]\n",
        "  df_detailed_stats = df_base.copy()\n",
        "  df_detailed_stats[\"Area (km^2)\"] = polygon_area_km2\n",
        "  for df_stats in df_stats_list:\n",
        "    if df_stats.equals(df_forest_cover_ha): stat_col = \"Forest cover (ha)\"\n",
        "    if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"Forest AGBD mean (Mg / ha)\"\n",
        "    if df_stats.equals(df_agbd_stdev_mg_ha): stat_col = \"Forest AGBD stdev (Mg / ha)\"\n",
        "    if df_stats.equals(df_agb_total_tg): stat_col = \"Forest AGB total (Tg)\"\n",
        "    if generate_uncertainty_stats:\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_ci95): stat_col = \"Forest AGBD CI95 (Mg / ha)\"\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_uncertainty): stat_col = \"Forest AGBD uncertainty (%)\"\n",
        "      if df_stats.equals(df_agb_total_tg_ci95): stat_col = \"Forest AGB total CI95 (Tg)\"\n",
        "    for stats_polygon_area in df_stats:\n",
        "      if stats_polygon_area == polygon_area:\n",
        "        df_stats_renamed = df_stats.rename(columns={stats_polygon_area:stat_col})\n",
        "        df_detailed_stats = pd.concat([df_detailed_stats, df_stats_renamed[stat_col]], axis=1)\n",
        "  df_detailed_stats.to_csv(join(detailed_stats_by_area_dir, f'{polygon_area}.csv'))\n",
        "\n",
        "# Generate detailed stats by scenario - build dictionary once then process\n",
        "scenarios = {}\n",
        "for stats_csv in os.listdir(detailed_stats_by_area_dir):\n",
        "    polygon_name = f\"{stats_csv[:-4]}\"\n",
        "    stats_csv_path = join(detailed_stats_by_area_dir, stats_csv)\n",
        "    stats_csv_df = pd.read_csv(stats_csv_path)\n",
        "    # Process all scenarios for this polygon in one pass\n",
        "    for scenario in stats_csv_df['scenario'].unique():\n",
        "        scenario_df = stats_csv_df[stats_csv_df['scenario'] == scenario].copy()\n",
        "        scenario_df.drop('scenario', axis=1, inplace=True)\n",
        "        scenario_df.insert(0, 'Name', polygon_name)\n",
        "        if scenario in scenarios:\n",
        "            scenarios[scenario] = pd.concat([scenarios[scenario], scenario_df], ignore_index=True)\n",
        "        else:\n",
        "            scenarios[scenario] = scenario_df\n",
        "\n",
        "# Write all scenario CSVs at once\n",
        "for scenario, scenario_df in scenarios.items():\n",
        "    output_file_path = join(detailed_stats_by_scenario_dir,f'{scenario}.csv')\n",
        "    scenario_df.to_csv(output_file_path, index=False)"
      ],
      "metadata": {
        "id": "exDLp760d8tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGzdNKRbo0H5"
      },
      "source": [
        "# Disturbance statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTpXfmfAo7hY"
      },
      "outputs": [],
      "source": [
        "# Create list of available disturbance rasters and scenarios\n",
        "if source_dir == scenarios_dir: dist_raster_dir = join(selected_model_dir, prediction_area, 'scenario_disturbance')\n",
        "if source_dir == predictions_dir: dist_raster_dir = join(selected_model_dir, f'scenarios_{prediction_area}', 'scenario_disturbance')\n",
        "\n",
        "dist_raster_dirs = []\n",
        "dists = set()\n",
        "for dist_raster in os.listdir(dist_raster_dir):\n",
        "  dist_raster_dirs.append(join(dist_raster_dir, dist_raster))\n",
        "  if source_dir == predictions_dir: dists.add(dist_raster.split(\"__\")[1])\n",
        "  if source_dir == scenarios_dir: dists.add(dist_raster.split(\"__\")[0])\n",
        "dists = sorted(list(dists))\n",
        "\n",
        "# Select disturbance rasters to calculate statistics\n",
        "print('selected_dists = [')\n",
        "for dist in dists:\n",
        "  print(f'  \"{dist}\",')\n",
        "print(']')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfyIlrVKwd6S"
      },
      "outputs": [],
      "source": [
        "selected_dists = [\n",
        "  \"2014_deforestation_total\",\n",
        "  \"2014_degradation_before_1990\",\n",
        "  \"2014_degradation_deforestation_total\",\n",
        "  \"2014_degradation_since_1990\",\n",
        "  \"2014_degradation_total\",\n",
        "  \"2024_deforestation_actual_since_2015\",\n",
        "  \"2024_deforestation_total\",\n",
        "  \"2024_degradation_before_2000\",\n",
        "  \"2024_degradation_before_2015\",\n",
        "  \"2024_degradation_deforestation_total\",\n",
        "  \"2024_degradation_since_2000\",\n",
        "  \"2024_degradation_since_2015\",\n",
        "  \"2024_degradation_total\",\n",
        "]\n",
        "\n",
        "# Filter to selected disturbances, and separate prediction and uncertainty rasters\n",
        "dist_rasters = []\n",
        "dist_uncertainty_rasters = []\n",
        "for dist_raster in dist_raster_dirs:\n",
        "  for dist in selected_dists:\n",
        "    if source_dir == predictions_dir:\n",
        "      if dist == dist_raster.split('/')[-1].split('__')[1] and 'mean__' in dist_raster:\n",
        "        dist_rasters.append(dist_raster)\n",
        "      if dist == dist_raster.split('/')[-1].split('__')[1] and 'uncertainty__' in dist_raster:\n",
        "        dist_uncertainty_rasters.append(dist_raster)\n",
        "    else: # If the source directory is scenarios_dir (without uncertainty values)\n",
        "      if dist == dist_raster.split('/')[-1].split('__')[0]: dist_rasters.append(dist_raster)\n",
        "\n",
        "# Sort rasters chronologically\n",
        "dist_rasters = sorted(dist_rasters)\n",
        "dist_uncertainty_rasters = sorted(dist_uncertainty_rasters)\n",
        "\n",
        "# Create lookup dictionary for faster uncertainty matching\n",
        "uncertainty_lookup = {}\n",
        "for uncertainty_raster in dist_uncertainty_rasters:\n",
        "  base_name = os.path.basename(uncertainty_raster).replace('uncertainty__', 'mean__')\n",
        "  uncertainty_lookup[base_name] = uncertainty_raster\n",
        "\n",
        "# Toggle whether to predict uncertainty stats\n",
        "generate_uncertainty_stats = bool(dist_uncertainty_rasters)\n",
        "\n",
        "# Generate empty dataframes for statistics\n",
        "df_base = pd.DataFrame(index = selected_dists)\n",
        "df_base.rename_axis('dist', inplace=True)\n",
        "df_agbd_mean_mg_ha, df_agb_total_tg = df_base.copy(), df_base.copy()\n",
        "# If uncertainty rasters are present, generate empty dataframes\n",
        "if generate_uncertainty_stats:\n",
        "  df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agb_total_tg_ci95 = df_base.copy(), df_base.copy(), df_base.copy()\n",
        "\n",
        "# Initialise polygon area dataframe\n",
        "df_polygon_area_km2 = pd.DataFrame(columns = [\"Name\", \"Area (km^2)\"])\n",
        "\n",
        "# Load cell area raster for accurate pixel-by-pixel area calculations\n",
        "cell_area_path = join(areas_dir, \"cell_area.tif\")\n",
        "\n",
        "# Loop through each polygon stored in GPKG to generate statistics\n",
        "for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "\n",
        "  # Define the polygon\n",
        "  sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"]\n",
        "  polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "  # Calculate sample_polygon_geometry area (ellipsoidal as opposed to planimetric)\n",
        "  sample_polygons_crs = selected_sample_polygons_gpkg.crs\n",
        "  temp_gdf = gpd.GeoDataFrame({'name': [sample_polygon_name], 'geometry': sample_polygon_geometry}, crs=sample_polygons_crs)\n",
        "  temp_gdf_utm = temp_gdf.estimate_utm_crs()\n",
        "  polygon_area_ha = np.divide(temp_gdf.to_crs(temp_gdf_utm).area[0], 10000, dtype='float64')\n",
        "\n",
        "  # Mask the cell area raster to the polygon once for efficiency\n",
        "  with rasterio.open(cell_area_path) as cell_area:\n",
        "    cell_area_masked, transform_1 = msk.mask(cell_area, polygons, crop=True, filled=False)\n",
        "\n",
        "  # Calculate total area of all pixels within polygon in hectares\n",
        "  pixel_area_sum_m2 = np.ma.sum(cell_area_masked, dtype='float64')\n",
        "  pixel_area_sum_ha = np.divide(pixel_area_sum_m2, 10000, dtype='float64')\n",
        "\n",
        "  # Calculate ratio between polygon and raster areas\n",
        "  area_ratio = np.divide(polygon_area_ha, pixel_area_sum_ha, dtype='float64')\n",
        "\n",
        "  # Apply area adjustment based on setting\n",
        "  if adjust_polygon_to_raster:\n",
        "    # Adjust polygon area to match raster area\n",
        "    adjusted_polygon_area_ha = pixel_area_sum_ha\n",
        "    # No adjustment needed for pixel values - keep original cell areas\n",
        "    adjusted_cell_area_masked = cell_area_masked\n",
        "  else:\n",
        "    # Keep original polygon area and adjust individual pixel areas\n",
        "    adjusted_polygon_area_ha = polygon_area_ha\n",
        "    # Adjust each pixel area by the area ratio to maintain total equal to polygon area\n",
        "    adjusted_cell_area_masked = np.multiply(cell_area_masked, area_ratio, dtype='float64')\n",
        "\n",
        "  # Convert adjusted cell areas from m² to ha\n",
        "  adjusted_cell_area_masked_ha = np.divide(adjusted_cell_area_masked, 10000, dtype='float64')\n",
        "\n",
        "  # Add polygon area to dataframe\n",
        "  new_row = pd.DataFrame([{'Name': sample_polygon_name, 'Area (km^2)': adjusted_polygon_area_ha / 100}], dtype=object)\n",
        "  df_polygon_area_km2 = pd.concat([df_polygon_area_km2, new_row], ignore_index=True, sort=False)\n",
        "\n",
        "  # Create empty lists for each disturbance raster statistic\n",
        "  values_agbd_mean_mg_ha, values_agb_total_tg = [], []\n",
        "\n",
        "  # If uncertainty rasters are present, create empty lists for uncertainty statistics\n",
        "  if generate_uncertainty_stats:\n",
        "    values_agbd_mean_mg_ha_ci95, values_agbd_mean_mg_ha_uncertainty, values_agb_total_tg_ci95 = [], [], []\n",
        "\n",
        "  # Loop through disturbance rasters\n",
        "  for dist_raster in dist_rasters:\n",
        "\n",
        "    # Mask feature to sample_polygon_geometry\n",
        "    with rasterio.open(dist_raster) as dist:\n",
        "      nodatavalue = int(dist.nodatavals[0])\n",
        "      dist_array_masked, transform_2 = msk.mask(dist, polygons, crop=True, filled=False)\n",
        "\n",
        "    # Extract forest pixels (non-masked pixels in disturbance array)\n",
        "    forest_pixels_mask = ~np.ma.getmaskarray(dist_array_masked)\n",
        "\n",
        "    # Calculate forest area by summing adjusted cell areas of forest pixels\n",
        "    forest_cell_areas_ha = np.ma.array(adjusted_cell_area_masked_ha.data, mask=~forest_pixels_mask)\n",
        "    forest_cover_ha = np.ma.sum(forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "    # Calculate total biomass by multiplying AGBD by pixel area for each pixel, then summing\n",
        "    pixel_biomass_mg = np.multiply(dist_array_masked, forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "    # Sum to get total biomass in Mg\n",
        "    agb_total_mg = np.ma.sum(pixel_biomass_mg, dtype='float64')\n",
        "\n",
        "    # Calculate mean AGBD (Mg/ha) by dividing total biomass by forest area\n",
        "    agbd_mean_mg_ha = np.divide(agb_total_mg, forest_cover_ha, dtype='float64') if forest_cover_ha > 0 else 0\n",
        "\n",
        "    # Convert total AGB from Mg to Tg\n",
        "    agb_total_tg = np.divide(agb_total_mg, 1000000, dtype='float64')\n",
        "\n",
        "    # Append results to statistics lists\n",
        "    values_agbd_mean_mg_ha.append(agbd_mean_mg_ha)\n",
        "    values_agb_total_tg.append(agb_total_tg)\n",
        "\n",
        "    if generate_uncertainty_stats:\n",
        "      # Use lookup dictionary to quickly find matching uncertainty raster\n",
        "      dist_basename = os.path.basename(dist_raster)\n",
        "      uncertainty_raster_present = dist_basename in uncertainty_lookup\n",
        "\n",
        "      if not uncertainty_raster_present:\n",
        "        print(f\"There is no uncertainty raster for {dist_basename}\")\n",
        "      else:\n",
        "        matching_uncertainty_raster = uncertainty_lookup[dist_basename]\n",
        "\n",
        "        # Open and mask uncertainty raster to polygon\n",
        "        with rasterio.open(matching_uncertainty_raster) as uncertainty:\n",
        "          nodatavalue = int(uncertainty.nodatavals[0])\n",
        "          uncertainty_array_masked, transform_2 = msk.mask(uncertainty, polygons, crop=True, filled=False)\n",
        "\n",
        "        # Calculate uncertainty with area-weighted approach\n",
        "        uncertainty_ratios = np.divide(uncertainty_array_masked, 100, dtype='float64')\n",
        "        # Multiply uncertainty % by AGBD by pixel area\n",
        "        pixel_ci95_mg = np.multiply(np.multiply(dist_array_masked, uncertainty_ratios, dtype='float64'),\n",
        "                                  forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "        # Sum to get total CI95 for the area (in Mg)\n",
        "        agb_total_ci95_mg = np.ma.sum(pixel_ci95_mg, dtype='float64')\n",
        "\n",
        "        # Calculate the mean CI95 as percentage of mean AGBD\n",
        "        if abs(agb_total_mg) > 0:  # Use absolute value for division check\n",
        "            # Uncertainty percentage is relative to absolute value\n",
        "            agb_total_ci95_percent = np.divide(abs(agb_total_ci95_mg), abs(agb_total_mg), dtype='float64') * 100\n",
        "            # Preserve sign of original measurement\n",
        "            sign = np.sign(agb_total_mg)\n",
        "            agbd_mean_mg_ha_ci95 = sign * np.multiply(abs(agbd_mean_mg_ha), np.divide(agb_total_ci95_percent, 100, dtype='float64'), dtype='float64')\n",
        "        else:\n",
        "            agb_total_ci95_percent = 0\n",
        "            agbd_mean_mg_ha_ci95 = 0\n",
        "\n",
        "        # Store uncertainty as percentage\n",
        "        agbd_mean_mg_ha_uncertainty = agb_total_ci95_percent\n",
        "\n",
        "        # Calculate total AGB CI95 in Tg - preserve sign\n",
        "        agb_total_tg_ci95 = np.divide(agb_total_ci95_mg, 1000000, dtype='float64')\n",
        "\n",
        "        # Append results to statistics list\n",
        "        values_agbd_mean_mg_ha_ci95.append(agbd_mean_mg_ha_ci95)\n",
        "        values_agbd_mean_mg_ha_uncertainty.append(agbd_mean_mg_ha_uncertainty)\n",
        "        values_agb_total_tg_ci95.append(agb_total_tg_ci95)\n",
        "\n",
        "  # Concatenate new columns to the main DataFrames for each statistic\n",
        "  df_agbd_mean_mg_ha = pd.concat([df_agbd_mean_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha}, index=df_agbd_mean_mg_ha.index)], axis=1)\n",
        "  df_agb_total_tg = pd.concat([df_agb_total_tg, pd.DataFrame({sample_polygon_name: values_agb_total_tg}, index=df_agb_total_tg.index)], axis=1)\n",
        "\n",
        "  if generate_uncertainty_stats:\n",
        "      df_agbd_mean_mg_ha_ci95 = pd.concat([df_agbd_mean_mg_ha_ci95, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_ci95}, index=df_agbd_mean_mg_ha_ci95.index)], axis=1)\n",
        "      df_agbd_mean_mg_ha_uncertainty = pd.concat([df_agbd_mean_mg_ha_uncertainty, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_uncertainty}, index=df_agbd_mean_mg_ha_uncertainty.index)], axis=1)\n",
        "      df_agb_total_tg_ci95 = pd.concat([df_agb_total_tg_ci95, pd.DataFrame({sample_polygon_name: values_agb_total_tg_ci95}, index=df_agb_total_tg_ci95.index)], axis=1)\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "  df_stats_list = [df_agbd_mean_mg_ha, df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agb_total_tg, df_agb_total_tg_ci95]\n",
        "else: df_stats_list = [df_agbd_mean_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Set index of the polygon area km2 dataframe to 'Name' of the polygon\n",
        "df_polygon_area_km2 = df_polygon_area_km2.set_index('Name')\n",
        "\n",
        "# Generate summary stats\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB (Tg)\")\n",
        "\n",
        "# Use list for more efficient concatenation\n",
        "summary_components = [df_polygon_area_km2, df_agb_total_tg_t]\n",
        "if generate_uncertainty_stats:\n",
        "  df_agb_total_tg_ci95_t = df_agb_total_tg_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB CI95 (Tg)\")\n",
        "  summary_components.append(df_agb_total_tg_ci95_t)\n",
        "\n",
        "summary_stats = pd.concat(summary_components, axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_stats.to_csv(join(sample_polygons_statistics_dir, 'summary_dist_stats.csv'))\n",
        "\n",
        "# Generate detailed stats by polygon\n",
        "for polygon_area in df_stats_list[0]:\n",
        "  polygon_area_km2 = df_polygon_area_km2.loc[polygon_area][\"Area (km^2)\"]\n",
        "  df_detailed_dist_stats = df_base.copy()\n",
        "  df_detailed_dist_stats[\"Area (km^2)\"] = polygon_area_km2\n",
        "  for df_stats in df_stats_list:\n",
        "    if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"Forest AGBD mean (Mg / ha)\"\n",
        "    if df_stats.equals(df_agb_total_tg): stat_col = \"Forest AGB total (Tg)\"\n",
        "    if generate_uncertainty_stats:\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_ci95): stat_col = \"Forest AGBD CI95 (Mg / ha)\"\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_uncertainty): stat_col = \"Forest AGBD uncertainty (%)\"\n",
        "      if df_stats.equals(df_agb_total_tg_ci95): stat_col = \"Forest AGB total CI95 (Tg)\"\n",
        "    for stats_polygon_area in df_stats:\n",
        "      if stats_polygon_area == polygon_area:\n",
        "        df_stats_renamed = df_stats.rename(columns={stats_polygon_area:stat_col})\n",
        "        df_detailed_dist_stats = pd.concat([df_detailed_dist_stats, df_stats_renamed[stat_col]], axis=1)\n",
        "  df_detailed_dist_stats.to_csv(join(detailed_dist_stats_by_area_dir, f'{polygon_area}.csv'))\n",
        "\n",
        "# Generate detailed stats by disturbance type - build dictionary once then process\n",
        "dists = {}\n",
        "for stats_csv in os.listdir(detailed_dist_stats_by_area_dir):\n",
        "    polygon_name = f\"{stats_csv[:-4]}\"\n",
        "    stats_csv_path = join(detailed_dist_stats_by_area_dir, stats_csv)\n",
        "    stats_csv_df = pd.read_csv(stats_csv_path)\n",
        "    # Process all disturbance types for this polygon in one pass\n",
        "    for dist in stats_csv_df['dist'].unique():\n",
        "        dist_df = stats_csv_df[stats_csv_df['dist'] == dist].copy()\n",
        "        dist_df.drop('dist', axis=1, inplace=True)\n",
        "        dist_df.insert(0, 'Name', polygon_name)\n",
        "        if dist in dists:\n",
        "            dists[dist] = pd.concat([dists[dist], dist_df], ignore_index=True)\n",
        "        else:\n",
        "            dists[dist] = dist_df\n",
        "\n",
        "# Write all disturbance CSVs at once\n",
        "for dist, dist_df in dists.items():\n",
        "    output_file_path = join(detailed_dist_stats_by_scenario_dir,f'{dist}.csv')\n",
        "    dist_df.to_csv(output_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intactness statistics"
      ],
      "metadata": {
        "id": "6t45TVipaVma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of available percentage change and intactness rasters\n",
        "if source_dir == scenarios_dir: intactness_dir = join(selected_model_dir, prediction_area, 'intactness')\n",
        "if source_dir == predictions_dir: intactness_dir = join(selected_model_dir, f'scenarios_{prediction_area}', 'intactness')\n",
        "\n",
        "percentage_change_rasters = []\n",
        "intactness_rasters = []\n",
        "for raster in os.listdir(intactness_dir):\n",
        "  if \"percentage_change__\" in raster and raster.endswith('tif'):\n",
        "    percentage_change_rasters.append(raster.split('/')[-1])\n",
        "  if \"intactness__\" in raster and raster.endswith('tif'):\n",
        "    intactness_rasters.append(raster.split('/')[-1])\n",
        "\n",
        "# Select intactness raster to calculate statistics\n",
        "print(\"# Select intactness raster to calculate statistics\")\n",
        "for raster in intactness_rasters:\n",
        "  print(f'intactness_raster = \"{raster}\"')\n",
        "print(\"\\n\")\n",
        "\n",
        "# Create list of available prediction rasters and scenarios. Rasters must already be masked (e.g. to forest).\n",
        "if source_dir == scenarios_dir: prediction_raster_dir = join(selected_model_dir, prediction_area, 'scenario_predictions')\n",
        "if source_dir == predictions_dir: prediction_raster_dir = join(selected_model_dir, f'scenarios_{prediction_area}', 'statistics_masked')\n",
        "prediction_raster_dirs = []\n",
        "for raster in os.listdir(prediction_raster_dir):\n",
        "  if raster.endswith('.tif') and 'uncertainty__' not in raster:\n",
        "    if source_dir == predictions_dir: prediction_raster_dirs.append(raster)\n",
        "    if source_dir == scenarios_dir: prediction_raster_dirs.append(raster)\n",
        "\n",
        "# Select all land scenario to calculate statistics\n",
        "print(\"# Select all land scenario to calculate statistics\")\n",
        "for raster in prediction_raster_dirs:\n",
        "  print(f'all_land_raster = \"{raster}\"')\n",
        "print(\"\\n\")\n",
        "\n",
        "# Load the CSV files\n",
        "summary_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_stats.csv'))\n",
        "summary_dist_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_dist_stats.csv'))\n",
        "\n",
        "# Check that all rows in both .csv files have the same strings (polygon areas) in column A\n",
        "polygon_areas_stats = summary_stats.iloc[:, 0]\n",
        "polygon_areas_dist_stats = summary_dist_stats.iloc[:, 0]\n",
        "\n",
        "assert all(polygon_areas_stats == polygon_areas_dist_stats), \"Polygon areas do not match between the two CSV files.\"\n",
        "\n",
        "# Print columns of summary_stats\n",
        "print(\"Columns in summary_stats:\")\n",
        "for i, col in enumerate(summary_stats.columns[1:]):\n",
        "    print(col, end=' ')\n",
        "    if (i+1) % 5 == 0:\n",
        "        print()\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print columns of summary_dist_stats\n",
        "print(\"Columns in summary_dist_stats:\")\n",
        "for i, col in enumerate(summary_dist_stats.columns[1:]):\n",
        "    print(col, end=' ')\n",
        "    if (i+1) % 5 == 0:\n",
        "        print()"
      ],
      "metadata": {
        "id": "a6p_6d-0adF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intactness_raster = \"intactness__intactness_wo_tn_10_quantiles.tif\"\n",
        "all_land_raster = \"mean__all_oldgrowth__asartr_agbd_historic_250429_223033.tif\"\n",
        "\n",
        "# old-growth AGB column (summary_stats)\n",
        "old_growth_agb_column = 'all_oldgrowth forest AGB (Tg)'\n",
        "\n",
        "# disturbance columns (summary_dist_stats)\n",
        "degradation_since_column = '2024_degradation_since_2000 forest AGB (Tg)'\n",
        "degradation_before_column = '2024_degradation_before_2000 forest AGB (Tg)'\n",
        "deforestation_total_column = '2024_deforestation_total forest AGB (Tg)'\n",
        "disturbance_total_column = '2024_degradation_deforestation_total forest AGB (Tg)'\n",
        "\n",
        "intactness_path = join(intactness_dir, intactness_raster)\n",
        "all_land_path = join(prediction_raster_dir, all_land_raster)\n",
        "summary_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_stats.csv'))\n",
        "summary_dist_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_dist_stats.csv'))\n",
        "\n",
        "# Load cell area raster for accurate pixel-by-pixel area calculations\n",
        "cell_area_path = join(areas_dir, \"cell_area.tif\")\n",
        "\n",
        "# Function to get values from statistics\n",
        "def get_value(df, idx, column_name):\n",
        "    try:\n",
        "        value = df.loc[idx, column_name]\n",
        "        if pd.isnull(value): return 0.0\n",
        "        else: return float(value)\n",
        "    except KeyError:\n",
        "        print(f\"Column '{column_name}' not found in the dataframe.\")\n",
        "        return 0.0\n",
        "\n",
        "# Function to calculate area-weighted statistics\n",
        "def weighted_stats(values, weights):\n",
        "    # Handle empty arrays\n",
        "    if len(values) == 0:\n",
        "        return None, None\n",
        "\n",
        "    # Calculate weighted mean\n",
        "    weighted_sum = np.sum(values * weights, dtype='float64')\n",
        "    sum_of_weights = np.sum(weights, dtype='float64')\n",
        "    weighted_mean = weighted_sum / sum_of_weights if sum_of_weights > 0 else 0\n",
        "\n",
        "    # Calculate weighted standard deviation\n",
        "    if sum_of_weights > 0:\n",
        "        variance = np.sum(weights * np.square(values - weighted_mean, dtype='float64'), dtype='float64') / sum_of_weights\n",
        "        weighted_std = np.sqrt(variance, dtype='float64')\n",
        "    else:\n",
        "        weighted_std = 0\n",
        "\n",
        "    return weighted_mean, weighted_std\n",
        "\n",
        "total_score = int(intactness_raster.split('_')[-2]) # Extracts the quantiles used for total score\n",
        "total_stdev = int(total_score/2)\n",
        "\n",
        "# Generate empty dataframe for statistics\n",
        "df_intactness_stats = pd.DataFrame(columns=[\"Name\",\n",
        "                                          'Percentage change degradation since 1990',\n",
        "                                          'Percentage change degradation before 1990',\n",
        "                                          'Percentage change deforestation total',\n",
        "                                          'Percentage change disturbance total',\n",
        "                                          f\"Intactness (remaining forest) mean / {total_score}\",\n",
        "                                          f\"Intactness (remaining forest) stdev / {total_stdev}\",\n",
        "                                          f\"Intactness (non-forest = 0) mean / {total_score}\",\n",
        "                                          f\"Intactness (non-forest = 0) stdev / {total_stdev}\"])\n",
        "\n",
        "# Loop through each polygon stored in GPKG to generate statistics\n",
        "for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "\n",
        "    # Define the polygon\n",
        "    sample_polygon_geometry = row[\"geometry\"]\n",
        "    sample_polygon_name = row[\"name\"]\n",
        "    polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "    # Calculate sample_polygon_geometry area (ellipsoidal as opposed to planimetric)\n",
        "    sample_polygons_crs = selected_sample_polygons_gpkg.crs\n",
        "    temp_gdf = gpd.GeoDataFrame({'name': [sample_polygon_name], 'geometry': sample_polygon_geometry}, crs=sample_polygons_crs)\n",
        "    temp_gdf_utm = temp_gdf.estimate_utm_crs()\n",
        "    polygon_area_ha = np.divide(temp_gdf.to_crs(temp_gdf_utm).area[0], 10000, dtype='float64')\n",
        "\n",
        "    # Mask the cell area raster to the polygon once for efficiency\n",
        "    with rasterio.open(cell_area_path) as cell_area:\n",
        "        cell_area_masked, transform_ca = msk.mask(cell_area, polygons, crop=True, filled=False)\n",
        "\n",
        "    # Calculate total area of all pixels within polygon in hectares\n",
        "    pixel_area_sum_m2 = np.ma.sum(cell_area_masked, dtype='float64')\n",
        "    pixel_area_sum_ha = np.divide(pixel_area_sum_m2, 10000, dtype='float64')\n",
        "\n",
        "    # Calculate ratio between polygon and raster areas\n",
        "    area_ratio = np.divide(polygon_area_ha, pixel_area_sum_ha, dtype='float64')\n",
        "\n",
        "    # Apply area adjustment based on setting\n",
        "    if adjust_polygon_to_raster:\n",
        "        # No adjustment needed for pixel values - keep original cell areas\n",
        "        adjusted_cell_area_masked = cell_area_masked\n",
        "    else:\n",
        "        # Adjust each pixel area by the area ratio to maintain total equal to polygon area\n",
        "        adjusted_cell_area_masked = np.multiply(cell_area_masked, area_ratio, dtype='float64')\n",
        "\n",
        "    # Convert adjusted cell areas from m² to ha for easier calculations\n",
        "    adjusted_cell_area_masked_ha = np.divide(adjusted_cell_area_masked, 10000, dtype='float64')\n",
        "\n",
        "    # Loop through each row (polygon area) in summary_stats to find matching polygon\n",
        "    found_in_summary_stats = False\n",
        "    for idx in summary_stats.index:\n",
        "        # Get the polygon name\n",
        "        polygon_name = summary_stats.iloc[idx, 0]\n",
        "\n",
        "        if polygon_name == sample_polygon_name:\n",
        "            found_in_summary_stats = True\n",
        "            # Get old-growth AGB values\n",
        "            old_growth_agb = get_value(summary_stats, idx, old_growth_agb_column)\n",
        "            break\n",
        "\n",
        "    if not found_in_summary_stats:\n",
        "        print(f\"Warning: {sample_polygon_name} not found in summary_stats\")\n",
        "        continue\n",
        "\n",
        "    # Loop through each row in summary_dist_stats to find matching polygon\n",
        "    found_in_dist_stats = False\n",
        "    for idx in summary_dist_stats.index:\n",
        "        # Get the polygon name\n",
        "        polygon_name = summary_dist_stats.iloc[idx, 0]\n",
        "        if polygon_name == sample_polygon_name:\n",
        "            found_in_dist_stats = True\n",
        "            # Get total_disturbance_value\n",
        "            degradation_since = get_value(summary_dist_stats, idx, degradation_since_column)\n",
        "            degradation_before = get_value(summary_dist_stats, idx, degradation_before_column)\n",
        "            deforestation_total = get_value(summary_dist_stats, idx, deforestation_total_column)\n",
        "            disturbance_total = get_value(summary_dist_stats, idx, disturbance_total_column)\n",
        "            break\n",
        "\n",
        "    if not found_in_dist_stats:\n",
        "        print(f\"Warning: {sample_polygon_name} not found in summary_dist_stats\")\n",
        "        degradation_since = degradation_before = deforestation_total = disturbance_total = 0.0\n",
        "\n",
        "    # Calculate total percentage change - properly handle disturbance values\n",
        "    # Disturbance values are typically negative, old_growth_agb is positive\n",
        "    if old_growth_agb != 0:\n",
        "        percent_dist_degradation_since = np.divide(degradation_since, old_growth_agb, dtype='float64') * 100\n",
        "        percent_dist_degradation_before = np.divide(degradation_before, old_growth_agb, dtype='float64') * 100\n",
        "        percent_dist_deforestation_total = np.divide(deforestation_total, old_growth_agb, dtype='float64') * 100\n",
        "        percent_dist_disturbance_total = np.divide(disturbance_total, old_growth_agb, dtype='float64') * 100\n",
        "    else:\n",
        "        percent_dist_degradation_since = percent_dist_degradation_before = percent_dist_deforestation_total = percent_dist_disturbance_total = 0.0\n",
        "\n",
        "    # Mask the 'all land' raster to the polygon\n",
        "    with rasterio.open(all_land_path) as all_land:\n",
        "        all_land_masked, transform_1 = msk.mask(all_land, polygons, crop=True, filled=False)\n",
        "\n",
        "    # Get the area of all land pixels\n",
        "    all_land_mask = ~np.ma.getmaskarray(all_land_masked)\n",
        "    all_land_area_ha = np.ma.array(adjusted_cell_area_masked_ha.data, mask=~all_land_mask)\n",
        "    all_land_area_ha_sum = np.ma.sum(all_land_area_ha, dtype='float64')\n",
        "\n",
        "    # Mask intactness array\n",
        "    with rasterio.open(intactness_path) as intactness:\n",
        "        intactness_masked, transform_2 = msk.mask(intactness, polygons, crop=True, filled=False)\n",
        "\n",
        "    # Get forest mask (where intactness values exist)\n",
        "    forest_mask = ~np.ma.getmaskarray(intactness_masked)\n",
        "\n",
        "    # Extract forest areas and values\n",
        "    forest_cell_areas_ha = np.ma.array(adjusted_cell_area_masked_ha.data, mask=~forest_mask)\n",
        "    forest_area_ha_sum = np.ma.sum(forest_cell_areas_ha, dtype='float64')\n",
        "\n",
        "    # Non-forest area\n",
        "    non_forest_area_ha = all_land_area_ha_sum - forest_area_ha_sum\n",
        "\n",
        "    if forest_area_ha_sum > 0:\n",
        "        # Extract intactness values for forest pixels\n",
        "        forest_intactness_values = np.ma.compressed(intactness_masked)\n",
        "        forest_intactness_weights = np.ma.compressed(forest_cell_areas_ha)\n",
        "\n",
        "        # Calculate area-weighted intactness statistics for remaining forest\n",
        "        intactness_remaining_mean, intactness_remaining_std = weighted_stats(forest_intactness_values, forest_intactness_weights)\n",
        "\n",
        "        # For all land including non-forest (treated as 0)\n",
        "        if non_forest_area_ha > 0:\n",
        "            # Calculate the weighted mean directly\n",
        "            all_mean_numerator = np.sum(forest_intactness_values * forest_intactness_weights)\n",
        "            all_mean_denominator = forest_area_ha_sum + non_forest_area_ha\n",
        "            intactness_all_mean = all_mean_numerator / all_mean_denominator\n",
        "\n",
        "            # Calculate the weighted variance directly\n",
        "            forest_variance_contribution = np.sum(forest_intactness_weights * np.square(forest_intactness_values - intactness_all_mean))\n",
        "            non_forest_variance_contribution = non_forest_area_ha * np.square(0 - intactness_all_mean)\n",
        "            all_variance = (forest_variance_contribution + non_forest_variance_contribution) / all_mean_denominator\n",
        "            intactness_all_std = np.sqrt(all_variance)\n",
        "        else:\n",
        "            # If no non-forest area, all-land stats are the same as forest stats\n",
        "            intactness_all_mean = intactness_remaining_mean\n",
        "            intactness_all_std = intactness_remaining_std\n",
        "    else:\n",
        "        intactness_remaining_mean = intactness_remaining_std = intactness_all_mean = intactness_all_std = None\n",
        "\n",
        "    # Create new row with statistics\n",
        "    new_row = pd.DataFrame([{\n",
        "        'Name': sample_polygon_name,\n",
        "        'Percentage change degradation since 1990': percent_dist_degradation_since,\n",
        "        'Percentage change degradation before 1990': percent_dist_degradation_before,\n",
        "        'Percentage change deforestation total': percent_dist_deforestation_total,\n",
        "        'Percentage change disturbance total': percent_dist_disturbance_total,\n",
        "        f'Intactness (non-forest = 0) mean / {total_score}': intactness_all_mean,\n",
        "        f'Intactness (non-forest = 0) stdev / {total_stdev}': intactness_all_std,\n",
        "        f'Intactness (remaining forest) mean / {total_score}': intactness_remaining_mean,\n",
        "        f'Intactness (remaining forest) stdev / {total_stdev}': intactness_remaining_std,\n",
        "    }], dtype=object)\n",
        "\n",
        "    # Append to main dataframe\n",
        "    df_intactness_stats = pd.concat([df_intactness_stats, new_row], ignore_index=True)\n",
        "\n",
        "# Set index to Name and save to CSV\n",
        "df_intactness_stats = df_intactness_stats.set_index('Name')\n",
        "df_intactness_stats.to_csv(join(sample_polygons_statistics_dir, 'summary_intactness_stats.csv'))"
      ],
      "metadata": {
        "id": "-59x0xOrfaPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report statistics"
      ],
      "metadata": {
        "id": "PC-rviHPi4q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define scenarios for report\n",
        "print(\"# Remember that order matters\\n\")\n",
        "print(\"scenario_list = [\")\n",
        "for csv in os.listdir(detailed_stats_by_scenario_dir):\n",
        "  print(f\"'{csv[:-4]}',\")\n",
        "print(\"]\")\n",
        "print(\"\")\n",
        "# Define disturbances for report\n",
        "print(\"disturbance_list = [\")\n",
        "for csv in os.listdir(detailed_dist_stats_by_scenario_dir):\n",
        "  print(f\"'{csv[:-4]}',\")\n",
        "print(\"]\")"
      ],
      "metadata": {
        "id": "BHpAf6qg1dEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scenario_list = [\n",
        "'2014',\n",
        "'2015',\n",
        "'2016',\n",
        "'2017',\n",
        "'2018',\n",
        "'2019',\n",
        "'2020',\n",
        "'2021',\n",
        "'2022',\n",
        "'2023',\n",
        "'2024',\n",
        "'2024_no_degradation_since_2000',\n",
        "'2024_oldgrowth',\n",
        "'all_oldgrowth',\n",
        "]\n",
        "\n",
        "disturbance_list = [\n",
        "# '2024_degradation_total',\n",
        "'2024_degradation_since_2000',\n",
        "'2024_degradation_before_2000',\n",
        "'2024_deforestation_total',\n",
        "'2024_degradation_deforestation_total',\n",
        "]\n",
        "\n",
        "report_year = '2024'\n",
        "\n",
        "all_land_scenario = None\n",
        "for scenario in scenario_list:\n",
        "  if 'all' in scenario:\n",
        "    all_land_scenario = scenario\n",
        "if all_land_scenario == None: print(\"No all land scenario exists in the detailed stats.\")\n",
        "\n",
        "# Read summary stats\n",
        "summary_stats_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_stats.csv'))\n",
        "summary_dist_stats_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_dist_stats.csv'))\n",
        "summary_intactness_stats_df = pd.read_csv(join(sample_polygons_statistics_dir, 'summary_intactness_stats.csv'))\n",
        "\n",
        "# Create attributes CSV\n",
        "attributes = pd.DataFrame()\n",
        "attributes['Name'] = summary_stats_df['Unnamed: 0']\n",
        "attributes['Area (km^2)'] = summary_stats_df['Area (km^2)']\n",
        "attributes[f'{report_year} forest cover (ha)'] = summary_stats_df[f'{report_year} forest cover (ha)']\n",
        "attributes[f'{all_land_scenario} forest cover (ha)'] = summary_stats_df[f'{all_land_scenario} forest cover (ha)']\n",
        "attributes.to_csv(join(report_statistics_dir, 'attributes.csv'), index=False)\n",
        "\n",
        "# Create scenarios total AGB CSV\n",
        "scenarios_total_agb = pd.DataFrame()\n",
        "scenarios_total_agb['Name'] = summary_stats_df['Unnamed: 0']\n",
        "for scenario in scenario_list:\n",
        "  scenarios_total_agb[f'{scenario} forest AGB (Tg)'] = summary_stats_df[f'{scenario} forest AGB (Tg)']\n",
        "if source_dir == predictions_dir:\n",
        "  for scenario in scenario_list:\n",
        "    scenarios_total_agb[f'{scenario} forest AGB CI95 (Tg)'] = summary_stats_df[f'{scenario} forest AGB CI95 (Tg)']\n",
        "scenarios_total_agb.to_csv(join(report_statistics_dir, 'scenarios_total_agb.csv'), index=False)\n",
        "\n",
        "# Create scenarios AGBD CSV\n",
        "scenarios_agbd = pd.DataFrame()\n",
        "scenarios_agbd['Name'] = summary_stats_df['Unnamed: 0']\n",
        "for scenario in scenario_list:\n",
        "  scenario_detailed_stats_df = pd.read_csv(join(detailed_stats_by_scenario_dir, f'{scenario}.csv'))\n",
        "  scenarios_agbd[f'{scenario} forest AGBD (Mg / ha)'] = scenario_detailed_stats_df['Forest AGBD mean (Mg / ha)']\n",
        "if source_dir == predictions_dir:\n",
        "  for scenario in scenario_list:\n",
        "    scenario_detailed_stats_df = pd.read_csv(join(detailed_stats_by_scenario_dir, f'{scenario}.csv'))\n",
        "    scenarios_agbd[f'{scenario} forest AGBD CI95 (Mg / ha)'] = scenario_detailed_stats_df['Forest AGBD CI95 (Mg / ha)']\n",
        "scenarios_agbd.to_csv(join(report_statistics_dir, 'scenarios_agbd.csv'), index=False)\n",
        "\n",
        "# Create disturbance total AGB CSV\n",
        "disturbance_total_agb = pd.DataFrame()\n",
        "disturbance_total_agb['Name'] = summary_dist_stats_df['Unnamed: 0']\n",
        "for disturbance in disturbance_list:\n",
        "  disturbance_total_agb[f'{disturbance} forest AGB (Tg)'] = summary_dist_stats_df[f'{disturbance} forest AGB (Tg)']\n",
        "if source_dir == predictions_dir:\n",
        "  for disturbance in disturbance_list:\n",
        "    disturbance_total_agb[f'{disturbance} forest AGB CI95 (Tg)'] = summary_dist_stats_df[f'{disturbance} forest AGB CI95 (Tg)']\n",
        "disturbance_total_agb.to_csv(join(report_statistics_dir, 'disturbance_total_agb.csv'), index=False)\n",
        "\n",
        "# Create disturbance AGBD CSV\n",
        "disturbance_agbd = pd.DataFrame()\n",
        "disturbance_agbd['Name'] = summary_stats_df['Unnamed: 0']\n",
        "for disturbance in disturbance_list:\n",
        "  disturbance_detailed_stats_df = pd.read_csv(join(detailed_dist_stats_by_scenario_dir, f'{disturbance}.csv'))\n",
        "  disturbance_agbd[f'{disturbance} forest AGBD (Mg / ha)'] = disturbance_detailed_stats_df['Forest AGBD mean (Mg / ha)']\n",
        "if source_dir == predictions_dir:\n",
        "  for disturbance in disturbance_list:\n",
        "    disturbance_detailed_stats_df = pd.read_csv(join(detailed_dist_stats_by_scenario_dir, f'{disturbance}.csv'))\n",
        "    disturbance_agbd[f'{disturbance} forest AGBD CI95 (Mg / ha)'] = disturbance_detailed_stats_df['Forest AGBD CI95 (Mg / ha)']\n",
        "disturbance_agbd.to_csv(join(report_statistics_dir, 'disturbance_agbd.csv'), index=False)\n",
        "\n",
        "# Create intactness CSV\n",
        "intactness = pd.DataFrame()\n",
        "intactness['Name'] = summary_intactness_stats_df['Name']\n",
        "intactness['Percentage change degradation since 1990'] = summary_intactness_stats_df['Percentage change degradation since 1990']\n",
        "intactness['Percentage change degradation before 1990'] = summary_intactness_stats_df['Percentage change degradation before 1990']\n",
        "intactness['Percentage change deforestation total'] = summary_intactness_stats_df['Percentage change deforestation total']\n",
        "intactness['Percentage change disturbance total'] = summary_intactness_stats_df['Percentage change disturbance total']\n",
        "intactness['Intactness (remaining forest) mean / 10'] = summary_intactness_stats_df['Intactness (remaining forest) mean / 10']\n",
        "intactness['Intactness (remaining forest) stdev / 5'] = summary_intactness_stats_df['Intactness (remaining forest) stdev / 5']\n",
        "intactness['Intactness (non-forest = 0) mean / 10'] = summary_intactness_stats_df['Intactness (non-forest = 0) mean / 10']\n",
        "intactness['Intactness (non-forest = 0) stdev / 5'] = summary_intactness_stats_df['Intactness (non-forest = 0) stdev / 5']\n",
        "intactness.to_csv(join(report_statistics_dir, 'intactness.csv'), index=False)"
      ],
      "metadata": {
        "id": "QCMvXcIa2Q0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sankey plots"
      ],
      "metadata": {
        "id": "b70oSO_VNtkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and create directories\n",
        "sankey_labelled = join(sample_polygons_statistics_dir, 'sankey_labelled')\n",
        "sankey_unlabelled = join(sample_polygons_statistics_dir, 'sankey_unlabelled')\n",
        "sankey_labelled_svg = join(sample_polygons_statistics_dir, 'sankey_labelled_svg')\n",
        "sankey_unlabelled_svg = join(sample_polygons_statistics_dir, 'sankey_unlabelled_svg')\n",
        "\n",
        "for dir in [sankey_labelled, sankey_unlabelled, sankey_labelled_svg, sankey_unlabelled_svg]:\n",
        "    makedirs(dir, exist_ok=True)\n",
        "\n",
        "# Load the CSV files\n",
        "summary_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_stats.csv'))\n",
        "summary_dist_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_dist_stats.csv'))\n",
        "\n",
        "# Check that all rows in both .csv files have the same strings (polygon areas) in column A\n",
        "polygon_areas_stats = summary_stats.iloc[:, 0]\n",
        "polygon_areas_dist_stats = summary_dist_stats.iloc[:, 0]\n",
        "\n",
        "assert all(polygon_areas_stats == polygon_areas_dist_stats), \"Polygon areas do not match between the two CSV files.\"\n",
        "\n",
        "# Print columns of summary_stats\n",
        "print(\"Columns in summary_stats:\")\n",
        "for i, col in enumerate(summary_stats.columns[1:]):\n",
        "    print(col, end=' ')\n",
        "    if (i+1) % 5 == 0:\n",
        "        print()\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print columns of summary_dist_stats\n",
        "print(\"Columns in summary_dist_stats:\")\n",
        "for i, col in enumerate(summary_dist_stats.columns[1:]):\n",
        "    print(col, end=' ')\n",
        "    if (i+1) % 5 == 0:\n",
        "        print()"
      ],
      "metadata": {
        "id": "jH2yiLQtmXIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot degradation and deforestation separately\n",
        "separate_disturbance = True\n",
        "# Plot degradation before and since a date separately\n",
        "separate_degradation = True\n",
        "\n",
        "# DPI (default is 96, output image will scale accordingly)\n",
        "dpi = 300\n",
        "# Relative width modifier (ratio, e.g. 0.5 or 2)\n",
        "width_modifier = 0.85\n",
        "\n",
        "# Title (polygon area), density and label variables (weight of 800 ~ bold, 400 ~ normal)\n",
        "show_title = True\n",
        "show_density = True\n",
        "show_labels = True\n",
        "left_axis_label = True\n",
        "svg_transparent_background = True\n",
        "title_font_size = 20\n",
        "title_font_weight = 600\n",
        "density_font_size = 17\n",
        "density_font_weight = 600\n",
        "label_font_size = 17\n",
        "label_font_weight = 600\n",
        "\n",
        "# Base columns and year (summary_stats)\n",
        "old_growth_agb_column = 'all_oldgrowth forest AGB (Tg)'\n",
        "current_agb_column = '2024 forest AGB (Tg)'\n",
        "current_year = current_agb_column.split(' ')[0] # Usually first word of current_agb_column\n",
        "\n",
        "# Disturbance columns (summary_dist_stats)\n",
        "degradation_before_column = '2024_degradation_before_2000 forest AGB (Tg)'\n",
        "degradation_since_column = '2024_degradation_since_2000 forest AGB (Tg)'\n",
        "degradation_total_column = '2024_degradation_total forest AGB (Tg)'\n",
        "deforestation_total_column = '2024_deforestation_total forest AGB (Tg)'\n",
        "disturbance_total_column = '2024_degradation_deforestation_total forest AGB (Tg)'\n",
        "\n",
        "# Node labels and colours\n",
        "remaining_name = f'Remaining in {current_year}:'\n",
        "remaining_colour = '#1a801a'  # Dark green\n",
        "degradation_before_name = 'Degradation before 2000'\n",
        "degradation_before_colour = '#8dc00d'  # Light green\n",
        "degradation_since_name = 'Degradation since 2000'\n",
        "degradation_since_colour = '#ffff00'  # Yellow\n",
        "degradation_total_name = 'Degradation'\n",
        "degradation_total_colour = '#ffff00'  # Yellow\n",
        "deforestation_total_name = 'Deforestation'\n",
        "deforestation_total_colour = '#ffffff'  # White\n",
        "disturbance_total_name = 'Disturbance'\n",
        "disturbance_total_colour = '#ffff00'  # Yellow\n",
        "\n",
        "# Assert checking separate_disturbance is True if separate_degradation is True\n",
        "assert not separate_degradation or separate_disturbance, \"separate_disturbance must be True if separate_degradation is True.\"\n",
        "\n",
        "# Function to get values from statistics\n",
        "def get_value(df, idx, column_name):\n",
        "    try:\n",
        "        value = df.loc[idx, column_name]\n",
        "        if pd.isnull(value): return 0.0\n",
        "        else: return float(value)\n",
        "    except KeyError:\n",
        "        print(f\"Column '{column_name}' not found in the dataframe.\")\n",
        "        return 0.0\n",
        "\n",
        "# Loop through each row (polygon area)\n",
        "for idx in summary_stats.index:\n",
        "\n",
        "    # Get the polygon name\n",
        "    polygon_name = summary_stats.iloc[idx, 0]\n",
        "\n",
        "    # Get old-growth and current AGB values\n",
        "    old_growth_agb = get_value(summary_stats, idx, old_growth_agb_column)\n",
        "    current_agb = get_value(summary_stats, idx, current_agb_column)\n",
        "\n",
        "    # Get values from summary_dist_stats\n",
        "    degradation_before = get_value(summary_dist_stats, idx, degradation_before_column)\n",
        "    degradation_since = get_value(summary_dist_stats, idx, degradation_since_column)\n",
        "    degradation_total = get_value(summary_dist_stats, idx, degradation_total_column)\n",
        "    deforestation_total = get_value(summary_dist_stats, idx, deforestation_total_column)\n",
        "    disturbance_total = get_value(summary_dist_stats, idx, disturbance_total_column)\n",
        "\n",
        "    # Load detailed stats to get mean AGBD and CI95 values\n",
        "    detailed_stats_df = pd.read_csv(join(detailed_stats_by_area_dir, f\"{polygon_name}.csv\"))\n",
        "    old_growth_index = detailed_stats_df.index[detailed_stats_df['scenario'] == f\"{old_growth_agb_column.split(' ')[0]}\"].item()\n",
        "    old_growth_mean_agbd = get_value(detailed_stats_df, old_growth_index, \"Forest AGBD mean (Mg / ha)\")\n",
        "    current_index = detailed_stats_df.index[detailed_stats_df['scenario'] == f\"{current_agb_column.split(' ')[0]}\"].item()\n",
        "    current_mean_agbd = get_value(detailed_stats_df, current_index, \"Forest AGBD mean (Mg / ha)\")\n",
        "    uncertainty = False # Uncertainty may not have been calculated\n",
        "    if 'Forest AGB total CI95 (Tg)' in detailed_stats_df.columns:\n",
        "      uncertainty = True # CI95 will be divided by 2 for margin of error\n",
        "      old_growth_agb_ci95 = get_value(detailed_stats_df, old_growth_index, \"Forest AGB total CI95 (Tg)\")\n",
        "      old_growth_mean_agbd_ci95 = get_value(detailed_stats_df, old_growth_index, \"Forest AGBD CI95 (Mg / ha)\")\n",
        "      current_agb_ci95 = get_value(detailed_stats_df, current_index, \"Forest AGB total CI95 (Tg)\")\n",
        "      current_mean_agbd_ci95 = get_value(detailed_stats_df, current_index, \"Forest AGBD CI95 (Mg / ha)\")\n",
        "\n",
        "    # Title line 1 name\n",
        "    title_name = f\"{polygon_name}\"\n",
        "\n",
        "    # Subtitle line 1 name\n",
        "    if uncertainty: subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} ± {old_growth_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "    else: subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} Mg / ha\"\n",
        "\n",
        "    # Subtitle line 2 name\n",
        "    if uncertainty: subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} ± {current_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "    else: subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} Mg / ha\"\n",
        "\n",
        "    # Left axis name\n",
        "    if left_axis_label:\n",
        "      if uncertainty: left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} ± {old_growth_agb_ci95:.2f} Tg\"\n",
        "      else: left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} Tg\"\n",
        "    else: left_axis = ''\n",
        "\n",
        "    # Update remaining_name with AGB\n",
        "    if uncertainty: remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} ± {current_agb_ci95:.2f} Tg\"\n",
        "    else: remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} Tg\"\n",
        "\n",
        "    # Depending on the settings, perform assertions and plot\n",
        "    if separate_disturbance and separate_degradation:\n",
        "        assert abs(degradation_before + degradation_since - degradation_total) < 1e-9, f\"{polygon_name}: degradation_before_column + degradation_since_column != degradation_total_column\"\n",
        "        assert abs(degradation_total + deforestation_total - disturbance_total) < 1e-9, f\"{polygon_name}: degradation_total_column + deforestation_total_column != disturbance_total_column\"\n",
        "        assert abs(current_agb - disturbance_total - old_growth_agb) < 1e-9, f\"{polygon_name}: current_agb_column - disturbance_total_column != old_growth_agb_column\"\n",
        "        # Define nodes\n",
        "        nodes = [left_axis, degradation_before_name, degradation_since_name, deforestation_total_name, remaining_name_agb]\n",
        "        # Calculate 'Remaining' value (should be equal to current_agb)\n",
        "        remaining_value = current_agb\n",
        "        # Define links\n",
        "        sources = [0, 0, 0, 0]\n",
        "        targets = [1, 2, 3, 4]\n",
        "        values = [-degradation_before, -degradation_since, -deforestation_total, remaining_value]\n",
        "        colors = [degradation_before_colour, degradation_since_colour, deforestation_total_colour, remaining_colour]\n",
        "        # Define node colors\n",
        "        node_colors = [remaining_colour] + colors\n",
        "\n",
        "    elif separate_disturbance and not separate_degradation:\n",
        "        assert abs(degradation_total + deforestation_total - disturbance_total) < 1e-9, f\"{polygon_name}: degradation_total_column + deforestation_total_column != disturbance_total_column\"\n",
        "        assert abs(current_agb - disturbance_total - old_growth_agb) < 1e-9, f\"{polygon_name}: current_agb_column - disturbance_total_column != old_growth_agb_column\"\n",
        "        # Define nodes\n",
        "        nodes = [left_axis, degradation_total_name, deforestation_total_name, remaining_name_agb]\n",
        "        # Calculate 'Remaining' value (should be equal to current_agb)\n",
        "        remaining_value = current_agb\n",
        "        # Define links\n",
        "        sources = [0, 0, 0]\n",
        "        targets = [1, 2, 3]\n",
        "        values = [-degradation_total, -deforestation_total, remaining_value]\n",
        "        colors = [degradation_total_colour, deforestation_total_colour, remaining_colour]\n",
        "        # Define node colors\n",
        "        node_colors = [remaining_colour] + colors\n",
        "\n",
        "    else:\n",
        "        # Both separate_disturbance and separate_degradation are False\n",
        "        assert abs(current_agb - disturbance_total - old_growth_agb) < 1e-9, f\"{polygon_name}: current_agb_column - disturbance_total_column != old_growth_agb_column\"\n",
        "        # Define nodes\n",
        "        nodes = [left_axis, disturbance_total_name, remaining_name_agb]\n",
        "        # Calculate 'Remaining' value (should be equal to current_agb)\n",
        "        remaining_value = current_agb\n",
        "        # Define links\n",
        "        sources = [0, 0]\n",
        "        targets = [1, 2]\n",
        "        values = [-disturbance_total, remaining_value]\n",
        "        colors = [disturbance_total_colour, remaining_colour]\n",
        "        # Define node colors\n",
        "        node_colors = [remaining_colour] + colors\n",
        "\n",
        "    # Calculate percentages and update right node labels\n",
        "    percentages = [(abs(val) / old_growth_agb * 100) for val in values]\n",
        "    for i in range(1, len(nodes)):\n",
        "        if i - 1 < len(percentages):\n",
        "            nodes[i] += f\" ({percentages[i-1]:.0f}%)\"\n",
        "\n",
        "    title_and_density = [\n",
        "            dict(\n",
        "                x=0,\n",
        "                y=1.28,\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                text=title_name,\n",
        "                showarrow=False,\n",
        "                xanchor='left',\n",
        "                align='left',\n",
        "                font=dict(\n",
        "                    family=\"arial, sans serif\",\n",
        "                    size=title_font_size,\n",
        "                    color=\"black\",\n",
        "                    weight=title_font_weight\n",
        "                )\n",
        "            ),\n",
        "            dict(\n",
        "                x=0,\n",
        "                y=1.19,\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                text=subtitle_1_name,\n",
        "                showarrow=False,\n",
        "                xanchor='left',\n",
        "                align='left',\n",
        "                font=dict(\n",
        "                    family=\"arial, sans serif\",\n",
        "                    size=density_font_size,\n",
        "                    color=\"black\",\n",
        "                    weight=density_font_weight\n",
        "                )\n",
        "            ),\n",
        "            dict(\n",
        "                x=0,\n",
        "                y=1.11,\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                text=subtitle_2_name,\n",
        "                showarrow=False,\n",
        "                xanchor='left',\n",
        "                align='left',\n",
        "                font=dict(\n",
        "                    family=\"arial, sans serif\",\n",
        "                    size=density_font_size,\n",
        "                    color=\"black\",\n",
        "                    weight=density_font_weight\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    if show_title and not show_density: title_and_density = title_and_density[0:1]\n",
        "    if not show_title and show_density: title_and_density = title_and_density[1:3]\n",
        "    if not show_title and not show_density: title_and_density = []\n",
        "\n",
        "    # If labels are toggled off, replace node labels with empty strings\n",
        "    if not show_labels: nodes = [''] * len(nodes)\n",
        "\n",
        "    # Create the Sankey diagram\n",
        "    fig = go.Figure(data=[go.Sankey(\n",
        "        arrangement=\"freeform\",\n",
        "        node=dict(\n",
        "            label=nodes,\n",
        "            color=node_colors,  # Set node colors\n",
        "            pad=15,\n",
        "            thickness=20,\n",
        "            line=dict(color=\"black\", width=1)\n",
        "        ),\n",
        "        link=dict(\n",
        "            source=sources,\n",
        "            target=targets,\n",
        "            value=values,\n",
        "            color=colors,\n",
        "            line=dict(color=\"black\", width=1),  # Add border to ribbons\n",
        "        )\n",
        "    )])\n",
        "\n",
        "    fig.update_layout(\n",
        "        width=700 * width_modifier,\n",
        "        height=500,\n",
        "        font=dict(\n",
        "            family=\"arial, sans serif\",\n",
        "            size=label_font_size,\n",
        "            color=\"black\",\n",
        "            weight=label_font_weight\n",
        "        ),\n",
        "        # Adjust the margins\n",
        "        margin=dict(\n",
        "            l=25,\n",
        "            r=25,\n",
        "            t=115,  # Increased top margin to accommodate title\n",
        "            b=25\n",
        "        ),\n",
        "        annotations=title_and_density\n",
        "    )\n",
        "\n",
        "    # Save labelled version (with user settings)\n",
        "    png_path = os.path.join(sankey_labelled, f'sankey_diagram_{polygon_name}.png')\n",
        "    fig.write_image(png_path, scale=dpi / 96)\n",
        "\n",
        "    if svg_transparent_background:\n",
        "        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "    svg_path = os.path.join(sankey_labelled_svg, f'sankey_diagram_vector_{polygon_name}.svg')\n",
        "    fig.write_image(svg_path, scale=dpi / 96)\n",
        "\n",
        "    # Create and save unlabelled version\n",
        "    fig_unlabelled = go.Figure(data=[go.Sankey(\n",
        "        arrangement=\"freeform\",\n",
        "        node=dict(label=[''] * len(nodes), color=node_colors, pad=15, thickness=20,\n",
        "                 line=dict(color=\"black\", width=1)),\n",
        "        link=dict(source=sources, target=targets, value=values, color=colors,\n",
        "                 line=dict(color=\"black\", width=1))\n",
        "    )])\n",
        "\n",
        "    fig_unlabelled.update_layout(\n",
        "        width=700 * width_modifier, height=500,\n",
        "        font=dict(family=\"arial, sans serif\", size=label_font_size, color=\"black\", weight=label_font_weight),\n",
        "        margin=dict(l=25, r=25, t=115, b=25)\n",
        "    )\n",
        "\n",
        "    png_path = os.path.join(sankey_unlabelled, f'sankey_diagram_{polygon_name}.png')\n",
        "    fig_unlabelled.write_image(png_path, scale=dpi / 96)\n",
        "\n",
        "    if svg_transparent_background:\n",
        "        fig_unlabelled.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
        "    svg_path = os.path.join(sankey_unlabelled_svg, f'sankey_diagram_vector_{polygon_name}.svg')\n",
        "    fig_unlabelled.write_image(svg_path, scale=dpi / 96)\n",
        "\n",
        "    print(f\"Statistical assertions and sankey diagram complete for {polygon_name}.\")\n",
        "\n",
        "    # Show the figure (with white background)\n",
        "    fig.update_layout(plot_bgcolor='white', paper_bgcolor='white')\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "FpW7W57iZEUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXAGIvpk_KWS"
      },
      "source": [
        "# Disconnected runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3qMsENC_MP2"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "6j0th2jhtnAR",
        "zjy-T1TqScbE",
        "PC-rviHPi4q5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}