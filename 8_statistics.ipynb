{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/8_statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j0th2jhtnAR"
      },
      "source": [
        "# Imports & Subdirectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9drq4euvG0MS"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "\n",
        "# Mount Google Drive and set base directory\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGWgfUqto0F"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Imports and upgrades\n",
        "!pip install geopandas\n",
        "!pip install kaleido\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MTERlyxuPd4"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# Imports\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import math\n",
        "import numpy as np\n",
        "from os.path import exists, join\n",
        "from os import makedirs\n",
        "from osgeo import gdal\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import rasterio\n",
        "from rasterio import mask as msk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwar9sxbts-3"
      },
      "outputs": [],
      "source": [
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "statistics_dir = join(base_dir, \"8_statistics\")\n",
        "sample_polygons_dir = join(statistics_dir, \"sample_polygons\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(statistics_dir, exist_ok=True)\n",
        "makedirs(sample_polygons_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atvGRSPYgwk9"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjy-T1TqScbE"
      },
      "source": [
        "# Select model, area and sample polygons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj5RbVSuEp-B"
      },
      "outputs": [],
      "source": [
        "# Select if to source predictions from scenarios_dir or uncertainty_dir\n",
        "source_dir = uncertainty_dir\n",
        "\n",
        "# Select the model\n",
        "for subdir in os.listdir(source_dir):\n",
        "  print(f\"selected_model = '{subdir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7UWN6aIL-dX"
      },
      "outputs": [],
      "source": [
        "selected_model = 'agbd_240926_030225'\n",
        "\n",
        "selected_model_dir = join(source_dir, selected_model)\n",
        "# Select the prediction area\n",
        "for subdir in os.listdir(selected_model_dir):\n",
        "  if source_dir == scenarios_dir and not subdir.endswith('.csv') and not subdir.endswith('.json'):\n",
        "    print(f\"prediction_area = '{subdir}'\")\n",
        "  if source_dir == uncertainty_dir and subdir != 'model_iterations':\n",
        "    print(f\"prediction_area = '{subdir[10:]}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJpsBoAKMc-y"
      },
      "outputs": [],
      "source": [
        "prediction_area = 'tekai'\n",
        "\n",
        "# Model-area stats directory\n",
        "model_area_statistics_dir = join(statistics_dir, f\"{selected_model}_{prediction_area}\")\n",
        "makedirs(model_area_statistics_dir, exist_ok=True)\n",
        "\n",
        "# Calculate precise pixel size for the centre of the area\n",
        "# See https://gis.stackexchange.com/questions/142326/calculating-longitude-length-in-miles\n",
        "# See https://thoughtco.com/degree-of-latitude-and-longitude-distance-4070616\n",
        "\n",
        "# Use a predictor of the prediction area as a template\n",
        "predictors_dir = join(scenarios_dir, selected_model, prediction_area, \"predictors\")\n",
        "prediction_area_template_path = join(predictors_dir, os.listdir(predictors_dir)[0])\n",
        "prediction_area_template = gdal.Open(prediction_area_template_path)\n",
        "pixel_height_deg = 0 - prediction_area_template.GetGeoTransform()[5]\n",
        "pixel_width_deg = prediction_area_template.GetGeoTransform()[1]\n",
        "\n",
        "# Create a raster without 'nodata' values.\n",
        "template_no_nodata_dir = join(model_area_statistics_dir, 'template_no_nodata.tif')\n",
        "if not exists(template_no_nodata_dir):\n",
        "  prediction_area_template_array = prediction_area_template.ReadAsArray()\n",
        "  template_nodatavalue = prediction_area_template.GetRasterBand(1).GetNoDataValue()\n",
        "  absent_nodatavalue = template_nodatavalue + 1 # Modify nodata value to one that's absent.\n",
        "  assert absent_nodatavalue not in prediction_area_template_array, \"New nodata value is present in the template. Change to a value that is absent.\"\n",
        "  print(\"A template without nodata values will be created.\")\n",
        "  export_array_as_tif(prediction_area_template_array, template_no_nodata_dir, template = prediction_area_template_path, nodatavalue=absent_nodatavalue, compress=False)\n",
        "else: print(\"Template without nodata values already exists.\")\n",
        "print(\"This is used for counting all pixels inside a polygon mask (which uses the 'nodata' values).\\n\")\n",
        "\n",
        "# Proxy distance of degrees latitude in m (actual is non-linear)\n",
        "lat_dist_equator_km = 110.567\n",
        "lat_dist_poles_km = 111.699\n",
        "lat_dist_diff = lat_dist_poles_km - lat_dist_equator_km\n",
        "lat_dist_change_deg_km = lat_dist_diff / 90\n",
        "long_dist_equator_km = 111.321 # Equation calculates at different latitudes\n",
        "\n",
        "# Approximate pixel size\n",
        "approx_resolution = (np.average([pixel_height_deg, pixel_width_deg]) * np.average([lat_dist_equator_km, lat_dist_poles_km]) * 1000)\n",
        "approx_pixel_size_ha = approx_resolution**2 / 10000\n",
        "\n",
        "print(f\"Without precise correction, the approximate resolution is {approx_resolution} m, while the approximate pixel area is {approx_pixel_size_ha} ha.\\n\")\n",
        "print(f\"The pixel size with be further corrected based on the position of each sample polygon.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txTXPYFLpH6v"
      },
      "outputs": [],
      "source": [
        "# Select sample area polygons. This should be a single .gpkg with the field 'name' differentiating polygons.\n",
        "sample_polygons = []\n",
        "for geopackage in os.listdir(sample_polygons_dir):\n",
        "  sample_polygons.append(geopackage)\n",
        "\n",
        "if len(sample_polygons) == 0:\n",
        "  print(f\"No sample areas found. Upload .gpkg polygons to {sample_polygons_dir}\")\n",
        "else:\n",
        "  for sample_polygon in sample_polygons: print(f\"selected_sample_polygons = '{sample_polygon}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwWvaX5qDANc"
      },
      "outputs": [],
      "source": [
        "selected_sample_polygons = 'tekai_sample_polygons.gpkg'\n",
        "\n",
        "selected_sample_polygons_dir = join(sample_polygons_dir, selected_sample_polygons)\n",
        "selected_sample_polygons_gpkg = gpd.read_file(selected_sample_polygons_dir)\n",
        "sample_polygons_statistics_dir = join(model_area_statistics_dir, selected_sample_polygons[:-5])\n",
        "makedirs(sample_polygons_statistics_dir, exist_ok=True)\n",
        "detailed_stats_dir = join(sample_polygons_statistics_dir, 'detailed_stats')\n",
        "makedirs(detailed_stats_dir, exist_ok=True)\n",
        "detailed_stats_scenario_dir = join(sample_polygons_statistics_dir, 'detailed_stats_scenario')\n",
        "makedirs(detailed_stats_scenario_dir, exist_ok=True)\n",
        "forecast_input_dir = join(sample_polygons_statistics_dir, 'forecast_input')\n",
        "makedirs(forecast_input_dir, exist_ok=True)\n",
        "detailed_diff_stats_dir = join(sample_polygons_statistics_dir, 'detailed_diff_stats')\n",
        "makedirs(detailed_diff_stats_dir, exist_ok=True)\n",
        "detailed_diff_stats_diff_dir = join(sample_polygons_statistics_dir, 'detailed_diff_stats_diff')\n",
        "makedirs(detailed_diff_stats_diff_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlQ2iLAiwQoB"
      },
      "source": [
        "# Scenario statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgC9laZXPB9Y"
      },
      "outputs": [],
      "source": [
        "# Create list of available prediction rasters and scenarios. Rasters must already be masked (e.g. to forest).\n",
        "if source_dir == scenarios_dir: prediction_raster_dir = join(selected_model_dir, prediction_area, 'scenario_predictions')\n",
        "if source_dir == uncertainty_dir: prediction_raster_dir = join(selected_model_dir, f'scenarios_{prediction_area}', 'statistics_masked')\n",
        "prediction_raster_dirs = []\n",
        "scenarios = set()\n",
        "for prediction_raster in os.listdir(prediction_raster_dir):\n",
        "  prediction_raster_dirs.append(join(prediction_raster_dir, prediction_raster))\n",
        "  if source_dir == uncertainty_dir: scenarios.add(prediction_raster.split(\"__\")[1])\n",
        "  if source_dir == scenarios_dir: scenarios.add(prediction_raster.split(\"__\")[0])\n",
        "scenarios = sorted(list(scenarios))\n",
        "\n",
        "# Select scenario predictions to calculate statistics\n",
        "print('selected_scenarios = [')\n",
        "for scenario in scenarios:\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(']\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jizdpzwLiqH7"
      },
      "outputs": [],
      "source": [
        "selected_scenarios = [\n",
        "  \"2022\",\n",
        "  \"2022_no_degradation_since_1990\",\n",
        "  \"2022_oldgrowth\",\n",
        "  \"2023\",\n",
        "  \"2023_no_degradation_since_1990\",\n",
        "  \"2023_oldgrowth\",\n",
        "  \"all_oldgrowth\",\n",
        "]\n",
        "\n",
        "# Filter to selected scenarios, and separate prediction and uncertainty rasters (if latter present)\n",
        "prediction_rasters = []\n",
        "uncertainty_rasters = []\n",
        "for prediction_raster in prediction_raster_dirs:\n",
        "  for scenario in selected_scenarios:\n",
        "    if source_dir == uncertainty_dir:\n",
        "      if scenario == prediction_raster.split('/')[-1].split('__')[1] and 'mean__' in prediction_raster:\n",
        "        prediction_rasters.append(prediction_raster)\n",
        "      if scenario == prediction_raster.split('/')[-1].split('__')[1] and 'uncertainty__' in prediction_raster:\n",
        "        uncertainty_rasters.append(prediction_raster)\n",
        "    else: # If the source directory is scenarios_dir (without uncertainty values)\n",
        "      if scenario == prediction_raster.split('/')[-1].split('__')[0]: prediction_rasters.append(prediction_raster)\n",
        "# Toggle whether to predict uncertainty stats\n",
        "if len(uncertainty_rasters) > 0: generate_uncertainty_stats = True\n",
        "else: generate_uncertainty_stats = False\n",
        "\n",
        "# Sort rasters chronologically (assuming year is first in the filename)\n",
        "prediction_rasters = sorted(prediction_rasters)\n",
        "uncertainty_rasters = sorted(uncertainty_rasters)\n",
        "\n",
        "# Generate empty dataframes for statistics\n",
        "df_base = pd.DataFrame(index = selected_scenarios)\n",
        "df_base.rename_axis('scenario', inplace=True)\n",
        "df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg = df_base.copy(), df_base.copy(), df_base.copy(), df_base.copy()\n",
        "# If uncertainty rasters are present, generate empty dataframes\n",
        "if generate_uncertainty_stats:\n",
        "  df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agb_total_tg_ci95 = df_base.copy(), df_base.copy(), df_base.copy()\n",
        "\n",
        "# Initialise polygon area dataframe\n",
        "df_polygon_area_km2 = pd.DataFrame(columns = [\"Name\", \"Area (km^2)\"])\n",
        "\n",
        "# Loop through each polygon stored in GPKG to generate statistics\n",
        "for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "\n",
        "  # Define the polygon\n",
        "  sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"] # Set name to start at 3rd character with [2:] (skipping number used for ordering)\n",
        "  polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "  # Latitude of the centroid\n",
        "  polygon_centroid_lat = sample_polygon_geometry.centroid.y\n",
        "\n",
        "  # Calculate latitude distance per degree at the polygon's latitude\n",
        "  latitude_m_per_degree = 1000 * (lat_dist_equator_km + (lat_dist_change_deg_km * polygon_centroid_lat))\n",
        "\n",
        "  # Rest of your calculations that depend on the latitude_m_per_degree follows here\n",
        "  # For example, if you're calculating pixel size in meters for each polygon based on its latitude:\n",
        "  precise_pixel_height_m = latitude_m_per_degree * pixel_height_deg\n",
        "  precise_pixel_width_m = (math.cos((math.pi / 180) * polygon_centroid_lat) * latitude_m_per_degree * pixel_width_deg)\n",
        "  precise_pixel_size_ha = precise_pixel_height_m * precise_pixel_width_m / 10000\n",
        "\n",
        "  # Mask the 'no nodata' raster to the polygon with an absent value to count all pixels within the polygon\n",
        "  with rasterio.open(template_no_nodata_dir) as template_no_nodata:\n",
        "    no_nodata_template_array_masked, transform_1 = msk.mask(template_no_nodata, polygons, crop=True, filled=False)\n",
        "\n",
        "  # Calculate sample_polygon_geometry area (ellipsoidal as opposed to planimetric)\n",
        "  sample_polygons_crs = selected_sample_polygons_gpkg.crs\n",
        "  temp_gdf = gpd.GeoDataFrame({'name': [sample_polygon_name], 'geometry': sample_polygon_geometry}, crs=sample_polygons_crs)\n",
        "  temp_gdf_utm = temp_gdf.estimate_utm_crs()\n",
        "  polygon_area_ha = np.divide(temp_gdf.to_crs(temp_gdf_utm).area[0], 10000, dtype='float64')\n",
        "  # Ensure the new_row has the correct data types\n",
        "  new_row = pd.DataFrame([{'Name': sample_polygon_name, 'Area (km^2)': polygon_area_ha / 100}], dtype=object)\n",
        "  df_polygon_area_km2 = pd.concat([df_polygon_area_km2, new_row], ignore_index=True, sort=False)\n",
        "\n",
        "  # Create an empty list for each prediction raster statistic\n",
        "  values_forest_cover_ha, values_agbd_mean_mg_ha, values_agbd_stdev_mg_ha, values_agb_total_tg = [], [], [], []\n",
        "\n",
        "  # If uncertainty rasters are present, create an empty list for each uncertainty raster statistic\n",
        "  if generate_uncertainty_stats:\n",
        "    values_agbd_mean_mg_ha_ci95, values_agbd_mean_mg_ha_uncertainty, values_agb_total_tg_ci95  = [], [], []\n",
        "\n",
        "  # Loop through prediction rasters\n",
        "  for prediction_raster in prediction_rasters:\n",
        "\n",
        "    # Mask predictor to sample_polygon_geometry\n",
        "    with rasterio.open(prediction_raster) as prediction:\n",
        "      nodatavalue = int(prediction.nodatavals[0])\n",
        "      prediction_array_masked, transform_2 = msk.mask(prediction, polygons, crop=True, filled=False)\n",
        "\n",
        "    # Count all (incl. nodata) pixels within polygon, and estimate their total area\n",
        "    pixel_count_polygon = np.ma.count(no_nodata_template_array_masked)\n",
        "    pixels_area_polygon_ha = np.multiply(pixel_count_polygon, precise_pixel_size_ha, dtype='float64')\n",
        "\n",
        "    # Correct pixel size to UTM ellipsoidal measure of the polygons position (which will decrease further North)\n",
        "    polygon_to_pixel_area_ratio = np.divide(pixels_area_polygon_ha, polygon_area_ha, dtype='float64')\n",
        "    pixel_size_ha_corrected = np.multiply(precise_pixel_size_ha, polygon_to_pixel_area_ratio, dtype='float64')\n",
        "\n",
        "    # Count pixels within polygon, excluding those previously masked i.e. nodata\n",
        "    pixel_count_polygon_masked = np.ma.count(prediction_array_masked)\n",
        "    pixels_area_polygon_masked_ha = np.multiply(pixel_count_polygon_masked, pixel_size_ha_corrected, dtype='float64')\n",
        "\n",
        "    # Calculate forest area\n",
        "    forest_cover_ha = pixels_area_polygon_masked_ha # Already masked to forest in current workflow\n",
        "\n",
        "    # Calculate total, mean and stdev of aboveground biomass\n",
        "    agbd_mean_mg_ha = np.ma.mean(prediction_array_masked, dtype='float64') # Float64 minimises error for large number of values\n",
        "    agbd_mean_stdev_ha = np.ma.std(prediction_array_masked, dtype='float64')\n",
        "    agb_total_mg = np.multiply(agbd_mean_mg_ha, forest_cover_ha, dtype='float64')\n",
        "    agb_total_tg = np.divide(agb_total_mg, 1000000, dtype='float64') # Convert Mg (megagram = ton) to Tg (teragram = megaton)\n",
        "\n",
        "    # Append results to statistics list\n",
        "    values_forest_cover_ha.append(forest_cover_ha)\n",
        "    values_agbd_mean_mg_ha.append(agbd_mean_mg_ha)\n",
        "    values_agbd_stdev_mg_ha.append(agbd_mean_stdev_ha)\n",
        "    values_agb_total_tg.append(agb_total_tg)\n",
        "\n",
        "    if generate_uncertainty_stats:\n",
        "      uncertainty_raster_present = False\n",
        "      for uncertainty_raster in uncertainty_rasters:\n",
        "        if prediction_raster.split('mean__')[1] in uncertainty_raster:\n",
        "          uncertainty_raster_present = True\n",
        "          matching_uncertainty_raster = uncertainty_raster\n",
        "\n",
        "      if not uncertainty_raster_present: print(f\"There is no uncertainty raster for {prediction_raster.split('/')[-1]}\")\n",
        "\n",
        "      if uncertainty_raster_present:\n",
        "        # Open and mask uncertainty raster to polygon\n",
        "          with rasterio.open(matching_uncertainty_raster) as uncertainty:\n",
        "            nodatavalue = int(uncertainty.nodatavals[0])\n",
        "            uncertainty_array_masked, transform_2 = msk.mask(uncertainty, polygons, crop=True, filled=False)\n",
        "\n",
        "          # See https://stats.stackexchange.com/questions/223924/how-to-add-up-partial-confidence-intervals-to-create-a-total-confidence-interval#comment426260_223924\n",
        "\n",
        "          # Compress masked array data to 1D\n",
        "          prediction_1d = np.ma.compressed(prediction_array_masked)\n",
        "          uncertainty_1d = np.ma.compressed(uncertainty_array_masked)\n",
        "          # Convert uncertainty percentages to ratios\n",
        "          uncertainty_ratios = np.divide(uncertainty_1d, 100, dtype='float64')\n",
        "          # Multiply the prediction values (mean AGBD Mg/ha) by uncertainty ratios for CI95 values\n",
        "          prediction_ci95s = np.multiply(prediction_1d, uncertainty_ratios, dtype='float64')\n",
        "\n",
        "          # Method 1 - Simple calculation of mean CI95 (higher estimate). Assumption: pixel values are completely correlated (all measure the same thing).\n",
        "          agbd_mean_mg_ha_ci95_1 = np.mean(prediction_ci95s, dtype = 'float64')\n",
        "          # Method 2 - Square prediction CI95s. Sum and then square root for total CI.\n",
        "          # Then divide by observations to calculate mean CI95. Assumption: pixel values are completely independent.\n",
        "          sum_squares = np.sum(np.square(prediction_ci95s, dtype='float64'), dtype='float64')\n",
        "          total_ci95 = np.sqrt(sum_squares, dtype='float64')\n",
        "          agbd_mean_mg_ha_ci95_2 = np.divide(total_ci95, np.ma.count(prediction_ci95s), dtype='float64')\n",
        "          # Method 3 - Used in Liang et al 2023 to calculate change uncertainty. Identical results to method 2.\n",
        "          predictions_x_uncertainties = np.multiply(prediction_1d, uncertainty_1d, dtype='float64')\n",
        "          sum_squares_pxu = np.sum(np.square(predictions_x_uncertainties, dtype='float64'), dtype='float64')\n",
        "          sqrt_divided_sum = np.sqrt(sum_squares_pxu, dtype='float64') / np.sum(prediction_1d, dtype='float64')\n",
        "          agbd_mean_mg_ha_ci95_3 = np.multiply(np.divide(sqrt_divided_sum, 100, dtype='float64'), agbd_mean_mg_ha, dtype='float64')\n",
        "\n",
        "          agbd_mean_mg_ha_ci95 = agbd_mean_mg_ha_ci95_1\n",
        "\n",
        "          # Calculate total AGB CI95\n",
        "          agb_total_mg_ci95 = np.multiply(agbd_mean_mg_ha_ci95, forest_cover_ha, dtype='float64')\n",
        "          agb_total_tg_ci95 = np.divide(agb_total_mg_ci95, 1000000, dtype='float64') # Convert total CI to Tg\n",
        "          # Calculate percentage uncertainty\n",
        "          agbd_mean_mg_ha_uncertainty = np.multiply(np.divide(agbd_mean_mg_ha_ci95, agbd_mean_mg_ha, dtype='float64'), 100, dtype='float64')\n",
        "          # Append results to statistics list\n",
        "          values_agbd_mean_mg_ha_ci95.append(agbd_mean_mg_ha_ci95)\n",
        "          values_agbd_mean_mg_ha_uncertainty.append(agbd_mean_mg_ha_uncertainty)\n",
        "          values_agb_total_tg_ci95.append(agb_total_tg_ci95)\n",
        "\n",
        "  # Concatenate new columns to the main DataFrames for each statistic\n",
        "  df_forest_cover_ha = pd.concat([df_forest_cover_ha, pd.DataFrame({sample_polygon_name: values_forest_cover_ha}, index=df_forest_cover_ha.index)], axis=1)\n",
        "  df_agbd_mean_mg_ha = pd.concat([df_agbd_mean_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha}, index=df_agbd_mean_mg_ha.index)], axis=1)\n",
        "  df_agbd_stdev_mg_ha = pd.concat([df_agbd_stdev_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_stdev_mg_ha}, index=df_agbd_stdev_mg_ha.index)], axis=1)\n",
        "  df_agb_total_tg = pd.concat([df_agb_total_tg, pd.DataFrame({sample_polygon_name: values_agb_total_tg}, index=df_agb_total_tg.index)], axis=1)\n",
        "\n",
        "  if generate_uncertainty_stats:\n",
        "      df_agbd_mean_mg_ha_ci95 = pd.concat([df_agbd_mean_mg_ha_ci95, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_ci95}, index=df_agbd_mean_mg_ha_ci95.index)], axis=1)\n",
        "      df_agbd_mean_mg_ha_uncertainty = pd.concat([df_agbd_mean_mg_ha_uncertainty, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_uncertainty}, index=df_agbd_mean_mg_ha_uncertainty.index)], axis=1)\n",
        "      df_agb_total_tg_ci95 = pd.concat([df_agb_total_tg_ci95, pd.DataFrame({sample_polygon_name: values_agb_total_tg_ci95}, index=df_agb_total_tg_ci95.index)], axis=1)\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "  df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_mean_mg_ha_ci95,\n",
        "                 df_agbd_mean_mg_ha_uncertainty, df_agbd_stdev_mg_ha, df_agb_total_tg, df_agb_total_tg_ci95]\n",
        "else: df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Set index of the polygon area km2 dataframe to 'Name' of the polygon\n",
        "df_polygon_area_km2 = df_polygon_area_km2.set_index('Name')\n",
        "\n",
        "# Export statistics for forecast input\n",
        "df_forecast_list = [df_forest_cover_ha, df_agb_total_tg]\n",
        "for df_forecast in df_forecast_list:\n",
        "  df_noalts = df_forecast[df_forecast.index.str.contains(\"_\") == False]\n",
        "  if df_forecast.equals(df_forest_cover_ha): df_filename = \"forest_cover_ha\"\n",
        "  # if df_stats.equals(df_agbd_mean_mg_ha): df_filename = \"agbd_mean_mg_ha\"\n",
        "  # if df_stats.equals(df_agbd_stdev_mg_ha): df_filename = \"agbd_stdev_mg_ha\"\n",
        "  if df_forecast.equals(df_agb_total_tg): df_filename = \"agb_total_tg\"\n",
        "  df_noalts.to_csv(join(forecast_input_dir, f'{df_filename}.csv'))\n",
        "\n",
        "# Generate summary stats\n",
        "df_forest_cover_ha_t = df_forest_cover_ha.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest cover (ha)\")\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB (Tg)\")\n",
        "if generate_uncertainty_stats:\n",
        "  df_agb_total_tg_ci95_t = df_agb_total_tg_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB CI95 (Tg)\")\n",
        "  summary_stats = pd.concat([df_polygon_area_km2, df_forest_cover_ha_t, df_agb_total_tg_t, df_agb_total_tg_ci95_t], axis=1).rename_axis(\"Name\", axis=1)\n",
        "else: summary_stats = pd.concat([df_polygon_area_km2, df_forest_cover_ha_t, df_agb_total_tg_t], axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_stats.to_csv(join(sample_polygons_statistics_dir, 'summary_stats.csv'))\n",
        "\n",
        "# Generate detailed stats by polygon (requires uncertainty stats)\n",
        "for polygon_area in df_stats_list[0]:\n",
        "  polygon_area_km2 = df_polygon_area_km2.loc[polygon_area][\"Area (km^2)\"]\n",
        "  df_detailed_stats = df_base\n",
        "  df_detailed_stats[\"Area (km^2)\"] = polygon_area_km2\n",
        "  for df_stats in df_stats_list:\n",
        "    if df_stats.equals(df_forest_cover_ha): stat_col = \"Forest cover (ha)\"\n",
        "    if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"Forest AGBD mean (Mg / ha)\"\n",
        "    if df_stats.equals(df_agbd_stdev_mg_ha): stat_col = \"Forest AGBD stdev (Mg / ha)\"\n",
        "    if df_stats.equals(df_agb_total_tg): stat_col = \"Forest AGB total (Tg)\"\n",
        "    if generate_uncertainty_stats:\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_ci95): stat_col = \"Forest AGBD CI95 (Mg / ha)\"\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_uncertainty): stat_col = \"Forest AGBD uncertainty (%)\"\n",
        "      if df_stats.equals(df_agb_total_tg_ci95): stat_col = \"Forest AGB total CI95 (Tg)\"\n",
        "    for stats_polygon_area in df_stats:\n",
        "      if stats_polygon_area == polygon_area:\n",
        "        df_stats_renamed = df_stats.rename(columns={stats_polygon_area:stat_col})\n",
        "        df_detailed_stats = pd.concat([df_detailed_stats, df_stats_renamed[stat_col]], axis=1)\n",
        "    df_detailed_stats.to_csv(join(detailed_stats_dir, f'{polygon_area}.csv'))\n",
        "\n",
        "# Generate detailed stats by scenario (requires uncertainty stats)\n",
        "scenarios = {}\n",
        "# Loop through all CSV files in the 'detailed_stats_dir'\n",
        "for stats_csv in os.listdir(detailed_stats_dir):\n",
        "    polygon_name = f\"{stats_csv[:-4]}\"\n",
        "    stats_csv_path = join(detailed_stats_dir, stats_csv)\n",
        "    stats_csv_df = pd.read_csv(stats_csv_path)\n",
        "    # Loop through each unique scenario in the file\n",
        "    for scenario in stats_csv_df['scenario'].unique():\n",
        "        # Filter the dataframe for the current scenario\n",
        "        scenario_df = stats_csv_df[stats_csv_df['scenario'] == scenario].copy()\n",
        "        # Drop the 'scenario' column and add the 'Name' column\n",
        "        scenario_df.drop('scenario', axis=1, inplace=True)\n",
        "        scenario_df.insert(0, 'Name', polygon_name)\n",
        "        # If this scenario's dataframe already exists, append to it; otherwise, create it\n",
        "        if scenario in scenarios: scenarios[scenario] = pd.concat([scenarios[scenario], scenario_df], ignore_index=True)\n",
        "        else: scenarios[scenario] = scenario_df\n",
        "for scenario, scenario_df in scenarios.items():\n",
        "    output_file_path = join(detailed_stats_scenario_dir,f'{scenario}.csv')\n",
        "    scenario_df.to_csv(output_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGzdNKRbo0H5"
      },
      "source": [
        "# Difference statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTpXfmfAo7hY"
      },
      "outputs": [],
      "source": [
        "# Create list of available difference rasters and scenarios\n",
        "if source_dir == scenarios_dir: diff_raster_dir = join(selected_model_dir, prediction_area, 'scenario_difference')\n",
        "if source_dir == uncertainty_dir: diff_raster_dir = join(selected_model_dir, f'scenarios_{prediction_area}', 'scenario_difference')\n",
        "\n",
        "diff_raster_dirs = []\n",
        "diffs = set()\n",
        "for diff_raster in os.listdir(diff_raster_dir):\n",
        "  diff_raster_dirs.append(join(diff_raster_dir, diff_raster))\n",
        "  if source_dir == uncertainty_dir: diffs.add(diff_raster.split(\"__\")[1])\n",
        "  if source_dir == scenarios_dir: diffs.add(diff_raster.split(\"__\")[0])\n",
        "diffs = sorted(list(diffs))\n",
        "\n",
        "# Select difference rasters to calculate statistics\n",
        "print('selected_diffs = [')\n",
        "for diff in diffs:\n",
        "  print(f'  \"{diff}\",')\n",
        "print(']')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfyIlrVKwd6S"
      },
      "outputs": [],
      "source": [
        "selected_diffs = [\n",
        "  \"2022_deforestation_total\",\n",
        "  \"2022_degradation_before_1990\",\n",
        "  \"2022_degradation_deforestation_total\",\n",
        "  \"2022_degradation_since_1990\",\n",
        "  \"2022_degradation_total\",\n",
        "  \"2023_deforestation_total\",\n",
        "  \"2023_degradation_before_1990\",\n",
        "  \"2023_degradation_deforestation_total\",\n",
        "  \"2023_degradation_since_1990\",\n",
        "  \"2023_degradation_total\",\n",
        "]\n",
        "\n",
        "# Filter to selected scenarios, and separate prediction and uncertainty rasters (if latter present)\n",
        "diff_rasters = []\n",
        "diff_uncertainty_rasters = []\n",
        "for diff_raster in diff_raster_dirs:\n",
        "  for diff in selected_diffs:\n",
        "    if source_dir == uncertainty_dir:\n",
        "      if diff == diff_raster.split('/')[-1].split('__')[1] and 'mean__' in diff_raster:\n",
        "        diff_rasters.append(diff_raster)\n",
        "      if diff == diff_raster.split('/')[-1].split('__')[1] and 'uncertainty__' in diff_raster:\n",
        "        diff_uncertainty_rasters.append(diff_raster)\n",
        "    else: # If the source directory is scenarios_dir (without uncertainty values)\n",
        "      if diff == diff_raster.split('/')[-1].split('__')[0]: diff_rasters.append(diff_raster)\n",
        "# Toggle whether to predict uncertainty stats\n",
        "if len(diff_uncertainty_rasters) > 0: generate_uncertainty_stats = True\n",
        "else: generate_uncertainty_stats = False\n",
        "\n",
        "# Sort rasters chronologically (assuming year is first in the filename)\n",
        "diff_rasters = sorted(diff_rasters)\n",
        "diff_uncertainty_rasters = sorted(diff_uncertainty_rasters)\n",
        "\n",
        "# Generate empty dataframes for statistics\n",
        "df_base = pd.DataFrame(index = selected_diffs)\n",
        "df_base.rename_axis('diff', inplace=True)\n",
        "df_agbd_mean_mg_ha, df_agb_total_tg = df_base.copy(), df_base.copy()\n",
        "# If uncertainty rasters are present, generate empty dataframes\n",
        "if generate_uncertainty_stats:\n",
        "  df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agb_total_tg_ci95 = df_base.copy(), df_base.copy(), df_base.copy()\n",
        "\n",
        "# Initialise polygon area dataframe\n",
        "df_polygon_area_km2 = pd.DataFrame(columns = [\"Name\", \"Area (km^2)\"])\n",
        "\n",
        "# Loop through each polygon stored in GPKG to generate statistics\n",
        "for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "\n",
        "  # Define the polygon\n",
        "  sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"] # Set name to start at 3rd character with [2:] (skipping number used for ordering)\n",
        "  polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "  # Latitude of the centroid\n",
        "  polygon_centroid_lat = sample_polygon_geometry.centroid.y\n",
        "\n",
        "  # Calculate latitude distance per degree at the polygon's latitude\n",
        "  latitude_m_per_degree = 1000 * (lat_dist_equator_km + (lat_dist_change_deg_km * polygon_centroid_lat))\n",
        "\n",
        "  # Rest of your calculations that depend on the latitude_m_per_degree follows here\n",
        "  # For example, if you're calculating pixel size in meters for each polygon based on its latitude:\n",
        "  precise_pixel_height_m = latitude_m_per_degree * pixel_height_deg\n",
        "  precise_pixel_width_m = (math.cos((math.pi / 180) * polygon_centroid_lat) * latitude_m_per_degree * pixel_width_deg)\n",
        "  precise_pixel_size_ha = precise_pixel_height_m * precise_pixel_width_m / 10000\n",
        "\n",
        "  # Mask the 'no nodata' raster to the polygon with an absent value to count all pixels within the polygon\n",
        "  with rasterio.open(template_no_nodata_dir) as template_no_nodata:\n",
        "    no_nodata_template_array_masked, transform_1 = msk.mask(template_no_nodata, polygons, crop=True, filled=False)\n",
        "\n",
        "  # Calculate sample_polygon_geometry area (ellipsoidal as opposed to planimetric)\n",
        "  sample_polygons_crs = selected_sample_polygons_gpkg.crs\n",
        "  temp_gdf = gpd.GeoDataFrame({'name': [sample_polygon_name], 'geometry': sample_polygon_geometry}, crs=sample_polygons_crs)\n",
        "  temp_gdf_utm = temp_gdf.estimate_utm_crs()\n",
        "  polygon_area_ha = np.divide(temp_gdf.to_crs(temp_gdf_utm).area[0], 10000, dtype='float64')\n",
        "  # Ensure the new_row has the correct data types\n",
        "  new_row = pd.DataFrame([{'Name': sample_polygon_name, 'Area (km^2)': polygon_area_ha / 100}], dtype=object)\n",
        "  df_polygon_area_km2 = pd.concat([df_polygon_area_km2, new_row], ignore_index=True, sort=False)\n",
        "\n",
        "  # Create an empty list for each prediction raster statistic\n",
        "  values_agbd_mean_mg_ha, values_agb_total_tg = [], []\n",
        "\n",
        "  # If uncertainty rasters are present, create an empty list for each uncertainty raster statistic\n",
        "  if generate_uncertainty_stats:\n",
        "    values_agbd_mean_mg_ha_ci95, values_agbd_mean_mg_ha_uncertainty, values_agb_total_tg_ci95  = [], [], []\n",
        "\n",
        "  # Loop through diff rasters\n",
        "  for diff_raster in diff_rasters:\n",
        "\n",
        "    # Mask predictor to sample_polygon_geometry\n",
        "    with rasterio.open(diff_raster) as diff:\n",
        "      nodatavalue = int(diff.nodatavals[0])\n",
        "      diff_array_masked, transform_2 = msk.mask(diff, polygons, crop=True, filled=False)\n",
        "\n",
        "    # Count all (incl. nodata) pixels within polygon, and estimate their total area\n",
        "    pixel_count_polygon = np.ma.count(no_nodata_template_array_masked)\n",
        "    pixels_area_polygon_ha = np.multiply(pixel_count_polygon, precise_pixel_size_ha, dtype='float64')\n",
        "\n",
        "    # Correct pixel size to UTM ellipsoidal measure of the polygons position (which will decrease further North)\n",
        "    polygon_to_pixel_area_ratio = np.divide(pixels_area_polygon_ha, polygon_area_ha, dtype='float64')\n",
        "    pixel_size_ha_corrected = np.multiply(precise_pixel_size_ha, polygon_to_pixel_area_ratio, dtype='float64')\n",
        "\n",
        "    # Count pixels within polygon, excluding those previously masked i.e. nodata\n",
        "    pixel_count_polygon_masked = np.ma.count(diff_array_masked)\n",
        "    pixels_area_polygon_masked_ha = np.multiply(pixel_count_polygon_masked, pixel_size_ha_corrected, dtype='float64')\n",
        "\n",
        "    # Calculate forest area\n",
        "    forest_cover_ha = pixels_area_polygon_masked_ha # Already masked to forest in current workflow\n",
        "\n",
        "    # Calculate total, mean and stdev of aboveground biomass\n",
        "    agbd_mean_mg_ha = np.ma.mean(diff_array_masked, dtype='float64') # Float64 minimises error for large number of values\n",
        "    agb_total_mg = np.multiply(agbd_mean_mg_ha, forest_cover_ha, dtype='float64')\n",
        "    agb_total_tg = np.divide(agb_total_mg, 1000000, dtype='float64') # Convert Mg (megagram = ton) to Tg (teragram = megaton)\n",
        "\n",
        "    # Append results to statistics list\n",
        "    values_agbd_mean_mg_ha.append(agbd_mean_mg_ha)\n",
        "    values_agb_total_tg.append(agb_total_tg)\n",
        "\n",
        "    if generate_uncertainty_stats:\n",
        "      diff_uncertainty_raster_present = False\n",
        "      for diff_uncertainty_raster in diff_uncertainty_rasters:\n",
        "        if diff_raster.split('mean__')[1] in diff_uncertainty_raster:\n",
        "          diff_uncertainty_raster_present = True\n",
        "          matching_diff_uncertainty_raster = diff_uncertainty_raster\n",
        "\n",
        "      if not diff_uncertainty_raster_present: print(f\"There is no uncertainty raster for {diff_raster.split('/')[-1]}\")\n",
        "\n",
        "      if diff_uncertainty_raster_present:\n",
        "        # Open and mask uncertainty raster to polygon\n",
        "          with rasterio.open(matching_diff_uncertainty_raster) as uncertainty:\n",
        "            nodatavalue = int(uncertainty.nodatavals[0])\n",
        "            uncertainty_array_masked, transform_2 = msk.mask(uncertainty, polygons, crop=True, filled=False)\n",
        "\n",
        "          # See https://stats.stackexchange.com/questions/223924/how-to-add-up-partial-confidence-intervals-to-create-a-total-confidence-interval#comment426260_223924\n",
        "\n",
        "          # Compress masked array data to 1D\n",
        "          diff_1d = np.ma.compressed(diff_array_masked)\n",
        "          uncertainty_1d = np.ma.compressed(uncertainty_array_masked)\n",
        "          # Convert uncertainty percentages to ratios\n",
        "          uncertainty_ratios = np.divide(uncertainty_1d, 100, dtype='float64')\n",
        "          # Multiply the diff values (mean AGBD Mg/ha) by uncertainty ratios for CI95 values\n",
        "          diff_ci95s = np.multiply(diff_1d, uncertainty_ratios, dtype='float64')\n",
        "\n",
        "          # Method 1 - Simple calculation of mean CI95 (higher estimate). Assumption: pixel values are completely correlated (all measure the same thing).\n",
        "          agbd_mean_mg_ha_ci95_1 = np.mean(diff_ci95s, dtype = 'float64')\n",
        "          # Method 2 - Square diff CI95s. Sum and then square root for total CI.\n",
        "          # Then divide by observations to calculate mean CI95. Assumption: pixel values are completely independent.\n",
        "          sum_squares = np.sum(np.square(diff_ci95s, dtype='float64'), dtype='float64')\n",
        "          total_ci95 = np.sqrt(sum_squares, dtype='float64')\n",
        "          agbd_mean_mg_ha_ci95_2 = np.divide(total_ci95, np.ma.count(diff_ci95s), dtype='float64')\n",
        "          # Method 3 - Used in Liang et al 2023 to calculate change uncertainty. Identical results to method 2.\n",
        "          diffs_x_uncertainties = np.multiply(diff_1d, uncertainty_1d, dtype='float64')\n",
        "          sum_squares_pxu = np.sum(np.square(diffs_x_uncertainties, dtype='float64'), dtype='float64')\n",
        "          sqrt_divided_sum = np.sqrt(sum_squares_pxu, dtype='float64') / np.sum(diff_1d, dtype='float64')\n",
        "          agbd_mean_mg_ha_ci95_3 = np.multiply(np.divide(sqrt_divided_sum, 100, dtype='float64'), agbd_mean_mg_ha, dtype='float64')\n",
        "\n",
        "          agbd_mean_mg_ha_ci95 = agbd_mean_mg_ha_ci95_1\n",
        "\n",
        "          # Calculate total AGB CI95\n",
        "          agb_total_mg_ci95 = np.multiply(agbd_mean_mg_ha_ci95, forest_cover_ha, dtype='float64')\n",
        "          agb_total_tg_ci95 = np.divide(agb_total_mg_ci95, 1000000, dtype='float64') # Convert total CI to Tg\n",
        "          # Calculate percentage uncertainty\n",
        "          agbd_mean_mg_ha_uncertainty = np.multiply(np.divide(agbd_mean_mg_ha_ci95, agbd_mean_mg_ha, dtype='float64'), 100, dtype='float64')\n",
        "          # Append results to statistics list\n",
        "          values_agbd_mean_mg_ha_ci95.append(agbd_mean_mg_ha_ci95)\n",
        "          values_agbd_mean_mg_ha_uncertainty.append(agbd_mean_mg_ha_uncertainty)\n",
        "          values_agb_total_tg_ci95.append(agb_total_tg_ci95)\n",
        "\n",
        "  # Concatenate new columns to the main DataFrames for each statistic\n",
        "  df_agbd_mean_mg_ha = pd.concat([df_agbd_mean_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha}, index=df_agbd_mean_mg_ha.index)], axis=1)\n",
        "  df_agb_total_tg = pd.concat([df_agb_total_tg, pd.DataFrame({sample_polygon_name: values_agb_total_tg}, index=df_agb_total_tg.index)], axis=1)\n",
        "\n",
        "  if generate_uncertainty_stats:\n",
        "      df_agbd_mean_mg_ha_ci95 = pd.concat([df_agbd_mean_mg_ha_ci95, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_ci95}, index=df_agbd_mean_mg_ha_ci95.index)], axis=1)\n",
        "      df_agbd_mean_mg_ha_uncertainty = pd.concat([df_agbd_mean_mg_ha_uncertainty, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_uncertainty}, index=df_agbd_mean_mg_ha_uncertainty.index)], axis=1)\n",
        "      df_agb_total_tg_ci95 = pd.concat([df_agb_total_tg_ci95, pd.DataFrame({sample_polygon_name: values_agb_total_tg_ci95}, index=df_agb_total_tg_ci95.index)], axis=1)\n",
        "\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "  df_stats_list = [df_agbd_mean_mg_ha, df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agb_total_tg, df_agb_total_tg_ci95]\n",
        "else: df_stats_list = [df_agbd_mean_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Set index of the polygon area km2 dataframe to 'Name' of the polygon\n",
        "df_polygon_area_km2 = df_polygon_area_km2.set_index('Name')\n",
        "\n",
        "# Generate summary stats\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB (Tg)\")\n",
        "if generate_uncertainty_stats:\n",
        "  df_agb_total_tg_ci95_t = df_agb_total_tg_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB CI95 (Tg)\")\n",
        "  summary_stats = pd.concat([df_polygon_area_km2, df_agb_total_tg_t, df_agb_total_tg_ci95_t], axis=1).rename_axis(\"Name\", axis=1)\n",
        "else: summary_stats = pd.concat([df_polygon_area_km2, df_agb_total_tg_t], axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_stats.to_csv(join(sample_polygons_statistics_dir, 'summary_diff_stats.csv'))\n",
        "\n",
        "# Generate detailed stats by polygon (requires uncertainty stats)\n",
        "for polygon_area in df_stats_list[0]:\n",
        "  polygon_area_km2 = df_polygon_area_km2.loc[polygon_area][\"Area (km^2)\"]\n",
        "  df_detailed_diff_stats = df_base\n",
        "  df_detailed_diff_stats[\"Area (km^2)\"] = polygon_area_km2\n",
        "  for df_stats in df_stats_list:\n",
        "    if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"Forest AGBD mean (Mg / ha)\"\n",
        "    if df_stats.equals(df_agb_total_tg): stat_col = \"Forest AGB total (Tg)\"\n",
        "    if generate_uncertainty_stats:\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_ci95): stat_col = \"Forest AGBD CI95 (Mg / ha)\"\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_uncertainty): stat_col = \"Forest AGBD uncertainty (%)\"\n",
        "      if df_stats.equals(df_agb_total_tg_ci95): stat_col = \"Forest AGB total CI95 (Tg)\"\n",
        "    for stats_polygon_area in df_stats:\n",
        "      if stats_polygon_area == polygon_area:\n",
        "        df_stats_renamed = df_stats.rename(columns={stats_polygon_area:stat_col})\n",
        "        df_detailed_diff_stats = pd.concat([df_detailed_diff_stats, df_stats_renamed[stat_col]], axis=1)\n",
        "    df_detailed_diff_stats.to_csv(join(detailed_diff_stats_dir, f'{polygon_area}.csv'))\n",
        "\n",
        "# Generate detailed stats by diff (requires uncertainty stats)\n",
        "diffs = {}\n",
        "# Loop through all CSV files in the 'detailed_diff_stats_dir'\n",
        "for stats_csv in os.listdir(detailed_diff_stats_dir):\n",
        "    polygon_name = f\"{stats_csv[:-4]}\"\n",
        "    stats_csv_path = join(detailed_diff_stats_dir, stats_csv)\n",
        "    stats_csv_df = pd.read_csv(stats_csv_path)\n",
        "    # Loop through each unique diff in the file\n",
        "    for diff in stats_csv_df['diff'].unique():\n",
        "        # Filter the dataframe for the current diff\n",
        "        diff_df = stats_csv_df[stats_csv_df['diff'] == diff].copy()\n",
        "        # Drop the 'diff' column and add the 'Name' column\n",
        "        diff_df.drop('diff', axis=1, inplace=True)\n",
        "        diff_df.insert(0, 'Name', polygon_name)\n",
        "        # If this diff's dataframe already exists, append to it; otherwise, create it\n",
        "        if diff in diffs: diffs[diff] = pd.concat([diffs[diff], diff_df], ignore_index=True)\n",
        "        else: diffs[diff] = diff_df\n",
        "for diff, diff_df in diffs.items():\n",
        "    output_file_path = join(detailed_diff_stats_diff_dir,f'{diff}.csv')\n",
        "    diff_df.to_csv(output_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sankey plots"
      ],
      "metadata": {
        "id": "b70oSO_VNtkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sankey_dir = join(sample_polygons_statistics_dir, 'sankey_diagrams')\n",
        "makedirs(sankey_dir, exist_ok=True)\n",
        "\n",
        "# Load the CSV files\n",
        "summary_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_stats.csv'))\n",
        "summary_diff_stats = pd.read_csv(join(sample_polygons_statistics_dir,'summary_diff_stats.csv'))\n",
        "\n",
        "# Check that all rows in both .csv files have the same strings (polygon areas) in column A\n",
        "polygon_areas_stats = summary_stats.iloc[:, 0]\n",
        "polygon_areas_diff_stats = summary_diff_stats.iloc[:, 0]\n",
        "\n",
        "assert all(polygon_areas_stats == polygon_areas_diff_stats), \"Polygon areas do not match between the two CSV files.\"\n",
        "\n",
        "# Print columns of summary_stats\n",
        "print(\"Columns in summary_stats:\")\n",
        "for i, col in enumerate(summary_stats.columns[1:]):\n",
        "    print(col, end=' ')\n",
        "    if (i+1) % 5 == 0:\n",
        "        print()\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print columns of summary_diff_stats\n",
        "print(\"Columns in summary_diff_stats:\")\n",
        "for i, col in enumerate(summary_diff_stats.columns[1:]):\n",
        "    print(col, end=' ')\n",
        "    if (i+1) % 5 == 0:\n",
        "        print()"
      ],
      "metadata": {
        "id": "jH2yiLQtmXIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot degradation and deforestation separately\n",
        "separate_disturbance = True\n",
        "\n",
        "# Plot degradation before and since a date separately\n",
        "separate_degradation = True\n",
        "\n",
        "# Scaling modifier for sizing non-text elements (ratio, e.g. 0.5 or 2)\n",
        "scaling_modifier = 1\n",
        "\n",
        "# Relative width modifier (ratio, e.g. 0.5 or 2)\n",
        "width_modifier = 0.7\n",
        "\n",
        "# Title (polygon area), density and label variables (weight of 800 ~ bold, 400 ~ normal)\n",
        "show_title = True\n",
        "show_labels = True\n",
        "title_font_size = 20\n",
        "title_font_weight = 600\n",
        "density_font_size = 17\n",
        "density_font_weight = 600\n",
        "label_font_size = 17\n",
        "label_font_weight = 600\n",
        "\n",
        "# Base columns (summary_stats)\n",
        "old_growth_agb_column = 'all_oldgrowth forest AGB (Tg)'\n",
        "current_agb_column = '2022 forest AGB (Tg)'\n",
        "\n",
        "current_year = current_agb_column.split(' ')[0] # Usually first word of current_agb_column\n",
        "\n",
        "# Difference columns (summary_diff_stats)\n",
        "degradation_before_column = '2022_degradation_before_1990 forest AGB (Tg)'\n",
        "degradation_since_column = '2022_degradation_since_1990 forest AGB (Tg)'\n",
        "degradation_total_column = '2022_degradation_total forest AGB (Tg)'\n",
        "deforestation_total_column = '2022_deforestation_total forest AGB (Tg)'\n",
        "disturbance_total_column = '2022_degradation_deforestation_total forest AGB (Tg)'\n",
        "\n",
        "# Node labels and colours\n",
        "remaining_name = f'Remaining in {current_year}:'\n",
        "remaining_colour = '#1a801a'  # Dark green\n",
        "degradation_before_name = 'Degradation before 1990'\n",
        "degradation_before_colour = '#8dc00d'  # Light green\n",
        "degradation_since_name = 'Degradation since 1990'\n",
        "degradation_since_colour = '#ffff00'  # Yellow\n",
        "degradation_total_name = 'Degradation'\n",
        "degradation_total_colour = '#ffff00'  # Yellow\n",
        "deforestation_total_name = 'Deforestation'\n",
        "deforestation_total_colour = '#ffffff'  # White\n",
        "disturbance_total_name = 'Disturbance'\n",
        "disturbance_total_colour = '#ffff00'  # Yellow\n",
        "\n",
        "left_axis_label = True\n",
        "\n",
        "# Assert checking separate_disturbance is True if separate_degradation is True\n",
        "assert not separate_degradation or separate_disturbance, \"separate_disturbance must be True if separate_degradation is True.\"\n",
        "\n",
        "# Function to get values from statistics\n",
        "def get_value(df, idx, column_name):\n",
        "    try:\n",
        "        value = df.loc[idx, column_name]\n",
        "        if pd.isnull(value): return 0.0\n",
        "        else: return float(value)\n",
        "    except KeyError:\n",
        "        print(f\"Column '{column_name}' not found in the dataframe.\")\n",
        "        return 0.0\n",
        "\n",
        "# Loop through each row (polygon area)\n",
        "for idx in summary_stats.index:\n",
        "\n",
        "    # Get the polygon name\n",
        "    polygon_name = summary_stats.iloc[idx, 0]\n",
        "\n",
        "    # Get old-growth and current AGB values\n",
        "    old_growth_agb = get_value(summary_stats, idx, old_growth_agb_column)\n",
        "    current_agb = get_value(summary_stats, idx, current_agb_column)\n",
        "\n",
        "    # Get values from summary_diff_stats\n",
        "    degradation_before = get_value(summary_diff_stats, idx, degradation_before_column)\n",
        "    degradation_since = get_value(summary_diff_stats, idx, degradation_since_column)\n",
        "    degradation_total = get_value(summary_diff_stats, idx, degradation_total_column)\n",
        "    deforestation_total = get_value(summary_diff_stats, idx, deforestation_total_column)\n",
        "    disturbance_total = get_value(summary_diff_stats, idx, disturbance_total_column)\n",
        "\n",
        "    # Load detailed stats to get mean AGBD and CI95 values\n",
        "    detailed_stats_df = pd.read_csv(join(detailed_stats_dir, f\"{polygon_name}.csv\"))\n",
        "    old_growth_index = detailed_stats_df.index[detailed_stats_df['scenario'] == f\"{old_growth_agb_column.split(' ')[0]}\"].item()\n",
        "    old_growth_mean_agbd = get_value(detailed_stats_df, old_growth_index, \"Forest AGBD mean (Mg / ha)\")\n",
        "    current_index = detailed_stats_df.index[detailed_stats_df['scenario'] == f\"{current_agb_column.split(' ')[0]}\"].item()\n",
        "    current_mean_agbd = get_value(detailed_stats_df, current_index, \"Forest AGBD mean (Mg / ha)\")\n",
        "    uncertainty = False # Uncertainty may not have been calculated\n",
        "    if 'Forest AGB total CI95 (Tg)' in detailed_stats_df.columns:\n",
        "      uncertainty = True\n",
        "      old_growth_agb_ci95 = get_value(detailed_stats_df, old_growth_index, \"Forest AGB total CI95 (Tg)\")\n",
        "      old_growth_mean_agbd_ci95 = get_value(detailed_stats_df, old_growth_index, \"Forest AGBD CI95 (Mg / ha)\")\n",
        "      current_agb_ci95 = get_value(detailed_stats_df, current_index, \"Forest AGB total CI95 (Tg)\")\n",
        "      current_mean_agbd_ci95 = get_value(detailed_stats_df, current_index, \"Forest AGBD CI95 (Mg / ha)\")\n",
        "\n",
        "    # Title line 1 name\n",
        "    title_name = f\"{polygon_name}\"\n",
        "\n",
        "    # Subtitle line 1 name\n",
        "    if uncertainty: subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} ± {old_growth_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "    else: subtitle_1_name = f\"Predicted old-growth AGBD: {old_growth_mean_agbd:.0f} Mg / ha\"\n",
        "\n",
        "    # Subtitle line 2 name\n",
        "    if uncertainty: subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} ± {current_mean_agbd_ci95:.1f} Mg / ha\"\n",
        "    else: subtitle_2_name = f\"{current_year} AGBD: {current_mean_agbd:.0f} Mg / ha\"\n",
        "\n",
        "    # Left axis name\n",
        "    if left_axis_label:\n",
        "      if uncertainty: left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} ± {old_growth_agb_ci95:.2f} Tg\"\n",
        "      else: left_axis = f\"Predicted<br>old-growth AGB:<br>{old_growth_agb:.1f} Tg\"\n",
        "    else: left_axis = ''\n",
        "\n",
        "    # Update remaining_name with AGB\n",
        "    if uncertainty: remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} ± {current_agb_ci95:.2f} Tg\"\n",
        "    else: remaining_name_agb = f\"{remaining_name}<br>{current_agb:.1f} Tg\"\n",
        "\n",
        "    # Depending on the settings, perform assertions and plot\n",
        "    if separate_disturbance and separate_degradation:\n",
        "        assert abs(degradation_before + degradation_since - degradation_total) < 1e-9, f\"{polygon_name}: degradation_before_column + degradation_since_column != degradation_total_column\"\n",
        "        assert abs(degradation_total + deforestation_total - disturbance_total) < 1e-9, f\"{polygon_name}: degradation_total_column + deforestation_total_column != disturbance_total_column\"\n",
        "        assert abs(current_agb - disturbance_total - old_growth_agb) < 1e-9, f\"{polygon_name}: current_agb_column - disturbance_total_column != old_growth_agb_column\"\n",
        "        # Define nodes\n",
        "        nodes = [left_axis, degradation_before_name, degradation_since_name, deforestation_total_name, remaining_name_agb]\n",
        "        # Calculate 'Remaining' value (should be equal to current_agb)\n",
        "        remaining_value = current_agb\n",
        "        # Define links\n",
        "        sources = [0, 0, 0, 0]\n",
        "        targets = [1, 2, 3, 4]\n",
        "        values = [-degradation_before, -degradation_since, -deforestation_total, remaining_value]\n",
        "        colors = [degradation_before_colour, degradation_since_colour, deforestation_total_colour, remaining_colour]\n",
        "        # Define node colors\n",
        "        node_colors = [remaining_colour] + colors\n",
        "\n",
        "    elif separate_disturbance and not separate_degradation:\n",
        "        assert abs(degradation_total + deforestation_total - disturbance_total) < 1e-9, f\"{polygon_name}: degradation_total_column + deforestation_total_column != disturbance_total_column\"\n",
        "        assert abs(current_agb - disturbance_total - old_growth_agb) < 1e-9, f\"{polygon_name}: current_agb_column - disturbance_total_column != old_growth_agb_column\"\n",
        "        # Define nodes\n",
        "        nodes = [left_axis, degradation_total_name, deforestation_total_name, remaining_name_agb]\n",
        "        # Calculate 'Remaining' value (should be equal to current_agb)\n",
        "        remaining_value = current_agb\n",
        "        # Define links\n",
        "        sources = [0, 0, 0]\n",
        "        targets = [1, 2, 3]\n",
        "        values = [-degradation_total, -deforestation_total, remaining_value]\n",
        "        colors = [degradation_total_colour, deforestation_total_colour, remaining_colour]\n",
        "        # Define node colors\n",
        "        node_colors = [remaining_colour] + colors\n",
        "\n",
        "    else:\n",
        "        # Both separate_disturbance and separate_degradation are False\n",
        "        assert abs(current_agb - disturbance_total - old_growth_agb) < 1e-9, f\"{polygon_name}: current_agb_column - disturbance_total_column != old_growth_agb_column\"\n",
        "        # Define nodes\n",
        "        nodes = [left_axis, disturbance_total_name, remaining_name_agb]\n",
        "        # Calculate 'Remaining' value (should be equal to current_agb)\n",
        "        remaining_value = current_agb\n",
        "        # Define links\n",
        "        sources = [0, 0]\n",
        "        targets = [1, 2]\n",
        "        values = [-disturbance_total, remaining_value]\n",
        "        colors = [disturbance_total_colour, remaining_colour]\n",
        "        # Define node colors\n",
        "        node_colors = [remaining_colour] + colors\n",
        "\n",
        "    # Calculate percentages and update right node labels\n",
        "    percentages = [(abs(val) / old_growth_agb * 100) for val in values]\n",
        "    for i in range(1, len(nodes)):\n",
        "        if i - 1 < len(percentages):\n",
        "            nodes[i] += f\" ({percentages[i-1]:.0f}%)\"\n",
        "\n",
        "    # If labels are toggled off, replace node labels with empty strings\n",
        "    if not show_labels: nodes = [''] * len(nodes)\n",
        "\n",
        "    # Create the Sankey diagram\n",
        "    fig = go.Figure(data=[go.Sankey(\n",
        "        arrangement=\"freeform\",\n",
        "        node=dict(\n",
        "            label=nodes,\n",
        "            color=node_colors,  # Set node colors\n",
        "            pad=15 * scaling_modifier,\n",
        "            thickness=20 * scaling_modifier,\n",
        "            line=dict(color=\"black\", width=1 * scaling_modifier)\n",
        "        ),\n",
        "        link=dict(\n",
        "            source=sources,\n",
        "            target=targets,\n",
        "            value=values,\n",
        "            color=colors,\n",
        "            line=dict(color=\"black\", width=1 * scaling_modifier),  # Add border to ribbons\n",
        "        )\n",
        "    )])\n",
        "\n",
        "    fig.update_layout(\n",
        "        width=700 * scaling_modifier * width_modifier,\n",
        "        height=500 * scaling_modifier,\n",
        "        font=dict(\n",
        "            family=\"helvetica, serif\",\n",
        "            size=label_font_size,\n",
        "            color=\"black\",\n",
        "            weight=label_font_weight\n",
        "        ),\n",
        "        # Adjust the margins\n",
        "        margin=dict(\n",
        "            l=25 * scaling_modifier,\n",
        "            r=25 * scaling_modifier,\n",
        "            t=115 * scaling_modifier,  # Increased top margin to accommodate title\n",
        "            b=25 * scaling_modifier\n",
        "        ),\n",
        "        # Annotations allow more customisable title with three lines\n",
        "        annotations=[\n",
        "            dict(\n",
        "                x=0,\n",
        "                y=1.28,\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                text=title_name,\n",
        "                showarrow=False,\n",
        "                xanchor='left',\n",
        "                align='left',\n",
        "                font=dict(\n",
        "                    family=\"helvetica, serif\",\n",
        "                    size=title_font_size,\n",
        "                    color=\"black\",\n",
        "                    weight=title_font_weight\n",
        "                )\n",
        "            ),\n",
        "            dict(\n",
        "                x=0,\n",
        "                y=1.19,\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                text=subtitle_1_name,\n",
        "                showarrow=False,\n",
        "                xanchor='left',\n",
        "                align='left',\n",
        "                font=dict(\n",
        "                    family=\"helvetica, serif\",\n",
        "                    size=density_font_size,\n",
        "                    color=\"black\",\n",
        "                    weight=density_font_weight\n",
        "                )\n",
        "            ),\n",
        "            dict(\n",
        "                x=0,\n",
        "                y=1.11,\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                text=subtitle_2_name,\n",
        "                showarrow=False,\n",
        "                xanchor='left',\n",
        "                align='left',\n",
        "                font=dict(\n",
        "                    family=\"helvetica, serif\",\n",
        "                    size=density_font_size,\n",
        "                    color=\"black\",\n",
        "                    weight=density_font_weight\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Save the figure\n",
        "    output_path = os.path.join(sankey_dir, f'sankey_diagram_{polygon_name}.png')\n",
        "    fig.write_image(output_path, scale=2)\n",
        "    print(f\"Statistical assertions and sankey diagram complete for {polygon_name}.\")\n",
        "\n",
        "    # Show the figure\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "FpW7W57iZEUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXAGIvpk_KWS"
      },
      "source": [
        "# Disconnected runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3qMsENC_MP2"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "6j0th2jhtnAR",
        "zjy-T1TqScbE",
        "hlQ2iLAiwQoB",
        "UGzdNKRbo0H5"
      ],
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}