{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/8_statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j0th2jhtnAR"
      },
      "source": [
        "# Imports & Subdirectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9drq4euvG0MS"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "\n",
        "# Mount Google Drive and set base directory\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGWgfUqto0F"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Imports and upgrades\n",
        "!pip install geopandas\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MTERlyxuPd4"
      },
      "outputs": [],
      "source": [
        "# Reload imports, replacing those in the cache\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# Imports\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "import math\n",
        "import numpy as np\n",
        "from os.path import exists, join\n",
        "from os import makedirs\n",
        "from osgeo import gdal\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio import mask as msk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwar9sxbts-3"
      },
      "outputs": [],
      "source": [
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "statistics_dir = join(base_dir, \"8_statistics\")\n",
        "sample_polygons_dir = join(statistics_dir, \"sample_polygons\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(statistics_dir, exist_ok=True)\n",
        "makedirs(sample_polygons_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)"
      ],
      "metadata": {
        "id": "atvGRSPYgwk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjy-T1TqScbE"
      },
      "source": [
        "# Select model, area and sample polygons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select if to source predictions from scenarios_dir or uncertainty_dir\n",
        "source_dir = uncertainty_dir\n",
        "\n",
        "# Select the model\n",
        "for subdir in os.listdir(source_dir):\n",
        "  print(f\"selected_model = '{subdir}'\")"
      ],
      "metadata": {
        "id": "nj5RbVSuEp-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_model = 'agbd_240819_091905'\n",
        "\n",
        "selected_model_dir = join(source_dir, selected_model)\n",
        "# Select the prediction area\n",
        "for subdir in os.listdir(selected_model_dir):\n",
        "  if source_dir == scenarios_dir and not subdir.endswith('.csv') and not subdir.endswith('.json'):\n",
        "    print(f\"prediction_area = '{subdir}'\")\n",
        "  if source_dir == uncertainty_dir and subdir != 'model_iterations':\n",
        "    print(f\"prediction_area = '{subdir[10:]}'\")"
      ],
      "metadata": {
        "id": "x7UWN6aIL-dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_area = 'taman'\n",
        "\n",
        "# Model-area stats directory\n",
        "model_area_statistics_dir = join(statistics_dir, f\"{selected_model}_{prediction_area}\")\n",
        "makedirs(model_area_statistics_dir, exist_ok=True)\n",
        "\n",
        "# Calculate precise pixel size for the centre of the area\n",
        "# See https://gis.stackexchange.com/questions/142326/calculating-longitude-length-in-miles\n",
        "# See https://thoughtco.com/degree-of-latitude-and-longitude-distance-4070616\n",
        "\n",
        "# Use a predictor of the prediction area as a template\n",
        "predictors_dir = join(scenarios_dir, selected_model, prediction_area, \"predictors\")\n",
        "prediction_area_template_path = join(predictors_dir, os.listdir(predictors_dir)[0])\n",
        "prediction_area_template = gdal.Open(prediction_area_template_path)\n",
        "pixel_height_deg = 0 - prediction_area_template.GetGeoTransform()[5]\n",
        "pixel_width_deg = prediction_area_template.GetGeoTransform()[1]\n",
        "\n",
        "# Create a raster without 'nodata' values.\n",
        "template_no_nodata_dir = join(model_area_statistics_dir, 'template_no_nodata.tif')\n",
        "if not exists(template_no_nodata_dir):\n",
        "  prediction_area_template_array = prediction_area_template.ReadAsArray()\n",
        "  template_nodatavalue = prediction_area_template.GetRasterBand(1).GetNoDataValue()\n",
        "  absent_nodatavalue = template_nodatavalue + 1 # Modify nodata value to one that's absent.\n",
        "  assert absent_nodatavalue not in prediction_area_template_array, \"New nodata value is present in the template. Change to a value that is absent.\"\n",
        "  print(\"A template without nodata values will be created.\")\n",
        "  export_array_as_tif(prediction_area_template_array, template_no_nodata_dir, template = prediction_area_template_path, nodatavalue=absent_nodatavalue, compress=False)\n",
        "else: print(\"Template without nodata values already exists.\")\n",
        "print(\"This is used for counting all pixels inside a polygon mask (which uses the 'nodata' values).\\n\")\n",
        "\n",
        "# Proxy distance of degrees latitude in m (actual is non-linear)\n",
        "lat_dist_equator_km = 110.567\n",
        "lat_dist_poles_km = 111.699\n",
        "lat_dist_diff = lat_dist_poles_km - lat_dist_equator_km\n",
        "lat_dist_change_deg_km = lat_dist_diff / 90\n",
        "long_dist_equator_km = 111.321 # Equation calculates at different latitudes\n",
        "\n",
        "# Approximate pixel size\n",
        "approx_resolution = (np.average([pixel_height_deg, pixel_width_deg]) * np.average([lat_dist_equator_km, lat_dist_poles_km]) * 1000)\n",
        "approx_pixel_size_ha = approx_resolution**2 / 10000\n",
        "\n",
        "print(f\"Without precise correction, the approximate resolution is {approx_resolution} m, while the approximate pixel area is {approx_pixel_size_ha} ha.\\n\")\n",
        "print(f\"The pixel size with be further corrected based on the position of each sample polygon.\")"
      ],
      "metadata": {
        "id": "yJpsBoAKMc-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select sample area polygons. This should be a single .gpkg with the field 'name' differentiating polygons.\n",
        "sample_polygons = []\n",
        "for geopackage in os.listdir(sample_polygons_dir):\n",
        "  sample_polygons.append(geopackage)\n",
        "\n",
        "if len(sample_polygons) == 0:\n",
        "  print(f\"No sample areas found. Upload .gpkg polygons to {sample_polygons_dir}\")\n",
        "else:\n",
        "  for sample_polygon in sample_polygons: print(f\"selected_sample_polygons = '{sample_polygon}'\")"
      ],
      "metadata": {
        "id": "txTXPYFLpH6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_sample_polygons = 'taman_sample_polygons.gpkg'\n",
        "\n",
        "selected_sample_polygons_dir = join(sample_polygons_dir, selected_sample_polygons)\n",
        "selected_sample_polygons_gpkg = gpd.read_file(selected_sample_polygons_dir)\n",
        "sample_polygons_statistics_dir = join(model_area_statistics_dir, selected_sample_polygons[:-5])\n",
        "makedirs(sample_polygons_statistics_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "cwWvaX5qDANc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario statistics"
      ],
      "metadata": {
        "id": "hlQ2iLAiwQoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define directories\n",
        "detailed_stats_dir = join(sample_polygons_statistics_dir, 'detailed_stats')\n",
        "makedirs(detailed_stats_dir, exist_ok=True)\n",
        "detailed_stats_scenario_dir = join(sample_polygons_statistics_dir, 'detailed_stats_scenario')\n",
        "makedirs(detailed_stats_scenario_dir, exist_ok=True)\n",
        "forecast_input_dir = join(sample_polygons_statistics_dir, 'forecast_input')\n",
        "makedirs(forecast_input_dir, exist_ok=True)\n",
        "\n",
        "# Create list of available prediction rasters and scenarios. Rasters must already be masked (e.g. to forest).\n",
        "if source_dir == scenarios_dir: prediction_raster_dir = join(selected_model_dir, prediction_area, 'scenario_predictions')\n",
        "if source_dir == uncertainty_dir: prediction_raster_dir = join(selected_model_dir, f'scenarios_{prediction_area}', 'statistics_masked')\n",
        "prediction_raster_dirs = []\n",
        "scenarios = set()\n",
        "for prediction_raster in os.listdir(prediction_raster_dir):\n",
        "  prediction_raster_dirs.append(join(prediction_raster_dir, prediction_raster))\n",
        "  scenarios.add(prediction_raster.split(\"__\")[0])\n",
        "scenarios = sorted(list(scenarios))\n",
        "\n",
        "# Select scenario predictions to calculate statistics\n",
        "print('selected_scenarios = [')\n",
        "for scenario in scenarios:\n",
        "  print(f'  \"{scenario}\",')\n",
        "print(']\\n')"
      ],
      "metadata": {
        "id": "BgC9laZXPB9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_scenarios = [\n",
        "  \"1990_oldgrowth\",\n",
        "  \"1990_oldgrowth_all_land\",\n",
        "  \"2023\",\n",
        "  \"2023_nodeg_nodef_since_1990\",\n",
        "  \"2023_nodeg_since_1990\",\n",
        "  \"2023_oldgrowth\",\n",
        "]\n",
        "\n",
        "\n",
        "# Filter to selected scenarios, and separate prediction and uncertainty rasters (if latter present)\n",
        "prediction_rasters = []\n",
        "uncertainty_rasters = []\n",
        "for prediction_raster in prediction_raster_dirs:\n",
        "  for scenario in selected_scenarios:\n",
        "    if source_dir == uncertainty_dir:\n",
        "      if f\"{scenario}__\" in prediction_raster and 'mean.tif' in prediction_raster:\n",
        "        prediction_rasters.append(prediction_raster)\n",
        "      if f\"{scenario}__\" in prediction_raster and 'uncertainty.tif' in prediction_raster:\n",
        "        uncertainty_rasters.append(prediction_raster)\n",
        "    else: # If the source directory is scenarios_dir (without uncertainty values)\n",
        "      if f\"{scenario}__\" in prediction_raster: prediction_rasters.append(prediction_raster)\n",
        "# Toggle whether to predict uncertainty stats\n",
        "if len(uncertainty_rasters) > 0: generate_uncertainty_stats = True\n",
        "else: generate_uncertainty_stats = False\n",
        "\n",
        "# Sort rasters chronologically (assuming year is first in the filename)\n",
        "prediction_rasters = sorted(prediction_rasters)\n",
        "uncertainty_rasters = sorted(uncertainty_rasters)\n",
        "\n",
        "# Generate empty dataframes for statistics\n",
        "df_base = pd.DataFrame(index = selected_scenarios)\n",
        "df_base.rename_axis('scenario', inplace=True)\n",
        "df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg = df_base.copy(), df_base.copy(), df_base.copy(), df_base.copy()\n",
        "# If uncertainty rasters are present, generate empty dataframes\n",
        "if generate_uncertainty_stats:\n",
        "  df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agb_total_tg_ci95 = df_base.copy(), df_base.copy(), df_base.copy()\n",
        "\n",
        "# Initialise polygon area dataframe\n",
        "df_polygon_area_km2 = pd.DataFrame(columns = [\"Name\", \"Area (km^2)\"])\n",
        "\n",
        "# Loop through each polygon stored in GPKG to generate statistics\n",
        "for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "\n",
        "  # Define the polygon\n",
        "  sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"] # Set name to start at 3rd character with [2:] (skipping number used for ordering)\n",
        "  polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "  # Latitude of the centroid\n",
        "  polygon_centroid_lat = sample_polygon_geometry.centroid.y\n",
        "\n",
        "  # Calculate latitude distance per degree at the polygon's latitude\n",
        "  latitude_m_per_degree = 1000 * (lat_dist_equator_km + (lat_dist_change_deg_km * polygon_centroid_lat))\n",
        "\n",
        "  # Rest of your calculations that depend on the latitude_m_per_degree follows here\n",
        "  # For example, if you're calculating pixel size in meters for each polygon based on its latitude:\n",
        "  precise_pixel_height_m = latitude_m_per_degree * pixel_height_deg\n",
        "  precise_pixel_width_m = (math.cos((math.pi / 180) * polygon_centroid_lat) * latitude_m_per_degree * pixel_width_deg)\n",
        "  precise_pixel_size_ha = precise_pixel_height_m * precise_pixel_width_m / 10000\n",
        "\n",
        "  # Mask the 'no nodata' raster to the polygon with an absent value to count all pixels within the polygon\n",
        "  with rasterio.open(template_no_nodata_dir) as template_no_nodata:\n",
        "    no_nodata_template_array_masked, transform_1 = msk.mask(template_no_nodata, polygons, crop=True, filled=False)\n",
        "\n",
        "  # Calculate sample_polygon_geometry area (ellipsoidal as opposed to planimetric)\n",
        "  sample_polygons_crs = selected_sample_polygons_gpkg.crs\n",
        "  temp_gdf = gpd.GeoDataFrame({'name': [sample_polygon_name], 'geometry': sample_polygon_geometry}, crs=sample_polygons_crs)\n",
        "  temp_gdf_utm = temp_gdf.estimate_utm_crs()\n",
        "  polygon_area_ha = np.divide(temp_gdf.to_crs(temp_gdf_utm).area[0], 10000, dtype='float64')\n",
        "  # Ensure the new_row has the correct data types\n",
        "  new_row = pd.DataFrame([{'Name': sample_polygon_name, 'Area (km^2)': polygon_area_ha / 100}], dtype=object)\n",
        "  df_polygon_area_km2 = pd.concat([df_polygon_area_km2, new_row], ignore_index=True, sort=False)\n",
        "\n",
        "  # Create an empty list for each prediction raster statistic\n",
        "  values_forest_cover_ha, values_agbd_mean_mg_ha, values_agbd_stdev_mg_ha, values_agb_total_tg = [], [], [], []\n",
        "\n",
        "  # If uncertainty rasters are present, create an empty list for each uncertainty raster statistic\n",
        "  if generate_uncertainty_stats:\n",
        "    values_agbd_mean_mg_ha_ci95, values_agbd_mean_mg_ha_uncertainty, values_agb_total_tg_ci95  = [], [], []\n",
        "\n",
        "  # Loop through prediction rasters\n",
        "  for prediction_raster in prediction_rasters:\n",
        "\n",
        "    # Mask predictor to sample_polygon_geometry\n",
        "    with rasterio.open(prediction_raster) as prediction:\n",
        "      nodatavalue = int(prediction.nodatavals[0])\n",
        "      prediction_array_masked, transform_2 = msk.mask(prediction, polygons, crop=True, filled=False)\n",
        "\n",
        "    # Count all (incl. nodata) pixels within polygon, and estimate their total area\n",
        "    pixel_count_polygon = np.ma.count(no_nodata_template_array_masked)\n",
        "    pixels_area_polygon_ha = np.multiply(pixel_count_polygon, precise_pixel_size_ha, dtype='float64')\n",
        "\n",
        "    # Correct pixel size to UTM ellipsoidal measure of the polygons position (which will decrease further North)\n",
        "    polygon_to_pixel_area_ratio = np.divide(pixels_area_polygon_ha, polygon_area_ha, dtype='float64')\n",
        "    pixel_size_ha_corrected = np.multiply(precise_pixel_size_ha, polygon_to_pixel_area_ratio, dtype='float64')\n",
        "\n",
        "    # Count pixels within polygon, excluding those previously masked i.e. nodata\n",
        "    pixel_count_polygon_masked = np.ma.count(prediction_array_masked)\n",
        "    pixels_area_polygon_masked_ha = np.multiply(pixel_count_polygon_masked, pixel_size_ha_corrected, dtype='float64')\n",
        "\n",
        "    # Calculate forest area\n",
        "    forest_cover_ha = pixels_area_polygon_masked_ha # Already masked to forest in current workflow\n",
        "\n",
        "    # Calculate total, mean and stdev of aboveground biomass\n",
        "    agbd_mean_mg_ha = np.ma.mean(prediction_array_masked, dtype='float64') # Float64 minimises error for large number of values\n",
        "    agbd_mean_stdev_ha = np.ma.std(prediction_array_masked, dtype='float64')\n",
        "    agb_total_mg = np.multiply(agbd_mean_mg_ha, forest_cover_ha, dtype='float64')\n",
        "    agb_total_tg = np.divide(agb_total_mg, 1000000, dtype='float64') # Convert Mg (megagram = ton) to Tg (teragram = megaton)\n",
        "\n",
        "    # Append results to statistics list\n",
        "    values_forest_cover_ha.append(forest_cover_ha)\n",
        "    values_agbd_mean_mg_ha.append(agbd_mean_mg_ha)\n",
        "    values_agbd_stdev_mg_ha.append(agbd_mean_stdev_ha)\n",
        "    values_agb_total_tg.append(agb_total_tg)\n",
        "\n",
        "    if generate_uncertainty_stats:\n",
        "      uncertainty_raster_present = False\n",
        "      for uncertainty_raster in uncertainty_rasters:\n",
        "        if prediction_raster.split(\"__mean.tif\")[0] in uncertainty_raster:\n",
        "          uncertainty_raster_present = True\n",
        "          matching_uncertainty_raster = uncertainty_raster\n",
        "\n",
        "      if not uncertainty_raster_present: print(f\"There is no uncertainty raster for {prediction_raster.split('/')[-1]}\")\n",
        "\n",
        "      if uncertainty_raster_present:\n",
        "        # Open and mask uncertainty raster to polygon\n",
        "          with rasterio.open(matching_uncertainty_raster) as uncertainty:\n",
        "            nodatavalue = int(uncertainty.nodatavals[0])\n",
        "            uncertainty_array_masked, transform_2 = msk.mask(uncertainty, polygons, crop=True, filled=False)\n",
        "\n",
        "          # See https://stats.stackexchange.com/questions/223924/how-to-add-up-partial-confidence-intervals-to-create-a-total-confidence-interval#comment426260_223924\n",
        "\n",
        "          # Compress masked array data to 1D\n",
        "          prediction_1d = np.ma.compressed(prediction_array_masked)\n",
        "          uncertainty_1d = np.ma.compressed(uncertainty_array_masked)\n",
        "          # Convert uncertainty percentages to ratios\n",
        "          uncertainty_ratios = np.divide(uncertainty_1d, 100, dtype='float64')\n",
        "          # Multiply the prediction values (mean AGBD Mg/ha) by uncertainty ratios for CI95 values\n",
        "          prediction_ci95s = np.multiply(prediction_1d, uncertainty_ratios, dtype='float64')\n",
        "\n",
        "          # Method 1 - Simple calculation of mean CI95 (higher estimate). Assumption: pixel values are completely correlated (all measure the same thing).\n",
        "          agbd_mean_mg_ha_ci95_1 = np.mean(prediction_ci95s, dtype = 'float64')\n",
        "          # Method 2 - Square prediction CI95s. Sum and then square root for total CI.\n",
        "          # Then divide by observations to calculate mean CI95. Assumption: pixel values are completely independent.\n",
        "          sum_squares = np.sum(np.square(prediction_ci95s, dtype='float64'), dtype='float64')\n",
        "          total_ci95 = np.sqrt(sum_squares, dtype='float64')\n",
        "          agbd_mean_mg_ha_ci95_2 = np.divide(total_ci95, np.ma.count(prediction_ci95s), dtype='float64')\n",
        "          # Method 3 - Used in Liang et al 2023 to calculate change uncertainty. Identical results to method 2.\n",
        "          predictions_x_uncertainties = np.multiply(prediction_1d, uncertainty_1d, dtype='float64')\n",
        "          sum_squares_pxu = np.sum(np.square(predictions_x_uncertainties, dtype='float64'), dtype='float64')\n",
        "          sqrt_divided_sum = np.sqrt(sum_squares_pxu, dtype='float64') / np.sum(prediction_1d, dtype='float64')\n",
        "          agbd_mean_mg_ha_ci95_3 = np.multiply(np.divide(sqrt_divided_sum, 100, dtype='float64'), agbd_mean_mg_ha, dtype='float64')\n",
        "\n",
        "          agbd_mean_mg_ha_ci95 = agbd_mean_mg_ha_ci95_1\n",
        "\n",
        "          # Calculate total AGB CI95\n",
        "          agb_total_mg_ci95 = np.multiply(agbd_mean_mg_ha_ci95, forest_cover_ha, dtype='float64')\n",
        "          agb_total_tg_ci95 = np.divide(agb_total_mg_ci95, 1000000, dtype='float64') # Convert total CI to Tg\n",
        "          # Calculate percentage uncertainty\n",
        "          agbd_mean_mg_ha_uncertainty = np.multiply(np.divide(agbd_mean_mg_ha_ci95, agbd_mean_mg_ha, dtype='float64'), 100, dtype='float64')\n",
        "          # Append results to statistics list\n",
        "          values_agbd_mean_mg_ha_ci95.append(agbd_mean_mg_ha_ci95)\n",
        "          values_agbd_mean_mg_ha_uncertainty.append(agbd_mean_mg_ha_uncertainty)\n",
        "          values_agb_total_tg_ci95.append(agb_total_tg_ci95)\n",
        "\n",
        "  # Concatenate new columns to the main DataFrames for each statistic\n",
        "  df_forest_cover_ha = pd.concat([df_forest_cover_ha, pd.DataFrame({sample_polygon_name: values_forest_cover_ha}, index=df_forest_cover_ha.index)], axis=1)\n",
        "  df_agbd_mean_mg_ha = pd.concat([df_agbd_mean_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha}, index=df_agbd_mean_mg_ha.index)], axis=1)\n",
        "  df_agbd_stdev_mg_ha = pd.concat([df_agbd_stdev_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_stdev_mg_ha}, index=df_agbd_stdev_mg_ha.index)], axis=1)\n",
        "  df_agb_total_tg = pd.concat([df_agb_total_tg, pd.DataFrame({sample_polygon_name: values_agb_total_tg}, index=df_agb_total_tg.index)], axis=1)\n",
        "\n",
        "  if generate_uncertainty_stats:\n",
        "      df_agbd_mean_mg_ha_ci95 = pd.concat([df_agbd_mean_mg_ha_ci95, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_ci95}, index=df_agbd_mean_mg_ha_ci95.index)], axis=1)\n",
        "      df_agbd_mean_mg_ha_uncertainty = pd.concat([df_agbd_mean_mg_ha_uncertainty, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_uncertainty}, index=df_agbd_mean_mg_ha_uncertainty.index)], axis=1)\n",
        "      df_agb_total_tg_ci95 = pd.concat([df_agb_total_tg_ci95, pd.DataFrame({sample_polygon_name: values_agb_total_tg_ci95}, index=df_agb_total_tg_ci95.index)], axis=1)\n",
        "\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "  df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_mean_mg_ha_ci95,\n",
        "                 df_agbd_mean_mg_ha_uncertainty, df_agbd_stdev_mg_ha, df_agb_total_tg, df_agb_total_tg_ci95]\n",
        "else: df_stats_list = [df_forest_cover_ha, df_agbd_mean_mg_ha, df_agbd_stdev_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Set index of the polygon area km2 dataframe to 'Name' of the polygon\n",
        "df_polygon_area_km2 = df_polygon_area_km2.set_index('Name')\n",
        "\n",
        "# Export statistics for forecast input\n",
        "df_forecast_list = [df_forest_cover_ha, df_agb_total_tg]\n",
        "for df_forecast in df_forecast_list:\n",
        "  df_noalts = df_forecast[df_forecast.index.str.contains(\"_\") == False]\n",
        "  if df_forecast.equals(df_forest_cover_ha): df_filename = \"forest_cover_ha\"\n",
        "  # if df_stats.equals(df_agbd_mean_mg_ha): df_filename = \"agbd_mean_mg_ha\"\n",
        "  # if df_stats.equals(df_agbd_stdev_mg_ha): df_filename = \"agbd_stdev_mg_ha\"\n",
        "  if df_forecast.equals(df_agb_total_tg): df_filename = \"agb_total_tg\"\n",
        "  df_noalts.to_csv(join(forecast_input_dir, f'{df_filename}.csv'))\n",
        "\n",
        "# Generate summary stats\n",
        "df_forest_cover_ha_t = df_forest_cover_ha.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest cover (ha)\")\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB (Tg)\")\n",
        "if generate_uncertainty_stats:\n",
        "  df_agb_total_tg_ci95_t = df_agb_total_tg_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB CI95 (Tg)\")\n",
        "  summary_stats = pd.concat([df_polygon_area_km2, df_forest_cover_ha_t, df_agb_total_tg_t, df_agb_total_tg_ci95_t], axis=1).rename_axis(\"Name\", axis=1)\n",
        "else: summary_stats = pd.concat([df_polygon_area_km2, df_forest_cover_ha_t, df_agb_total_tg_t], axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_stats.to_csv(join(sample_polygons_statistics_dir, 'summary_stats.csv'))\n",
        "\n",
        "# Generate detailed stats by polygon (requires uncertainty stats)\n",
        "for polygon_area in df_stats_list[0]:\n",
        "  polygon_area_km2 = df_polygon_area_km2.loc[polygon_area][\"Area (km^2)\"]\n",
        "  df_detailed_stats = df_base\n",
        "  df_detailed_stats[\"Area (km^2)\"] = polygon_area_km2\n",
        "  for df_stats in df_stats_list:\n",
        "    if df_stats.equals(df_forest_cover_ha): stat_col = \"Forest cover (ha)\"\n",
        "    if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"Forest AGBD mean (Mg / ha)\"\n",
        "    if df_stats.equals(df_agbd_stdev_mg_ha): stat_col = \"Forest AGBD stdev (Mg / ha)\"\n",
        "    if df_stats.equals(df_agb_total_tg): stat_col = \"Forest AGB total (Tg)\"\n",
        "    if generate_uncertainty_stats:\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_ci95): stat_col = \"Forest AGBD CI95 (Mg / ha)\"\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_uncertainty): stat_col = \"Forest AGBD uncertainty (%)\"\n",
        "      if df_stats.equals(df_agb_total_tg_ci95): stat_col = \"Forest AGB total CI95 (Tg)\"\n",
        "    for stats_polygon_area in df_stats:\n",
        "      if stats_polygon_area == polygon_area:\n",
        "        df_stats_renamed = df_stats.rename(columns={stats_polygon_area:stat_col})\n",
        "        df_detailed_stats = pd.concat([df_detailed_stats, df_stats_renamed[stat_col]], axis=1)\n",
        "    df_detailed_stats.to_csv(join(detailed_stats_dir, f'{polygon_area}.csv'))\n",
        "\n",
        "# Generate detailed stats by scenario (requires uncertainty stats)\n",
        "scenarios = {}\n",
        "# Loop through all CSV files in the 'detailed_stats_dir'\n",
        "for stats_csv in os.listdir(detailed_stats_dir):\n",
        "    polygon_name = f\"{stats_csv[:-4]}\"\n",
        "    stats_csv_path = join(detailed_stats_dir, stats_csv)\n",
        "    stats_csv_df = pd.read_csv(stats_csv_path)\n",
        "    # Loop through each unique scenario in the file\n",
        "    for scenario in stats_csv_df['scenario'].unique():\n",
        "        # Filter the dataframe for the current scenario\n",
        "        scenario_df = stats_csv_df[stats_csv_df['scenario'] == scenario].copy()\n",
        "        # Drop the 'scenario' column and add the 'Name' column\n",
        "        scenario_df.drop('scenario', axis=1, inplace=True)\n",
        "        scenario_df.insert(0, 'Name', polygon_name)\n",
        "        # If this scenario's dataframe already exists, append to it; otherwise, create it\n",
        "        if scenario in scenarios: scenarios[scenario] = pd.concat([scenarios[scenario], scenario_df], ignore_index=True)\n",
        "        else: scenarios[scenario] = scenario_df\n",
        "for scenario, scenario_df in scenarios.items():\n",
        "    output_file_path = join(detailed_stats_scenario_dir,f'{scenario}.csv')\n",
        "    scenario_df.to_csv(output_file_path, index=False)"
      ],
      "metadata": {
        "id": "jizdpzwLiqH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Difference statistics"
      ],
      "metadata": {
        "id": "UGzdNKRbo0H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define directories\n",
        "detailed_diff_stats_dir = join(sample_polygons_statistics_dir, 'detailed_diff_stats')\n",
        "makedirs(detailed_diff_stats_dir, exist_ok=True)\n",
        "detailed_diff_stats_diff_dir = join(sample_polygons_statistics_dir, 'detailed_diff_stats_diff')\n",
        "makedirs(detailed_diff_stats_diff_dir, exist_ok=True)\n",
        "\n",
        "# Create list of available difference rasters and scenarios\n",
        "if source_dir == scenarios_dir: diff_raster_dir = join(selected_model_dir, prediction_area, 'scenario_difference')\n",
        "if source_dir == uncertainty_dir: diff_raster_dir = join(selected_model_dir, f'scenarios_{prediction_area}', 'scenario_difference')\n",
        "diff_raster_dirs = []\n",
        "diffs = set()\n",
        "for diff_raster in os.listdir(diff_raster_dir):\n",
        "  diff_raster_dirs.append(join(diff_raster_dir, diff_raster))\n",
        "  diffs.add(diff_raster.split(\"__\")[0])\n",
        "diffs = sorted(list(diffs))\n",
        "\n",
        "# Select difference rasters to calculate statistics\n",
        "print('selected_diffs = [')\n",
        "for diff in diffs:\n",
        "  print(f'  \"{diff}\",')\n",
        "print(']')"
      ],
      "metadata": {
        "id": "FTpXfmfAo7hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_diffs = [\n",
        "  \"1990_oldgrowth_-_1990_oldgrowth_all_land\",\n",
        "  \"2023_-_1990_oldgrowth_all_land\",\n",
        "  \"2023_-_2023_nodeg_nodef_since_1990\",\n",
        "  \"2023_-_2023_nodeg_since_1990\",\n",
        "  \"2023_-_2023_oldgrowth\",\n",
        "  \"2023_nodeg_nodef_since_1990_-_1990_oldgrowth_all_land\",\n",
        "  \"2023_nodeg_since_1990_-_2023_oldgrowth\",\n",
        "  \"2023_oldgrowth_-_1990_oldgrowth\",\n",
        "  \"2023_oldgrowth_-_1990_oldgrowth_all_land\",\n",
        "]\n",
        "\n",
        "# Filter to selected scenarios, and separate prediction and uncertainty rasters (if latter present)\n",
        "diff_rasters = []\n",
        "diff_uncertainty_rasters = []\n",
        "for diff_raster in diff_raster_dirs:\n",
        "  for diff in diffs:\n",
        "    if source_dir == uncertainty_dir:\n",
        "      if f\"{diff}__\" in diff_raster and 'mean.tif' in diff_raster:\n",
        "        diff_rasters.append(diff_raster)\n",
        "      if f\"{diff}__\" in diff_raster and 'uncertainty.tif' in diff_raster:\n",
        "        diff_uncertainty_rasters.append(diff_raster)\n",
        "    else: # If the source directory is scenarios_dir (without uncertainty values)\n",
        "      if f\"{diff}__\" in diff_raster: diff_rasters.append(diff_raster)\n",
        "# Toggle whether to predict uncertainty stats\n",
        "if len(diff_uncertainty_rasters) > 0: generate_uncertainty_stats = True\n",
        "else: generate_uncertainty_stats = False\n",
        "\n",
        "# Sort rasters chronologically (assuming year is first in the filename)\n",
        "diff_rasters = sorted(diff_rasters)\n",
        "diff_uncertainty_rasters = sorted(diff_uncertainty_rasters)\n",
        "\n",
        "# Generate empty dataframes for statistics\n",
        "df_base = pd.DataFrame(index = selected_diffs)\n",
        "df_base.rename_axis('diff', inplace=True)\n",
        "df_agbd_mean_mg_ha, df_agb_total_tg = df_base.copy(), df_base.copy()\n",
        "# If uncertainty rasters are present, generate empty dataframes\n",
        "if generate_uncertainty_stats:\n",
        "  df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agb_total_tg_ci95 = df_base.copy(), df_base.copy(), df_base.copy()\n",
        "\n",
        "# Initialise polygon area dataframe\n",
        "df_polygon_area_km2 = pd.DataFrame(columns = [\"Name\", \"Area (km^2)\"])\n",
        "\n",
        "# Loop through each polygon stored in GPKG to generate statistics\n",
        "for index, row in selected_sample_polygons_gpkg.iterrows():\n",
        "\n",
        "  # Define the polygon\n",
        "  sample_polygon_geometry, sample_polygon_name = row[\"geometry\"], row[\"name\"] # Set name to start at 3rd character with [2:] (skipping number used for ordering)\n",
        "  polygons = [polygon for polygon in sample_polygon_geometry.geoms]\n",
        "\n",
        "  # Latitude of the centroid\n",
        "  polygon_centroid_lat = sample_polygon_geometry.centroid.y\n",
        "\n",
        "  # Calculate latitude distance per degree at the polygon's latitude\n",
        "  latitude_m_per_degree = 1000 * (lat_dist_equator_km + (lat_dist_change_deg_km * polygon_centroid_lat))\n",
        "\n",
        "  # Rest of your calculations that depend on the latitude_m_per_degree follows here\n",
        "  # For example, if you're calculating pixel size in meters for each polygon based on its latitude:\n",
        "  precise_pixel_height_m = latitude_m_per_degree * pixel_height_deg\n",
        "  precise_pixel_width_m = (math.cos((math.pi / 180) * polygon_centroid_lat) * latitude_m_per_degree * pixel_width_deg)\n",
        "  precise_pixel_size_ha = precise_pixel_height_m * precise_pixel_width_m / 10000\n",
        "\n",
        "  # Mask the 'no nodata' raster to the polygon with an absent value to count all pixels within the polygon\n",
        "  with rasterio.open(template_no_nodata_dir) as template_no_nodata:\n",
        "    no_nodata_template_array_masked, transform_1 = msk.mask(template_no_nodata, polygons, crop=True, filled=False)\n",
        "\n",
        "  # Calculate sample_polygon_geometry area (ellipsoidal as opposed to planimetric)\n",
        "  sample_polygons_crs = selected_sample_polygons_gpkg.crs\n",
        "  temp_gdf = gpd.GeoDataFrame({'name': [sample_polygon_name], 'geometry': sample_polygon_geometry}, crs=sample_polygons_crs)\n",
        "  temp_gdf_utm = temp_gdf.estimate_utm_crs()\n",
        "  polygon_area_ha = np.divide(temp_gdf.to_crs(temp_gdf_utm).area[0], 10000, dtype='float64')\n",
        "  # Ensure the new_row has the correct data types\n",
        "  new_row = pd.DataFrame([{'Name': sample_polygon_name, 'Area (km^2)': polygon_area_ha / 100}], dtype=object)\n",
        "  df_polygon_area_km2 = pd.concat([df_polygon_area_km2, new_row], ignore_index=True, sort=False)\n",
        "\n",
        "  # Create an empty list for each prediction raster statistic\n",
        "  values_agbd_mean_mg_ha, values_agb_total_tg = [], []\n",
        "\n",
        "  # If uncertainty rasters are present, create an empty list for each uncertainty raster statistic\n",
        "  if generate_uncertainty_stats:\n",
        "    values_agbd_mean_mg_ha_ci95, values_agbd_mean_mg_ha_uncertainty, values_agb_total_tg_ci95  = [], [], []\n",
        "\n",
        "  # Loop through diff rasters\n",
        "  for diff_raster in diff_rasters:\n",
        "\n",
        "    # Mask predictor to sample_polygon_geometry\n",
        "    with rasterio.open(diff_raster) as diff:\n",
        "      nodatavalue = int(diff.nodatavals[0])\n",
        "      diff_array_masked, transform_2 = msk.mask(diff, polygons, crop=True, filled=False)\n",
        "\n",
        "    # Count all (incl. nodata) pixels within polygon, and estimate their total area\n",
        "    pixel_count_polygon = np.ma.count(no_nodata_template_array_masked)\n",
        "    pixels_area_polygon_ha = np.multiply(pixel_count_polygon, precise_pixel_size_ha, dtype='float64')\n",
        "\n",
        "    # Correct pixel size to UTM ellipsoidal measure of the polygons position (which will decrease further North)\n",
        "    polygon_to_pixel_area_ratio = np.divide(pixels_area_polygon_ha, polygon_area_ha, dtype='float64')\n",
        "    pixel_size_ha_corrected = np.multiply(precise_pixel_size_ha, polygon_to_pixel_area_ratio, dtype='float64')\n",
        "\n",
        "    # Count pixels within polygon, excluding those previously masked i.e. nodata\n",
        "    pixel_count_polygon_masked = np.ma.count(diff_array_masked)\n",
        "    pixels_area_polygon_masked_ha = np.multiply(pixel_count_polygon_masked, pixel_size_ha_corrected, dtype='float64')\n",
        "\n",
        "    # Calculate forest area\n",
        "    forest_cover_ha = pixels_area_polygon_masked_ha # Already masked to forest in current workflow\n",
        "\n",
        "    # Calculate total, mean and stdev of aboveground biomass\n",
        "    agbd_mean_mg_ha = np.ma.mean(diff_array_masked, dtype='float64') # Float64 minimises error for large number of values\n",
        "    agb_total_mg = np.multiply(agbd_mean_mg_ha, forest_cover_ha, dtype='float64')\n",
        "    agb_total_tg = np.divide(agb_total_mg, 1000000, dtype='float64') # Convert Mg (megagram = ton) to Tg (teragram = megaton)\n",
        "\n",
        "    # Append results to statistics list\n",
        "    values_agbd_mean_mg_ha.append(agbd_mean_mg_ha)\n",
        "    values_agb_total_tg.append(agb_total_tg)\n",
        "\n",
        "    if generate_uncertainty_stats:\n",
        "      diff_uncertainty_raster_present = False\n",
        "      for diff_uncertainty_raster in diff_uncertainty_rasters:\n",
        "        if diff_raster.split(\"__mean.tif\")[0] in diff_uncertainty_raster:\n",
        "          diff_uncertainty_raster_present = True\n",
        "          matching_diff_uncertainty_raster = diff_uncertainty_raster\n",
        "\n",
        "      if not diff_uncertainty_raster_present: print(f\"There is no uncertainty raster for {diff_raster.split('/')[-1]}\")\n",
        "\n",
        "      if diff_uncertainty_raster_present:\n",
        "        # Open and mask uncertainty raster to polygon\n",
        "          with rasterio.open(matching_diff_uncertainty_raster) as uncertainty:\n",
        "            nodatavalue = int(uncertainty.nodatavals[0])\n",
        "            uncertainty_array_masked, transform_2 = msk.mask(uncertainty, polygons, crop=True, filled=False)\n",
        "\n",
        "          # See https://stats.stackexchange.com/questions/223924/how-to-add-up-partial-confidence-intervals-to-create-a-total-confidence-interval#comment426260_223924\n",
        "\n",
        "          # Compress masked array data to 1D\n",
        "          diff_1d = np.ma.compressed(diff_array_masked)\n",
        "          uncertainty_1d = np.ma.compressed(uncertainty_array_masked)\n",
        "          # Convert uncertainty percentages to ratios\n",
        "          uncertainty_ratios = np.divide(uncertainty_1d, 100, dtype='float64')\n",
        "          # Multiply the diff values (mean AGBD Mg/ha) by uncertainty ratios for CI95 values\n",
        "          diff_ci95s = np.multiply(diff_1d, uncertainty_ratios, dtype='float64')\n",
        "\n",
        "          # Method 1 - Simple calculation of mean CI95 (higher estimate). Assumption: pixel values are completely correlated (all measure the same thing).\n",
        "          agbd_mean_mg_ha_ci95_1 = np.mean(diff_ci95s, dtype = 'float64')\n",
        "          # Method 2 - Square diff CI95s. Sum and then square root for total CI.\n",
        "          # Then divide by observations to calculate mean CI95. Assumption: pixel values are completely independent.\n",
        "          sum_squares = np.sum(np.square(diff_ci95s, dtype='float64'), dtype='float64')\n",
        "          total_ci95 = np.sqrt(sum_squares, dtype='float64')\n",
        "          agbd_mean_mg_ha_ci95_2 = np.divide(total_ci95, np.ma.count(diff_ci95s), dtype='float64')\n",
        "          # Method 3 - Used in Liang et al 2023 to calculate change uncertainty. Identical results to method 2.\n",
        "          diffs_x_uncertainties = np.multiply(diff_1d, uncertainty_1d, dtype='float64')\n",
        "          sum_squares_pxu = np.sum(np.square(diffs_x_uncertainties, dtype='float64'), dtype='float64')\n",
        "          sqrt_divided_sum = np.sqrt(sum_squares_pxu, dtype='float64') / np.sum(diff_1d, dtype='float64')\n",
        "          agbd_mean_mg_ha_ci95_3 = np.multiply(np.divide(sqrt_divided_sum, 100, dtype='float64'), agbd_mean_mg_ha, dtype='float64')\n",
        "\n",
        "          agbd_mean_mg_ha_ci95 = agbd_mean_mg_ha_ci95_1\n",
        "\n",
        "          # Calculate total AGB CI95\n",
        "          agb_total_mg_ci95 = np.multiply(agbd_mean_mg_ha_ci95, forest_cover_ha, dtype='float64')\n",
        "          agb_total_tg_ci95 = np.divide(agb_total_mg_ci95, 1000000, dtype='float64') # Convert total CI to Tg\n",
        "          # Calculate percentage uncertainty\n",
        "          agbd_mean_mg_ha_uncertainty = np.multiply(np.divide(agbd_mean_mg_ha_ci95, agbd_mean_mg_ha, dtype='float64'), 100, dtype='float64')\n",
        "          # Append results to statistics list\n",
        "          values_agbd_mean_mg_ha_ci95.append(agbd_mean_mg_ha_ci95)\n",
        "          values_agbd_mean_mg_ha_uncertainty.append(agbd_mean_mg_ha_uncertainty)\n",
        "          values_agb_total_tg_ci95.append(agb_total_tg_ci95)\n",
        "\n",
        "  # Concatenate new columns to the main DataFrames for each statistic\n",
        "  df_agbd_mean_mg_ha = pd.concat([df_agbd_mean_mg_ha, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha}, index=df_agbd_mean_mg_ha.index)], axis=1)\n",
        "  df_agb_total_tg = pd.concat([df_agb_total_tg, pd.DataFrame({sample_polygon_name: values_agb_total_tg}, index=df_agb_total_tg.index)], axis=1)\n",
        "\n",
        "  if generate_uncertainty_stats:\n",
        "      df_agbd_mean_mg_ha_ci95 = pd.concat([df_agbd_mean_mg_ha_ci95, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_ci95}, index=df_agbd_mean_mg_ha_ci95.index)], axis=1)\n",
        "      df_agbd_mean_mg_ha_uncertainty = pd.concat([df_agbd_mean_mg_ha_uncertainty, pd.DataFrame({sample_polygon_name: values_agbd_mean_mg_ha_uncertainty}, index=df_agbd_mean_mg_ha_uncertainty.index)], axis=1)\n",
        "      df_agb_total_tg_ci95 = pd.concat([df_agb_total_tg_ci95, pd.DataFrame({sample_polygon_name: values_agb_total_tg_ci95}, index=df_agb_total_tg_ci95.index)], axis=1)\n",
        "\n",
        "\n",
        "# Create stats list\n",
        "if generate_uncertainty_stats:\n",
        "  df_stats_list = [df_agbd_mean_mg_ha, df_agbd_mean_mg_ha_ci95, df_agbd_mean_mg_ha_uncertainty, df_agb_total_tg, df_agb_total_tg_ci95]\n",
        "else: df_stats_list = [df_agbd_mean_mg_ha, df_agb_total_tg]\n",
        "\n",
        "# Set index of the polygon area km2 dataframe to 'Name' of the polygon\n",
        "df_polygon_area_km2 = df_polygon_area_km2.set_index('Name')\n",
        "\n",
        "# Generate summary stats\n",
        "df_agb_total_tg_t = df_agb_total_tg.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB (Tg)\")\n",
        "if generate_uncertainty_stats:\n",
        "  df_agb_total_tg_ci95_t = df_agb_total_tg_ci95.T.rename_axis(\"Name\", axis=1).add_suffix(\" forest AGB CI95 (Tg)\")\n",
        "  summary_stats = pd.concat([df_polygon_area_km2, df_agb_total_tg_t, df_agb_total_tg_ci95_t], axis=1).rename_axis(\"Name\", axis=1)\n",
        "else: summary_stats = pd.concat([df_polygon_area_km2, df_agb_total_tg_t], axis=1).rename_axis(\"Name\", axis=1)\n",
        "summary_stats.to_csv(join(sample_polygons_statistics_dir, 'summary_diff_stats.csv'))\n",
        "\n",
        "# Generate detailed stats by polygon (requires uncertainty stats)\n",
        "for polygon_area in df_stats_list[0]:\n",
        "  polygon_area_km2 = df_polygon_area_km2.loc[polygon_area][\"Area (km^2)\"]\n",
        "  df_detailed_diff_stats = df_base\n",
        "  df_detailed_diff_stats[\"Area (km^2)\"] = polygon_area_km2\n",
        "  for df_stats in df_stats_list:\n",
        "    if df_stats.equals(df_agbd_mean_mg_ha): stat_col = \"Forest AGBD mean (Mg / ha)\"\n",
        "    if df_stats.equals(df_agb_total_tg): stat_col = \"Forest AGB total (Tg)\"\n",
        "    if generate_uncertainty_stats:\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_ci95): stat_col = \"Forest AGBD CI95 (Mg / ha)\"\n",
        "      if df_stats.equals(df_agbd_mean_mg_ha_uncertainty): stat_col = \"Forest AGBD uncertainty (%)\"\n",
        "      if df_stats.equals(df_agb_total_tg_ci95): stat_col = \"Forest AGB total CI95 (Tg)\"\n",
        "    for stats_polygon_area in df_stats:\n",
        "      if stats_polygon_area == polygon_area:\n",
        "        df_stats_renamed = df_stats.rename(columns={stats_polygon_area:stat_col})\n",
        "        df_detailed_diff_stats = pd.concat([df_detailed_diff_stats, df_stats_renamed[stat_col]], axis=1)\n",
        "    df_detailed_diff_stats.to_csv(join(detailed_diff_stats_dir, f'{polygon_area}.csv'))\n",
        "\n",
        "# Generate detailed stats by diff (requires uncertainty stats)\n",
        "diffs = {}\n",
        "# Loop through all CSV files in the 'detailed_diff_stats_dir'\n",
        "for stats_csv in os.listdir(detailed_diff_stats_dir):\n",
        "    polygon_name = f\"{stats_csv[:-4]}\"\n",
        "    stats_csv_path = join(detailed_diff_stats_dir, stats_csv)\n",
        "    stats_csv_df = pd.read_csv(stats_csv_path)\n",
        "    # Loop through each unique diff in the file\n",
        "    for diff in stats_csv_df['diff'].unique():\n",
        "        # Filter the dataframe for the current diff\n",
        "        diff_df = stats_csv_df[stats_csv_df['diff'] == diff].copy()\n",
        "        # Drop the 'diff' column and add the 'Name' column\n",
        "        diff_df.drop('diff', axis=1, inplace=True)\n",
        "        diff_df.insert(0, 'Name', polygon_name)\n",
        "        # If this diff's dataframe already exists, append to it; otherwise, create it\n",
        "        if diff in diffs: diffs[diff] = pd.concat([diffs[diff], diff_df], ignore_index=True)\n",
        "        else: diffs[diff] = diff_df\n",
        "for diff, diff_df in diffs.items():\n",
        "    output_file_path = join(detailed_diff_stats_diff_dir,f'{diff}.csv')\n",
        "    diff_df.to_csv(output_file_path, index=False)"
      ],
      "metadata": {
        "id": "FfyIlrVKwd6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disconnected runtime"
      ],
      "metadata": {
        "id": "wXAGIvpk_KWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "y3qMsENC_MP2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "collapsed_sections": [
        "zjy-T1TqScbE",
        "hlQ2iLAiwQoB"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}