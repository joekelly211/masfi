{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/8_differences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8jLRKRSfgb"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkvV803eSUQY"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEpdIjOwSiB1"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!apt-get install -y gdal-bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQDSsvGeSi4u"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "from os import makedirs\n",
        "from os.path import join, exists\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "from shutil import copyfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxgT3K1Sjum"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "differences_dir = join(base_dir, \"8_differences\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(differences_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96FAfeyooMLz"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = vector = None\n",
        "    try:\n",
        "        raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "        vector = ogr.Open(polygon_path)\n",
        "        if not raster or not vector:\n",
        "            raise ValueError(\"Cannot open input files\")\n",
        "        layer = vector.GetLayer()\n",
        "        options = [\"ALL_TOUCHED=TRUE\"] if all_touched else []\n",
        "        if fixed:\n",
        "            gdal.RasterizeLayer(raster, [1], layer, burn_values=[fixed_value], options=options)\n",
        "        else:\n",
        "            attr_name = column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()\n",
        "            options.append(f\"ATTRIBUTE={attr_name}\")\n",
        "            gdal.RasterizeLayer(raster, [1], layer, options=options)\n",
        "    finally:\n",
        "        if raster: raster.FlushCache()\n",
        "        raster = vector = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_OkHaUkTCyB"
      },
      "source": [
        "# Select source and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mB0XrPNYU_I"
      },
      "outputs": [],
      "source": [
        "# Select if to source predictions from scenarios_dir or uncertainty_dir\n",
        "# If available, uncertainty_dir should be selected so that uncertainty can\n",
        "# be propagated and scenario 'mean' iteration values used.\n",
        "\n",
        "source_dir = uncertainty_dir\n",
        "# source_dir = scenarios_dir\n",
        "\n",
        "print(f\"{source_dir.split('/')[-1]} has been selected as the source directory for predictions\")\n",
        "print(\"to calculate disturbance and intactness.\\n\")\n",
        "\n",
        "# If uncertainty selected, check it exists\n",
        "if not exists(uncertainty_dir) and source_dir == uncertainty_dir:\n",
        "  print(\"The uncertainty directory does not yet exist. Defaulting to scenarios directory.\")\n",
        "  source_dir = scenarios_dir\n",
        "\n",
        "source_dir_name = f\"{source_dir.split('_')[-1]}_dir\"\n",
        "\n",
        "# Select the model\n",
        "for subdir in os.listdir(source_dir):\n",
        "  if 'scenario_masks' not in subdir:\n",
        "    print(f\"selected_model = '{subdir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbXY6iP_qjYc"
      },
      "outputs": [],
      "source": [
        "selected_model = 'agbd_251203_161707'\n",
        "\n",
        "selected_model_dir = join(source_dir, selected_model)\n",
        "if source_dir == scenarios_dir: predictions_dir = join(selected_model_dir, 'scenario_predictions')\n",
        "if source_dir == uncertainty_dir:\n",
        "  predictions_dir = join(selected_model_dir, 'uncertainty_predictions')\n",
        "  predictions_unmasked_dir = join(selected_model_dir, 'uncertainty_predictions_unmasked')\n",
        "\n",
        "scenario_masks_dir = join(scenarios_dir, selected_model, \"scenario_masks\")\n",
        "\n",
        "# Check predictions exist to calculate differences\n",
        "if len(os.listdir(predictions_dir)) < 2: print(f\"At least 2 predictions must exist in {source_dir} to calculate differences.\")\n",
        "else:\n",
        "  model_differences_dir = join(differences_dir, f\"{selected_model}_{source_dir_name}\")\n",
        "  disturbance_dir = join(model_differences_dir, 'disturbance')\n",
        "  intactness_dir = join(model_differences_dir, 'intactness')\n",
        "  restoration_dir = join(model_differences_dir, 'restoration')\n",
        "  makedirs(model_differences_dir, exist_ok=True)\n",
        "  makedirs(disturbance_dir, exist_ok=True)\n",
        "  makedirs(intactness_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECHaTfrs0n7i"
      },
      "source": [
        "# Scenario differences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgPVQgfTpy21"
      },
      "source": [
        "## Define type and period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUaYCCymYI2Y"
      },
      "outputs": [],
      "source": [
        "# Forest change is measured as absolute AGBD loss\n",
        "# This block builds dictionaries of options based on available files/\n",
        "# All dictionaries output 2-tuples: (deforestation, degradation) or degradation-only strings\n",
        "# Decomposition uses nodata patterns:\n",
        "# - Deforestation: pixel transitions from forest (data) to non-forest (nodata)\n",
        "# - Degradation: pixel remains forest but loses AGBD\n",
        "# Deforestation AGBD loss only calculated for oldgrowth and area-based baselines.\n",
        "# Deforestation is the cumulative process from intact forest to complete removal.\n",
        "\n",
        "# Extract all available scenarios from scenarios predictions directory\n",
        "if source_dir == scenarios_dir:\n",
        "    scenarios = set()\n",
        "    for file in os.listdir(predictions_dir):\n",
        "        scenarios.add(file.split(\"__\")[0])\n",
        "# Or extract all available scenarios from uncertainty predictions directory\n",
        "if source_dir == uncertainty_dir:\n",
        "    prediction_stats = {}\n",
        "    for file in os.listdir(predictions_dir):\n",
        "        parts = file.split(\"__\")\n",
        "        if len(parts) >= 2:\n",
        "            stat, scenario = parts[0], parts[1]\n",
        "            if scenario not in prediction_stats:\n",
        "                prediction_stats[scenario] = set()\n",
        "            prediction_stats[scenario].add(stat)\n",
        "    scenarios = {scenario for scenario, stats in prediction_stats.items()\n",
        "                 if 'uncertainty' in stats and 'mean' in stats}\n",
        "# Categorise years from scenarios\n",
        "years = set()\n",
        "plain_years = set()\n",
        "for s in scenarios:\n",
        "    if s.isdigit():\n",
        "        years.add(int(s))\n",
        "        plain_years.add(int(s))\n",
        "    elif \"_no_disturbance_since_\" in s:\n",
        "        year = s.split(\"_\")[0]\n",
        "        if year.isdigit():\n",
        "            years.add(int(year))\n",
        "        since_part = s.split(\"_since_\")[1]\n",
        "        if since_part.isdigit():\n",
        "            years.add(int(since_part) - 1)\n",
        "years_sorted = sorted(list(years))\n",
        "\n",
        "\n",
        "# 1. Disturbance since dictionary\n",
        "# Compares actual AGBD against no_disturbance counterfactual.\n",
        "# Keys: (actual_year, counterfactual_scenario)\n",
        "# Values: (deforestation_name, degradation_name) for oldgrowth, degradation_name otherwise\n",
        "print(\"disturbance_since_dictionary = {\\n\")\n",
        "disturbance_since_dictionary = {}\n",
        "for year_a in years_sorted:\n",
        "    a_str = str(year_a)\n",
        "    if year_a not in plain_years: continue\n",
        "    # Oldgrowth entry first\n",
        "    counterfactual_oldgrowth = f\"{a_str}_no_disturbance_since_oldgrowth\"\n",
        "    if counterfactual_oldgrowth in scenarios:\n",
        "        print(f\"  # Disturbance in {a_str} caused by events since oldgrowth\")\n",
        "        print(f\"  ('{a_str}', '{counterfactual_oldgrowth}'):\")\n",
        "        print(f\"    ('{a_str}_deforestation_since_oldgrowth',\")\n",
        "        print(f\"     '{a_str}_degradation_since_oldgrowth'),\")\n",
        "        print(\"\")\n",
        "        disturbance_since_dictionary[(a_str, counterfactual_oldgrowth)] = (\n",
        "            f\"{a_str}_deforestation_since_oldgrowth\",\n",
        "            f\"{a_str}_degradation_since_oldgrowth\")\n",
        "    # Year-based entries in chronological order\n",
        "    for year_b in years_sorted:\n",
        "        if year_a <= year_b: continue\n",
        "        b_plus1 = str(year_b + 1)\n",
        "        counterfactual_scenario = f\"{a_str}_no_disturbance_since_{b_plus1}\"\n",
        "        if counterfactual_scenario in scenarios:\n",
        "            print(f\"  # Disturbance in {a_str} caused by events since {b_plus1}\")\n",
        "            print(f\"  ('{a_str}', '{counterfactual_scenario}'):\")\n",
        "            print(f\"    '{a_str}_degradation_since_{b_plus1}',\")\n",
        "            print(\"\")\n",
        "            disturbance_since_dictionary[(a_str, counterfactual_scenario)] = \\\n",
        "                f\"{a_str}_degradation_since_{b_plus1}\"\n",
        "print(\"}\\n\")\n",
        "\n",
        "\n",
        "# 2. Degradation interval dictionary\n",
        "# Calculates degradation over a multi-year interval from a baseline\n",
        "# (oldgrowth or earliest available counterfactual) to a recent year.\n",
        "# Single-year intervals handled by degradation_single_year_dictionary.\n",
        "# Keys: (counterfactual_recent, counterfactual_baseline) counterfactual pairs\n",
        "# Values: degradation_name\n",
        "print(\"degradation_interval_dictionary = {\\n\")\n",
        "degradation_interval_dictionary = {}\n",
        "for y in years_sorted:\n",
        "    y_str = str(y)\n",
        "    if y not in plain_years: continue\n",
        "    # Collect available no_disturbance counterfactuals\n",
        "    counterfactual_dist = {}\n",
        "    if f\"{y_str}_no_disturbance_since_oldgrowth\" in scenarios:\n",
        "        counterfactual_dist[\"oldgrowth\"] = f\"{y_str}_no_disturbance_since_oldgrowth\"\n",
        "    for since_y in years_sorted:\n",
        "        if since_y >= y:\n",
        "            continue\n",
        "        sp = str(since_y + 1)\n",
        "        dist_counterfactual = f\"{y_str}_no_disturbance_since_{sp}\"\n",
        "        if dist_counterfactual in scenarios:\n",
        "            counterfactual_dist[sp] = dist_counterfactual\n",
        "    if len(counterfactual_dist) < 2: continue\n",
        "    since_parts = sorted([k for k in counterfactual_dist if k != \"oldgrowth\"], key=int)\n",
        "    # Check if any multi-year intervals exist for this year\n",
        "    has_entries = False\n",
        "    if \"oldgrowth\" in counterfactual_dist and since_parts:\n",
        "        has_entries = True\n",
        "    if len(since_parts) > 1:\n",
        "        earliest = min(since_parts, key=int)\n",
        "        for sp in since_parts:\n",
        "            if sp != earliest and int(sp) - int(earliest) > 1:\n",
        "                has_entries = True\n",
        "                break\n",
        "    if not has_entries: continue\n",
        "    # Oldgrowth baseline pairs first\n",
        "    if \"oldgrowth\" in counterfactual_dist:\n",
        "        for sp in since_parts:\n",
        "            recent_year = str(int(sp) - 1)\n",
        "            print(f\"  # Degradation in {y_str} from events since oldgrowth to {recent_year}\")\n",
        "            print(f\"  ('{counterfactual_dist[sp]}', '{counterfactual_dist['oldgrowth']}'):\")\n",
        "            print(f\"    '{y_str}_degradation_from_oldgrowth_to_{recent_year}',\")\n",
        "            print(\"\")\n",
        "            degradation_interval_dictionary[(counterfactual_dist[sp], counterfactual_dist[\"oldgrowth\"])] = \\\n",
        "                f\"{y_str}_degradation_from_oldgrowth_to_{recent_year}\"\n",
        "    # Earliest year baseline pairs (excluding single-year differences)\n",
        "    if len(since_parts) > 1:\n",
        "        earliest = min(since_parts, key=int)\n",
        "        for sp in since_parts:\n",
        "            if sp != earliest and int(sp) - int(earliest) > 1:\n",
        "                recent_year = str(int(sp) - 1)\n",
        "                print(f\"  # Degradation in {y_str} from events in {earliest} to {recent_year}\")\n",
        "                print(f\"  ('{counterfactual_dist[sp]}', '{counterfactual_dist[earliest]}'):\")\n",
        "                print(f\"    '{y_str}_degradation_from_{earliest}_to_{recent_year}',\")\n",
        "                print(\"\")\n",
        "                degradation_interval_dictionary[(counterfactual_dist[sp], counterfactual_dist[earliest])] = \\\n",
        "                    f\"{y_str}_degradation_from_{earliest}_to_{recent_year}\"\n",
        "print(\"}\\n\")\n",
        "\n",
        "\n",
        "# 3. Degradation single year dictionary\n",
        "# Calculates degradation effect from a single year using consecutive counterfactuals.\n",
        "# Keys: (counterfactual_next, counterfactual_current) or (actual, counterfactual_current) for same-year\n",
        "# Values: degradation_name\n",
        "print(\"degradation_single_year_dictionary = {\\n\")\n",
        "degradation_single_year_dictionary = {}\n",
        "for y in years_sorted:\n",
        "    y_str = str(y)\n",
        "    if y not in plain_years: continue\n",
        "    # Collect available no_disturbance counterfactuals\n",
        "    counterfactual_dist = {}\n",
        "    for since_y in years_sorted:\n",
        "        if since_y >= y: continue\n",
        "        sp = str(since_y + 1)\n",
        "        dist_counterfactual = f\"{y_str}_no_disturbance_since_{sp}\"\n",
        "        if dist_counterfactual in scenarios:\n",
        "            counterfactual_dist[int(sp)] = dist_counterfactual\n",
        "    if not counterfactual_dist: continue\n",
        "    since_years = sorted(counterfactual_dist.keys())\n",
        "    # Consecutive year pairs\n",
        "    for i in range(len(since_years) - 1):\n",
        "        current_year = since_years[i]\n",
        "        next_year = since_years[i + 1]\n",
        "        if next_year == current_year + 1:\n",
        "            effect_year = current_year\n",
        "            print(f\"  # Degradation in {y_str} from events in {effect_year}\")\n",
        "            print(f\"  ('{counterfactual_dist[next_year]}', '{counterfactual_dist[current_year]}'):\")\n",
        "            print(f\"    '{y_str}_effect_of_degradation_in_{effect_year}',\")\n",
        "            print(\"\")\n",
        "            degradation_single_year_dictionary[(counterfactual_dist[next_year], counterfactual_dist[current_year])] = \\\n",
        "                f\"{y_str}_effect_of_degradation_in_{effect_year}\"\n",
        "    # Same-year case (actual vs no_disturbance_since_year)\n",
        "    max_since = max(since_years)\n",
        "    if max_since == y:\n",
        "        print(f\"  # Degradation in {y_str} from events in {y}\")\n",
        "        print(f\"  ('{y_str}', '{counterfactual_dist[max_since]}'):\")\n",
        "        print(f\"    '{y_str}_effect_of_degradation_in_{y}',\")\n",
        "        print(\"\")\n",
        "        degradation_single_year_dictionary[(y_str, counterfactual_dist[max_since])] = \\\n",
        "            f\"{y_str}_effect_of_degradation_in_{y}\"\n",
        "print(\"}\\n\")\n",
        "\n",
        "\n",
        "# 4. Disturbance area dictionary\n",
        "# Calculates disturbance from polygon-based alternate scenarios.\n",
        "# Unlike historical counterfactuals, these represent forecast scenarios of potential\n",
        "# loss, so AGBD loss from deforestation is calculated.\n",
        "# Keys: (alternate_scenario, actual_year)\n",
        "# Values: (deforestation_name, degradation_name)\n",
        "print(\"disturbance_area_dictionary = {\\n\")\n",
        "disturbance_area_dictionary = {}\n",
        "polygon_names = set()\n",
        "if os.path.exists(polygons_dir):\n",
        "    for file in os.listdir(polygons_dir):\n",
        "        if file.endswith('.gpkg'):\n",
        "            polygon_names.add(file[:-5])\n",
        "area_based_entries = []\n",
        "for scenario in scenarios:\n",
        "    parts = scenario.split('_')\n",
        "    # Check for deforestation scenarios (ends with \"Xm_degradation_buffer\")\n",
        "    if len(parts) >= 5 and parts[-1] == 'buffer' and parts[-2] == 'degradation' and parts[-3].endswith('m'):\n",
        "        alt_year, year_affix, dist_type = parts[0], parts[-4], parts[-5]\n",
        "        polygon_name = '_'.join(parts[1:-5])\n",
        "        if polygon_name in polygon_names and dist_type == 'deforestation':\n",
        "            output_base = f\"{alt_year}_deforestation_of_{polygon_name}_{year_affix}\"\n",
        "            area_based_entries.append((scenario, alt_year, output_base))\n",
        "    # Check for degradation scenarios (ends with \"degradation_YYYY\")\n",
        "    elif len(parts) >= 3 and parts[-2] == 'degradation' and parts[-1].isdigit() and len(parts[-1]) == 4:\n",
        "        alt_year, year_affix = parts[0], parts[-1]\n",
        "        polygon_name = '_'.join(parts[1:-2])\n",
        "        if polygon_name in polygon_names:\n",
        "            output_base = f\"{alt_year}_degradation_of_{polygon_name}_{year_affix}\"\n",
        "            area_based_entries.append((scenario, alt_year, output_base))\n",
        "if area_based_entries:\n",
        "    for scenario, alt_year, output_base in sorted(area_based_entries):\n",
        "        print(f\"  # Area-based disturbance: {output_base}\")\n",
        "        print(f\"  ('{scenario}', '{alt_year}'):\")\n",
        "        print(f\"    ('{output_base}_deforestation',\")\n",
        "        print(f\"     '{output_base}_degradation'),\")\n",
        "        print(\"\")\n",
        "        disturbance_area_dictionary[(scenario, alt_year)] = (\n",
        "            f\"{output_base}_deforestation\",\n",
        "            f\"{output_base}_degradation\")\n",
        "\n",
        "print(\"}\\n\")\n",
        "\n",
        "\n",
        "# 5. Restoration potential dictionary\n",
        "# Calculates potential AGBD gain from restoration to oldgrowth state.\n",
        "# Unlike disturbance dictionaries, output is positive (gain not loss).\n",
        "# Recovery: gain in existing forest (oldgrowth_recovery scenario).\n",
        "# Reforestation and recovery: gain including cleared areas (no_disturbance_since_oldgrowth).\n",
        "# Keys: (restoration_scenario, actual_year)\n",
        "# Values: restoration_potential_name\n",
        "print(\"restoration_potential_dictionary = {\\n\")\n",
        "restoration_potential_dictionary = {}\n",
        "for year_a in years_sorted:\n",
        "    a_str = str(year_a)\n",
        "    if year_a not in plain_years: continue\n",
        "    # Recovery potential (existing forest only)\n",
        "    recovery_scenario = f\"{a_str}_oldgrowth_recovery\"\n",
        "    if recovery_scenario in scenarios:\n",
        "        print(f\"  # Recovery potential in {a_str}\")\n",
        "        print(f\"  ('{recovery_scenario}', '{a_str}'):\")\n",
        "        print(f\"    '{a_str}_recovery_potential',\")\n",
        "        print(\"\")\n",
        "        restoration_potential_dictionary[(recovery_scenario, a_str)] = \\\n",
        "            f\"{a_str}_recovery_potential\"\n",
        "    # Reforestation and recovery potential (including cleared areas)\n",
        "    counterfactual_oldgrowth = f\"{a_str}_no_disturbance_since_oldgrowth\"\n",
        "    if counterfactual_oldgrowth in scenarios:\n",
        "        print(f\"  # Reforestation and recovery potential in {a_str}\")\n",
        "        print(f\"  ('{counterfactual_oldgrowth}', '{a_str}'):\")\n",
        "        print(f\"    '{a_str}_reforestation_and_recovery_potential',\")\n",
        "        print(\"\")\n",
        "        restoration_potential_dictionary[(counterfactual_oldgrowth, a_str)] = \\\n",
        "            f\"{a_str}_reforestation_and_recovery_potential\"\n",
        "print(\"}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disturbance_since_dictionary = {\n",
        "\n",
        "  # Disturbance in 2021 caused by events since oldgrowth\n",
        "  ('2021', '2021_no_disturbance_since_oldgrowth'):\n",
        "    ('2021_deforestation_since_oldgrowth',\n",
        "     '2021_degradation_since_oldgrowth'),\n",
        "\n",
        "  # Disturbance in 2021 caused by events since 1993\n",
        "  ('2021', '2021_no_disturbance_since_1993'):\n",
        "    '2021_degradation_since_1993',\n",
        "\n",
        "  # Disturbance in 2024 caused by events since oldgrowth\n",
        "  ('2024', '2024_no_disturbance_since_oldgrowth'):\n",
        "    ('2024_deforestation_since_oldgrowth',\n",
        "     '2024_degradation_since_oldgrowth'),\n",
        "\n",
        "  # Disturbance in 2024 caused by events since 1996\n",
        "  ('2024', '2024_no_disturbance_since_1996'):\n",
        "    '2024_degradation_since_1996',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 1997\n",
        "  # ('2024', '2024_no_disturbance_since_1997'):\n",
        "  #   '2024_degradation_since_1997',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 1998\n",
        "  # ('2024', '2024_no_disturbance_since_1998'):\n",
        "  #   '2024_degradation_since_1998',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 1999\n",
        "  # ('2024', '2024_no_disturbance_since_1999'):\n",
        "  #   '2024_degradation_since_1999',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2000\n",
        "  # ('2024', '2024_no_disturbance_since_2000'):\n",
        "  #   '2024_degradation_since_2000',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2001\n",
        "  # ('2024', '2024_no_disturbance_since_2001'):\n",
        "  #   '2024_degradation_since_2001',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2002\n",
        "  # ('2024', '2024_no_disturbance_since_2002'):\n",
        "  #   '2024_degradation_since_2002',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2003\n",
        "  # ('2024', '2024_no_disturbance_since_2003'):\n",
        "  #   '2024_degradation_since_2003',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2004\n",
        "  # ('2024', '2024_no_disturbance_since_2004'):\n",
        "  #   '2024_degradation_since_2004',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2005\n",
        "  # ('2024', '2024_no_disturbance_since_2005'):\n",
        "  #   '2024_degradation_since_2005',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2006\n",
        "  # ('2024', '2024_no_disturbance_since_2006'):\n",
        "  #   '2024_degradation_since_2006',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2007\n",
        "  # ('2024', '2024_no_disturbance_since_2007'):\n",
        "  #   '2024_degradation_since_2007',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2008\n",
        "  # ('2024', '2024_no_disturbance_since_2008'):\n",
        "  #   '2024_degradation_since_2008',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2009\n",
        "  # ('2024', '2024_no_disturbance_since_2009'):\n",
        "  #   '2024_degradation_since_2009',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2010\n",
        "  # ('2024', '2024_no_disturbance_since_2010'):\n",
        "  #   '2024_degradation_since_2010',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2011\n",
        "  # ('2024', '2024_no_disturbance_since_2011'):\n",
        "  #   '2024_degradation_since_2011',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2012\n",
        "  # ('2024', '2024_no_disturbance_since_2012'):\n",
        "  #   '2024_degradation_since_2012',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2013\n",
        "  # ('2024', '2024_no_disturbance_since_2013'):\n",
        "  #   '2024_degradation_since_2013',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2014\n",
        "  # ('2024', '2024_no_disturbance_since_2014'):\n",
        "  #   '2024_degradation_since_2014',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2015\n",
        "  # ('2024', '2024_no_disturbance_since_2015'):\n",
        "  #   '2024_degradation_since_2015',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2016\n",
        "  # ('2024', '2024_no_disturbance_since_2016'):\n",
        "  #   '2024_degradation_since_2016',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2017\n",
        "  # ('2024', '2024_no_disturbance_since_2017'):\n",
        "  #   '2024_degradation_since_2017',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2018\n",
        "  # ('2024', '2024_no_disturbance_since_2018'):\n",
        "  #   '2024_degradation_since_2018',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2019\n",
        "  # ('2024', '2024_no_disturbance_since_2019'):\n",
        "  #   '2024_degradation_since_2019',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2020\n",
        "  # ('2024', '2024_no_disturbance_since_2020'):\n",
        "  #   '2024_degradation_since_2020',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2021\n",
        "  # ('2024', '2024_no_disturbance_since_2021'):\n",
        "  #   '2024_degradation_since_2021',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2022\n",
        "  # ('2024', '2024_no_disturbance_since_2022'):\n",
        "  #   '2024_degradation_since_2022',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2023\n",
        "  # ('2024', '2024_no_disturbance_since_2023'):\n",
        "  #   '2024_degradation_since_2023',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2024\n",
        "  # ('2024', '2024_no_disturbance_since_2024'):\n",
        "  #   '2024_degradation_since_2024',\n",
        "\n",
        "}\n",
        "\n",
        "degradation_interval_dictionary = {\n",
        "\n",
        "  # Degradation in 2021 from events since oldgrowth to 1992\n",
        "  ('2021_no_disturbance_since_1993', '2021_no_disturbance_since_oldgrowth'):\n",
        "    '2021_degradation_from_oldgrowth_to_1992',\n",
        "\n",
        "  # Degradation in 2024 from events since oldgrowth to 1995\n",
        "  ('2024_no_disturbance_since_1996', '2024_no_disturbance_since_oldgrowth'):\n",
        "    '2024_degradation_from_oldgrowth_to_1995',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 1996\n",
        "  # ('2024_no_disturbance_since_1997', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_1996',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 1997\n",
        "  # ('2024_no_disturbance_since_1998', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_1997',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 1998\n",
        "  # ('2024_no_disturbance_since_1999', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_1998',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 1999\n",
        "  # ('2024_no_disturbance_since_2000', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_1999',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2000\n",
        "  # ('2024_no_disturbance_since_2001', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2000',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2001\n",
        "  # ('2024_no_disturbance_since_2002', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2001',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2002\n",
        "  # ('2024_no_disturbance_since_2003', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2002',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2003\n",
        "  # ('2024_no_disturbance_since_2004', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2003',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2004\n",
        "  # ('2024_no_disturbance_since_2005', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2004',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2005\n",
        "  # ('2024_no_disturbance_since_2006', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2005',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2006\n",
        "  # ('2024_no_disturbance_since_2007', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2006',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2007\n",
        "  # ('2024_no_disturbance_since_2008', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2007',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2008\n",
        "  # ('2024_no_disturbance_since_2009', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2008',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2009\n",
        "  # ('2024_no_disturbance_since_2010', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2009',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2010\n",
        "  # ('2024_no_disturbance_since_2011', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2010',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2011\n",
        "  # ('2024_no_disturbance_since_2012', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2011',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2012\n",
        "  # ('2024_no_disturbance_since_2013', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2012',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2013\n",
        "  # ('2024_no_disturbance_since_2014', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2013',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2014\n",
        "  # ('2024_no_disturbance_since_2015', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2014',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2015\n",
        "  # ('2024_no_disturbance_since_2016', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2015',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2016\n",
        "  # ('2024_no_disturbance_since_2017', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2016',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2017\n",
        "  # ('2024_no_disturbance_since_2018', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2017',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2018\n",
        "  # ('2024_no_disturbance_since_2019', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2018',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2019\n",
        "  # ('2024_no_disturbance_since_2020', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2019',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2020\n",
        "  # ('2024_no_disturbance_since_2021', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2020',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2021\n",
        "  # ('2024_no_disturbance_since_2022', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2021',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2022\n",
        "  # ('2024_no_disturbance_since_2023', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2022',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2023\n",
        "  # ('2024_no_disturbance_since_2024', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2023',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 1997\n",
        "  # ('2024_no_disturbance_since_1998', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_1997',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 1998\n",
        "  # ('2024_no_disturbance_since_1999', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_1998',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 1999\n",
        "  # ('2024_no_disturbance_since_2000', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_1999',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2000\n",
        "  # ('2024_no_disturbance_since_2001', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2000',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2001\n",
        "  # ('2024_no_disturbance_since_2002', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2001',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2002\n",
        "  # ('2024_no_disturbance_since_2003', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2002',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2003\n",
        "  # ('2024_no_disturbance_since_2004', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2003',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2004\n",
        "  # ('2024_no_disturbance_since_2005', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2004',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2005\n",
        "  # ('2024_no_disturbance_since_2006', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2005',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2006\n",
        "  # ('2024_no_disturbance_since_2007', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2006',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2007\n",
        "  # ('2024_no_disturbance_since_2008', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2007',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2008\n",
        "  # ('2024_no_disturbance_since_2009', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2008',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2009\n",
        "  # ('2024_no_disturbance_since_2010', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2009',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2010\n",
        "  # ('2024_no_disturbance_since_2011', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2010',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2011\n",
        "  # ('2024_no_disturbance_since_2012', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2011',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2012\n",
        "  # ('2024_no_disturbance_since_2013', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2012',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2013\n",
        "  # ('2024_no_disturbance_since_2014', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2013',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2014\n",
        "  # ('2024_no_disturbance_since_2015', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2014',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2015\n",
        "  # ('2024_no_disturbance_since_2016', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2015',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2016\n",
        "  # ('2024_no_disturbance_since_2017', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2016',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2017\n",
        "  # ('2024_no_disturbance_since_2018', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2017',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2018\n",
        "  # ('2024_no_disturbance_since_2019', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2018',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2019\n",
        "  # ('2024_no_disturbance_since_2020', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2019',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2020\n",
        "  # ('2024_no_disturbance_since_2021', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2020',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2021\n",
        "  # ('2024_no_disturbance_since_2022', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2021',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2022\n",
        "  # ('2024_no_disturbance_since_2023', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2022',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2023\n",
        "  # ('2024_no_disturbance_since_2024', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2023',\n",
        "\n",
        "}\n",
        "\n",
        "degradation_single_year_dictionary = {\n",
        "\n",
        "  # Degradation in 2024 from events in 1996\n",
        "  ('2024_no_disturbance_since_1997', '2024_no_disturbance_since_1996'):\n",
        "    '2024_effect_of_degradation_in_1996',\n",
        "\n",
        "  # Degradation in 2024 from events in 1997\n",
        "  ('2024_no_disturbance_since_1998', '2024_no_disturbance_since_1997'):\n",
        "    '2024_effect_of_degradation_in_1997',\n",
        "\n",
        "  # Degradation in 2024 from events in 1998\n",
        "  ('2024_no_disturbance_since_1999', '2024_no_disturbance_since_1998'):\n",
        "    '2024_effect_of_degradation_in_1998',\n",
        "\n",
        "  # Degradation in 2024 from events in 1999\n",
        "  ('2024_no_disturbance_since_2000', '2024_no_disturbance_since_1999'):\n",
        "    '2024_effect_of_degradation_in_1999',\n",
        "\n",
        "  # Degradation in 2024 from events in 2000\n",
        "  ('2024_no_disturbance_since_2001', '2024_no_disturbance_since_2000'):\n",
        "    '2024_effect_of_degradation_in_2000',\n",
        "\n",
        "  # Degradation in 2024 from events in 2001\n",
        "  ('2024_no_disturbance_since_2002', '2024_no_disturbance_since_2001'):\n",
        "    '2024_effect_of_degradation_in_2001',\n",
        "\n",
        "  # Degradation in 2024 from events in 2002\n",
        "  ('2024_no_disturbance_since_2003', '2024_no_disturbance_since_2002'):\n",
        "    '2024_effect_of_degradation_in_2002',\n",
        "\n",
        "  # Degradation in 2024 from events in 2003\n",
        "  ('2024_no_disturbance_since_2004', '2024_no_disturbance_since_2003'):\n",
        "    '2024_effect_of_degradation_in_2003',\n",
        "\n",
        "  # Degradation in 2024 from events in 2004\n",
        "  ('2024_no_disturbance_since_2005', '2024_no_disturbance_since_2004'):\n",
        "    '2024_effect_of_degradation_in_2004',\n",
        "\n",
        "  # Degradation in 2024 from events in 2005\n",
        "  ('2024_no_disturbance_since_2006', '2024_no_disturbance_since_2005'):\n",
        "    '2024_effect_of_degradation_in_2005',\n",
        "\n",
        "  # Degradation in 2024 from events in 2006\n",
        "  ('2024_no_disturbance_since_2007', '2024_no_disturbance_since_2006'):\n",
        "    '2024_effect_of_degradation_in_2006',\n",
        "\n",
        "  # Degradation in 2024 from events in 2007\n",
        "  ('2024_no_disturbance_since_2008', '2024_no_disturbance_since_2007'):\n",
        "    '2024_effect_of_degradation_in_2007',\n",
        "\n",
        "  # Degradation in 2024 from events in 2008\n",
        "  ('2024_no_disturbance_since_2009', '2024_no_disturbance_since_2008'):\n",
        "    '2024_effect_of_degradation_in_2008',\n",
        "\n",
        "  # Degradation in 2024 from events in 2009\n",
        "  ('2024_no_disturbance_since_2010', '2024_no_disturbance_since_2009'):\n",
        "    '2024_effect_of_degradation_in_2009',\n",
        "\n",
        "  # Degradation in 2024 from events in 2010\n",
        "  ('2024_no_disturbance_since_2011', '2024_no_disturbance_since_2010'):\n",
        "    '2024_effect_of_degradation_in_2010',\n",
        "\n",
        "  # Degradation in 2024 from events in 2011\n",
        "  ('2024_no_disturbance_since_2012', '2024_no_disturbance_since_2011'):\n",
        "    '2024_effect_of_degradation_in_2011',\n",
        "\n",
        "  # Degradation in 2024 from events in 2012\n",
        "  ('2024_no_disturbance_since_2013', '2024_no_disturbance_since_2012'):\n",
        "    '2024_effect_of_degradation_in_2012',\n",
        "\n",
        "  # Degradation in 2024 from events in 2013\n",
        "  ('2024_no_disturbance_since_2014', '2024_no_disturbance_since_2013'):\n",
        "    '2024_effect_of_degradation_in_2013',\n",
        "\n",
        "  # Degradation in 2024 from events in 2014\n",
        "  ('2024_no_disturbance_since_2015', '2024_no_disturbance_since_2014'):\n",
        "    '2024_effect_of_degradation_in_2014',\n",
        "\n",
        "  # Degradation in 2024 from events in 2015\n",
        "  ('2024_no_disturbance_since_2016', '2024_no_disturbance_since_2015'):\n",
        "    '2024_effect_of_degradation_in_2015',\n",
        "\n",
        "  # Degradation in 2024 from events in 2016\n",
        "  ('2024_no_disturbance_since_2017', '2024_no_disturbance_since_2016'):\n",
        "    '2024_effect_of_degradation_in_2016',\n",
        "\n",
        "  # Degradation in 2024 from events in 2017\n",
        "  ('2024_no_disturbance_since_2018', '2024_no_disturbance_since_2017'):\n",
        "    '2024_effect_of_degradation_in_2017',\n",
        "\n",
        "  # Degradation in 2024 from events in 2018\n",
        "  ('2024_no_disturbance_since_2019', '2024_no_disturbance_since_2018'):\n",
        "    '2024_effect_of_degradation_in_2018',\n",
        "\n",
        "  # Degradation in 2024 from events in 2019\n",
        "  ('2024_no_disturbance_since_2020', '2024_no_disturbance_since_2019'):\n",
        "    '2024_effect_of_degradation_in_2019',\n",
        "\n",
        "  # Degradation in 2024 from events in 2020\n",
        "  ('2024_no_disturbance_since_2021', '2024_no_disturbance_since_2020'):\n",
        "    '2024_effect_of_degradation_in_2020',\n",
        "\n",
        "  # Degradation in 2024 from events in 2021\n",
        "  ('2024_no_disturbance_since_2022', '2024_no_disturbance_since_2021'):\n",
        "    '2024_effect_of_degradation_in_2021',\n",
        "\n",
        "  # Degradation in 2024 from events in 2022\n",
        "  ('2024_no_disturbance_since_2023', '2024_no_disturbance_since_2022'):\n",
        "    '2024_effect_of_degradation_in_2022',\n",
        "\n",
        "  # Degradation in 2024 from events in 2023\n",
        "  ('2024_no_disturbance_since_2024', '2024_no_disturbance_since_2023'):\n",
        "    '2024_effect_of_degradation_in_2023',\n",
        "\n",
        "  # Degradation in 2024 from events in 2024\n",
        "  ('2024', '2024_no_disturbance_since_2024'):\n",
        "    '2024_effect_of_degradation_in_2024',\n",
        "\n",
        "}\n",
        "\n",
        "disturbance_area_dictionary = {\n",
        "\n",
        "  # Area-based disturbance: 2024_deforestation_of_road_mat_daling_2023\n",
        "  ('2024_road_mat_daling_deforestation_2023_30m_degradation_buffer', '2024'):\n",
        "    ('2024_deforestation_of_road_mat_daling_2023_deforestation',\n",
        "     '2024_deforestation_of_road_mat_daling_2023_degradation'),\n",
        "\n",
        "}\n",
        "\n",
        "restoration_potential_dictionary = {\n",
        "\n",
        "  # Recovery potential in 2021\n",
        "  ('2021_oldgrowth_recovery', '2021'):\n",
        "    '2021_recovery_potential',\n",
        "\n",
        "  # Reforestation and recovery potential in 2021\n",
        "  ('2021_no_disturbance_since_oldgrowth', '2021'):\n",
        "    '2021_reforestation_and_recovery_potential',\n",
        "\n",
        "  # Recovery potential in 2024\n",
        "  ('2024_oldgrowth_recovery', '2024'):\n",
        "    '2024_recovery_potential',\n",
        "\n",
        "  # Reforestation and recovery potential in 2024\n",
        "  ('2024_no_disturbance_since_oldgrowth', '2024'):\n",
        "    '2024_reforestation_and_recovery_potential',\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "g96l3kC13-Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozToZSzop-V8"
      },
      "source": [
        "## Calculate difference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision settings for output rasters\n",
        "mean_precision = 2\n",
        "ci_precision = 2\n",
        "uncertainty_precision = 2\n",
        "confidence_level = 95\n",
        "\n",
        "def load_raster(path):\n",
        "    ds = gdal.Open(path)\n",
        "    arr = ds.ReadAsArray()\n",
        "    ds = None\n",
        "    return arr\n",
        "\n",
        "def round_array(arr, precision):\n",
        "    rounded = np.round(arr, precision)\n",
        "    return rounded.astype(np.int16) if precision == 0 else rounded\n",
        "\n",
        "# Propagate uncertainty for forest AGBD loss calculations using confidence intervals.\n",
        "# Measures uncertainty of forest AGBD change from disturbance events only.\n",
        "\n",
        "# Mathematical basis: For difference Z = X - Y with confidence intervals CI_x, CI_y:\n",
        "# Combined CI: CI_z = [CI_x + CI_y] (IPCC 2006, Eq. 3.2; 2019, Eq. 3.2A)\n",
        "# Relative uncertainty: CI_z / |Z| = CI_z / |X - Y|\n",
        "\n",
        "# Note: Liang et al. (2023) incorrectly used |X + Y| as denominator, violating standard\n",
        "# uncertainty propagation theory for differences. IPCC guidelines (2006 Section 3.2.3.1,\n",
        "# 2019 Section 3.2.3.1) specify the denominator must be the absolute value of the\n",
        "# difference |X - Y| for mathematically correct relative uncertainty calculations.\n",
        "\n",
        "# Limitation: This approach assumes independence between scenario uncertainties, but\n",
        "# scenarios using identical models and predictors are highly correlated. This results\n",
        "# in conservative (overestimated) uncertainty bounds. Liang et al. (2023) has the same\n",
        "# correlation limitation plus the mathematical error noted above.\n",
        "\n",
        "# Forest classification from external dataset determines data availability per scenario.\n",
        "# External disturbance classification determines whether forest AGBD change occurred.\n",
        "# Uncertainty quantifies confidence in magnitude of forest AGBD change from disturbance.\n",
        "\n",
        "# References:\n",
        "# - IPCC (2006) Guidelines Vol.1 Ch.3: Uncertainties, Section 3.2.3.1\n",
        "# - IPCC (2019) Refinement Vol.1 Ch.3: Uncertainties, Section 3.2.3.1\n",
        "# - Liang et al. (2023) Remote Sensing of Environment 284:113367\n",
        "\n",
        "# Propagate uncertainty function\n",
        "# mean1, mean2: Forest AGBD values for two scenarios/timepoints (Mg/ha)\n",
        "# ci1, ci2: Confidence interval half-widths (Mg/ha)\n",
        "# is_restoration: If True, expect positive diff (gain); if False, expect negative diff (loss)\n",
        "# relative_uncertainty: Percentage (0-100)\n",
        "# ci_combined: Absolute CI for downstream calculations\n",
        "def propagate_uncertainty(mean1, ci1, mean2, ci2, is_restoration=False):\n",
        "    mean_diff = mean1 - mean2\n",
        "    # Forest/non-forest transitions where one scenario has nodata (converted to 0 mean, 0 CI).\n",
        "    # Uncertainty reflects confidence in original forest AGBD estimate, not the forest mask.\n",
        "    deforestation_case = (ci1 == 0) & (mean1 == 0) & (ci2 != 0) & (mean2 != 0)\n",
        "    reforestation_case = (ci1 != 0) & (mean1 != 0) & (ci2 == 0) & (mean2 == 0)\n",
        "    # Combine absolute uncertainties using IPCC error propagation formula\n",
        "    ci_combined = np.sqrt(np.square(ci1) + np.square(ci2))\n",
        "    # Relative uncertainty using absolute difference as denominator (IPCC standard)\n",
        "    denominator = np.abs(mean_diff)\n",
        "    standard_rel_unc = np.divide(ci_combined, denominator,\n",
        "                                 out=np.zeros_like(ci_combined, dtype=np.float64),\n",
        "                                 where=(denominator != 0))\n",
        "    # Forest transition uncertainties\n",
        "    defor_rel_unc = np.divide(ci2, np.abs(mean2),\n",
        "                              out=np.zeros_like(ci2, dtype=np.float64),\n",
        "                              where=(mean2 != 0))\n",
        "    refor_rel_unc = np.divide(ci1, np.abs(mean1),\n",
        "                              out=np.zeros_like(ci1, dtype=np.float64),\n",
        "                              where=(mean1 != 0))\n",
        "    # Zero uncertainty when unexpected sign: gain for disturbance, loss for restoration\n",
        "    unexpected_sign = (mean_diff < 0) if is_restoration else (mean_diff > 0)\n",
        "    relative_uncertainty = np.where(\n",
        "        deforestation_case, defor_rel_unc,\n",
        "        np.where(reforestation_case, refor_rel_unc,\n",
        "                 np.where(unexpected_sign | (denominator == 0), 0, standard_rel_unc)))\n",
        "    return relative_uncertainty * 100.0, ci_combined\n",
        "\n",
        "# Calculate forest disturbance components using nodata-based decomposition.\n",
        "# recent: scenario with more disturbance (lower AGBD)\n",
        "# baseline: scenario with less disturbance (higher AGBD)\n",
        "# calculate_deforestation: whether to compute deforestation component\n",
        "\n",
        "# Decomposition logic (diff = fill(recent, 0) - baseline):\n",
        "#   Deforestation: nodata if baseline_nodata, elif recent_nodata: diff, else 0\n",
        "#   Degradation:   nodata if recent_nodata OR baseline_nodata, else diff\n",
        "def calculate_forest_disturbance_components(recent_mean, baseline_mean, nodata, calculate_deforestation,\n",
        "                                       recent_ci=None, baseline_ci=None):\n",
        "    recent_nodata = (recent_mean == nodata)\n",
        "    baseline_nodata = (baseline_mean == nodata)\n",
        "    # Fill recent where baseline has data (deforestation case)\n",
        "    fill_mask = recent_nodata & ~baseline_nodata\n",
        "    recent_mean_filled = np.where(fill_mask, 0, recent_mean)\n",
        "    diff = recent_mean_filled - baseline_mean\n",
        "    # Deforestation: AGBD loss from forest-to-nonforest transitions\n",
        "    if calculate_deforestation:\n",
        "        defor = np.where(baseline_nodata, nodata, np.where(recent_nodata, diff, 0))\n",
        "    # Degradation: AGBD loss within persistent forest\n",
        "    deg = np.where(recent_nodata | baseline_nodata, nodata, diff)\n",
        "    if recent_ci is None:\n",
        "        if calculate_deforestation: return defor, deg\n",
        "        return deg\n",
        "    # Propagate uncertainty\n",
        "    recent_ci_filled = np.where(fill_mask, 0, recent_ci)\n",
        "    rel_unc, ci_combined = propagate_uncertainty(recent_mean_filled, recent_ci_filled,\n",
        "                                                 baseline_mean, baseline_ci)\n",
        "    if calculate_deforestation:\n",
        "        defor_ci = np.where(baseline_nodata, nodata, np.where(recent_nodata, ci_combined, 0))\n",
        "        defor_unc = np.where(baseline_nodata, nodata, np.where(recent_nodata, rel_unc, 0))\n",
        "    deg_ci = np.where(recent_nodata | baseline_nodata, nodata, ci_combined)\n",
        "    deg_unc = np.where(recent_nodata | baseline_nodata, nodata, rel_unc)\n",
        "    if calculate_deforestation:\n",
        "        return ((defor, defor_ci, defor_unc), (deg, deg_ci, deg_unc))\n",
        "    return (deg, deg_ci, deg_unc)\n",
        "\n",
        "# Calculate restoration potential using nodata-based masking.\n",
        "# restoration: scenario representing potential restored state (higher AGBD)\n",
        "# actual: current state (lower AGBD)\n",
        "# Output masked by restoration scenario nodata only.\n",
        "\n",
        "# Logic (diff = restoration - fill(actual, 0)):\n",
        "#   Reforestation case: actual nodata filled with 0 where restoration has data\n",
        "#   Restoration potential: nodata if restoration_nodata, else diff\n",
        "def calculate_restoration_potential(restoration_mean, actual_mean, nodata,\n",
        "                                    restoration_ci=None, actual_ci=None):\n",
        "    restoration_nodata = (restoration_mean == nodata)\n",
        "    actual_nodata = (actual_mean == nodata)\n",
        "    # Fill actual where restoration has data (reforestation case)\n",
        "    fill_mask = actual_nodata & ~restoration_nodata\n",
        "    actual_mean_filled = np.where(fill_mask, 0, actual_mean)\n",
        "    diff = restoration_mean - actual_mean_filled\n",
        "    # Restoration potential: mask by restoration scenario\n",
        "    pot = np.where(restoration_nodata, nodata, diff)\n",
        "    if restoration_ci is None:\n",
        "        return pot\n",
        "    # Propagate uncertainty\n",
        "    actual_ci_filled = np.where(fill_mask, 0, actual_ci)\n",
        "    rel_unc, ci_combined = propagate_uncertainty(restoration_mean, restoration_ci,\n",
        "                                                 actual_mean_filled, actual_ci_filled,\n",
        "                                                 is_restoration=True)\n",
        "    pot_ci = np.where(restoration_nodata, nodata, ci_combined)\n",
        "    pot_unc = np.where(restoration_nodata, nodata, rel_unc)\n",
        "    return (pot, pot_ci, pot_unc)\n",
        "\n",
        "# Determine processing mode\n",
        "use_uncertainty = source_dir == uncertainty_dir\n",
        "\n",
        "# Combine all disturbance dictionaries\n",
        "all_disturbance_dictionaries = {}\n",
        "all_disturbance_dictionaries.update(disturbance_since_dictionary)\n",
        "all_disturbance_dictionaries.update(degradation_interval_dictionary)\n",
        "all_disturbance_dictionaries.update(degradation_single_year_dictionary)\n",
        "all_disturbance_dictionaries.update(disturbance_area_dictionary)\n",
        "all_disturbance_dictionaries.update(restoration_potential_dictionary)\n",
        "\n",
        "# Progress tracking\n",
        "total_operations = len(all_disturbance_dictionaries)\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Forest disturbance calculation progress: {progress_index}/{total_operations}\")\n",
        "display(progress_label)\n",
        "\n",
        "# Process all forest disturbance calculations\n",
        "for (recent_key, baseline_key), value in all_disturbance_dictionaries.items():\n",
        "    # Determine output type from dictionary value structure\n",
        "    if isinstance(value, tuple):\n",
        "        defor_name, deg_name = value\n",
        "        has_deforestation = True\n",
        "    else:\n",
        "        deg_name = value\n",
        "        has_deforestation = False\n",
        "    # Determine calculation type\n",
        "    is_restoration = (recent_key, baseline_key) in restoration_potential_dictionary\n",
        "    is_interval = (recent_key, baseline_key) in degradation_interval_dictionary\n",
        "    output_dir = restoration_dir if is_restoration else disturbance_dir\n",
        "    # Define output paths and check existence\n",
        "    if use_uncertainty:\n",
        "        output_paths = {\n",
        "            'deg_mean': join(output_dir, f\"mean__{deg_name}__{selected_model}.tif\"),\n",
        "            'deg_ci': join(output_dir, f\"ci_{confidence_level}__{deg_name}__{selected_model}.tif\"),\n",
        "            'deg_unc': join(output_dir, f\"uncertainty__{deg_name}__{selected_model}.tif\"),}\n",
        "        if has_deforestation:\n",
        "            output_paths.update({\n",
        "                'defor_mean': join(output_dir, f\"mean__{defor_name}__{selected_model}.tif\"),\n",
        "                'defor_ci': join(output_dir, f\"ci_{confidence_level}__{defor_name}__{selected_model}.tif\"),\n",
        "                'defor_unc': join(output_dir, f\"uncertainty__{defor_name}__{selected_model}.tif\"),})\n",
        "    else:\n",
        "        output_paths = {'deg': join(output_dir, f\"{deg_name}__{selected_model}.tif\")}\n",
        "        if has_deforestation:\n",
        "            output_paths['defor'] = join(output_dir, f\"{defor_name}__{selected_model}.tif\")\n",
        "    if all(exists(p) for p in output_paths.values()):\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"Forest disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "        continue\n",
        "    # Validate input paths\n",
        "    if use_uncertainty:\n",
        "        recent_mean_path = join(predictions_dir, f\"mean__{recent_key}__{selected_model}.tif\")\n",
        "        recent_ci_path = join(predictions_dir, f\"ci_{confidence_level}__{recent_key}__{selected_model}.tif\")\n",
        "        baseline_mean_path = join(predictions_dir, f\"mean__{baseline_key}__{selected_model}.tif\")\n",
        "        baseline_ci_path = join(predictions_dir, f\"ci_{confidence_level}__{baseline_key}__{selected_model}.tif\")\n",
        "        assert exists(recent_mean_path), f\"Missing: {recent_mean_path}\"\n",
        "        assert exists(recent_ci_path), f\"Missing: {recent_ci_path}\"\n",
        "        assert exists(baseline_mean_path), f\"Missing: {baseline_mean_path}\"\n",
        "        assert exists(baseline_ci_path), f\"Missing: {baseline_ci_path}\"\n",
        "    else:\n",
        "        recent_path = join(predictions_dir, f\"{recent_key}__{selected_model}.tif\")\n",
        "        baseline_path = join(predictions_dir, f\"{baseline_key}__{selected_model}.tif\")\n",
        "        assert exists(recent_path), f\"Missing: {recent_path}\"\n",
        "        assert exists(baseline_path), f\"Missing: {baseline_path}\"\n",
        "    # Interval pairs: apply actual year mask to degradation\n",
        "    if is_interval:\n",
        "        year = recent_key.split('_')[0]\n",
        "        interval_mask_arr = load_raster(join(scenario_masks_dir, f\"{year}.tif\"))\n",
        "    # Load inputs and calculate components\n",
        "    if use_uncertainty:\n",
        "        recent_mean = load_raster(recent_mean_path)\n",
        "        recent_ci = load_raster(recent_ci_path)\n",
        "        baseline_mean = load_raster(baseline_mean_path)\n",
        "        baseline_ci = load_raster(baseline_ci_path)\n",
        "        template_path = recent_mean_path if is_restoration else baseline_mean_path\n",
        "        if is_restoration:\n",
        "            deg, deg_ci, deg_unc = calculate_restoration_potential(\n",
        "                recent_mean, baseline_mean, nodatavalue, recent_ci, baseline_ci)\n",
        "        else:\n",
        "            result = calculate_forest_disturbance_components(\n",
        "                recent_mean, baseline_mean, nodatavalue, has_deforestation, recent_ci, baseline_ci)\n",
        "            if has_deforestation: (defor, defor_ci, defor_unc), (deg, deg_ci, deg_unc) = result\n",
        "            else: deg, deg_ci, deg_unc = result\n",
        "        # Interval pairs: apply actual year mask to degradation\n",
        "        if is_interval:\n",
        "            deg = np.where(interval_mask_arr == nodatavalue, nodatavalue, deg)\n",
        "            deg_ci = np.where(interval_mask_arr == nodatavalue, nodatavalue, deg_ci)\n",
        "            deg_unc = np.where(interval_mask_arr == nodatavalue, nodatavalue, deg_unc)\n",
        "        # Round degradation mean, zero uncertainty where mean rounds to zero\n",
        "        deg_rounded = round_array(deg, mean_precision)\n",
        "        deg_ci = np.where(deg_rounded == 0, 0, deg_ci)\n",
        "        deg_unc = np.where(deg_rounded == 0, 0, deg_unc)\n",
        "        # Export degradation\n",
        "        export_array_as_tif(deg_rounded, output_paths['deg_mean'], template=template_path)\n",
        "        export_array_as_tif(round_array(deg_ci, ci_precision), output_paths['deg_ci'], template=template_path)\n",
        "        export_array_as_tif(round_array(deg_unc, uncertainty_precision), output_paths['deg_unc'], template=template_path)\n",
        "        # Export deforestation\n",
        "        if has_deforestation:\n",
        "            defor_rounded = round_array(defor, mean_precision)\n",
        "            defor_ci = np.where(defor_rounded == 0, 0, defor_ci)\n",
        "            defor_unc = np.where(defor_rounded == 0, 0, defor_unc)\n",
        "            export_array_as_tif(defor_rounded, output_paths['defor_mean'], template=template_path)\n",
        "            export_array_as_tif(round_array(defor_ci, ci_precision), output_paths['defor_ci'], template=template_path)\n",
        "            export_array_as_tif(round_array(defor_unc, uncertainty_precision), output_paths['defor_unc'], template=template_path)\n",
        "    else:\n",
        "        recent_arr = load_raster(recent_path)\n",
        "        baseline_arr = load_raster(baseline_path)\n",
        "        template_path = recent_path if is_restoration else baseline_path\n",
        "        if is_restoration:\n",
        "            deg = calculate_restoration_potential(recent_arr, baseline_arr, nodatavalue)\n",
        "        else:\n",
        "            result = calculate_forest_disturbance_components(recent_arr, baseline_arr, nodatavalue, has_deforestation)\n",
        "            if has_deforestation: defor, deg = result\n",
        "            else: deg = result\n",
        "        # Interval pairs: apply actual year mask to degradation\n",
        "        if is_interval:\n",
        "            deg = np.where(interval_mask_arr == nodatavalue, nodatavalue, deg)\n",
        "        if has_deforestation:\n",
        "            export_array_as_tif(round_array(defor, mean_precision), output_paths['defor'], template=template_path)\n",
        "        export_array_as_tif(round_array(deg, mean_precision), output_paths['deg'], template=template_path)\n",
        "\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Forest disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "print(\"All forest disturbance calculations complete.\\n\")"
      ],
      "metadata": {
        "id": "dpgsXhrX8rZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81OCJi98NDwj"
      },
      "source": [
        "# Intactness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJyxcVvLqCdW"
      },
      "source": [
        "## Percentage loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsend_eeNIll"
      },
      "outputs": [],
      "source": [
        "# Intactness is measured as relative percentage loss of AGBD within an area of interest\n",
        "\n",
        "# Build dictionary of degradation rasters and their corresponding baseline predictions.\n",
        "# Degradation: {year}_degradation_since_{baseline}\n",
        "# Baseline: {year}_no_disturbance_since_oldgrowth or {year}_no_disturbance_since_{year}\n",
        "\n",
        "intactness_pairs = {}\n",
        "\n",
        "for deg_file in os.listdir(disturbance_dir):\n",
        "    is_mean = 'mean__' in deg_file\n",
        "    if source_dir == uncertainty_dir and not is_mean: continue\n",
        "    if '_degradation_since_' not in deg_file: continue\n",
        "    deg_scenario = deg_file.split('__')[1 if is_mean else 0]\n",
        "    year, since = deg_scenario.split('_degradation_since_')\n",
        "    baseline_scenario = f\"{year}_no_disturbance_since_oldgrowth\" if since == 'oldgrowth' else f\"{year}_no_disturbance_since_{since}\"\n",
        "    baseline_file = f\"{'mean__' if is_mean else ''}{baseline_scenario}__{selected_model}.tif\"\n",
        "    if baseline_file in os.listdir(predictions_dir):\n",
        "        intactness_pairs[deg_scenario] = baseline_scenario\n",
        "print(\"intactness_pairs = {\")\n",
        "for deg, baseline in sorted(intactness_pairs.items()):\n",
        "    print(f\"    '{deg}': '{baseline}',\")\n",
        "print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87Q6aMSdNIye"
      },
      "outputs": [],
      "source": [
        "intactness_pairs = {\n",
        "    '2021_degradation_since_1993': '2021_no_disturbance_since_1993',\n",
        "    '2021_degradation_since_oldgrowth': '2021_no_disturbance_since_oldgrowth',\n",
        "    '2024_degradation_since_1996': '2024_no_disturbance_since_1996',\n",
        "    '2024_degradation_since_oldgrowth': '2024_no_disturbance_since_oldgrowth',\n",
        "}\n",
        "\n",
        "percentage_loss_precision = 0\n",
        "\n",
        "for deg_scenario, baseline_scenario in intactness_pairs.items():\n",
        "    year = deg_scenario.split('_')[0]\n",
        "    base_name = f\"{baseline_scenario}__{deg_scenario}\"\n",
        "    intactness_subdir = join(intactness_dir, base_name)\n",
        "    makedirs(intactness_subdir, exist_ok=True)\n",
        "    percentage_filename = f\"percentage_change__{base_name}__{selected_model}.tif\"\n",
        "    percentage_path = join(intactness_subdir, percentage_filename)\n",
        "    if exists(percentage_path):\n",
        "        print(f\"{percentage_filename} already exists.\")\n",
        "        continue\n",
        "\n",
        "    # Define input paths\n",
        "    prefix = \"mean__\" if source_dir == uncertainty_dir else \"\"\n",
        "    baseline_path = join(predictions_dir, f\"{prefix}{baseline_scenario}__{selected_model}.tif\")\n",
        "    deg_path = join(disturbance_dir, f\"{prefix}{deg_scenario}__{selected_model}.tif\")\n",
        "    mask_path = join(scenario_masks_dir, f\"{year}.tif\")\n",
        "\n",
        "    # Load arrays\n",
        "    ds = gdal.Open(baseline_path)\n",
        "    baseline_arr = ds.ReadAsArray()\n",
        "    ds = gdal.Open(deg_path)\n",
        "    deg_arr = ds.ReadAsArray()\n",
        "    ds = gdal.Open(mask_path)\n",
        "    mask_arr = ds.ReadAsArray()\n",
        "    ds = None\n",
        "\n",
        "    # Calculate percentage: degradation / baseline * 100\n",
        "    percentage_arr = np.full_like(baseline_arr, nodatavalue, dtype=np.float64)\n",
        "    valid = (mask_arr != nodatavalue) & (baseline_arr != nodatavalue) & (deg_arr != nodatavalue)\n",
        "    nonzero = valid & (baseline_arr != 0)\n",
        "    percentage_arr[valid & (baseline_arr == 0)] = 0\n",
        "    percentage_arr[nonzero] = (deg_arr[nonzero] / baseline_arr[nonzero]) * 100\n",
        "    percentage_arr = np.round(percentage_arr, percentage_loss_precision)\n",
        "    export_array_as_tif(percentage_arr, percentage_path, template=baseline_path)\n",
        "    print(f\"{percentage_filename} exported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ugXALeIqLkt"
      },
      "source": [
        "## Quantiles (relative intactness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwXRZTew1r39"
      },
      "outputs": [],
      "source": [
        "# Use additional polygons for masking relative intactness quantiles\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "\n",
        "# Select baseline / disturbance pairs to measure relative intactness\n",
        "print(\"baseline_disturbance_pairs = [\")\n",
        "for dir in os.listdir(intactness_dir):\n",
        "  print(f\"'{dir}',\")\n",
        "print(\"]\\n\")\n",
        "\n",
        "# Select polygons to mask and calculate quantiles\n",
        "print(\"mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"'{polygon}',\")\n",
        "print(None)\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7-X8cXopVo3"
      },
      "outputs": [],
      "source": [
        "baseline_disturbance_pairs = [\n",
        "'2021_no_disturbance_since_1993__2021_degradation_since_1993',\n",
        "'2021_no_disturbance_since_oldgrowth__2021_degradation_since_oldgrowth',\n",
        "'2024_no_disturbance_since_1996__2024_degradation_since_1996',\n",
        "'2024_no_disturbance_since_oldgrowth__2024_degradation_since_oldgrowth',\n",
        "]\n",
        "\n",
        "mask_polygons = [\n",
        "# 'project_area.gpkg',\n",
        "# 'peninsular_malaysia.gpkg',\n",
        "# 'lu_yong.gpkg',\n",
        "# 'lu_yong_lipis.gpkg',\n",
        "# 'lu_berkelah_jerantut.gpkg',\n",
        "# 'lu_tekai_tembeling.gpkg',\n",
        "# 'lu_ais.gpkg',\n",
        "# 'lu_pa_taman_negara_krau.gpkg',\n",
        "# 'lu_tekam.gpkg',\n",
        "# 'lu_berkelah_temerloh.gpkg',\n",
        "# 'lu_remen_chereh.gpkg',\n",
        "# 'lu_berkelah_kuantan.gpkg',\n",
        "'forest_reserves.gpkg',\n",
        "'gedi_area.gpkg',\n",
        "# None\n",
        "]\n",
        "\n",
        "# Convert nodata values inside the mask to a score of 0 (representing non-forest areas)\n",
        "# Otherwise both non-forest and masked areas will be nodatavalue\n",
        "convert_non_forest_nodatavalue_to_0 = True\n",
        "\n",
        "# Define top score for intactness rating (e.g. 10 for 1 - 10 scale)\n",
        "top_score = 10\n",
        "\n",
        "# Calculate actual number of quantiles for non-zero values\n",
        "num_quantiles = top_score - 1\n",
        "\n",
        "print(f\"Calculating {num_quantiles} quantiles for negative percentage change (scores 1-{num_quantiles}), with score {top_score} reserved for 0% change.\\n\")\n",
        "\n",
        "# Create polygon mask array using template tif\n",
        "template = gdal.Open(template_tif_path)\n",
        "template_array = template.ReadAsArray()\n",
        "template = None\n",
        "polygon_mask_array = np.ones_like(template_array, dtype=bool)\n",
        "\n",
        "polygon_masks = {}\n",
        "for mask_polygon in mask_polygons:\n",
        "  if mask_polygon is not None:\n",
        "    # Create an inverse project area path for masking\n",
        "    template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "    inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "    if not exists(inverse_polygon_path):\n",
        "      polygon_path = join(polygons_dir, mask_polygon)\n",
        "      template_polygon = gpd.read_file(template_polygon_path)\n",
        "      polygon_read = gpd.read_file(polygon_path)\n",
        "      polygon_crs = polygon_read.crs.to_epsg()\n",
        "      inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "      inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "      inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "      print(f\"An inverse masking polygon for {mask_polygon} has been created in {polygons_dir}.\")\n",
        "    else: print(f\"An inverse masking polygon for {mask_polygon} already exists.\")\n",
        "\n",
        "    # Create and store individual mask for this polygon\n",
        "    print(f\"Creating polygon mask for {mask_polygon}.\")\n",
        "    inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "    temp_mask_path = join(intactness_dir, f\"temp_mask_{mask_polygon[:-5]}.tif\")\n",
        "    copyfile(template_tif_path, temp_mask_path)\n",
        "    burn_polygon_to_raster(temp_mask_path, inverse_polygon_path, fixed_value=nodatavalue, all_touched=False)\n",
        "    temp_mask = gdal.Open(temp_mask_path)\n",
        "    temp_mask_array = temp_mask.ReadAsArray()\n",
        "    temp_mask = None\n",
        "    del temp_mask\n",
        "    individual_mask = np.ones_like(template_array, dtype=bool)\n",
        "    individual_mask[temp_mask_array == nodatavalue] = False\n",
        "    polygon_masks[mask_polygon] = individual_mask\n",
        "    os.remove(temp_mask_path)\n",
        "\n",
        "for base_dist_name in baseline_disturbance_pairs:\n",
        "  intactness_baseline_dist_dir = join(intactness_dir, base_dist_name)\n",
        "  percentage_filename = f\"percentage_change__{base_dist_name}__{selected_model}\"\n",
        "  percentage_path = join(intactness_baseline_dist_dir, f\"{percentage_filename}.tif\")\n",
        "\n",
        "  for mask_polygon in mask_polygons:\n",
        "\n",
        "    if mask_polygon is not None:\n",
        "      # Copy the percentage raster for potential masking\n",
        "      percentage_masked_filename = f\"{percentage_filename}__masked_{mask_polygon[:-5]}.tif\"\n",
        "      percentage_masked_path = join(intactness_baseline_dist_dir, percentage_masked_filename)\n",
        "      if not exists(percentage_masked_path):\n",
        "        print(f\"Copying {percentage_filename} for masking...\")\n",
        "        copyfile(percentage_path, percentage_masked_path)\n",
        "        print(f\"Masking {percentage_filename} with {mask_polygon}...\")\n",
        "        inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "        burn_polygon_to_raster(percentage_masked_path, inverse_polygon_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        # Recompress the prediction after burning the polygon masks\n",
        "        percentage_masked = gdal.Open(percentage_masked_path)\n",
        "        percentage_masked_array = percentage_masked.ReadAsArray()\n",
        "        del percentage_masked\n",
        "        percentage_masked = None\n",
        "        export_array_as_tif(percentage_masked_array, percentage_masked_path)\n",
        "        print(f\"{percentage_filename} masked.\")\n",
        "      else: print(f\"{percentage_masked_filename} already exists.\")\n",
        "\n",
        "    # Define paths and arrays\n",
        "    if mask_polygon is None: relative_intactness_name = f'intactness__{top_score}_quantiles__{base_dist_name}__{selected_model}'\n",
        "    else: relative_intactness_name = f'intactness__{mask_polygon[:-5]}_{top_score}_quantiles__{base_dist_name}__{selected_model}'\n",
        "    relative_intactness_path = join(intactness_baseline_dist_dir, f'{relative_intactness_name}.tif')\n",
        "    if not exists(relative_intactness_path):\n",
        "      # Always track originally nodata pixels from the original percentage raster\n",
        "      original_percentage = gdal.Open(percentage_path)\n",
        "      original_percentage_array = original_percentage.ReadAsArray()\n",
        "      original_percentage = None\n",
        "      originally_nodata_mask = original_percentage_array == nodatavalue\n",
        "\n",
        "      # Apply polygon masking to percentage array using pre-created mask\n",
        "      if mask_polygon is None:\n",
        "        percentage_array = original_percentage_array.copy()\n",
        "      else:\n",
        "        percentage_array = original_percentage_array.copy()\n",
        "        percentage_array[~polygon_masks[mask_polygon]] = nodatavalue\n",
        "\n",
        "      # Capture original data for histogram before conversions\n",
        "      original_valid_elements = percentage_array[percentage_array != nodatavalue]\n",
        "\n",
        "      relative_intactness_array = np.full_like(percentage_array, nodatavalue, dtype=np.int16)\n",
        "\n",
        "      # Set all values above 0 to 0\n",
        "      percentage_array[percentage_array > 0] = 0\n",
        "\n",
        "      # Separate valid and invalid (nodatavalue) elements\n",
        "      valid_elements = percentage_array[percentage_array != nodatavalue]\n",
        "\n",
        "      # Separate zero and non-zero values, excluding originally nodata pixels from quantile calculation\n",
        "      zero_elements = percentage_array == 0\n",
        "      # Exclude pixels that were originally nodata from quantile calculation\n",
        "      quantile_mask = (percentage_array != nodatavalue) & (~originally_nodata_mask) & (percentage_array != 0)\n",
        "      non_zero_valid_elements = percentage_array[quantile_mask]\n",
        "\n",
        "      # Calculate quantiles for non-zero valid elements only\n",
        "      quantiles = np.percentile(non_zero_valid_elements, np.linspace(0, 100, num_quantiles + 1)[1:-1]) if len(non_zero_valid_elements) > 0 else []\n",
        "\n",
        "      # Assign scores 1 to num_quantiles for non-zero values\n",
        "      for i in range(1, num_quantiles + 1):\n",
        "          lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "          upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "          relative_intactness_array[\n",
        "              (percentage_array > lower_bound) & (percentage_array <= upper_bound) &\n",
        "              (percentage_array != 0) & (percentage_array != nodatavalue)] = i\n",
        "\n",
        "      # Set all zero values to top score\n",
        "      relative_intactness_array[zero_elements] = top_score\n",
        "\n",
        "      # Set areas outside polygon to nodatavalue using pre-created mask\n",
        "      if mask_polygon is not None:\n",
        "        relative_intactness_array[~polygon_masks[mask_polygon]] = nodatavalue\n",
        "\n",
        "      # Convert non-forest areas inside polygon to 0\n",
        "      if convert_non_forest_nodatavalue_to_0:\n",
        "        if mask_polygon is None:\n",
        "          non_forest_inside_polygon = originally_nodata_mask\n",
        "        else:\n",
        "          non_forest_inside_polygon = originally_nodata_mask & polygon_masks[mask_polygon]\n",
        "        relative_intactness_array[non_forest_inside_polygon] = 0\n",
        "\n",
        "      export_array_as_tif(relative_intactness_array, relative_intactness_path)\n",
        "\n",
        "      # Prepare data for CSV: Collect lower and upper bounds for each category\n",
        "      ranges_data = {'Score': [], 'Lower_Bound': [], 'Upper_Bound': []}\n",
        "\n",
        "      # Add ranges for scores 1 to num_quantiles (non-zero values)\n",
        "      for i in range(1, num_quantiles + 1):\n",
        "          lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "          if i == num_quantiles:\n",
        "            upper_bound = -0.000000001\n",
        "          else: upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "          ranges_data['Score'].append(i)\n",
        "          ranges_data['Lower_Bound'].append(lower_bound)\n",
        "          ranges_data['Upper_Bound'].append(upper_bound)\n",
        "\n",
        "      # Add entry for top score (value of 0)\n",
        "      ranges_data['Score'].append(top_score)\n",
        "      ranges_data['Lower_Bound'].append(0)\n",
        "      ranges_data['Upper_Bound'].append(0)\n",
        "\n",
        "      # Create DataFrame and save to CSV\n",
        "      relative_intactness_df = pd.DataFrame(ranges_data)\n",
        "      relative_intactness_csv_path = os.path.join(intactness_baseline_dist_dir, f'{relative_intactness_name}.csv')\n",
        "      relative_intactness_df.to_csv(relative_intactness_csv_path, index=False)\n",
        "\n",
        "      # Generate and save histogram for converted data as .png\n",
        "      histogram_path = join(intactness_baseline_dist_dir, f'{relative_intactness_name}.png')\n",
        "      plt.figure()\n",
        "      counts, bins, patches = plt.hist(original_valid_elements.flatten(), bins=100)\n",
        "\n",
        "      # Count how many values became 0 after conversions\n",
        "      zero_count_after_conversion = np.sum(valid_elements == 0)\n",
        "\n",
        "      # Find the zero bin and set its frequency to 0\n",
        "      zero_idx = next((i for i, (l, r) in enumerate(zip(bins[:-1], bins[1:])) if l <= 0 <= r), None)\n",
        "      if zero_idx is not None:\n",
        "          counts[zero_idx] = 0\n",
        "          plt.clf()\n",
        "          plt.bar(bins[:-1], counts, width=np.diff(bins), align='edge')\n",
        "          x_center = (bins.min() + bins.max()) / 2\n",
        "          y_max = max(counts)\n",
        "          plt.text(x_center, y_max * 0.9,\n",
        "                  f'0 value frequency = {zero_count_after_conversion:,}',\n",
        "                  ha='center', va='center', fontweight='bold',\n",
        "                  bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9))\n",
        "      plt.title(f'{relative_intactness_name} Histogram')\n",
        "      plt.xlabel('Value')\n",
        "      plt.ylabel('Frequency')\n",
        "      plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(histogram_path)\n",
        "      plt.close()\n",
        "\n",
        "    else: print(f\"{relative_intactness_name} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-7ctfgdoF2"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "qcuArY00N9Mc"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QJ8jLRKRSfgb",
        "ECHaTfrs0n7i"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}