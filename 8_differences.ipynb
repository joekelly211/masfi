{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/8_differences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8jLRKRSfgb"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkvV803eSUQY"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEpdIjOwSiB1"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!apt-get install -y gdal-bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQDSsvGeSi4u"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "from os import makedirs\n",
        "from os.path import join, exists\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "import re\n",
        "from shutil import copyfile\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxgT3K1Sjum"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "differences_dir = join(base_dir, \"8_differences\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(differences_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96FAfeyooMLz"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = vector = None\n",
        "    try:\n",
        "        raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "        vector = ogr.Open(polygon_path)\n",
        "        if not raster or not vector:\n",
        "            raise ValueError(\"Cannot open input files\")\n",
        "        layer = vector.GetLayer()\n",
        "        options = [\"ALL_TOUCHED=TRUE\"] if all_touched else []\n",
        "        if fixed:\n",
        "            gdal.RasterizeLayer(raster, [1], layer, burn_values=[fixed_value], options=options)\n",
        "        else:\n",
        "            attr_name = column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()\n",
        "            options.append(f\"ATTRIBUTE={attr_name}\")\n",
        "            gdal.RasterizeLayer(raster, [1], layer, options=options)\n",
        "    finally:\n",
        "        if raster: raster.FlushCache()\n",
        "        raster = vector = None\n",
        "\n",
        "# Global function: read raster as array\n",
        "def read_raster_as_array(path):\n",
        "    ds = gdal.Open(path)\n",
        "    arr = ds.ReadAsArray()\n",
        "    ds = None\n",
        "    return arr\n",
        "\n",
        "# Global function: round array\n",
        "def round_array(arr, precision):\n",
        "    rounded = np.round(arr, precision)\n",
        "    return rounded.astype(np.int16) if precision == 0 else rounded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_OkHaUkTCyB"
      },
      "source": [
        "# Select source and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mB0XrPNYU_I"
      },
      "outputs": [],
      "source": [
        "# Select if to source predictions from scenarios_dir or uncertainty_dir\n",
        "# If available, uncertainty_dir should be selected so that uncertainty can\n",
        "# be propagated and scenario 'mean' iteration values used.\n",
        "\n",
        "source_dir = uncertainty_dir\n",
        "# source_dir = scenarios_dir\n",
        "\n",
        "print(f\"{source_dir.split('/')[-1]} has been selected as the source directory for predictions\")\n",
        "print(\"to calculate disturbance, intactness and restoration.\\n\")\n",
        "\n",
        "# If uncertainty selected, check it exists\n",
        "if not exists(uncertainty_dir) and source_dir == uncertainty_dir:\n",
        "  print(\"The uncertainty directory does not yet exist. Defaulting to scenarios directory.\")\n",
        "  source_dir = scenarios_dir\n",
        "\n",
        "source_dir_name = f\"{source_dir.split('_')[-1]}_dir\"\n",
        "\n",
        "# Select the model\n",
        "for subdir in os.listdir(source_dir):\n",
        "  if 'scenario_masks' not in subdir:\n",
        "    print(f\"selected_model = '{subdir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbXY6iP_qjYc"
      },
      "outputs": [],
      "source": [
        "selected_model = 'agbd_251203_161707'\n",
        "\n",
        "selected_model_dir = join(source_dir, selected_model)\n",
        "if source_dir == scenarios_dir: predictions_dir = join(selected_model_dir, 'scenario_predictions')\n",
        "if source_dir == uncertainty_dir:\n",
        "  predictions_dir = join(selected_model_dir, 'uncertainty_predictions')\n",
        "  scenario_iterations_dir = join(selected_model_dir, 'scenario_iterations')\n",
        "\n",
        "scenario_masks_dir = join(scenarios_dir, selected_model, \"scenario_masks\")\n",
        "\n",
        "# Check predictions exist to calculate differences\n",
        "if len(os.listdir(predictions_dir)) < 2: print(f\"At least 2 predictions must exist in {source_dir} to calculate differences.\")\n",
        "else:\n",
        "  model_differences_dir = join(differences_dir, f\"{selected_model}_{source_dir_name}\")\n",
        "  disturbance_dir = join(model_differences_dir, 'disturbance')\n",
        "  intactness_dir = join(model_differences_dir, 'intactness')\n",
        "  restoration_dir = join(model_differences_dir, 'restoration')\n",
        "  makedirs(model_differences_dir, exist_ok=True)\n",
        "  makedirs(disturbance_dir, exist_ok=True)\n",
        "  makedirs(intactness_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECHaTfrs0n7i"
      },
      "source": [
        "# Disturbance and restoration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgPVQgfTpy21"
      },
      "source": [
        "## Define type and period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUaYCCymYI2Y"
      },
      "outputs": [],
      "source": [
        "# Forest disturbance and restoration measured as absolute AGBD change.\n",
        "# This block builds dictionaries of options based on available files.\n",
        "# Disturbance dictionaries output a tuple (deforestation, degradation) or degradation-only strings.\n",
        "# Restoration dictionary outputs a tuple (reforestation, recovery) or recovery-only strings.\n",
        "# Disturbance: Both deforestation and degradation.\n",
        "# - Deforestation: pixel changes from forest (data) to non-forest (nodata)\n",
        "# - Degradation: pixel remains forest but potential AGBD loss\n",
        "# Restoration: Both reforestation and recovery.\n",
        "# - Reforestation: pixel changes from non-forest (nodata) to forest (data)\n",
        "# - Recovery: pixel was already forest but can have potential AGBD gains\n",
        "\n",
        "# Dictionaries:\n",
        "# 1. disturbance_since_dictionary: actual scenario vs no_disturbance_since alternate scenarios\n",
        "# 2. degradation_interval_dictionary: multi-year intervals between alternate scenarios\n",
        "# 3. degradation_single_year_dictionary: consecutive alternate scenarios for single-year effects\n",
        "# 4. disturbance_area_dictionary: polygon-based alternate scenarios\n",
        "# 5. restoration_potential_dictionary:\n",
        "# - Restoration and reforestation potential uses no_disturbance_since alternate scenarios\n",
        "# - Recovery potential with edge effects uses recovery alternate scenarios,\n",
        "# accounting for actual scenario forest edge effects.\n",
        "\n",
        "# Extract all available scenarios from scenarios predictions directory\n",
        "if source_dir == scenarios_dir:\n",
        "    scenarios = set()\n",
        "    for file in os.listdir(predictions_dir):\n",
        "        scenarios.add(file.split(\"__\")[0])\n",
        "# Or extract all available scenarios from uncertainty predictions directory\n",
        "if source_dir == uncertainty_dir:\n",
        "    prediction_stats = {}\n",
        "    for file in os.listdir(predictions_dir):\n",
        "        parts = file.split(\"__\")\n",
        "        if len(parts) >= 2:\n",
        "            stat, scenario = parts[0], parts[1]\n",
        "            if scenario not in prediction_stats:\n",
        "                prediction_stats[scenario] = set()\n",
        "            prediction_stats[scenario].add(stat)\n",
        "    scenarios = {scenario for scenario, stats in prediction_stats.items()\n",
        "                 if 'uncertainty' in stats and 'mean' in stats}\n",
        "# Categorise years from scenarios\n",
        "years = set()\n",
        "plain_years = set()\n",
        "for s in scenarios:\n",
        "    if s.isdigit():\n",
        "        years.add(int(s))\n",
        "        plain_years.add(int(s))\n",
        "    elif \"_no_disturbance_since_\" in s:\n",
        "        year = s.split(\"_\")[0]\n",
        "        if year.isdigit():\n",
        "            years.add(int(year))\n",
        "        since_part = s.split(\"_since_\")[1]\n",
        "        if since_part.isdigit():\n",
        "            years.add(int(since_part) - 1)\n",
        "years_sorted = sorted(list(years))\n",
        "\n",
        "\n",
        "# 1. Disturbance since dictionary\n",
        "# Compares actual AGBD against no_disturbance alternate scenario.\n",
        "# Keys: (actual_year, alternate_scenario_scenario)\n",
        "# Values: (deforestation_name, degradation_name) for oldgrowth, degradation_name otherwise\n",
        "print(\"disturbance_since_dictionary = {\\n\")\n",
        "disturbance_since_dictionary = {}\n",
        "for year_a in years_sorted:\n",
        "    a_str = str(year_a)\n",
        "    if year_a not in plain_years: continue\n",
        "    # Oldgrowth entry first\n",
        "    alternate_scenario_oldgrowth = f\"{a_str}_no_disturbance_since_oldgrowth\"\n",
        "    if alternate_scenario_oldgrowth in scenarios:\n",
        "        print(f\"  # Disturbance in {a_str} caused by events since oldgrowth\")\n",
        "        print(f\"  ('{a_str}', '{alternate_scenario_oldgrowth}'):\")\n",
        "        print(f\"    ('{a_str}_deforestation_since_oldgrowth',\")\n",
        "        print(f\"     '{a_str}_degradation_since_oldgrowth'),\")\n",
        "        print(\"\")\n",
        "        disturbance_since_dictionary[(a_str, alternate_scenario_oldgrowth)] = (\n",
        "            f\"{a_str}_deforestation_since_oldgrowth\",\n",
        "            f\"{a_str}_degradation_since_oldgrowth\")\n",
        "    # Year-based entries in chronological order\n",
        "    for year_b in years_sorted:\n",
        "        if year_a <= year_b: continue\n",
        "        b_plus1 = str(year_b + 1)\n",
        "        alternate_scenario_scenario = f\"{a_str}_no_disturbance_since_{b_plus1}\"\n",
        "        if alternate_scenario_scenario in scenarios:\n",
        "            print(f\"  # Disturbance in {a_str} caused by events since {b_plus1}\")\n",
        "            print(f\"  ('{a_str}', '{alternate_scenario_scenario}'):\")\n",
        "            print(f\"    '{a_str}_degradation_since_{b_plus1}',\")\n",
        "            print(\"\")\n",
        "            disturbance_since_dictionary[(a_str, alternate_scenario_scenario)] = \\\n",
        "                f\"{a_str}_degradation_since_{b_plus1}\"\n",
        "print(\"}\\n\")\n",
        "\n",
        "\n",
        "# 2. Degradation interval dictionary\n",
        "# Calculates degradation over a multi-year interval from an alternate scenario baseline\n",
        "# (oldgrowth or earliest available year) to an actual year.\n",
        "# Single-year intervals handled by degradation_single_year_dictionary.\n",
        "# Keys: (alternate_scenario_recent, alternate_scenario_baseline) alternate scenario pairs\n",
        "# Values: degradation_name\n",
        "print(\"degradation_interval_dictionary = {\\n\")\n",
        "degradation_interval_dictionary = {}\n",
        "for y in years_sorted:\n",
        "    y_str = str(y)\n",
        "    if y not in plain_years: continue\n",
        "    # Collect available no_disturbance alternate scenarios\n",
        "    alternate_scenario_dist = {}\n",
        "    if f\"{y_str}_no_disturbance_since_oldgrowth\" in scenarios:\n",
        "        alternate_scenario_dist[\"oldgrowth\"] = f\"{y_str}_no_disturbance_since_oldgrowth\"\n",
        "    for since_y in years_sorted:\n",
        "        if since_y >= y:\n",
        "            continue\n",
        "        sp = str(since_y + 1)\n",
        "        dist_alternate_scenario = f\"{y_str}_no_disturbance_since_{sp}\"\n",
        "        if dist_alternate_scenario in scenarios:\n",
        "            alternate_scenario_dist[sp] = dist_alternate_scenario\n",
        "    if len(alternate_scenario_dist) < 2: continue\n",
        "    since_parts = sorted([k for k in alternate_scenario_dist if k != \"oldgrowth\"], key=int)\n",
        "    # Check if any multi-year intervals exist for this year\n",
        "    has_entries = False\n",
        "    if \"oldgrowth\" in alternate_scenario_dist and since_parts:\n",
        "        has_entries = True\n",
        "    if len(since_parts) > 1:\n",
        "        earliest = min(since_parts, key=int)\n",
        "        for sp in since_parts:\n",
        "            if sp != earliest and int(sp) - int(earliest) > 1:\n",
        "                has_entries = True\n",
        "                break\n",
        "    if not has_entries: continue\n",
        "    # Oldgrowth alternate scenario pairs first\n",
        "    if \"oldgrowth\" in alternate_scenario_dist:\n",
        "        for sp in since_parts:\n",
        "            recent_year = str(int(sp) - 1)\n",
        "            print(f\"  # Degradation in {y_str} from events since oldgrowth to {recent_year}\")\n",
        "            print(f\"  ('{alternate_scenario_dist[sp]}', '{alternate_scenario_dist['oldgrowth']}'):\")\n",
        "            print(f\"    '{y_str}_degradation_from_oldgrowth_to_{recent_year}',\")\n",
        "            print(\"\")\n",
        "            degradation_interval_dictionary[(alternate_scenario_dist[sp], alternate_scenario_dist[\"oldgrowth\"])] = \\\n",
        "                f\"{y_str}_degradation_from_oldgrowth_to_{recent_year}\"\n",
        "    # Earliest year alternate scenario pairs (excluding single-year differences)\n",
        "    if len(since_parts) > 1:\n",
        "        earliest = min(since_parts, key=int)\n",
        "        for sp in since_parts:\n",
        "            if sp != earliest and int(sp) - int(earliest) > 1:\n",
        "                recent_year = str(int(sp) - 1)\n",
        "                print(f\"  # Degradation in {y_str} from events in {earliest} to {recent_year}\")\n",
        "                print(f\"  ('{alternate_scenario_dist[sp]}', '{alternate_scenario_dist[earliest]}'):\")\n",
        "                print(f\"    '{y_str}_degradation_from_{earliest}_to_{recent_year}',\")\n",
        "                print(\"\")\n",
        "                degradation_interval_dictionary[(alternate_scenario_dist[sp], alternate_scenario_dist[earliest])] = \\\n",
        "                    f\"{y_str}_degradation_from_{earliest}_to_{recent_year}\"\n",
        "print(\"}\\n\")\n",
        "\n",
        "\n",
        "# 3. Degradation single year dictionary\n",
        "# Calculates degradation effect from a single year using consecutive alternate scenarios.\n",
        "# Keys: (alternate_scenario_next, alternate_scenario_current) or (actual, alternate_scenario_current) for same-year\n",
        "# Values: degradation_name\n",
        "print(\"degradation_single_year_dictionary = {\\n\")\n",
        "degradation_single_year_dictionary = {}\n",
        "for y in years_sorted:\n",
        "    y_str = str(y)\n",
        "    if y not in plain_years: continue\n",
        "    # Collect available no_disturbance alternate scenarios\n",
        "    alternate_scenario_dist = {}\n",
        "    for since_y in years_sorted:\n",
        "        if since_y >= y: continue\n",
        "        sp = str(since_y + 1)\n",
        "        dist_alternate_scenario = f\"{y_str}_no_disturbance_since_{sp}\"\n",
        "        if dist_alternate_scenario in scenarios:\n",
        "            alternate_scenario_dist[int(sp)] = dist_alternate_scenario\n",
        "    if not alternate_scenario_dist: continue\n",
        "    since_years = sorted(alternate_scenario_dist.keys())\n",
        "    # Consecutive year pairs\n",
        "    for i in range(len(since_years) - 1):\n",
        "        current_year = since_years[i]\n",
        "        next_year = since_years[i + 1]\n",
        "        if next_year == current_year + 1:\n",
        "            effect_year = current_year\n",
        "            print(f\"  # Degradation in {y_str} from events in {effect_year}\")\n",
        "            print(f\"  ('{alternate_scenario_dist[next_year]}', '{alternate_scenario_dist[current_year]}'):\")\n",
        "            print(f\"    '{y_str}_effect_of_degradation_in_{effect_year}',\")\n",
        "            print(\"\")\n",
        "            degradation_single_year_dictionary[(alternate_scenario_dist[next_year], alternate_scenario_dist[current_year])] = \\\n",
        "                f\"{y_str}_effect_of_degradation_in_{effect_year}\"\n",
        "    # Same-year case (actual vs no_disturbance_since_year)\n",
        "    max_since = max(since_years)\n",
        "    if max_since == y:\n",
        "        print(f\"  # Degradation in {y_str} from events in {y}\")\n",
        "        print(f\"  ('{y_str}', '{alternate_scenario_dist[max_since]}'):\")\n",
        "        print(f\"    '{y_str}_effect_of_degradation_in_{y}',\")\n",
        "        print(\"\")\n",
        "        degradation_single_year_dictionary[(y_str, alternate_scenario_dist[max_since])] = \\\n",
        "            f\"{y_str}_effect_of_degradation_in_{y}\"\n",
        "print(\"}\\n\")\n",
        "\n",
        "\n",
        "# 4. Disturbance area dictionary\n",
        "# Calculates disturbance from polygon-based alternate scenarios.\n",
        "# Unlike historical alternate scenarios, these represent alternate scenarios of potential\n",
        "# loss, so AGBD loss from deforestation is calculated.\n",
        "# Keys: (alternate_scenario, actual_year)\n",
        "# Values: (deforestation_name, degradation_name)\n",
        "print(\"disturbance_area_dictionary = {\\n\")\n",
        "disturbance_area_dictionary = {}\n",
        "polygon_names = set()\n",
        "if os.path.exists(polygons_dir):\n",
        "    for file in os.listdir(polygons_dir):\n",
        "        if file.endswith('.gpkg'):\n",
        "            polygon_names.add(file[:-5])\n",
        "area_based_entries = []\n",
        "for scenario in scenarios:\n",
        "    parts = scenario.split('_')\n",
        "    # Check for deforestation scenarios (ends with \"Xm_degradation_buffer\")\n",
        "    if len(parts) >= 5 and parts[-1] == 'buffer' and parts[-2] == 'degradation' and parts[-3].endswith('m'):\n",
        "        alt_year, year_affix, dist_type = parts[0], parts[-4], parts[-5]\n",
        "        polygon_name = '_'.join(parts[1:-5])\n",
        "        if polygon_name in polygon_names and dist_type == 'deforestation':\n",
        "            output_base = f\"{alt_year}_deforestation_of_{polygon_name}_{year_affix}\"\n",
        "            area_based_entries.append((scenario, alt_year, output_base))\n",
        "    # Check for degradation scenarios (ends with \"degradation_YYYY\")\n",
        "    elif len(parts) >= 3 and parts[-2] == 'degradation' and parts[-1].isdigit() and len(parts[-1]) == 4:\n",
        "        alt_year, year_affix = parts[0], parts[-1]\n",
        "        polygon_name = '_'.join(parts[1:-2])\n",
        "        if polygon_name in polygon_names:\n",
        "            output_base = f\"{alt_year}_degradation_of_{polygon_name}_{year_affix}\"\n",
        "            area_based_entries.append((scenario, alt_year, output_base))\n",
        "if area_based_entries:\n",
        "    for scenario, alt_year, output_base in sorted(area_based_entries):\n",
        "        print(f\"  # Area-based disturbance: {output_base}\")\n",
        "        print(f\"  ('{scenario}', '{alt_year}'):\")\n",
        "        print(f\"    ('{output_base}_deforestation',\")\n",
        "        print(f\"     '{output_base}_degradation'),\")\n",
        "        print(\"\")\n",
        "        disturbance_area_dictionary[(scenario, alt_year)] = (\n",
        "            f\"{output_base}_deforestation\",\n",
        "            f\"{output_base}_degradation\")\n",
        "\n",
        "print(\"}\\n\")\n",
        "\n",
        "\n",
        "# 5. Restoration potential dictionary\n",
        "# Calculates potential AGBD gain from restoration to oldgrowth state.\n",
        "# Unlike disturbance dictionaries, output is positive (gain not loss).\n",
        "# Recovery with edge effects: gain in existing forest (oldgrowth_recovery scenario).\n",
        "# Restoration potential: total gain including cleared areas (no_disturbance_since_oldgrowth).\n",
        "# Reforestation potential: gain from non-forest to forest transitions only.\n",
        "# Keys: (restoration_scenario, actual_year)\n",
        "# Values: (restoration_name, reforestation_name) for no_disturbance_since, recovery_name otherwise\n",
        "print(\"restoration_potential_dictionary = {\\n\")\n",
        "restoration_potential_dictionary = {}\n",
        "for year_a in years_sorted:\n",
        "    a_str = str(year_a)\n",
        "    if year_a not in plain_years: continue\n",
        "    # Recovery potential with edge effects (existing forest only)\n",
        "    recovery_scenario = f\"{a_str}_oldgrowth_recovery\"\n",
        "    if recovery_scenario in scenarios:\n",
        "        print(f\"  # Recovery potential with edge effects in {a_str}\")\n",
        "        print(f\"  ('{recovery_scenario}', '{a_str}'):\")\n",
        "        print(f\"    '{a_str}_recovery_potential_with_edge_effects',\")\n",
        "        print(\"\")\n",
        "        restoration_potential_dictionary[(recovery_scenario, a_str)] = \\\n",
        "            f\"{a_str}_recovery_potential_with_edge_effects\"\n",
        "    # Restoration and reforestation potential (including cleared areas)\n",
        "    alternate_scenario_oldgrowth = f\"{a_str}_no_disturbance_since_oldgrowth\"\n",
        "    if alternate_scenario_oldgrowth in scenarios:\n",
        "        print(f\"  # Restoration and reforestation potential in {a_str}\")\n",
        "        print(f\"  ('{alternate_scenario_oldgrowth}', '{a_str}'):\")\n",
        "        print(f\"    ('{a_str}_restoration_potential',\")\n",
        "        print(f\"     '{a_str}_reforestation_potential'),\")\n",
        "        print(\"\")\n",
        "        restoration_potential_dictionary[(alternate_scenario_oldgrowth, a_str)] = (\n",
        "            f\"{a_str}_restoration_potential\",\n",
        "            f\"{a_str}_reforestation_potential\")\n",
        "print(\"}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disturbance_since_dictionary = {\n",
        "\n",
        "  # Disturbance in 2021 caused by events since oldgrowth\n",
        "  ('2021', '2021_no_disturbance_since_oldgrowth'):\n",
        "    ('2021_deforestation_since_oldgrowth',\n",
        "     '2021_degradation_since_oldgrowth'),\n",
        "\n",
        "  # Disturbance in 2021 caused by events since 1993\n",
        "  ('2021', '2021_no_disturbance_since_1993'):\n",
        "    '2021_degradation_since_1993',\n",
        "\n",
        "  # Disturbance in 2024 caused by events since oldgrowth\n",
        "  ('2024', '2024_no_disturbance_since_oldgrowth'):\n",
        "    ('2024_deforestation_since_oldgrowth',\n",
        "     '2024_degradation_since_oldgrowth'),\n",
        "\n",
        "  # Disturbance in 2024 caused by events since 1996\n",
        "  ('2024', '2024_no_disturbance_since_1996'):\n",
        "    '2024_degradation_since_1996',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 1997\n",
        "  # ('2024', '2024_no_disturbance_since_1997'):\n",
        "  #   '2024_degradation_since_1997',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 1998\n",
        "  # ('2024', '2024_no_disturbance_since_1998'):\n",
        "  #   '2024_degradation_since_1998',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 1999\n",
        "  # ('2024', '2024_no_disturbance_since_1999'):\n",
        "  #   '2024_degradation_since_1999',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2000\n",
        "  # ('2024', '2024_no_disturbance_since_2000'):\n",
        "  #   '2024_degradation_since_2000',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2001\n",
        "  # ('2024', '2024_no_disturbance_since_2001'):\n",
        "  #   '2024_degradation_since_2001',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2002\n",
        "  # ('2024', '2024_no_disturbance_since_2002'):\n",
        "  #   '2024_degradation_since_2002',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2003\n",
        "  # ('2024', '2024_no_disturbance_since_2003'):\n",
        "  #   '2024_degradation_since_2003',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2004\n",
        "  # ('2024', '2024_no_disturbance_since_2004'):\n",
        "  #   '2024_degradation_since_2004',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2005\n",
        "  # ('2024', '2024_no_disturbance_since_2005'):\n",
        "  #   '2024_degradation_since_2005',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2006\n",
        "  # ('2024', '2024_no_disturbance_since_2006'):\n",
        "  #   '2024_degradation_since_2006',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2007\n",
        "  # ('2024', '2024_no_disturbance_since_2007'):\n",
        "  #   '2024_degradation_since_2007',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2008\n",
        "  # ('2024', '2024_no_disturbance_since_2008'):\n",
        "  #   '2024_degradation_since_2008',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2009\n",
        "  # ('2024', '2024_no_disturbance_since_2009'):\n",
        "  #   '2024_degradation_since_2009',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2010\n",
        "  # ('2024', '2024_no_disturbance_since_2010'):\n",
        "  #   '2024_degradation_since_2010',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2011\n",
        "  # ('2024', '2024_no_disturbance_since_2011'):\n",
        "  #   '2024_degradation_since_2011',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2012\n",
        "  # ('2024', '2024_no_disturbance_since_2012'):\n",
        "  #   '2024_degradation_since_2012',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2013\n",
        "  # ('2024', '2024_no_disturbance_since_2013'):\n",
        "  #   '2024_degradation_since_2013',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2014\n",
        "  # ('2024', '2024_no_disturbance_since_2014'):\n",
        "  #   '2024_degradation_since_2014',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2015\n",
        "  # ('2024', '2024_no_disturbance_since_2015'):\n",
        "  #   '2024_degradation_since_2015',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2016\n",
        "  # ('2024', '2024_no_disturbance_since_2016'):\n",
        "  #   '2024_degradation_since_2016',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2017\n",
        "  # ('2024', '2024_no_disturbance_since_2017'):\n",
        "  #   '2024_degradation_since_2017',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2018\n",
        "  # ('2024', '2024_no_disturbance_since_2018'):\n",
        "  #   '2024_degradation_since_2018',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2019\n",
        "  # ('2024', '2024_no_disturbance_since_2019'):\n",
        "  #   '2024_degradation_since_2019',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2020\n",
        "  # ('2024', '2024_no_disturbance_since_2020'):\n",
        "  #   '2024_degradation_since_2020',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2021\n",
        "  # ('2024', '2024_no_disturbance_since_2021'):\n",
        "  #   '2024_degradation_since_2021',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2022\n",
        "  # ('2024', '2024_no_disturbance_since_2022'):\n",
        "  #   '2024_degradation_since_2022',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2023\n",
        "  # ('2024', '2024_no_disturbance_since_2023'):\n",
        "  #   '2024_degradation_since_2023',\n",
        "\n",
        "  # # Disturbance in 2024 caused by events since 2024\n",
        "  # ('2024', '2024_no_disturbance_since_2024'):\n",
        "  #   '2024_degradation_since_2024',\n",
        "\n",
        "}\n",
        "\n",
        "degradation_interval_dictionary = {\n",
        "\n",
        "  # Degradation in 2021 from events since oldgrowth to 1992\n",
        "  ('2021_no_disturbance_since_1993', '2021_no_disturbance_since_oldgrowth'):\n",
        "    '2021_degradation_from_oldgrowth_to_1992',\n",
        "\n",
        "  # Degradation in 2024 from events since oldgrowth to 1995\n",
        "  ('2024_no_disturbance_since_1996', '2024_no_disturbance_since_oldgrowth'):\n",
        "    '2024_degradation_from_oldgrowth_to_1995',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 1996\n",
        "  # ('2024_no_disturbance_since_1997', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_1996',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 1997\n",
        "  # ('2024_no_disturbance_since_1998', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_1997',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 1998\n",
        "  # ('2024_no_disturbance_since_1999', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_1998',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 1999\n",
        "  # ('2024_no_disturbance_since_2000', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_1999',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2000\n",
        "  # ('2024_no_disturbance_since_2001', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2000',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2001\n",
        "  # ('2024_no_disturbance_since_2002', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2001',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2002\n",
        "  # ('2024_no_disturbance_since_2003', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2002',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2003\n",
        "  # ('2024_no_disturbance_since_2004', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2003',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2004\n",
        "  # ('2024_no_disturbance_since_2005', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2004',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2005\n",
        "  # ('2024_no_disturbance_since_2006', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2005',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2006\n",
        "  # ('2024_no_disturbance_since_2007', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2006',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2007\n",
        "  # ('2024_no_disturbance_since_2008', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2007',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2008\n",
        "  # ('2024_no_disturbance_since_2009', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2008',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2009\n",
        "  # ('2024_no_disturbance_since_2010', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2009',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2010\n",
        "  # ('2024_no_disturbance_since_2011', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2010',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2011\n",
        "  # ('2024_no_disturbance_since_2012', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2011',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2012\n",
        "  # ('2024_no_disturbance_since_2013', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2012',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2013\n",
        "  # ('2024_no_disturbance_since_2014', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2013',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2014\n",
        "  # ('2024_no_disturbance_since_2015', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2014',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2015\n",
        "  # ('2024_no_disturbance_since_2016', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2015',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2016\n",
        "  # ('2024_no_disturbance_since_2017', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2016',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2017\n",
        "  # ('2024_no_disturbance_since_2018', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2017',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2018\n",
        "  # ('2024_no_disturbance_since_2019', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2018',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2019\n",
        "  # ('2024_no_disturbance_since_2020', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2019',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2020\n",
        "  # ('2024_no_disturbance_since_2021', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2020',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2021\n",
        "  # ('2024_no_disturbance_since_2022', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2021',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2022\n",
        "  # ('2024_no_disturbance_since_2023', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2022',\n",
        "\n",
        "  # # Degradation in 2024 from events since oldgrowth to 2023\n",
        "  # ('2024_no_disturbance_since_2024', '2024_no_disturbance_since_oldgrowth'):\n",
        "  #   '2024_degradation_from_oldgrowth_to_2023',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 1997\n",
        "  # ('2024_no_disturbance_since_1998', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_1997',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 1998\n",
        "  # ('2024_no_disturbance_since_1999', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_1998',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 1999\n",
        "  # ('2024_no_disturbance_since_2000', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_1999',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2000\n",
        "  # ('2024_no_disturbance_since_2001', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2000',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2001\n",
        "  # ('2024_no_disturbance_since_2002', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2001',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2002\n",
        "  # ('2024_no_disturbance_since_2003', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2002',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2003\n",
        "  # ('2024_no_disturbance_since_2004', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2003',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2004\n",
        "  # ('2024_no_disturbance_since_2005', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2004',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2005\n",
        "  # ('2024_no_disturbance_since_2006', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2005',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2006\n",
        "  # ('2024_no_disturbance_since_2007', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2006',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2007\n",
        "  # ('2024_no_disturbance_since_2008', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2007',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2008\n",
        "  # ('2024_no_disturbance_since_2009', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2008',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2009\n",
        "  # ('2024_no_disturbance_since_2010', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2009',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2010\n",
        "  # ('2024_no_disturbance_since_2011', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2010',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2011\n",
        "  # ('2024_no_disturbance_since_2012', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2011',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2012\n",
        "  # ('2024_no_disturbance_since_2013', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2012',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2013\n",
        "  # ('2024_no_disturbance_since_2014', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2013',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2014\n",
        "  # ('2024_no_disturbance_since_2015', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2014',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2015\n",
        "  # ('2024_no_disturbance_since_2016', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2015',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2016\n",
        "  # ('2024_no_disturbance_since_2017', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2016',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2017\n",
        "  # ('2024_no_disturbance_since_2018', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2017',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2018\n",
        "  # ('2024_no_disturbance_since_2019', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2018',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2019\n",
        "  # ('2024_no_disturbance_since_2020', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2019',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2020\n",
        "  # ('2024_no_disturbance_since_2021', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2020',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2021\n",
        "  # ('2024_no_disturbance_since_2022', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2021',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2022\n",
        "  # ('2024_no_disturbance_since_2023', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2022',\n",
        "\n",
        "  # # Degradation in 2024 from events in 1996 to 2023\n",
        "  # ('2024_no_disturbance_since_2024', '2024_no_disturbance_since_1996'):\n",
        "  #   '2024_degradation_from_1996_to_2023',\n",
        "\n",
        "}\n",
        "\n",
        "degradation_single_year_dictionary = {\n",
        "\n",
        "  # Degradation in 2024 from events in 1996\n",
        "  ('2024_no_disturbance_since_1997', '2024_no_disturbance_since_1996'):\n",
        "    '2024_effect_of_degradation_in_1996',\n",
        "\n",
        "  # Degradation in 2024 from events in 1997\n",
        "  ('2024_no_disturbance_since_1998', '2024_no_disturbance_since_1997'):\n",
        "    '2024_effect_of_degradation_in_1997',\n",
        "\n",
        "  # Degradation in 2024 from events in 1998\n",
        "  ('2024_no_disturbance_since_1999', '2024_no_disturbance_since_1998'):\n",
        "    '2024_effect_of_degradation_in_1998',\n",
        "\n",
        "  # Degradation in 2024 from events in 1999\n",
        "  ('2024_no_disturbance_since_2000', '2024_no_disturbance_since_1999'):\n",
        "    '2024_effect_of_degradation_in_1999',\n",
        "\n",
        "  # Degradation in 2024 from events in 2000\n",
        "  ('2024_no_disturbance_since_2001', '2024_no_disturbance_since_2000'):\n",
        "    '2024_effect_of_degradation_in_2000',\n",
        "\n",
        "  # Degradation in 2024 from events in 2001\n",
        "  ('2024_no_disturbance_since_2002', '2024_no_disturbance_since_2001'):\n",
        "    '2024_effect_of_degradation_in_2001',\n",
        "\n",
        "  # Degradation in 2024 from events in 2002\n",
        "  ('2024_no_disturbance_since_2003', '2024_no_disturbance_since_2002'):\n",
        "    '2024_effect_of_degradation_in_2002',\n",
        "\n",
        "  # Degradation in 2024 from events in 2003\n",
        "  ('2024_no_disturbance_since_2004', '2024_no_disturbance_since_2003'):\n",
        "    '2024_effect_of_degradation_in_2003',\n",
        "\n",
        "  # Degradation in 2024 from events in 2004\n",
        "  ('2024_no_disturbance_since_2005', '2024_no_disturbance_since_2004'):\n",
        "    '2024_effect_of_degradation_in_2004',\n",
        "\n",
        "  # Degradation in 2024 from events in 2005\n",
        "  ('2024_no_disturbance_since_2006', '2024_no_disturbance_since_2005'):\n",
        "    '2024_effect_of_degradation_in_2005',\n",
        "\n",
        "  # Degradation in 2024 from events in 2006\n",
        "  ('2024_no_disturbance_since_2007', '2024_no_disturbance_since_2006'):\n",
        "    '2024_effect_of_degradation_in_2006',\n",
        "\n",
        "  # Degradation in 2024 from events in 2007\n",
        "  ('2024_no_disturbance_since_2008', '2024_no_disturbance_since_2007'):\n",
        "    '2024_effect_of_degradation_in_2007',\n",
        "\n",
        "  # Degradation in 2024 from events in 2008\n",
        "  ('2024_no_disturbance_since_2009', '2024_no_disturbance_since_2008'):\n",
        "    '2024_effect_of_degradation_in_2008',\n",
        "\n",
        "  # Degradation in 2024 from events in 2009\n",
        "  ('2024_no_disturbance_since_2010', '2024_no_disturbance_since_2009'):\n",
        "    '2024_effect_of_degradation_in_2009',\n",
        "\n",
        "  # Degradation in 2024 from events in 2010\n",
        "  ('2024_no_disturbance_since_2011', '2024_no_disturbance_since_2010'):\n",
        "    '2024_effect_of_degradation_in_2010',\n",
        "\n",
        "  # Degradation in 2024 from events in 2011\n",
        "  ('2024_no_disturbance_since_2012', '2024_no_disturbance_since_2011'):\n",
        "    '2024_effect_of_degradation_in_2011',\n",
        "\n",
        "  # Degradation in 2024 from events in 2012\n",
        "  ('2024_no_disturbance_since_2013', '2024_no_disturbance_since_2012'):\n",
        "    '2024_effect_of_degradation_in_2012',\n",
        "\n",
        "  # Degradation in 2024 from events in 2013\n",
        "  ('2024_no_disturbance_since_2014', '2024_no_disturbance_since_2013'):\n",
        "    '2024_effect_of_degradation_in_2013',\n",
        "\n",
        "  # Degradation in 2024 from events in 2014\n",
        "  ('2024_no_disturbance_since_2015', '2024_no_disturbance_since_2014'):\n",
        "    '2024_effect_of_degradation_in_2014',\n",
        "\n",
        "  # Degradation in 2024 from events in 2015\n",
        "  ('2024_no_disturbance_since_2016', '2024_no_disturbance_since_2015'):\n",
        "    '2024_effect_of_degradation_in_2015',\n",
        "\n",
        "  # Degradation in 2024 from events in 2016\n",
        "  ('2024_no_disturbance_since_2017', '2024_no_disturbance_since_2016'):\n",
        "    '2024_effect_of_degradation_in_2016',\n",
        "\n",
        "  # Degradation in 2024 from events in 2017\n",
        "  ('2024_no_disturbance_since_2018', '2024_no_disturbance_since_2017'):\n",
        "    '2024_effect_of_degradation_in_2017',\n",
        "\n",
        "  # Degradation in 2024 from events in 2018\n",
        "  ('2024_no_disturbance_since_2019', '2024_no_disturbance_since_2018'):\n",
        "    '2024_effect_of_degradation_in_2018',\n",
        "\n",
        "  # Degradation in 2024 from events in 2019\n",
        "  ('2024_no_disturbance_since_2020', '2024_no_disturbance_since_2019'):\n",
        "    '2024_effect_of_degradation_in_2019',\n",
        "\n",
        "  # Degradation in 2024 from events in 2020\n",
        "  ('2024_no_disturbance_since_2021', '2024_no_disturbance_since_2020'):\n",
        "    '2024_effect_of_degradation_in_2020',\n",
        "\n",
        "  # Degradation in 2024 from events in 2021\n",
        "  ('2024_no_disturbance_since_2022', '2024_no_disturbance_since_2021'):\n",
        "    '2024_effect_of_degradation_in_2021',\n",
        "\n",
        "  # Degradation in 2024 from events in 2022\n",
        "  ('2024_no_disturbance_since_2023', '2024_no_disturbance_since_2022'):\n",
        "    '2024_effect_of_degradation_in_2022',\n",
        "\n",
        "  # Degradation in 2024 from events in 2023\n",
        "  ('2024_no_disturbance_since_2024', '2024_no_disturbance_since_2023'):\n",
        "    '2024_effect_of_degradation_in_2023',\n",
        "\n",
        "  # Degradation in 2024 from events in 2024\n",
        "  ('2024', '2024_no_disturbance_since_2024'):\n",
        "    '2024_effect_of_degradation_in_2024',\n",
        "\n",
        "}\n",
        "\n",
        "disturbance_area_dictionary = {\n",
        "\n",
        "  # Area-based disturbance: 2024_deforestation_of_road_mat_daling_2023\n",
        "  ('2024_road_mat_daling_deforestation_2023_30m_degradation_buffer', '2024'):\n",
        "    ('2024_deforestation_of_road_mat_daling_2023_deforestation',\n",
        "     '2024_deforestation_of_road_mat_daling_2023_degradation'),\n",
        "\n",
        "}\n",
        "\n",
        "restoration_potential_dictionary = {\n",
        "\n",
        "  # Recovery potential with edge effects in 2021\n",
        "  ('2021_oldgrowth_recovery', '2021'):\n",
        "    '2021_recovery_potential_with_edge_effects',\n",
        "\n",
        "  # Restoration and reforestation potential in 2021\n",
        "  ('2021_no_disturbance_since_oldgrowth', '2021'):\n",
        "    ('2021_restoration_potential',\n",
        "     '2021_reforestation_potential'),\n",
        "\n",
        "  # Recovery potential with edge effects in 2024\n",
        "  ('2024_oldgrowth_recovery', '2024'):\n",
        "    '2024_recovery_potential_with_edge_effects',\n",
        "\n",
        "  # Restoration and reforestation potential in 2024\n",
        "  ('2024_no_disturbance_since_oldgrowth', '2024'):\n",
        "    ('2024_restoration_potential',\n",
        "     '2024_reforestation_potential'),\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "g96l3kC13-Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozToZSzop-V8"
      },
      "source": [
        "## Calculate change"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision settings for output rasters\n",
        "mean_precision = 2\n",
        "ci_precision = 2\n",
        "uncertainty_precision = 2\n",
        "\n",
        "# Disturbance and restoration calculations quantify AGBD change as absolute differences\n",
        "# between a baseline alternate scenario and an actual or more recent alternate scenario.\n",
        "# Positive differences indicate restoration (recovery and reforestation) and negative\n",
        "# indicate disturbance (degradation and deforestation).\n",
        "\n",
        "# Disturbance\n",
        "# - Deforestation:\n",
        "# -- Forest to non-forest transition (baseline has data, actual / recent alternate has nodata).\n",
        "# -- The actual / recent nodata is replaced with 0 where baseline has data to calculate deforestation magnitude.\n",
        "# - Degradation:\n",
        "# -- AGBD loss without changing from forest (both scenarios in the pair have data).\n",
        "def calculate_disturbance(recent_mean, baseline_mean, nodata, calculate_deforestation,\n",
        "                          recent_ci=None, baseline_ci=None):\n",
        "    recent_nodata = (recent_mean == nodata)\n",
        "    baseline_nodata = (baseline_mean == nodata)\n",
        "    # Fill recent where alternate scenario has data (deforestation case)\n",
        "    fill_mask = recent_nodata & ~baseline_nodata\n",
        "    recent_mean_filled = np.where(fill_mask, 0, recent_mean)\n",
        "    diff = recent_mean_filled - baseline_mean\n",
        "    # Deforestation: AGBD loss from forest-to-nonforest transitions\n",
        "    if calculate_deforestation:\n",
        "        deforestation = np.where(baseline_nodata, nodata, np.where(recent_nodata, diff, 0))\n",
        "    # Degradation: AGBD loss within persistent forest\n",
        "    degradation = np.where(recent_nodata | baseline_nodata, nodata, diff)\n",
        "    if recent_ci is None:\n",
        "        if calculate_deforestation:\n",
        "            return deforestation, degradation\n",
        "        return degradation\n",
        "    # Propagate uncertainty\n",
        "    recent_ci_filled = np.where(fill_mask, 0, recent_ci)\n",
        "    relative_uncertainty, ci_halfwidth = propagate_uncertainty(\n",
        "        recent_mean_filled, recent_ci_filled, baseline_mean, baseline_ci)\n",
        "    if calculate_deforestation:\n",
        "        deforestation_ci = np.where(baseline_nodata, nodata, np.where(recent_nodata, ci_halfwidth, 0))\n",
        "        deforestation_uncertainty = np.where(baseline_nodata, nodata, np.where(recent_nodata, relative_uncertainty, 0))\n",
        "    degradation_ci = np.where(recent_nodata | baseline_nodata, nodata, ci_halfwidth)\n",
        "    degradation_uncertainty = np.where(recent_nodata | baseline_nodata, nodata, relative_uncertainty)\n",
        "    if calculate_deforestation:\n",
        "        return ((deforestation, deforestation_ci, deforestation_uncertainty),\n",
        "                (degradation, degradation_ci, degradation_uncertainty))\n",
        "    return (degradation, degradation_ci, degradation_uncertainty)\n",
        "\n",
        "# Restoration\n",
        "# - Reforestation:\n",
        "# -- Non-forest to forest transition (actual has no-data, alternate scenario has data).\n",
        "# -- The actual nodata is replaced with 0 where the alternate has data to calculate reforestation magnitude.\n",
        "# - Recovery:\n",
        "# -- AGBD gain within existing forest (both scenarios in the pair have data).\n",
        "def calculate_restoration(scenario_mean, actual_mean, nodata, calculate_reforestation,\n",
        "                          scenario_ci=None, actual_ci=None):\n",
        "    scenario_nodata = (scenario_mean == nodata)\n",
        "    actual_nodata = (actual_mean == nodata)\n",
        "    # Fill actual where alternate scenario has data (reforestation case)\n",
        "    fill_mask = actual_nodata & ~scenario_nodata\n",
        "    actual_mean_filled = np.where(fill_mask, 0, actual_mean)\n",
        "    diff = scenario_mean - actual_mean_filled\n",
        "    # Reforestation: AGBD gain from nonforest-to-forest transitions\n",
        "    if calculate_reforestation:\n",
        "        reforestation = np.where(scenario_nodata, nodata, np.where(actual_nodata, diff, 0))\n",
        "    # Restoration potential: total gain masked by alternate scenario\n",
        "    restoration = np.where(scenario_nodata, nodata, diff)\n",
        "    if scenario_ci is None:\n",
        "        if calculate_reforestation:\n",
        "            return restoration, reforestation\n",
        "        return restoration\n",
        "    # Propagate uncertainty\n",
        "    actual_ci_filled = np.where(fill_mask, 0, actual_ci)\n",
        "    relative_uncertainty, ci_halfwidth = propagate_uncertainty(\n",
        "        scenario_mean, scenario_ci, actual_mean_filled, actual_ci_filled, is_restoration=True)\n",
        "    if calculate_reforestation:\n",
        "        reforestation_ci = np.where(scenario_nodata, nodata, np.where(actual_nodata, ci_halfwidth, 0))\n",
        "        reforestation_uncertainty = np.where(scenario_nodata, nodata, np.where(actual_nodata, relative_uncertainty, 0))\n",
        "    restoration_ci = np.where(scenario_nodata, nodata, ci_halfwidth)\n",
        "    restoration_uncertainty = np.where(scenario_nodata, nodata, relative_uncertainty)\n",
        "    if calculate_reforestation:\n",
        "        return ((restoration, restoration_ci, restoration_uncertainty),\n",
        "                (reforestation, reforestation_ci, reforestation_uncertainty))\n",
        "    return (restoration, restoration_ci, restoration_uncertainty)\n",
        "\n",
        "# Uncertainty (if used as source_dir) is propagated for AGBD disturbance / restoration\n",
        "# magnitude, not forest state transitions. The Landsat-derived classification products\n",
        "# (e.g. TMF) used for masking non-forest have their own measures of uncertainty.\n",
        "# For absolute differences (Z = X - Y) IPCC Approach 1 (Equation 3.2) applies:\n",
        "# CI_z = sqrt(CI_x**2 + CI_y**2)\n",
        "# This holds because variances add for differences of normals, and CIs scale linearly\n",
        "# with standard deviation under normality, preserving the CI interval symmetry.\n",
        "# Output CI values represent half-widths of symmetric 95% confidence intervals.\n",
        "# The limitation is that this assumes independence between scenario uncertainties,\n",
        "# when scenarios predicted with identical models and features have some covariance.\n",
        "# The results are therefore conservative (overestimated) uncertainty bounds.\n",
        "# Values of 0 or the wrong sign (+ for disturbance, - for restoration) do not have\n",
        "# uncertainty propagated. 0 indicates neither disturbance nor restoration took place,\n",
        "# i.e. there is no magnitude for uncertainty, while the wrong sign is a rare indication\n",
        "# of model instability given the constraints of the scenario pairs.\n",
        "\n",
        "# Percentage uncertainty is calculated as CI_z / |X - Y|. The denominator is different\n",
        "# from that published Liang et al. (2023), which report |X + Y| (which may be a typo).\n",
        "\n",
        "# References:\n",
        "# IPCC (2006) Guidelines Vol.1 Ch.3: Uncertainties, Section 3.2.3.1\n",
        "# IPCC (2019) Refinement Vol.1 Ch.3: Uncertainties, Section 3.2.3.1\n",
        "# Liang et al. (2023) Remote Sensing of Environment 284:113367\n",
        "\n",
        "def propagate_uncertainty(mean1, ci1, mean2, ci2, is_restoration=False):\n",
        "    mean_diff = mean1 - mean2\n",
        "    # Forest transitions: one scenario has nodata (converted to 0 mean, 0 CI)\n",
        "    deforestation_case = (ci1 == 0) & (mean1 == 0) & (ci2 != 0) & (mean2 != 0)\n",
        "    reforestation_case = (ci1 != 0) & (mean1 != 0) & (ci2 == 0) & (mean2 == 0)\n",
        "    # Combine absolute uncertainties: CI_z = sqrt(CI_x**2 + CI_y**2)\n",
        "    ci_halfwidth = np.sqrt(np.square(ci1) + np.square(ci2))\n",
        "    # Relative uncertainty: CI_z / |X - Y|\n",
        "    denominator = np.abs(mean_diff)\n",
        "    standard_relative_uncertainty = np.divide(\n",
        "        ci_halfwidth, denominator,\n",
        "        out=np.zeros_like(ci_halfwidth, dtype=np.float64),\n",
        "        where=(denominator != 0))\n",
        "    # Forest transition: uncertainty from forested scenario only\n",
        "    deforestation_relative_uncertainty = np.divide(\n",
        "        ci2, np.abs(mean2),\n",
        "        out=np.zeros_like(ci2, dtype=np.float64),\n",
        "        where=(mean2 != 0))\n",
        "    reforestation_relative_uncertainty = np.divide(\n",
        "        ci1, np.abs(mean1),\n",
        "        out=np.zeros_like(ci1, dtype=np.float64),\n",
        "        where=(mean1 != 0))\n",
        "    # Zero uncertainty for unexpected sign\n",
        "    unexpected_sign = (mean_diff < 0) if is_restoration else (mean_diff > 0)\n",
        "    percentage_uncertainty = np.where(\n",
        "        deforestation_case, deforestation_relative_uncertainty,\n",
        "        np.where(reforestation_case, reforestation_relative_uncertainty,\n",
        "                 np.where(unexpected_sign | (denominator == 0), 0, standard_relative_uncertainty)))\n",
        "    return percentage_uncertainty * 100.0, ci_halfwidth\n",
        "\n",
        "# Determine processing mode\n",
        "use_uncertainty = source_dir == uncertainty_dir\n",
        "\n",
        "# Combine all disturbance dictionaries\n",
        "all_disturbance_dictionaries = {}\n",
        "all_disturbance_dictionaries.update(disturbance_since_dictionary)\n",
        "all_disturbance_dictionaries.update(degradation_interval_dictionary)\n",
        "all_disturbance_dictionaries.update(degradation_single_year_dictionary)\n",
        "all_disturbance_dictionaries.update(disturbance_area_dictionary)\n",
        "all_disturbance_dictionaries.update(restoration_potential_dictionary)\n",
        "\n",
        "# Progress tracking\n",
        "total_operations = len(all_disturbance_dictionaries)\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Forest disturbance calculation progress: {progress_index}/{total_operations}\")\n",
        "display(progress_label)\n",
        "\n",
        "# Process all forest disturbance calculations\n",
        "for (recent_key, baseline_key), value in all_disturbance_dictionaries.items():\n",
        "    # Determine calculation type\n",
        "    is_restoration = (recent_key, baseline_key) in restoration_potential_dictionary\n",
        "    is_interval = (recent_key, baseline_key) in degradation_interval_dictionary\n",
        "    output_dir = restoration_dir if is_restoration else disturbance_dir\n",
        "    # Determine output type from dictionary value structure\n",
        "    # Tuples indicate forest state transitions (deforestation/reforestation) are calculated\n",
        "    if isinstance(value, tuple):\n",
        "        if is_restoration:\n",
        "            restoration_name, reforestation_name = value\n",
        "            has_reforestation = True\n",
        "            has_deforestation = False\n",
        "        else:\n",
        "            deforestation_name, degradation_name = value\n",
        "            has_deforestation = True\n",
        "            has_reforestation = False\n",
        "    else:\n",
        "        has_deforestation = False\n",
        "        has_reforestation = False\n",
        "        if is_restoration:\n",
        "            restoration_name = value\n",
        "        else:\n",
        "            degradation_name = value\n",
        "    # Define output paths and check existence\n",
        "    if use_uncertainty:\n",
        "        if is_restoration:\n",
        "            output_paths = {\n",
        "                'restoration_mean': join(output_dir, f\"mean__{restoration_name}__{selected_model}.tif\"),\n",
        "                'restoration_ci': join(output_dir, f\"ci_95__{restoration_name}__{selected_model}.tif\"),\n",
        "                'restoration_uncertainty': join(output_dir, f\"uncertainty__{restoration_name}__{selected_model}.tif\")}\n",
        "            if has_reforestation:\n",
        "                output_paths.update({\n",
        "                    'reforestation_mean': join(output_dir, f\"mean__{reforestation_name}__{selected_model}.tif\"),\n",
        "                    'reforestation_ci': join(output_dir, f\"ci_95__{reforestation_name}__{selected_model}.tif\"),\n",
        "                    'reforestation_uncertainty': join(output_dir, f\"uncertainty__{reforestation_name}__{selected_model}.tif\")})\n",
        "        else:\n",
        "            output_paths = {\n",
        "                'degradation_mean': join(output_dir, f\"mean__{degradation_name}__{selected_model}.tif\"),\n",
        "                'degradation_ci': join(output_dir, f\"ci_95__{degradation_name}__{selected_model}.tif\"),\n",
        "                'degradation_uncertainty': join(output_dir, f\"uncertainty__{degradation_name}__{selected_model}.tif\")}\n",
        "            if has_deforestation:\n",
        "                output_paths.update({\n",
        "                    'deforestation_mean': join(output_dir, f\"mean__{deforestation_name}__{selected_model}.tif\"),\n",
        "                    'deforestation_ci': join(output_dir, f\"ci_95__{deforestation_name}__{selected_model}.tif\"),\n",
        "                    'deforestation_uncertainty': join(output_dir, f\"uncertainty__{deforestation_name}__{selected_model}.tif\")})\n",
        "    else:\n",
        "        if is_restoration:\n",
        "            output_paths = {'restoration': join(output_dir, f\"{restoration_name}__{selected_model}.tif\")}\n",
        "            if has_reforestation:\n",
        "                output_paths['reforestation'] = join(output_dir, f\"{reforestation_name}__{selected_model}.tif\")\n",
        "        else:\n",
        "            output_paths = {'degradation': join(output_dir, f\"{degradation_name}__{selected_model}.tif\")}\n",
        "            if has_deforestation:\n",
        "                output_paths['deforestation'] = join(output_dir, f\"{deforestation_name}__{selected_model}.tif\")\n",
        "    if all(exists(p) for p in output_paths.values()):\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"Forest disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "        continue\n",
        "    # Validate input paths\n",
        "    if use_uncertainty:\n",
        "        recent_mean_path = join(predictions_dir, f\"mean__{recent_key}__{selected_model}.tif\")\n",
        "        recent_ci_path = join(predictions_dir, f\"ci_95__{recent_key}__{selected_model}.tif\")\n",
        "        baseline_mean_path = join(predictions_dir, f\"mean__{baseline_key}__{selected_model}.tif\")\n",
        "        baseline_ci_path = join(predictions_dir, f\"ci_95__{baseline_key}__{selected_model}.tif\")\n",
        "        assert exists(recent_mean_path), f\"Missing: {recent_mean_path}\"\n",
        "        assert exists(recent_ci_path), f\"Missing: {recent_ci_path}\"\n",
        "        assert exists(baseline_mean_path), f\"Missing: {baseline_mean_path}\"\n",
        "        assert exists(baseline_ci_path), f\"Missing: {baseline_ci_path}\"\n",
        "    else:\n",
        "        recent_path = join(predictions_dir, f\"{recent_key}__{selected_model}.tif\")\n",
        "        baseline_path = join(predictions_dir, f\"{baseline_key}__{selected_model}.tif\")\n",
        "        assert exists(recent_path), f\"Missing: {recent_path}\"\n",
        "        assert exists(baseline_path), f\"Missing: {baseline_path}\"\n",
        "    # Interval pairs: apply actual year mask to degradation\n",
        "    if is_interval:\n",
        "        year = recent_key.split('_')[0]\n",
        "        interval_mask_arr = read_raster_as_array(join(scenario_masks_dir, f\"{year}.tif\"))\n",
        "    # Load inputs and calculate components\n",
        "    if use_uncertainty:\n",
        "        recent_mean = read_raster_as_array(recent_mean_path)\n",
        "        recent_ci = read_raster_as_array(recent_ci_path)\n",
        "        baseline_mean = read_raster_as_array(baseline_mean_path)\n",
        "        baseline_ci = read_raster_as_array(baseline_ci_path)\n",
        "        template_path = recent_mean_path if is_restoration else baseline_mean_path\n",
        "        if is_restoration:\n",
        "            result = calculate_restoration(\n",
        "                recent_mean, baseline_mean, nodatavalue, has_reforestation, recent_ci, baseline_ci)\n",
        "            if has_reforestation:\n",
        "                (restoration, restoration_ci, restoration_uncertainty), \\\n",
        "                (reforestation, reforestation_ci, reforestation_uncertainty) = result\n",
        "            else:\n",
        "                restoration, restoration_ci, restoration_uncertainty = result\n",
        "        else:\n",
        "            result = calculate_disturbance(\n",
        "                recent_mean, baseline_mean, nodatavalue, has_deforestation, recent_ci, baseline_ci)\n",
        "            if has_deforestation:\n",
        "                (deforestation, deforestation_ci, deforestation_uncertainty), \\\n",
        "                (degradation, degradation_ci, degradation_uncertainty) = result\n",
        "            else:\n",
        "                degradation, degradation_ci, degradation_uncertainty = result\n",
        "        # Interval pairs: apply actual year mask to degradation\n",
        "        if is_interval:\n",
        "            degradation = np.where(interval_mask_arr == nodatavalue, nodatavalue, degradation)\n",
        "            degradation_ci = np.where(interval_mask_arr == nodatavalue, nodatavalue, degradation_ci)\n",
        "            degradation_uncertainty = np.where(interval_mask_arr == nodatavalue, nodatavalue, degradation_uncertainty)\n",
        "        # Round restoration mean, zero uncertainty where mean rounds to zero\n",
        "        if is_restoration:\n",
        "            restoration_rounded = round_array(restoration, mean_precision)\n",
        "            restoration_ci = np.where(restoration_rounded == 0, 0, restoration_ci)\n",
        "            restoration_uncertainty = np.where(restoration_rounded == 0, 0, restoration_uncertainty)\n",
        "            # Export restoration\n",
        "            export_array_as_tif(restoration_rounded, output_paths['restoration_mean'], template=template_path)\n",
        "            export_array_as_tif(round_array(restoration_ci, ci_precision), output_paths['restoration_ci'], template=template_path)\n",
        "            export_array_as_tif(round_array(restoration_uncertainty, uncertainty_precision), output_paths['restoration_uncertainty'], template=template_path)\n",
        "            # Round reforestation mean, zero uncertainty where mean rounds to zero\n",
        "            if has_reforestation:\n",
        "                reforestation_rounded = round_array(reforestation, mean_precision)\n",
        "                reforestation_ci = np.where(reforestation_rounded == 0, 0, reforestation_ci)\n",
        "                reforestation_uncertainty = np.where(reforestation_rounded == 0, 0, reforestation_uncertainty)\n",
        "                # Export reforestation\n",
        "                export_array_as_tif(reforestation_rounded, output_paths['reforestation_mean'], template=template_path)\n",
        "                export_array_as_tif(round_array(reforestation_ci, ci_precision), output_paths['reforestation_ci'], template=template_path)\n",
        "                export_array_as_tif(round_array(reforestation_uncertainty, uncertainty_precision), output_paths['reforestation_uncertainty'], template=template_path)\n",
        "        # Round degradation mean, zero uncertainty where mean rounds to zero\n",
        "        else:\n",
        "            degradation_rounded = round_array(degradation, mean_precision)\n",
        "            degradation_ci = np.where(degradation_rounded == 0, 0, degradation_ci)\n",
        "            degradation_uncertainty = np.where(degradation_rounded == 0, 0, degradation_uncertainty)\n",
        "            # Export degradation\n",
        "            export_array_as_tif(degradation_rounded, output_paths['degradation_mean'], template=template_path)\n",
        "            export_array_as_tif(round_array(degradation_ci, ci_precision), output_paths['degradation_ci'], template=template_path)\n",
        "            export_array_as_tif(round_array(degradation_uncertainty, uncertainty_precision), output_paths['degradation_uncertainty'], template=template_path)\n",
        "            # Round deforestation mean, zero uncertainty where mean rounds to zero\n",
        "            if has_deforestation:\n",
        "                deforestation_rounded = round_array(deforestation, mean_precision)\n",
        "                deforestation_ci = np.where(deforestation_rounded == 0, 0, deforestation_ci)\n",
        "                deforestation_uncertainty = np.where(deforestation_rounded == 0, 0, deforestation_uncertainty)\n",
        "                # Export deforestation\n",
        "                export_array_as_tif(deforestation_rounded, output_paths['deforestation_mean'], template=template_path)\n",
        "                export_array_as_tif(round_array(deforestation_ci, ci_precision), output_paths['deforestation_ci'], template=template_path)\n",
        "                export_array_as_tif(round_array(deforestation_uncertainty, uncertainty_precision), output_paths['deforestation_uncertainty'], template=template_path)\n",
        "    else:\n",
        "        recent_arr = read_raster_as_array(recent_path)\n",
        "        baseline_arr = read_raster_as_array(baseline_path)\n",
        "        template_path = recent_path if is_restoration else baseline_path\n",
        "        if is_restoration:\n",
        "            result = calculate_restoration(recent_arr, baseline_arr, nodatavalue, has_reforestation)\n",
        "            if has_reforestation:\n",
        "                restoration, reforestation = result\n",
        "            else:\n",
        "                restoration = result\n",
        "        else:\n",
        "            result = calculate_disturbance(recent_arr, baseline_arr, nodatavalue, has_deforestation)\n",
        "            if has_deforestation:\n",
        "                deforestation, degradation = result\n",
        "            else:\n",
        "                degradation = result\n",
        "        # Interval pairs: apply actual year mask to degradation\n",
        "        if is_interval:\n",
        "            degradation = np.where(interval_mask_arr == nodatavalue, nodatavalue, degradation)\n",
        "        # Export\n",
        "        if is_restoration:\n",
        "            export_array_as_tif(round_array(restoration, mean_precision), output_paths['restoration'], template=template_path)\n",
        "            if has_reforestation:\n",
        "                export_array_as_tif(round_array(reforestation, mean_precision), output_paths['reforestation'], template=template_path)\n",
        "        else:\n",
        "            export_array_as_tif(round_array(degradation, mean_precision), output_paths['degradation'], template=template_path)\n",
        "            if has_deforestation:\n",
        "                export_array_as_tif(round_array(deforestation, mean_precision), output_paths['deforestation'], template=template_path)\n",
        "\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Forest disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "print(\"All forest disturbance calculations complete.\\n\")"
      ],
      "metadata": {
        "id": "dpgsXhrX8rZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81OCJi98NDwj"
      },
      "source": [
        "# Intactness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJyxcVvLqCdW"
      },
      "source": [
        "## Percentage loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Intactness is measured as relative percentage loss of AGBD within an area of interest\n",
        "\n",
        "# Build list of alternate scenario baselines for intactness calculation\n",
        "# Actual scenario is derived from alternate scenario (year prefix)\n",
        "use_uncertainty = source_dir == uncertainty_dir\n",
        "\n",
        "intactness_alternate_baselines = set()\n",
        "\n",
        "for file in os.listdir(predictions_dir):\n",
        "    is_mean = 'mean__' in file\n",
        "    if use_uncertainty and not is_mean: continue\n",
        "    if '_no_disturbance_since_' not in file: continue\n",
        "    baseline_scenario = file.split('__')[1 if is_mean else 0]\n",
        "    year = baseline_scenario.split('_')[0]\n",
        "    actual_file = f\"{'mean__' if is_mean else ''}{year}__{selected_model}.tif\"\n",
        "    if exists(join(predictions_dir, actual_file)):\n",
        "        intactness_alternate_baselines.add(baseline_scenario)\n",
        "\n",
        "print(\"intactness_alternate_baselines = [\")\n",
        "for baseline in sorted(intactness_alternate_baselines):\n",
        "    print(f\"    '{baseline}',\")\n",
        "print(\"]\")"
      ],
      "metadata": {
        "id": "-6RjW6jgtsws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intactness_alternate_baselines = [\n",
        "    '2021_no_disturbance_since_1993',\n",
        "    '2021_no_disturbance_since_oldgrowth',\n",
        "    '2024_no_disturbance_since_1996',\n",
        "    '2024_no_disturbance_since_oldgrowth',\n",
        "]"
      ],
      "metadata": {
        "id": "WQwmfY3BtzHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percentage_loss_precision = 0\n",
        "percentage_ci_precision = 2\n",
        "\n",
        "# Percentage loss quantifies AGBD change relative to baseline: (actual / baseline) * 100 - 100.\n",
        "# Values >0 indicate AGBD gain, which (within scenario constraints) are artefacts of\n",
        "# model instability. These are clipped to 0.\n",
        "\n",
        "# Uncertainty (if used as source_dir) of percentage loss is propagated for the magnitude\n",
        "# of percentage loss from AGBD disturbance, not forest state transitions.\n",
        "# The Landsat-derived classification products (e.g. TMF) used for masking non-forest\n",
        "# have their own measures of uncertainty. Therefore values of -100 % (deforestation)\n",
        "# are set to a CI value of 0. Magnitude of deforestation is not measured in this case,\n",
        "# only degradation (>-100 <0). Similarly, percentage loss of 0 is also set to a\n",
        "# percentage loss CI of 0.\n",
        "\n",
        "# For absolute differences (Z = X - Y) used for disturbance and restoration,\n",
        "# IPCC Approach 1 (Equation 3.1) applies: CI_z = sqrt(CI_x**2 + CI_y**2).\n",
        "# This holds because variances add for differences of normals, and CIs scale linearly\n",
        "# with standard deviation under normality, preserving the CI interval symmetry.\n",
        "\n",
        "# For ratios like percentage loss (Z = X / Y), there is no equivalent formula for CI_z.\n",
        "# IPCC Approach 1 is not applicable to divisions like percentage loss.\n",
        "# The distribution of the quotient is not normal even when X and Y are both normal.\n",
        "# The resulting distribution has heavy tails and is inherently asymmetric.\n",
        "# Monte Carlo simulation (IPCC Approach 2, IPCC 2006 and 2019) is therefore used instead,\n",
        "# as in the initial uncertainty calculations of scenarios. Percentage loss for each\n",
        "# prediction iteration pair is calculated, then confidence intervals from percentiles\n",
        "# are derived from the empirical (actual rather than theoretical) distribution.\n",
        "# Percentiles directly characterise the distribution without assuming normality,\n",
        "# are robust to outliers, and handle bounded distributions (percentage loss <= 0).\n",
        "# CI95 upper and lower are stored alongside the half-width for inspection of asymmetry.\n",
        "\n",
        "# References:\n",
        "# IPCC (2006) Guidelines Vol.1 Ch.3: Uncertainties, Section 3.2.3.2\n",
        "# IPCC (2019) Refinement Vol.1 Ch.3: Uncertainties, Section 3.2.3.2\n",
        "\n",
        "# Sort key for iteration files (1..100)\n",
        "def iteration_sort_key(path):\n",
        "    match = re.search(r'iteration_(\\d+)', path)\n",
        "    return int(match.group(1)) if match else 0\n",
        "\n",
        "# Get sorted iteration paths for a scenario\n",
        "def get_iteration_paths(scenario):\n",
        "    iterations_dir = join(scenario_iterations_dir, f\"{scenario}_iterations\")\n",
        "    return sorted(\n",
        "        [join(iterations_dir, f) for f in os.listdir(iterations_dir) if f.endswith(\".tif\")],\n",
        "        key=iteration_sort_key)\n",
        "\n",
        "# Calculate percentage loss from actual and baseline arrays\n",
        "def calculate_percentage(actual, baseline, nodata):\n",
        "    actual = np.where(actual == nodata, np.nan, actual).astype(np.float64)\n",
        "    baseline = np.where((baseline == nodata) | (baseline == 0), np.nan, baseline).astype(np.float64)\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
        "        return (actual / baseline) * 100 - 100\n",
        "\n",
        "# Progress labels\n",
        "n_baselines = len(intactness_alternate_baselines)\n",
        "baseline_progress_label = widgets.Label(value=f\"Baseline progress: 0 / {n_baselines}\")\n",
        "iter_progress_label = widgets.Label(value=\"Iteration progress: -\")\n",
        "display(baseline_progress_label, iter_progress_label)\n",
        "\n",
        "for baseline_index, baseline_scenario in enumerate(intactness_alternate_baselines):\n",
        "    # Parse actual scenario from baseline scenario name\n",
        "    year = baseline_scenario.split('_')[0]\n",
        "    actual_scenario = year\n",
        "    intactness_subdir = join(intactness_dir, baseline_scenario)\n",
        "    makedirs(intactness_subdir, exist_ok=True)\n",
        "\n",
        "    # Define output paths\n",
        "    percentage_filename = f\"percentage_loss__{baseline_scenario}__{selected_model}.tif\"\n",
        "    percentage_path = join(intactness_subdir, percentage_filename)\n",
        "    ci_halfwidth_path = None\n",
        "    ci_upper_path = None\n",
        "    ci_lower_path = None\n",
        "    if use_uncertainty:\n",
        "        ci_halfwidth_path = join(intactness_subdir, f\"ci_95_halfwidth__percentage_loss__{baseline_scenario}__{selected_model}.tif\")\n",
        "        ci_upper_path = join(intactness_subdir, f\"ci_95_upper__percentage_loss__{baseline_scenario}__{selected_model}.tif\")\n",
        "        ci_lower_path = join(intactness_subdir, f\"ci_95_lower__percentage_loss__{baseline_scenario}__{selected_model}.tif\")\n",
        "    outputs_exist = exists(percentage_path) and (not use_uncertainty or (exists(ci_halfwidth_path) and exists(ci_upper_path) and exists(ci_lower_path)))\n",
        "    if outputs_exist:\n",
        "        iter_progress_label.value = \"Iteration progress: skipped (exists)\"\n",
        "        baseline_progress_label.value = f\"Baseline progress: {baseline_index + 1} / {n_baselines}\"\n",
        "        continue\n",
        "\n",
        "    if use_uncertainty:\n",
        "        # Collect iteration paths for both scenarios\n",
        "        actual_paths = get_iteration_paths(actual_scenario)\n",
        "        baseline_paths = get_iteration_paths(baseline_scenario)\n",
        "        n_iterations = len(actual_paths)\n",
        "        iter_progress_label.value = f\"Iteration progress: 0 / {n_iterations}\"\n",
        "\n",
        "        # Load first pair to define masks (consistent across all iterations)\n",
        "        first_actual = read_raster_as_array(actual_paths[0])\n",
        "        first_baseline = read_raster_as_array(baseline_paths[0])\n",
        "        actual_nodata_mask = first_actual == nodatavalue\n",
        "        baseline_nodata_mask = first_baseline == nodatavalue\n",
        "        both_valid = ~baseline_nodata_mask & ~actual_nodata_mask\n",
        "        deforestation_mask = ~baseline_nodata_mask & actual_nodata_mask\n",
        "        zero_baseline_mask = both_valid & (first_baseline == 0)\n",
        "        template_path = actual_paths[0]\n",
        "\n",
        "        # Calculate percentage loss per iteration pair, accumulate for percentile calculation\n",
        "        percentage_arrays = []\n",
        "        for iter_index, (actual_path, baseline_path) in enumerate(zip(actual_paths, baseline_paths)):\n",
        "            actual_iter = read_raster_as_array(actual_path)\n",
        "            baseline_iter = read_raster_as_array(baseline_path)\n",
        "            percentage_iter = calculate_percentage(actual_iter, baseline_iter, nodatavalue)\n",
        "            percentage_arrays.append(percentage_iter)\n",
        "            iter_progress_label.value = f\"Iteration progress: {iter_index + 1} / {n_iterations}\"\n",
        "\n",
        "        # Stack and calculate mean and 95% CI from distribution\n",
        "        percentage_stack = np.stack(percentage_arrays, axis=0)\n",
        "        percentage_arrays = None\n",
        "\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
        "            percentage_arr = np.nanmean(percentage_stack, axis=0)\n",
        "\n",
        "            # Sort once, index at percentile positions. Faster than np.nanpercentile.\n",
        "            # This works because nodata mask is identical across iterations.\n",
        "            sorted_stack = np.sort(percentage_stack, axis=0)\n",
        "            percentage_stack = None\n",
        "            n = sorted_stack.shape[0]\n",
        "            lower_idx = 0.025 * (n - 1)\n",
        "            upper_idx = 0.975 * (n - 1)\n",
        "            lower_floor = int(np.floor(lower_idx))\n",
        "            upper_floor = int(np.floor(upper_idx))\n",
        "            lower_frac = lower_idx - lower_floor\n",
        "            upper_frac = upper_idx - upper_floor\n",
        "            ci_lower_arr = (sorted_stack[lower_floor] * (1 - lower_frac) +\n",
        "                            sorted_stack[min(lower_floor + 1, n - 1)] * lower_frac)\n",
        "            ci_upper_arr = (sorted_stack[upper_floor] * (1 - upper_frac) +\n",
        "                            sorted_stack[min(upper_floor + 1, n - 1)] * upper_frac)\n",
        "            sorted_stack = None\n",
        "\n",
        "        # CI halfwidth (symmetric approximation)\n",
        "        ci_halfwidth_arr = (ci_upper_arr - ci_lower_arr) / 2\n",
        "\n",
        "        # Set nodata pixels\n",
        "        percentage_arr = np.where(baseline_nodata_mask, nodatavalue, percentage_arr)\n",
        "        ci_halfwidth_arr = np.where(baseline_nodata_mask, nodatavalue, ci_halfwidth_arr)\n",
        "        ci_upper_arr = np.where(baseline_nodata_mask, nodatavalue, ci_upper_arr)\n",
        "        ci_lower_arr = np.where(baseline_nodata_mask, nodatavalue, ci_lower_arr)\n",
        "\n",
        "    else:\n",
        "        iter_progress_label.value = \"Iteration progress: n/a (no uncertainty)\"\n",
        "\n",
        "        # Load single prediction rasters from scenarios dir\n",
        "        baseline_path_in = join(predictions_dir, f\"{baseline_scenario}__{selected_model}.tif\")\n",
        "        actual_path_in = join(predictions_dir, f\"{actual_scenario}__{selected_model}.tif\")\n",
        "        baseline_arr = read_raster_as_array(baseline_path_in)\n",
        "        actual_arr = read_raster_as_array(actual_path_in)\n",
        "        template_path = baseline_path_in\n",
        "\n",
        "        # Define masks\n",
        "        actual_nodata_mask = actual_arr == nodatavalue\n",
        "        baseline_nodata_mask = baseline_arr == nodatavalue\n",
        "        both_valid = ~baseline_nodata_mask & ~actual_nodata_mask\n",
        "        deforestation_mask = ~baseline_nodata_mask & actual_nodata_mask\n",
        "        zero_baseline_mask = both_valid & (baseline_arr == 0)\n",
        "\n",
        "        # Calculate percentage loss\n",
        "        percentage_arr = calculate_percentage(actual_arr, baseline_arr, nodatavalue)\n",
        "        percentage_arr = np.where(baseline_nodata_mask, nodatavalue, percentage_arr)\n",
        "\n",
        "    # Deforestation: -100% loss, zero CI\n",
        "    percentage_arr[deforestation_mask] = -100\n",
        "    if use_uncertainty:\n",
        "        ci_halfwidth_arr[deforestation_mask] = 0\n",
        "        ci_upper_arr[deforestation_mask] = -100\n",
        "        ci_lower_arr[deforestation_mask] = -100\n",
        "\n",
        "    # Zero baseline: 0% loss, zero CI\n",
        "    percentage_arr[zero_baseline_mask] = 0\n",
        "    if use_uncertainty:\n",
        "        ci_halfwidth_arr[zero_baseline_mask] = 0\n",
        "        ci_upper_arr[zero_baseline_mask] = 0\n",
        "        ci_lower_arr[zero_baseline_mask] = 0\n",
        "\n",
        "    # Clip percentage to [-100, 0]\n",
        "    percentage_arr[both_valid] = np.clip(percentage_arr[both_valid], -100, 0)\n",
        "\n",
        "    # Round outputs. Zero CI where percentage rounds to zero.\n",
        "    percentage_arr = round_array(percentage_arr, percentage_loss_precision)\n",
        "    if use_uncertainty:\n",
        "        ci_halfwidth_arr = np.where(percentage_arr == 0, 0, ci_halfwidth_arr)\n",
        "        ci_upper_arr = np.where(percentage_arr == 0, 0, ci_upper_arr)\n",
        "        ci_lower_arr = np.where(percentage_arr == 0, 0, ci_lower_arr)\n",
        "        ci_halfwidth_arr = round_array(ci_halfwidth_arr, percentage_ci_precision)\n",
        "        ci_upper_arr = round_array(ci_upper_arr, percentage_ci_precision)\n",
        "        ci_lower_arr = round_array(ci_lower_arr, percentage_ci_precision)\n",
        "\n",
        "    # Export outputs\n",
        "    export_array_as_tif(percentage_arr, percentage_path, template=template_path)\n",
        "    if use_uncertainty:\n",
        "        export_array_as_tif(ci_halfwidth_arr, ci_halfwidth_path, template=template_path)\n",
        "        export_array_as_tif(ci_upper_arr, ci_upper_path, template=template_path)\n",
        "        export_array_as_tif(ci_lower_arr, ci_lower_path, template=template_path)\n",
        "\n",
        "    baseline_progress_label.value = f\"Baseline progress: {baseline_index + 1} / {n_baselines}\"\n",
        "\n",
        "print(\"Percentage loss calculations complete.\")"
      ],
      "metadata": {
        "id": "O2FwcMip7jHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ugXALeIqLkt"
      },
      "source": [
        "## Quantiles (relative intactness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwXRZTew1r39"
      },
      "outputs": [],
      "source": [
        "# Use additional polygons for masking relative intactness quantiles\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg', 'prediction_area.gpkg']\n",
        "\n",
        "# Select alternate scenario / degradation pairs to measure relative intactness\n",
        "print(\"baseline_percentage_loss_list = [\")\n",
        "for dir in os.listdir(intactness_dir):\n",
        "  print(f\"'{dir}',\")\n",
        "print(\"]\\n\")\n",
        "\n",
        "# Select polygons to mask and calculate quantiles\n",
        "print(\"mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"'{polygon}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_percentage_loss_list = [\n",
        "'2021_no_disturbance_since_1993',\n",
        "'2021_no_disturbance_since_oldgrowth',\n",
        "'2024_no_disturbance_since_1996',\n",
        "'2024_no_disturbance_since_oldgrowth',\n",
        "]\n",
        "\n",
        "mask_polygons = [\n",
        "# 'project_area.gpkg',\n",
        "# 'peninsular_malaysia.gpkg',\n",
        "# 'lu_yong.gpkg',\n",
        "# 'lu_yong_lipis.gpkg',\n",
        "# 'lu_berkelah_jerantut.gpkg',\n",
        "# 'lu_tekai_tembeling.gpkg',\n",
        "# 'lu_ais.gpkg',\n",
        "# 'lu_tekam.gpkg',\n",
        "# 'lu_berkelah_temerloh.gpkg',\n",
        "# 'lu_remen_chereh.gpkg',\n",
        "# 'lu_berkelah_kuantan.gpkg',\n",
        "'forest_reserves.gpkg',\n",
        "# 'lu_old-growth_protected_areas.gpkg',\n",
        "# 'road_mat_daling.gpkg',\n",
        "# 'road_mat_daling_buffered_30.gpkg',\n",
        "# 'asartr_phase_2.gpkg',\n",
        "# 'tekai_tembeling.gpkg',\n",
        "# 'non_forest_biome.gpkg',\n",
        "]"
      ],
      "metadata": {
        "id": "sf03ZoqBjDbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7-X8cXopVo3"
      },
      "outputs": [],
      "source": [
        "# Define top score for intactness rating (e.g. 10 for 1 - 10 scale)\n",
        "top_score = 10\n",
        "\n",
        "# Calculate actual number of quantiles for non-zero values\n",
        "num_quantiles = top_score - 1\n",
        "\n",
        "print(f\"Calculating {num_quantiles} quantiles for negative percentage loss (scores 1-{num_quantiles}), with score {top_score} reserved for 0% difference.\\n\")\n",
        "\n",
        "# Ensure prediction_area.gpkg is always processed, plus any selected mask_polygons\n",
        "required_polygon = 'prediction_area.gpkg'\n",
        "if required_polygon not in mask_polygons:\n",
        "    mask_polygons_full = [required_polygon] + [p for p in mask_polygons if p is not None]\n",
        "else:\n",
        "    mask_polygons_full = [p for p in mask_polygons if p is not None]\n",
        "\n",
        "# Create polygon mask array using template tif\n",
        "template_array = read_raster_as_array(template_tif_path)\n",
        "\n",
        "polygon_masks = {}\n",
        "for mask_polygon in mask_polygons_full:\n",
        "    # Create an inverse project area path for masking\n",
        "    template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "    inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "    if not exists(inverse_polygon_path):\n",
        "        polygon_path = join(polygons_dir, mask_polygon)\n",
        "        template_polygon = gpd.read_file(template_polygon_path)\n",
        "        polygon_read = gpd.read_file(polygon_path)\n",
        "        polygon_crs = polygon_read.crs.to_epsg()\n",
        "        inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "        inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "        inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "        print(f\"An inverse masking polygon for {mask_polygon} has been created in {polygons_dir}.\")\n",
        "    else: print(f\"An inverse masking polygon for {mask_polygon} already exists.\")\n",
        "\n",
        "    # Create and store individual mask for this polygon\n",
        "    print(f\"Creating polygon mask for {mask_polygon}.\")\n",
        "    temp_mask_path = join(intactness_dir, f\"temp_mask_{mask_polygon[:-5]}.tif\")\n",
        "    copyfile(template_tif_path, temp_mask_path)\n",
        "    burn_polygon_to_raster(temp_mask_path, inverse_polygon_path, fixed_value=nodatavalue, all_touched=False)\n",
        "    temp_mask_array = read_raster_as_array(temp_mask_path)\n",
        "    individual_mask = np.ones_like(template_array, dtype=bool)\n",
        "    individual_mask[temp_mask_array == nodatavalue] = False\n",
        "    polygon_masks[mask_polygon] = individual_mask\n",
        "    os.remove(temp_mask_path)\n",
        "\n",
        "for baseline_name in baseline_percentage_loss_list:\n",
        "    intactness_baseline_dir = join(intactness_dir, baseline_name)\n",
        "    percentage_filename = f\"percentage_loss__{baseline_name}__{selected_model}\"\n",
        "    percentage_path = join(intactness_baseline_dir, f\"{percentage_filename}.tif\")\n",
        "\n",
        "    # Load scenario mask for this year\n",
        "    year = baseline_name[:4]\n",
        "    scenario_mask_path = join(scenario_masks_dir, f\"{year}.tif\")\n",
        "    scenario_mask_array = read_raster_as_array(scenario_mask_path)\n",
        "    scenario_valid_mask = scenario_mask_array == 1\n",
        "\n",
        "    for mask_polygon in mask_polygons_full:\n",
        "        # Create subdirectory for this polygon within baseline directory\n",
        "        polygon_subdir = join(intactness_baseline_dir, mask_polygon[:-5])\n",
        "        makedirs(polygon_subdir, exist_ok=True)\n",
        "\n",
        "        # Copy the percentage raster for potential masking\n",
        "        percentage_masked_filename = f\"{percentage_filename}__masked_{mask_polygon[:-5]}.tif\"\n",
        "        percentage_masked_path = join(polygon_subdir, percentage_masked_filename)\n",
        "        if not exists(percentage_masked_path):\n",
        "            print(f\"Copying {percentage_filename} for masking...\")\n",
        "            copyfile(percentage_path, percentage_masked_path)\n",
        "            print(f\"Masking {percentage_filename} with {mask_polygon}...\")\n",
        "            inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "            burn_polygon_to_raster(percentage_masked_path, inverse_polygon_path, fixed_value=nodatavalue, all_touched=False)\n",
        "            # Recompress the prediction after burning the polygon masks\n",
        "            percentage_masked_array = read_raster_as_array(percentage_masked_path)\n",
        "            export_array_as_tif(percentage_masked_array, percentage_masked_path)\n",
        "            print(f\"{percentage_filename} masked.\")\n",
        "        else: print(f\"{percentage_masked_filename} already exists.\")\n",
        "\n",
        "        # Define output paths\n",
        "        relative_intactness_name = f'intactness__{mask_polygon[:-5]}_{top_score}_quantiles__{baseline_name}__{selected_model}'\n",
        "        relative_intactness_path = join(polygon_subdir, f'{relative_intactness_name}.tif')\n",
        "        if exists(relative_intactness_path):\n",
        "            print(f\"{relative_intactness_name} already exists.\")\n",
        "            continue\n",
        "\n",
        "        # Load original percentage raster and track nodata pixels\n",
        "        original_percentage_array = read_raster_as_array(percentage_path)\n",
        "        originally_nodata_mask = original_percentage_array == nodatavalue\n",
        "\n",
        "        # Apply polygon masking to percentage array using pre-created mask\n",
        "        percentage_array = original_percentage_array.copy()\n",
        "        percentage_array[~polygon_masks[mask_polygon]] = nodatavalue\n",
        "\n",
        "        # Identify deforestation: valid percentage data outside scenario mask\n",
        "        deforestation_mask = (percentage_array != nodatavalue) & (~scenario_valid_mask)\n",
        "\n",
        "        # Capture degradation data for histogram (within scenario mask only)\n",
        "        original_valid_elements = percentage_array[(percentage_array != nodatavalue) & scenario_valid_mask]\n",
        "\n",
        "        relative_intactness_array = np.full_like(percentage_array, nodatavalue, dtype=np.int16)\n",
        "\n",
        "        # Set all values above 0 to 0\n",
        "        percentage_array[percentage_array > 0] = 0\n",
        "\n",
        "        # Separate zero and non-zero values within scenario mask only\n",
        "        zero_elements = (percentage_array == 0) & scenario_valid_mask\n",
        "        # Exclude nodata and deforestation from quantile calculation\n",
        "        quantile_mask = (percentage_array != nodatavalue) & (~originally_nodata_mask) & (percentage_array != 0) & scenario_valid_mask\n",
        "        non_zero_valid_elements = percentage_array[quantile_mask]\n",
        "\n",
        "        # Calculate quantiles for non-zero valid elements only\n",
        "        quantiles = np.percentile(non_zero_valid_elements, np.linspace(0, 100, num_quantiles + 1)[1:-1]) if len(non_zero_valid_elements) > 0 else []\n",
        "\n",
        "        # Assign scores 1 to num_quantiles for non-zero values\n",
        "        for i in range(1, num_quantiles + 1):\n",
        "            lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "            upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "            relative_intactness_array[\n",
        "                (percentage_array > lower_bound) & (percentage_array <= upper_bound) &\n",
        "                (percentage_array != 0) & (percentage_array != nodatavalue) & scenario_valid_mask] = i\n",
        "\n",
        "        # Set zero percentage loss to top score\n",
        "        relative_intactness_array[zero_elements] = top_score\n",
        "\n",
        "        # Set deforestation areas to score 0\n",
        "        relative_intactness_array[deforestation_mask] = 0\n",
        "\n",
        "        # Set areas outside polygon to nodatavalue using pre-created mask\n",
        "        relative_intactness_array[~polygon_masks[mask_polygon]] = nodatavalue\n",
        "\n",
        "        export_array_as_tif(relative_intactness_array, relative_intactness_path)\n",
        "\n",
        "        # Prepare data for CSV: Collect lower and upper bounds for each category\n",
        "        ranges_data = {'Score': [], 'Lower_Bound': [], 'Upper_Bound': []}\n",
        "\n",
        "        # Add ranges for scores 1 to num_quantiles (non-zero values)\n",
        "        for i in range(1, num_quantiles + 1):\n",
        "            lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "            if i == num_quantiles: upper_bound = -0.000000001\n",
        "            else: upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "            ranges_data['Score'].append(i)\n",
        "            ranges_data['Lower_Bound'].append(lower_bound)\n",
        "            ranges_data['Upper_Bound'].append(upper_bound)\n",
        "\n",
        "        # Add entry for top score (value of 0)\n",
        "        ranges_data['Score'].append(top_score)\n",
        "        ranges_data['Lower_Bound'].append(0)\n",
        "        ranges_data['Upper_Bound'].append(0)\n",
        "\n",
        "        # Create DataFrame and save to CSV\n",
        "        relative_intactness_df = pd.DataFrame(ranges_data)\n",
        "        relative_intactness_csv_path = join(polygon_subdir, f'{relative_intactness_name}.csv')\n",
        "        relative_intactness_df.to_csv(relative_intactness_csv_path, index=False)\n",
        "\n",
        "        # Generate and save histogram for degradation data as .png\n",
        "        histogram_path = join(polygon_subdir, f'{relative_intactness_name}.png')\n",
        "        plt.figure()\n",
        "        counts, bins, patches = plt.hist(original_valid_elements.flatten(), bins=100)\n",
        "\n",
        "        # Count deforested pixels (score 0)\n",
        "        deforestation_count = np.sum(deforestation_mask)\n",
        "\n",
        "        # Find the zero bin and set its frequency to 0 for display\n",
        "        zero_idx = next((i for i, (l, r) in enumerate(zip(bins[:-1], bins[1:])) if l <= 0 <= r), None)\n",
        "        if zero_idx is not None:\n",
        "            counts[zero_idx] = 0\n",
        "            plt.clf()\n",
        "            plt.bar(bins[:-1], counts, width=np.diff(bins), align='edge')\n",
        "            x_center = (bins.min() + bins.max()) / 2\n",
        "            y_max = max(counts)\n",
        "            plt.text(x_center, y_max * 0.9,\n",
        "                    f'Deforested pixels (score 0) = {deforestation_count:,}',\n",
        "                    ha='center', va='center', fontweight='bold',\n",
        "                    bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9))\n",
        "        plt.title(f'{relative_intactness_name} Histogram')\n",
        "        plt.xlabel('Value')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(histogram_path)\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-7ctfgdoF2"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcuArY00N9Mc"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V6E1",
      "toc_visible": true,
      "collapsed_sections": [
        "QJ8jLRKRSfgb",
        "y_OkHaUkTCyB",
        "MJyxcVvLqCdW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}