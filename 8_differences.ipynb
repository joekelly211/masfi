{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/8_differences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8jLRKRSfgb"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkvV803eSUQY"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEpdIjOwSiB1"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!apt-get install -y gdal-bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQDSsvGeSi4u"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "from os import makedirs\n",
        "from os.path import join, exists\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "from shutil import copyfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxgT3K1Sjum"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "masks_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "differences_dir = join(base_dir, \"8_differences\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(differences_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96FAfeyooMLz"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = [\"COMPRESS=DEFLATE\", \"PREDICTOR=3\", \"ZLEVEL=9\"]\n",
        "    else: options = []\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = vector = None\n",
        "    try:\n",
        "        raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "        vector = ogr.Open(polygon_path)\n",
        "        if not raster or not vector:\n",
        "            raise ValueError(\"Cannot open input files\")\n",
        "        layer = vector.GetLayer()\n",
        "        options = [\"ALL_TOUCHED=TRUE\"] if all_touched else []\n",
        "        if fixed:\n",
        "            gdal.RasterizeLayer(raster, [1], layer, burn_values=[fixed_value], options=options)\n",
        "        else:\n",
        "            attr_name = column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()\n",
        "            options.append(f\"ATTRIBUTE={attr_name}\")\n",
        "            gdal.RasterizeLayer(raster, [1], layer, options=options)\n",
        "    finally:\n",
        "        if raster: raster.FlushCache()\n",
        "        raster = vector = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_OkHaUkTCyB"
      },
      "source": [
        "# Select source and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mB0XrPNYU_I"
      },
      "outputs": [],
      "source": [
        "# Select if to source predictions from scenarios_dir or uncertainty_dir\n",
        "# If available, uncertainty_dir should be selected so that uncertainty can\n",
        "# be propagated and scenario 'mean' iteration values used.\n",
        "\n",
        "source_dir = uncertainty_dir\n",
        "# source_dir = scenarios_dir\n",
        "\n",
        "print(f\"{source_dir.split('/')[-1]} has been selected as the source directory for predictions\")\n",
        "print(\"to calculate disturbance and intactness.\\n\")\n",
        "\n",
        "# If uncertainty selected, check it exists\n",
        "if not exists(uncertainty_dir) and source_dir == uncertainty_dir:\n",
        "  print(\"The uncertainty directory does not yet exist. Defaulting to scenarios directory.\")\n",
        "  source_dir = scenarios_dir\n",
        "\n",
        "source_dir_name = f\"{source_dir.split('_')[-1]}_dir\"\n",
        "\n",
        "# Select the model\n",
        "for subdir in os.listdir(source_dir):\n",
        "  if 'scenario_masks' not in subdir:\n",
        "    print(f\"selected_model = '{subdir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbXY6iP_qjYc"
      },
      "outputs": [],
      "source": [
        "selected_model = 'agbd_tekai_250625_003858'\n",
        "\n",
        "selected_model_dir = join(source_dir, selected_model)\n",
        "if source_dir == scenarios_dir: predictions_dir = join(selected_model_dir, 'scenario_predictions')\n",
        "if source_dir == uncertainty_dir: predictions_dir = join(selected_model_dir, 'uncertainty_predictions')\n",
        "\n",
        "# Check predictions exist to calculate differences\n",
        "if len(os.listdir(predictions_dir)) < 2: print(f\"At least 2 predictions must exist in {source_dir} to calculate differences.\")\n",
        "else:\n",
        "  model_differences_dir = join(differences_dir, f\"{selected_model}_{source_dir_name}\")\n",
        "  disturbance_dir = join(model_differences_dir, 'disturbance')\n",
        "  intactness_dir = join(model_differences_dir, 'intactness')\n",
        "  makedirs(model_differences_dir, exist_ok=True)\n",
        "  makedirs(disturbance_dir, exist_ok=True)\n",
        "  makedirs(intactness_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECHaTfrs0n7i"
      },
      "source": [
        "# Disturbance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define type and period"
      ],
      "metadata": {
        "id": "hgPVQgfTpy21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUaYCCymYI2Y"
      },
      "outputs": [],
      "source": [
        "# Disturbance is measured as absolute AGBD loss\n",
        "# This block builds dictionaries of disturbance options based on available files\n",
        "\n",
        "# Extract all available scenarios from scenarios predictions directory\n",
        "if source_dir == scenarios_dir:\n",
        "  scenarios = set()\n",
        "  for file in os.listdir(predictions_dir):\n",
        "      scenarios.add(file.split(\"__\")[0])\n",
        "\n",
        "# OR Extract all available scenarios from uncertainty predictions directory\n",
        "if source_dir == uncertainty_dir:\n",
        "  prediction_stats = {}\n",
        "  for file in os.listdir(predictions_dir):\n",
        "      parts = file.split(\"__\")\n",
        "      if len(parts) >= 2:\n",
        "          stat, scenario = parts[0], parts[1]\n",
        "          if scenario not in prediction_stats:\n",
        "              prediction_stats[scenario] = set()\n",
        "          prediction_stats[scenario].add(stat)\n",
        "  # Only keep scenarios that have both 'uncertainty' and 'mean' prediction stats\n",
        "  scenarios = {prediction for prediction, stats in prediction_stats.items()\n",
        "              if 'uncertainty' in stats and 'mean' in stats}\n",
        "\n",
        "# Categorise years from scenarios\n",
        "years = set()\n",
        "plain_years = set()\n",
        "oldgrowth_years = set()\n",
        "oldgrowth_all_land_years = set()\n",
        "for s in scenarios:\n",
        "    if s.isdigit():\n",
        "        years.add(int(s))\n",
        "        plain_years.add(int(s))\n",
        "    elif \"_oldgrowth_all_land\" in s:\n",
        "        year = s.split(\"_oldgrowth_all_land\")[0]\n",
        "        if year.isdigit():\n",
        "            years.add(int(year))\n",
        "            oldgrowth_all_land_years.add(int(year))\n",
        "    elif \"_oldgrowth\" in s:\n",
        "        year = s.split(\"_oldgrowth\")[0]\n",
        "        if year.isdigit():\n",
        "            years.add(int(year))\n",
        "            oldgrowth_years.add(int(year))\n",
        "    elif any(pattern in s for pattern in [\"_no_disturbance_since_\", \"_no_degradation_since_\"]):\n",
        "        year = s.split(\"_\")[0]\n",
        "        if year.isdigit():\n",
        "            years.add(int(year))\n",
        "        if \"_since_\" in s:\n",
        "            since_year = s.split(\"_since_\")[1]\n",
        "            if since_year.isdigit():\n",
        "                years.add(int(since_year) - 1)\n",
        "years_sorted = sorted(list(years))\n",
        "\n",
        "# Output dictionaries\n",
        "disturbance_since_dictionary = {}\n",
        "degradation_since_dictionary = {}\n",
        "deforestation_since_dictionary = {}\n",
        "print(\"disturbance_since_dictionary = {\")\n",
        "print(\"\")\n",
        "\n",
        "# 1. Process disturbance_since scenarios\n",
        "for year_a in sorted(years_sorted):\n",
        "    a_str = str(year_a)\n",
        "    for year_b in sorted(years_sorted):\n",
        "        if year_a <= year_b:\n",
        "            continue\n",
        "        b_str, b_plus1 = str(year_b), str(year_b + 1)\n",
        "\n",
        "        if year_a in plain_years and f\"{a_str}_no_disturbance_since_{b_plus1}\" in scenarios:\n",
        "            print(f\"# Disturbance in {a_str} caused by events since {b_plus1}\")\n",
        "            print(f\"  ('{a_str}', '{a_str}_no_disturbance_since_{b_plus1}'):\")\n",
        "            print(f\"    '{a_str}_disturbance_since_{b_plus1}',\")\n",
        "            print(\"\")\n",
        "            disturbance_since_dictionary[(a_str, f\"{a_str}_no_disturbance_since_{b_plus1}\")] = f\"{a_str}_disturbance_since_{b_plus1}\"\n",
        "# Process disturbance_since_oldgrowth scenarios\n",
        "for year in years_sorted:\n",
        "    y_str = str(year)\n",
        "    if (year in plain_years and year in oldgrowth_all_land_years and f\"{y_str}_oldgrowth_all_land\" in scenarios):\n",
        "        print(f\"# Disturbance in {y_str} caused by events since an oldgrowth state.\")\n",
        "        print(f\"  ('{y_str}', '{y_str}_oldgrowth_all_land'):\")\n",
        "        print(f\"    '{y_str}_disturbance_since_oldgrowth',\")\n",
        "        print(\"\")\n",
        "        disturbance_since_dictionary[(y_str, f\"{y_str}_oldgrowth_all_land\")] = f\"{y_str}_disturbance_since_oldgrowth\"\n",
        "print(\"}\\n\")\n",
        "\n",
        "# 2. Degradation since dictionary\n",
        "print(\"degradation_since_dictionary = {\\n\")\n",
        "for year_a in sorted(years_sorted):\n",
        "    a_str = str(year_a)\n",
        "    for year_b in sorted(years_sorted):\n",
        "        if year_a <= year_b:\n",
        "            continue\n",
        "        b_str, b_plus1 = str(year_b), str(year_b + 1)\n",
        "        if year_a in plain_years and f\"{a_str}_no_degradation_since_{b_plus1}\" in scenarios:\n",
        "            print(f\"# Degradation in {a_str} caused by events since {b_plus1}\")\n",
        "            print(f\"  ('{a_str}', '{a_str}_no_degradation_since_{b_plus1}'):\")\n",
        "            print(f\"    '{a_str}_degradation_since_{b_plus1}',\")\n",
        "            print(\"\")\n",
        "            degradation_since_dictionary[(a_str, f\"{a_str}_no_degradation_since_{b_plus1}\")] = f\"{a_str}_degradation_since_{b_plus1}\"\n",
        "# Process degradation_since_oldgrowth scenarios\n",
        "for year in years_sorted:\n",
        "    y_str = str(year)\n",
        "    if (year in plain_years and year in oldgrowth_years and f\"{y_str}_oldgrowth\" in scenarios):\n",
        "        print(f\"# Degradation in {y_str} caused by events since an old-growth state\")\n",
        "        print(f\"  ('{y_str}', '{y_str}_oldgrowth'):\")\n",
        "        print(f\"    '{y_str}_degradation_since_oldgrowth',\")\n",
        "        print(\"\")\n",
        "        degradation_since_dictionary[(y_str, f\"{y_str}_oldgrowth\")] = f\"{y_str}_degradation_since_oldgrowth\"\n",
        "print(\"}\\n\")\n",
        "\n",
        "# 3. Deforestation since dictionary\n",
        "print(\"deforestation_since_dictionary = {\\n\")\n",
        "for year_a in sorted(years_sorted):\n",
        "    a_str = str(year_a)\n",
        "    for year_b in sorted(years_sorted):\n",
        "        if year_a <= year_b:\n",
        "            continue\n",
        "        b_str, b_plus1 = str(year_b), str(year_b + 1)\n",
        "        deg_key = (a_str, f\"{a_str}_no_degradation_since_{b_plus1}\")\n",
        "        dist_key = (a_str, f\"{a_str}_no_disturbance_since_{b_plus1}\")\n",
        "        if deg_key in degradation_since_dictionary and dist_key in disturbance_since_dictionary:\n",
        "            deg_result = degradation_since_dictionary[deg_key]\n",
        "            dist_result = disturbance_since_dictionary[dist_key]\n",
        "            defor_result = f\"{a_str}_deforestation_since_{b_plus1}\"\n",
        "            print(f\"# Deforestation in {a_str} caused by events since {b_plus1}\")\n",
        "            print(f\"  ('{deg_result}', '{dist_result}'):\")\n",
        "            print(f\"    '{defor_result}',\")\n",
        "            print(\"\")\n",
        "            deforestation_since_dictionary[(deg_result, dist_result)] = defor_result\n",
        "# Process deforestation_since_oldgrowth scenarios\n",
        "for year in years_sorted:\n",
        "    y_str = str(year)\n",
        "    deg_key = (y_str, f\"{y_str}_oldgrowth\")\n",
        "    dist_key = (y_str, f\"{y_str}_oldgrowth_all_land\")\n",
        "\n",
        "    if deg_key in degradation_since_dictionary and dist_key in disturbance_since_dictionary:\n",
        "        print(f\"# Deforestation in {y_str} caused by events since an old-growth state\")\n",
        "        deg_result = degradation_since_dictionary[deg_key]\n",
        "        dist_result = disturbance_since_dictionary[dist_key]\n",
        "        defor_result = f\"{y_str}_deforestation_since_oldgrowth\"\n",
        "        print(f\"  ('{deg_result}', '{dist_result}'):\")\n",
        "        print(f\"    '{defor_result}',\")\n",
        "        print(\"\")\n",
        "        deforestation_since_dictionary[(deg_result, dist_result)] = defor_result\n",
        "print(\"}\\n\")\n",
        "\n",
        "\n",
        "# 4. Specific year effects dictionary\n",
        "print(\"specific_year_effects_dictionary = {\\n\")\n",
        "# Collect all since results and organise by year of interest and disturbance type\n",
        "effects_by_year = {}\n",
        "# Process degradation since results\n",
        "for result_name in degradation_since_dictionary.values():\n",
        "    if \"_degradation_since_\" in result_name and \"_oldgrowth\" not in result_name:\n",
        "        parts = result_name.split(\"_degradation_since_\")\n",
        "        year_of_interest = parts[0]\n",
        "        baseline_year = int(parts[1])\n",
        "        if year_of_interest not in effects_by_year:\n",
        "            effects_by_year[year_of_interest] = {}\n",
        "        if 'degradation' not in effects_by_year[year_of_interest]:\n",
        "            effects_by_year[year_of_interest]['degradation'] = {}\n",
        "        effects_by_year[year_of_interest]['degradation'][baseline_year] = result_name\n",
        "# Process disturbance since results\n",
        "for result_name in disturbance_since_dictionary.values():\n",
        "    if \"_disturbance_since_\" in result_name and \"_oldgrowth\" not in result_name:\n",
        "        parts = result_name.split(\"_disturbance_since_\")\n",
        "        year_of_interest = parts[0]\n",
        "        baseline_year = int(parts[1])\n",
        "        if year_of_interest not in effects_by_year:\n",
        "            effects_by_year[year_of_interest] = {}\n",
        "        if 'disturbance' not in effects_by_year[year_of_interest]:\n",
        "            effects_by_year[year_of_interest]['disturbance'] = {}\n",
        "        effects_by_year[year_of_interest]['disturbance'][baseline_year] = result_name\n",
        "# Process deforestation since results\n",
        "for result_name in deforestation_since_dictionary.values():\n",
        "    if \"_deforestation_since_\" in result_name and \"_oldgrowth\" not in result_name:\n",
        "        parts = result_name.split(\"_deforestation_since_\")\n",
        "        year_of_interest = parts[0]\n",
        "        baseline_year = int(parts[1])\n",
        "        if year_of_interest not in effects_by_year:\n",
        "            effects_by_year[year_of_interest] = {}\n",
        "        if 'deforestation' not in effects_by_year[year_of_interest]:\n",
        "            effects_by_year[year_of_interest]['deforestation'] = {}\n",
        "        effects_by_year[year_of_interest]['deforestation'][baseline_year] = result_name\n",
        "# Output dictionary entries grouped by year of interest and disturbance type\n",
        "specific_year_effects_dictionary = {}\n",
        "for year_of_interest in sorted(effects_by_year.keys()):\n",
        "    year_effects = effects_by_year[year_of_interest]\n",
        "\n",
        "    # Build all effects for this year first\n",
        "    year_has_effects = False\n",
        "    all_type_effects = {}\n",
        "\n",
        "    # Process each disturbance type\n",
        "    for dist_type in ['degradation', 'deforestation', 'disturbance']:\n",
        "        if dist_type in year_effects:\n",
        "            baseline_years = sorted(year_effects[dist_type].keys())\n",
        "            # Find consecutive year pairs for specific year effects\n",
        "            type_effects = []\n",
        "            for i in range(len(baseline_years) - 1):\n",
        "                current_year = baseline_years[i]\n",
        "                next_year = baseline_years[i + 1]\n",
        "                if next_year == current_year + 1:\n",
        "                    since_current = year_effects[dist_type][current_year]\n",
        "                    since_next = year_effects[dist_type][next_year]\n",
        "                    effect_name = f\"{year_of_interest}_effect_of_{dist_type}_in_{current_year}\"\n",
        "                    type_effects.append((since_current, since_next, effect_name, current_year))\n",
        "                    specific_year_effects_dictionary[(since_current, since_next)] = effect_name\n",
        "            # Add same-year effect (copy and rename)\n",
        "            if baseline_years:\n",
        "                last_year = max(baseline_years)\n",
        "                if last_year == int(year_of_interest):\n",
        "                    since_same_year = year_effects[dist_type][last_year]\n",
        "                    same_year_effect = f\"{year_of_interest}_effect_of_{dist_type}_in_{last_year}\"\n",
        "                    type_effects.append((since_same_year, None, same_year_effect, last_year))\n",
        "                    specific_year_effects_dictionary[(since_same_year,)] = same_year_effect\n",
        "\n",
        "            if type_effects:\n",
        "                all_type_effects[dist_type] = type_effects\n",
        "                year_has_effects = True\n",
        "\n",
        "    # Only print if there are effects for this year\n",
        "    if year_has_effects:\n",
        "        print(f\"# Effects in {year_of_interest}\")\n",
        "        for dist_type in ['degradation', 'deforestation', 'disturbance']:\n",
        "            if dist_type in all_type_effects:\n",
        "                print(f\"  # {dist_type.capitalize()} effects\")\n",
        "                # Sort by effect year chronologically\n",
        "                sorted_effects = sorted(all_type_effects[dist_type], key=lambda x: x[3])\n",
        "                for since_current, since_next, effect_name, effect_year in sorted_effects:\n",
        "                    if since_next is None:  # Same-year effect (copy and rename)\n",
        "                        print(f\"  ('{since_current}',):\")\n",
        "                        print(f\"    '{effect_name}',\")\n",
        "                    else:  # Regular subtraction effect\n",
        "                        print(f\"  ('{since_current}', '{since_next}'):\")\n",
        "                        print(f\"    '{effect_name}',\")\n",
        "                print(\"\")\n",
        "print(\"}\\n\")\n",
        "\n",
        "# 5. Area-based dictionary\n",
        "print(\"area_based_dictionary = {\")\n",
        "# Get polygon names from polygons directory\n",
        "polygon_names = set()\n",
        "if os.path.exists(polygons_dir):\n",
        "    for file in os.listdir(polygons_dir):\n",
        "        if file.endswith('.gpkg'):\n",
        "            polygon_names.add(file[:-5])\n",
        "area_based_entries = []\n",
        "for scenario in scenarios:\n",
        "    parts = scenario.split('_')\n",
        "    # Check for deforestation (ends with \"Xm_degradation_buffer\")\n",
        "    if len(parts) >= 5 and parts[-1] == 'buffer' and parts[-2] == 'degradation' and parts[-3].endswith('m'):\n",
        "        alt_year, year_affix, dist_type = parts[0], parts[-4], parts[-5]\n",
        "        polygon_name = '_'.join(parts[1:-5])\n",
        "        if polygon_name in polygon_names and dist_type == 'deforestation':\n",
        "            output_name = f\"{alt_year}_deforestation_of_{polygon_name}_{year_affix}\"\n",
        "            area_based_entries.append((scenario, alt_year, output_name))\n",
        "    # Check for degradation (ends with \"degradation_YYYY\")\n",
        "    elif len(parts) >= 3 and parts[-2] == 'degradation' and parts[-1].isdigit() and len(parts[-1]) == 4:\n",
        "        alt_year, year_affix = parts[0], parts[-1]\n",
        "        polygon_name = '_'.join(parts[1:-2])\n",
        "        if polygon_name in polygon_names:\n",
        "            output_name = f\"{alt_year}_degradation_of_{polygon_name}_{year_affix}\"\n",
        "            area_based_entries.append((scenario, alt_year, output_name))\n",
        "if area_based_entries:\n",
        "    print(\"\\n# Area-based disturbance from alternate scenarios\")\n",
        "    for scenario, alt_year, output_name in sorted(area_based_entries):\n",
        "        print(f\"  ('{scenario}', '{alt_year}'):\")\n",
        "        print(f\"    '{output_name}',\")\n",
        "print(\"}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFNUYdP3Fibv"
      },
      "outputs": [],
      "source": [
        "disturbance_since_dictionary = {\n",
        "\n",
        "# Disturbance in 2021 caused by events since 1993\n",
        "  ('2021', '2021_no_disturbance_since_1993'):\n",
        "    '2021_disturbance_since_1993',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 1996\n",
        "  ('2024', '2024_no_disturbance_since_1996'):\n",
        "    '2024_disturbance_since_1996',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 1997\n",
        "  ('2024', '2024_no_disturbance_since_1997'):\n",
        "    '2024_disturbance_since_1997',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 1998\n",
        "  ('2024', '2024_no_disturbance_since_1998'):\n",
        "    '2024_disturbance_since_1998',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 1999\n",
        "  ('2024', '2024_no_disturbance_since_1999'):\n",
        "    '2024_disturbance_since_1999',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2000\n",
        "  ('2024', '2024_no_disturbance_since_2000'):\n",
        "    '2024_disturbance_since_2000',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2001\n",
        "  ('2024', '2024_no_disturbance_since_2001'):\n",
        "    '2024_disturbance_since_2001',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2002\n",
        "  ('2024', '2024_no_disturbance_since_2002'):\n",
        "    '2024_disturbance_since_2002',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2003\n",
        "  ('2024', '2024_no_disturbance_since_2003'):\n",
        "    '2024_disturbance_since_2003',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2004\n",
        "  ('2024', '2024_no_disturbance_since_2004'):\n",
        "    '2024_disturbance_since_2004',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2005\n",
        "  ('2024', '2024_no_disturbance_since_2005'):\n",
        "    '2024_disturbance_since_2005',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2006\n",
        "  ('2024', '2024_no_disturbance_since_2006'):\n",
        "    '2024_disturbance_since_2006',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2007\n",
        "  ('2024', '2024_no_disturbance_since_2007'):\n",
        "    '2024_disturbance_since_2007',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2008\n",
        "  ('2024', '2024_no_disturbance_since_2008'):\n",
        "    '2024_disturbance_since_2008',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2009\n",
        "  ('2024', '2024_no_disturbance_since_2009'):\n",
        "    '2024_disturbance_since_2009',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2010\n",
        "  ('2024', '2024_no_disturbance_since_2010'):\n",
        "    '2024_disturbance_since_2010',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2011\n",
        "  ('2024', '2024_no_disturbance_since_2011'):\n",
        "    '2024_disturbance_since_2011',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2012\n",
        "  ('2024', '2024_no_disturbance_since_2012'):\n",
        "    '2024_disturbance_since_2012',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2013\n",
        "  ('2024', '2024_no_disturbance_since_2013'):\n",
        "    '2024_disturbance_since_2013',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2014\n",
        "  ('2024', '2024_no_disturbance_since_2014'):\n",
        "    '2024_disturbance_since_2014',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2015\n",
        "  ('2024', '2024_no_disturbance_since_2015'):\n",
        "    '2024_disturbance_since_2015',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2016\n",
        "  ('2024', '2024_no_disturbance_since_2016'):\n",
        "    '2024_disturbance_since_2016',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2017\n",
        "  ('2024', '2024_no_disturbance_since_2017'):\n",
        "    '2024_disturbance_since_2017',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2018\n",
        "  ('2024', '2024_no_disturbance_since_2018'):\n",
        "    '2024_disturbance_since_2018',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2019\n",
        "  ('2024', '2024_no_disturbance_since_2019'):\n",
        "    '2024_disturbance_since_2019',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2020\n",
        "  ('2024', '2024_no_disturbance_since_2020'):\n",
        "    '2024_disturbance_since_2020',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2021\n",
        "  ('2024', '2024_no_disturbance_since_2021'):\n",
        "    '2024_disturbance_since_2021',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2022\n",
        "  ('2024', '2024_no_disturbance_since_2022'):\n",
        "    '2024_disturbance_since_2022',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2023\n",
        "  ('2024', '2024_no_disturbance_since_2023'):\n",
        "    '2024_disturbance_since_2023',\n",
        "\n",
        "# Disturbance in 2024 caused by events since 2024\n",
        "  ('2024', '2024_no_disturbance_since_2024'):\n",
        "    '2024_disturbance_since_2024',\n",
        "\n",
        "# Disturbance in 2021 caused by events since an oldgrowth state.\n",
        "  ('2021', '2021_oldgrowth_all_land'):\n",
        "    '2021_disturbance_since_oldgrowth',\n",
        "\n",
        "# Disturbance in 2024 caused by events since an oldgrowth state.\n",
        "  ('2024', '2024_oldgrowth_all_land'):\n",
        "    '2024_disturbance_since_oldgrowth',\n",
        "\n",
        "}\n",
        "\n",
        "degradation_since_dictionary = {\n",
        "\n",
        "# Degradation in 2021 caused by events since 1993\n",
        "  ('2021', '2021_no_degradation_since_1993'):\n",
        "    '2021_degradation_since_1993',\n",
        "\n",
        "# Degradation in 2024 caused by events since 1996\n",
        "  ('2024', '2024_no_degradation_since_1996'):\n",
        "    '2024_degradation_since_1996',\n",
        "\n",
        "# Degradation in 2021 caused by events since an old-growth state\n",
        "  ('2021', '2021_oldgrowth'):\n",
        "    '2021_degradation_since_oldgrowth',\n",
        "\n",
        "# Degradation in 2024 caused by events since an old-growth state\n",
        "  ('2024', '2024_oldgrowth'):\n",
        "    '2024_degradation_since_oldgrowth',\n",
        "\n",
        "}\n",
        "\n",
        "deforestation_since_dictionary = {\n",
        "\n",
        "# Deforestation in 2021 caused by events since 1993\n",
        "  ('2021_degradation_since_1993', '2021_disturbance_since_1993'):\n",
        "    '2021_deforestation_since_1993',\n",
        "\n",
        "# Deforestation in 2024 caused by events since 1996\n",
        "  ('2024_degradation_since_1996', '2024_disturbance_since_1996'):\n",
        "    '2024_deforestation_since_1996',\n",
        "\n",
        "# Deforestation in 2021 caused by events since an old-growth state\n",
        "  ('2021_degradation_since_oldgrowth', '2021_disturbance_since_oldgrowth'):\n",
        "    '2021_deforestation_since_oldgrowth',\n",
        "\n",
        "# Deforestation in 2024 caused by events since an old-growth state\n",
        "  ('2024_degradation_since_oldgrowth', '2024_disturbance_since_oldgrowth'):\n",
        "    '2024_deforestation_since_oldgrowth',\n",
        "\n",
        "}\n",
        "\n",
        "specific_year_effects_dictionary = {\n",
        "\n",
        "# Effects in 2024\n",
        "  # Disturbance effects\n",
        "  ('2024_disturbance_since_1996', '2024_disturbance_since_1997'):\n",
        "    '2024_effect_of_disturbance_in_1996',\n",
        "  ('2024_disturbance_since_1997', '2024_disturbance_since_1998'):\n",
        "    '2024_effect_of_disturbance_in_1997',\n",
        "  ('2024_disturbance_since_1998', '2024_disturbance_since_1999'):\n",
        "    '2024_effect_of_disturbance_in_1998',\n",
        "  ('2024_disturbance_since_1999', '2024_disturbance_since_2000'):\n",
        "    '2024_effect_of_disturbance_in_1999',\n",
        "  ('2024_disturbance_since_2000', '2024_disturbance_since_2001'):\n",
        "    '2024_effect_of_disturbance_in_2000',\n",
        "  ('2024_disturbance_since_2001', '2024_disturbance_since_2002'):\n",
        "    '2024_effect_of_disturbance_in_2001',\n",
        "  ('2024_disturbance_since_2002', '2024_disturbance_since_2003'):\n",
        "    '2024_effect_of_disturbance_in_2002',\n",
        "  ('2024_disturbance_since_2003', '2024_disturbance_since_2004'):\n",
        "    '2024_effect_of_disturbance_in_2003',\n",
        "  ('2024_disturbance_since_2004', '2024_disturbance_since_2005'):\n",
        "    '2024_effect_of_disturbance_in_2004',\n",
        "  ('2024_disturbance_since_2005', '2024_disturbance_since_2006'):\n",
        "    '2024_effect_of_disturbance_in_2005',\n",
        "  ('2024_disturbance_since_2006', '2024_disturbance_since_2007'):\n",
        "    '2024_effect_of_disturbance_in_2006',\n",
        "  ('2024_disturbance_since_2007', '2024_disturbance_since_2008'):\n",
        "    '2024_effect_of_disturbance_in_2007',\n",
        "  ('2024_disturbance_since_2008', '2024_disturbance_since_2009'):\n",
        "    '2024_effect_of_disturbance_in_2008',\n",
        "  ('2024_disturbance_since_2009', '2024_disturbance_since_2010'):\n",
        "    '2024_effect_of_disturbance_in_2009',\n",
        "  ('2024_disturbance_since_2010', '2024_disturbance_since_2011'):\n",
        "    '2024_effect_of_disturbance_in_2010',\n",
        "  ('2024_disturbance_since_2011', '2024_disturbance_since_2012'):\n",
        "    '2024_effect_of_disturbance_in_2011',\n",
        "  ('2024_disturbance_since_2012', '2024_disturbance_since_2013'):\n",
        "    '2024_effect_of_disturbance_in_2012',\n",
        "  ('2024_disturbance_since_2013', '2024_disturbance_since_2014'):\n",
        "    '2024_effect_of_disturbance_in_2013',\n",
        "  ('2024_disturbance_since_2014', '2024_disturbance_since_2015'):\n",
        "    '2024_effect_of_disturbance_in_2014',\n",
        "  ('2024_disturbance_since_2015', '2024_disturbance_since_2016'):\n",
        "    '2024_effect_of_disturbance_in_2015',\n",
        "  ('2024_disturbance_since_2016', '2024_disturbance_since_2017'):\n",
        "    '2024_effect_of_disturbance_in_2016',\n",
        "  ('2024_disturbance_since_2017', '2024_disturbance_since_2018'):\n",
        "    '2024_effect_of_disturbance_in_2017',\n",
        "  ('2024_disturbance_since_2018', '2024_disturbance_since_2019'):\n",
        "    '2024_effect_of_disturbance_in_2018',\n",
        "  ('2024_disturbance_since_2019', '2024_disturbance_since_2020'):\n",
        "    '2024_effect_of_disturbance_in_2019',\n",
        "  ('2024_disturbance_since_2020', '2024_disturbance_since_2021'):\n",
        "    '2024_effect_of_disturbance_in_2020',\n",
        "  ('2024_disturbance_since_2021', '2024_disturbance_since_2022'):\n",
        "    '2024_effect_of_disturbance_in_2021',\n",
        "  ('2024_disturbance_since_2022', '2024_disturbance_since_2023'):\n",
        "    '2024_effect_of_disturbance_in_2022',\n",
        "  ('2024_disturbance_since_2023', '2024_disturbance_since_2024'):\n",
        "    '2024_effect_of_disturbance_in_2023',\n",
        "  ('2024_disturbance_since_2024',):\n",
        "    '2024_effect_of_disturbance_in_2024',\n",
        "\n",
        "}\n",
        "\n",
        "area_based_dictionary = {\n",
        "\n",
        "# Area-based disturbance from alternate scenarios\n",
        "  ('2024_road_mat_daling_deforestation_2023_30m_degradation_buffer', '2024'):\n",
        "    '2024_deforestation_of_road_mat_daling_2023',\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate disturbance"
      ],
      "metadata": {
        "id": "ozToZSzop-V8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P-0bj40AxZ2"
      },
      "outputs": [],
      "source": [
        "# Apply floor constraint preventing degradation from exceeding total disturbance.\n",
        "# Conceptually, disturbance = degradation + deforestation.\n",
        "# This can happen in rare cases where edge effects from non-forest have a positive\n",
        "# predicted impact on AGBD, especially at high elevation.\n",
        "apply_degradation_floor = False\n",
        "# Caps all positive differences, again mainly from rare edge effects cases.\n",
        "# Conceptually this is disturbance loss rather than effect of disturbance.\n",
        "cap_positive_differences = False\n",
        "# Uncertainty propagation will result in changes to original precision (unlike AGBD mean)\n",
        "uncertainty_precision = 1\n",
        "\n",
        "# Calculate AGBD loss between two scenarios (array1 - array2)\n",
        "# Returns negative values for disturbance losses\n",
        "# Gains will be negligible artifacts of float precision if scenario dictionary correct\n",
        "def subtract_arrays(array1, array2):\n",
        "  diff_array = array1 - array2\n",
        "  # Remove 'positive' artefacts from Monte Carlo averaging\n",
        "  return np.where(diff_array > 0 , 0, diff_array) if cap_positive_differences else diff_array\n",
        "\n",
        "# Propagate uncertainty for forest AGBD loss calculations using standard error propagation\n",
        "# Measures uncertainty of forest AGBD change from disturbance events only\n",
        "\n",
        "# Mathematical basis: For difference Z = X - Y with relative uncertainties u_x, u_y:\n",
        "# Absolute uncertainty: σ_z = √[(X×u_x)² + (Y×u_y)²] (IPCC 2006, Eq. 3.2; 2019, Eq. 3.2A)\n",
        "# Relative uncertainty: σ_z / |Z| = σ_z / |X - Y|\n",
        "\n",
        "# Note: Liang et al. (2023) incorrectly used |X + Y| as denominator, violating standard\n",
        "# uncertainty propagation theory for differences. IPCC guidelines (2006 Section 3.2.3.1,\n",
        "# 2019 Section 3.2.3.1) specify the denominator must be the absolute value of the\n",
        "# difference |X - Y| for mathematically correct relative uncertainty calculations.\n",
        "\n",
        "# Limitation: This approach assumes independence between scenario uncertainties, but\n",
        "# scenarios using identical models and predictors are highly correlated. This results\n",
        "# in conservative (overestimated) uncertainty bounds. Liang et al. (2023) has the same\n",
        "# correlation limitation plus the mathematical error noted above.\n",
        "\n",
        "# Forest classification from external dataset determines data availability per scenario\n",
        "# External disturbance classification determines whether forest AGBD change occurred\n",
        "# Uncertainty quantifies confidence in magnitude of forest AGBD change from disturbance\n",
        "\n",
        "# References:\n",
        "# - IPCC (2006) Guidelines Vol.1 Ch.3: Uncertainties, Section 3.2.3.1\n",
        "# - IPCC (2019) Refinement Vol.1 Ch.3: Uncertainties, Section 3.2.3.1\n",
        "# - Liang et al. (2023) Remote Sensing of Environment 284:113367\n",
        "\n",
        "# Parameters:\n",
        "#   mean1, mean2: Forest AGBD values for two scenarios/timepoints (Mg/ha)\n",
        "#   uncertainty1, uncertainty2: Relative uncertainties as percentages (0-100)\n",
        "# Returns: Relative uncertainty of forest AGBD change from disturbance as percentage (0-100)\n",
        "def propagate_uncertainty(mean1, uncertainty1, mean2, uncertainty2):\n",
        "  mean_diff = mean1 - mean2\n",
        "  # Convert percentage uncertainties to decimals\n",
        "  unc1_decimal = uncertainty1 / 100.0\n",
        "  unc2_decimal = uncertainty2 / 100.0\n",
        "\n",
        "  # Handle forest/non-forest transitions where one scenario has nodata (converted to 0 mean, 0 uncertainty)\n",
        "  # Uncertainty reflects confidence in original forest AGBD estimate, not the forest mask\n",
        "  deforestation_case = (unc1_decimal == 0) & (mean1 == 0) & (unc2_decimal != 0) & (mean2 != 0)\n",
        "  reforestation_case = (unc1_decimal != 0) & (mean1 != 0) & (unc2_decimal == 0) & (mean2 == 0)\n",
        "\n",
        "  # Convert to absolute uncertainties (same units as measurements) because IPCC error propagation\n",
        "  # formula requires absolute values - relative uncertainties cannot be combined directly\n",
        "  absolute_uncertainty1 = np.multiply(mean1, unc1_decimal)\n",
        "  absolute_uncertainty2 = np.multiply(mean2, unc2_decimal)\n",
        "\n",
        "  # Combine absolute uncertainties using IPCC error propagation formula\n",
        "  # Applies when both scenarios contain forest AGBD estimates\n",
        "  absolute_uncertainty_combined = np.sqrt(\n",
        "      np.square(absolute_uncertainty1) +\n",
        "      np.square(absolute_uncertainty2)\n",
        "  )\n",
        "\n",
        "  # Calculate relative uncertainty using |difference| as denominator (IPCC standard)\n",
        "  denominator = np.abs(mean_diff)\n",
        "  # Avoid division by zero using np.divide with where parameter\n",
        "  standard_uncertainty = np.divide(absolute_uncertainty_combined, denominator,\n",
        "                                 out=np.zeros_like(absolute_uncertainty_combined),\n",
        "                                 where=(denominator != 0))\n",
        "\n",
        "  # Apply uncertainty logic for forest AGBD change from disturbance measurements\n",
        "  # Zero uncertainty when no disturbance classified: scenarios definitionally identical, not measurement-based\n",
        "  # Uncertainty measures confidence in forest AGBD change magnitude given that disturbance occurred\n",
        "  relative_uncertainty = np.where(\n",
        "      deforestation_case, unc2_decimal,  # Forest > non-forest: uncertainty equals original forest AGBD uncertainty\n",
        "      np.where(reforestation_case, unc1_decimal,  # Non-forest > forest: uncertainty equals new forest AGBD uncertainty\n",
        "               np.where((mean_diff > 0) | (denominator == 0),  # No disturbance classified or forest AGBD gains\n",
        "                       0,  # Scenarios definitionally identical when no forest disturbance occurred\n",
        "                       standard_uncertainty)))  # Standard propagation for forest degradation cases\n",
        "\n",
        "  # Convert back to percentage\n",
        "  return np.round(relative_uncertainty * 100.0, uncertainty_precision)\n",
        "\n",
        "# Determine processing mode based on source directory\n",
        "use_uncertainty = source_dir == uncertainty_dir\n",
        "\n",
        "# Progress tracking\n",
        "total_operations = len(disturbance_since_dictionary) + len(degradation_since_dictionary) + len(deforestation_since_dictionary) + len(specific_year_effects_dictionary) + len(area_based_dictionary)\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Disturbance calculation progress: {progress_index}/{total_operations}\")\n",
        "\n",
        "display(progress_label)\n",
        "\n",
        "# 1. Process disturbance_since calculations\n",
        "for (scenario1, scenario2), disturbance_name in disturbance_since_dictionary.items():\n",
        "  if use_uncertainty:\n",
        "      # Define filenames and paths for disturbance mean and uncertainty\n",
        "      mean_filename = f\"mean__{disturbance_name}__{selected_model}.tif\"\n",
        "      mean_path = join(disturbance_dir, mean_filename)\n",
        "      uncertainty_filename = f\"uncertainty__{disturbance_name}__{selected_model}.tif\"\n",
        "      uncertainty_path = join(disturbance_dir, uncertainty_filename)\n",
        "      # Skip if both files already exist\n",
        "      if exists(mean_path) and exists(uncertainty_path):\n",
        "          progress_index += 1\n",
        "          progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "          continue\n",
        "\n",
        "      scenario1_base = f\"{scenario1}__{selected_model}\"\n",
        "      scenario2_base = f\"{scenario2}__{selected_model}\"\n",
        "      # Define scenario paths, assert that both exist for both scenarios\n",
        "      scenario1_mean_path = join(predictions_dir, f\"mean__{scenario1_base}.tif\")\n",
        "      scenario1_uncertainty_path = join(predictions_dir, f\"uncertainty__{scenario1_base}.tif\")\n",
        "      scenario2_mean_path = join(predictions_dir, f\"mean__{scenario2_base}.tif\")\n",
        "      scenario2_uncertainty_path = join(predictions_dir, f\"uncertainty__{scenario2_base}.tif\")\n",
        "      assert exists(scenario1_mean_path), f\"mean__{scenario1_base}.tif does not exist.\"\n",
        "      assert exists(scenario1_uncertainty_path), f\"uncertainty__{scenario1_base}.tif does not exist.\"\n",
        "      assert exists(scenario2_mean_path), f\"mean__{scenario2_base}.tif does not exist.\"\n",
        "      assert exists(scenario2_uncertainty_path), f\"uncertainty__{scenario2_base}.tif does not exist.\"\n",
        "      # Read arrays\n",
        "      scenario1_mean = gdal.Open(scenario1_mean_path).ReadAsArray()\n",
        "      scenario1_uncertainty = gdal.Open(scenario1_uncertainty_path).ReadAsArray()\n",
        "      scenario2_mean = gdal.Open(scenario2_mean_path).ReadAsArray()\n",
        "      scenario2_uncertainty = gdal.Open(scenario2_uncertainty_path).ReadAsArray()\n",
        "      # Fill scenario nodata values with 0 if they are not nodatavalues in the other scenario\n",
        "      scenario1_mean = np.where((scenario1_mean == nodatavalue) & (scenario2_mean != nodatavalue), 0, scenario1_mean)\n",
        "      scenario1_uncertainty = np.where((scenario1_uncertainty == nodatavalue) & (scenario2_uncertainty != nodatavalue), 0, scenario1_uncertainty)\n",
        "      scenario2_mean = np.where((scenario2_mean == nodatavalue) & (scenario1_mean != nodatavalue), 0, scenario2_mean)\n",
        "      scenario2_uncertainty = np.where((scenario2_uncertainty == nodatavalue) & (scenario1_uncertainty != nodatavalue), 0, scenario2_uncertainty)\n",
        "      # Create disturbance arrays where the value is not 'nodatavalue' in both scenarios\n",
        "      dist_mean_array = np.where(scenario1_mean == nodatavalue, nodatavalue, subtract_arrays(scenario1_mean, scenario2_mean))\n",
        "      dist_uncertainty_array = np.where(scenario1_mean == nodatavalue, nodatavalue,\n",
        "                                       propagate_uncertainty(scenario1_mean, scenario1_uncertainty, scenario2_mean, scenario2_uncertainty))\n",
        "      # Round arrays before export\n",
        "      dist_mean_array_rounded = np.round(dist_mean_array)\n",
        "      # Set uncertainty to 0 where rounded difference is 0\n",
        "      dist_uncertainty_array = np.where(dist_mean_array_rounded == 0, 0, dist_uncertainty_array)\n",
        "      # Export disturbance rasters\n",
        "      export_array_as_tif(dist_mean_array_rounded, mean_path, template=scenario1_mean_path)\n",
        "      export_array_as_tif(dist_uncertainty_array, uncertainty_path, template=scenario1_mean_path)\n",
        "  else:\n",
        "      # Define filenames and paths for disturbance\n",
        "      dist_filename = f\"{disturbance_name}__{selected_model}.tif\"\n",
        "      dist_path = join(disturbance_dir, dist_filename)\n",
        "      # Skip if file already exists\n",
        "      if exists(dist_path):\n",
        "          progress_index += 1\n",
        "          progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "          continue\n",
        "      # Define scenario paths, assert that both exist for both scenarios\n",
        "      scenario1_path = join(predictions_dir, f\"{scenario1}__{selected_model}.tif\")\n",
        "      assert exists(scenario1_path), f\"{scenario1_path} does not exist.\"\n",
        "      scenario2_path = join(predictions_dir, f\"{scenario2}__{selected_model}.tif\")\n",
        "      assert exists(scenario2_path), f\"{scenario2_path} does not exist.\"\n",
        "      # Read arrays\n",
        "      scenario1_array_temp = gdal.Open(scenario1_path).ReadAsArray()\n",
        "      scenario2_array_temp = gdal.Open(scenario2_path).ReadAsArray()\n",
        "      # Fill scenario nodata values with 0 if they are not nodatavalues in the other scenario\n",
        "      scenario1_array = np.where((scenario1_array_temp == nodatavalue) & (scenario2_array_temp != nodatavalue), 0, scenario1_array_temp)\n",
        "      scenario2_array = np.where((scenario2_array_temp == nodatavalue) & (scenario1_array != nodatavalue), 0, scenario2_array_temp)\n",
        "      # Create disturbance arrays where the value is not 'nodatavalue' in both scenarios\n",
        "      dist_array = np.where(scenario1_array==nodatavalue, nodatavalue, subtract_arrays(scenario1_array, scenario2_array))\n",
        "      # Round array before export\n",
        "      dist_array_rounded = np.round(dist_array)\n",
        "      # Export disturbance raster\n",
        "      export_array_as_tif(dist_array_rounded, dist_path, template = scenario1_path)\n",
        "\n",
        "  # Update progress\n",
        "  progress_index += 1\n",
        "  progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "# 2. Process degradation_since calculations\n",
        "for (scenario1, scenario2), disturbance_name in degradation_since_dictionary.items():\n",
        "  if use_uncertainty:\n",
        "      # Define filenames and paths for disturbance mean and uncertainty\n",
        "      mean_filename = f\"mean__{disturbance_name}__{selected_model}.tif\"\n",
        "      mean_path = join(disturbance_dir, mean_filename)\n",
        "      uncertainty_filename = f\"uncertainty__{disturbance_name}__{selected_model}.tif\"\n",
        "      uncertainty_path = join(disturbance_dir, uncertainty_filename)\n",
        "      # Skip if both files already exist\n",
        "      if exists(mean_path) and exists(uncertainty_path):\n",
        "          progress_index += 1\n",
        "          progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "          continue\n",
        "\n",
        "      scenario1_base = f\"{scenario1}__{selected_model}\"\n",
        "      scenario2_base = f\"{scenario2}__{selected_model}\"\n",
        "      # Define scenario paths, assert that both exist for both scenarios\n",
        "      scenario1_mean_path = join(predictions_dir, f\"mean__{scenario1_base}.tif\")\n",
        "      scenario1_uncertainty_path = join(predictions_dir, f\"uncertainty__{scenario1_base}.tif\")\n",
        "      scenario2_mean_path = join(predictions_dir, f\"mean__{scenario2_base}.tif\")\n",
        "      scenario2_uncertainty_path = join(predictions_dir, f\"uncertainty__{scenario2_base}.tif\")\n",
        "      assert exists(scenario1_mean_path), f\"mean__{scenario1_base}.tif does not exist.\"\n",
        "      assert exists(scenario1_uncertainty_path), f\"uncertainty__{scenario1_base}.tif does not exist.\"\n",
        "      assert exists(scenario2_mean_path), f\"mean__{scenario2_base}.tif does not exist.\"\n",
        "      assert exists(scenario2_uncertainty_path), f\"uncertainty__{scenario2_base}.tif does not exist.\"\n",
        "      # Read arrays\n",
        "      scenario1_mean = gdal.Open(scenario1_mean_path).ReadAsArray()\n",
        "      scenario1_uncertainty = gdal.Open(scenario1_uncertainty_path).ReadAsArray()\n",
        "      scenario2_mean = gdal.Open(scenario2_mean_path).ReadAsArray()\n",
        "      scenario2_uncertainty = gdal.Open(scenario2_uncertainty_path).ReadAsArray()\n",
        "      # Fill scenario nodata values with 0 if they are not nodatavalues in the other scenario\n",
        "      scenario1_mean = np.where((scenario1_mean == nodatavalue) & (scenario2_mean != nodatavalue), 0, scenario1_mean)\n",
        "      scenario1_uncertainty = np.where((scenario1_uncertainty == nodatavalue) & (scenario2_uncertainty != nodatavalue), 0, scenario1_uncertainty)\n",
        "      scenario2_mean = np.where((scenario2_mean == nodatavalue) & (scenario1_mean != nodatavalue), 0, scenario2_mean)\n",
        "      scenario2_uncertainty = np.where((scenario2_uncertainty == nodatavalue) & (scenario1_uncertainty != nodatavalue), 0, scenario2_uncertainty)\n",
        "      # Create disturbance arrays where the value is not 'nodatavalue' in both scenarios\n",
        "      dist_mean_array = np.where(scenario1_mean == nodatavalue, nodatavalue, subtract_arrays(scenario1_mean, scenario2_mean))\n",
        "      dist_uncertainty_array = np.where(scenario1_mean == nodatavalue, nodatavalue,\n",
        "                                       propagate_uncertainty(scenario1_mean, scenario1_uncertainty, scenario2_mean, scenario2_uncertainty))\n",
        "\n",
        "      # Check for matching disturbance file and apply floor constraint\n",
        "      if apply_degradation_floor and 'degradation_since' in disturbance_name:\n",
        "          # construct matching disturbance filename\n",
        "          equiv_disturbance_name = disturbance_name.replace('degradation_since', 'disturbance_since')\n",
        "          equiv_dist_mean_filename = f\"mean__{equiv_disturbance_name}__{selected_model}.tif\"\n",
        "          equiv_dist_uncertainty_filename = f\"uncertainty__{equiv_disturbance_name}__{selected_model}.tif\"\n",
        "\n",
        "          # get list of files in disturbance directory\n",
        "          disturbance_files = os.listdir(disturbance_dir)\n",
        "\n",
        "          # check if matching files exist\n",
        "          if equiv_dist_mean_filename in disturbance_files and equiv_dist_uncertainty_filename in disturbance_files:\n",
        "              print(f\"Applying floor constraint: {disturbance_name} will be constrained by {equiv_disturbance_name}\")\n",
        "\n",
        "              equiv_dist_mean_path = join(disturbance_dir, equiv_dist_mean_filename)\n",
        "              equiv_dist_uncertainty_path = join(disturbance_dir, equiv_dist_uncertainty_filename)\n",
        "\n",
        "              equiv_dist_mean = gdal.Open(equiv_dist_mean_path).ReadAsArray()\n",
        "              equiv_dist_uncertainty = gdal.Open(equiv_dist_uncertainty_path).ReadAsArray()\n",
        "\n",
        "              # Apply floor constraint: degradation cannot be more negative than disturbance\n",
        "              floor_applied = ((dist_mean_array != nodatavalue) & (equiv_dist_mean != nodatavalue) &\n",
        "                             (dist_mean_array < equiv_dist_mean))\n",
        "              dist_mean_array = np.where(floor_applied, equiv_dist_mean, dist_mean_array)\n",
        "              # Use disturbance uncertainty where floor constraint applied\n",
        "              dist_uncertainty_array = np.where(floor_applied, equiv_dist_uncertainty, dist_uncertainty_array)\n",
        "          else:\n",
        "              print(f\"No floor constraint applied: {equiv_disturbance_name} files not found for {disturbance_name}\")\n",
        "\n",
        "      # Round arrays before export\n",
        "      dist_mean_array_rounded = np.round(dist_mean_array)\n",
        "      # Set uncertainty to 0 where rounded difference is 0\n",
        "      dist_uncertainty_array = np.where(dist_mean_array_rounded == 0, 0, dist_uncertainty_array)\n",
        "      # Export disturbance rasters\n",
        "      export_array_as_tif(dist_mean_array_rounded, mean_path, template=scenario1_mean_path)\n",
        "      export_array_as_tif(dist_uncertainty_array, uncertainty_path, template=scenario1_mean_path)\n",
        "  else:\n",
        "      # Define filenames and paths for disturbance\n",
        "      dist_filename = f\"{disturbance_name}__{selected_model}.tif\"\n",
        "      dist_path = join(disturbance_dir, dist_filename)\n",
        "      # Skip if file already exists\n",
        "      if exists(dist_path):\n",
        "          progress_index += 1\n",
        "          progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "          continue\n",
        "      # Define scenario paths, assert that both exist for both scenarios\n",
        "      scenario1_path = join(predictions_dir, f\"{scenario1}__{selected_model}.tif\")\n",
        "      assert exists(scenario1_path), f\"{scenario1_path} does not exist.\"\n",
        "      scenario2_path = join(predictions_dir, f\"{scenario2}__{selected_model}.tif\")\n",
        "      assert exists(scenario2_path), f\"{scenario2_path} does not exist.\"\n",
        "      # Read arrays\n",
        "      scenario1_array_temp = gdal.Open(scenario1_path).ReadAsArray()\n",
        "      scenario2_array_temp = gdal.Open(scenario2_path).ReadAsArray()\n",
        "      # Fill scenario nodata values with 0 if they are not nodatavalues in the other scenario\n",
        "      scenario1_array = np.where((scenario1_array_temp == nodatavalue) & (scenario2_array_temp != nodatavalue), 0, scenario1_array_temp)\n",
        "      scenario2_array = np.where((scenario2_array_temp == nodatavalue) & (scenario1_array != nodatavalue), 0, scenario2_array_temp)\n",
        "      # Create disturbance arrays where the value is not 'nodatavalue' in both scenarios\n",
        "      dist_array = np.where(scenario1_array==nodatavalue, nodatavalue, subtract_arrays(scenario1_array, scenario2_array))\n",
        "\n",
        "      # Check for matching disturbance file and apply floor constraint\n",
        "      if apply_degradation_floor and 'degradation_since' in disturbance_name:\n",
        "          # construct matching disturbance filename\n",
        "          equiv_disturbance_name = disturbance_name.replace('degradation_since', 'disturbance_since')\n",
        "          equiv_dist_filename = f\"{equiv_disturbance_name}__{selected_model}.tif\"\n",
        "\n",
        "          # get list of files in disturbance directory\n",
        "          disturbance_files = os.listdir(disturbance_dir)\n",
        "\n",
        "          # check if matching file exists\n",
        "          if equiv_dist_filename in disturbance_files:\n",
        "              print(f\"Applying floor constraint: {disturbance_name} will be constrained by {equiv_disturbance_name}\")\n",
        "\n",
        "              equiv_dist_path = join(disturbance_dir, equiv_dist_filename)\n",
        "              equiv_dist_array = gdal.Open(equiv_dist_path).ReadAsArray()\n",
        "\n",
        "              # Apply floor constraint: degradation cannot be more negative than disturbance\n",
        "              dist_array = np.where(\n",
        "                  (dist_array != nodatavalue) & (equiv_dist_array != nodatavalue) & (dist_array < equiv_dist_array),\n",
        "                  equiv_dist_array,\n",
        "                  dist_array\n",
        "              )\n",
        "          else:\n",
        "              print(f\"No floor constraint applied: {equiv_disturbance_name} file not found for {disturbance_name}\")\n",
        "\n",
        "      # Round array before export\n",
        "      dist_array_rounded = np.round(dist_array)\n",
        "      # Export disturbance raster\n",
        "      export_array_as_tif(dist_array_rounded, dist_path, template = scenario1_path)\n",
        "\n",
        "  # Update progress\n",
        "  progress_index += 1\n",
        "  progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "# 3. Process deforestation_since calculations\n",
        "for (dist1_name, dist2_name), disturbance_name in deforestation_since_dictionary.items():\n",
        "  if use_uncertainty:\n",
        "      # Define filenames and paths of disturbance .tifs\n",
        "      mean_filename = f\"mean__{disturbance_name}__{selected_model}.tif\"\n",
        "      mean_path = join(disturbance_dir, mean_filename)\n",
        "      uncertainty_filename = f\"uncertainty__{disturbance_name}__{selected_model}.tif\"\n",
        "      uncertainty_path = join(disturbance_dir, uncertainty_filename)\n",
        "      # Skip if both files already exist\n",
        "      if exists(mean_path) and exists(uncertainty_path):\n",
        "          progress_index += 1\n",
        "          progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "          continue\n",
        "      # Define disturbance paths, assert that both exist\n",
        "      dist1_mean_path = join(disturbance_dir, f\"mean__{dist1_name}__{selected_model}.tif\")\n",
        "      dist1_uncertainty_path = join(disturbance_dir, f\"uncertainty__{dist1_name}__{selected_model}.tif\")\n",
        "      dist2_mean_path = join(disturbance_dir, f\"mean__{dist2_name}__{selected_model}.tif\")\n",
        "      dist2_uncertainty_path = join(disturbance_dir, f\"uncertainty__{dist2_name}__{selected_model}.tif\")\n",
        "      assert exists(dist1_mean_path), f\"{dist1_mean_path} does not exist.\"\n",
        "      assert exists(dist1_uncertainty_path), f\"{dist1_uncertainty_path} does not exist.\"\n",
        "      assert exists(dist2_mean_path), f\"{dist2_mean_path} does not exist.\"\n",
        "      assert exists(dist2_uncertainty_path), f\"{dist2_uncertainty_path} does not exist.\"\n",
        "      # Read arrays\n",
        "      dist1_mean = gdal.Open(dist1_mean_path).ReadAsArray()\n",
        "      dist1_uncertainty = gdal.Open(dist1_uncertainty_path).ReadAsArray()\n",
        "      dist2_mean = gdal.Open(dist2_mean_path).ReadAsArray()\n",
        "      dist2_uncertainty = gdal.Open(dist2_uncertainty_path).ReadAsArray()\n",
        "      # Fill disturbance nodata values with 0 if they are not nodatavalues in the other disturbance\n",
        "      dist1_mean = np.where((dist1_mean == nodatavalue) & (dist2_mean != nodatavalue), 0, dist1_mean)\n",
        "      dist1_uncertainty = np.where((dist1_uncertainty == nodatavalue) & (dist2_uncertainty != nodatavalue), 0, dist1_uncertainty)\n",
        "      dist2_mean = np.where((dist2_mean == nodatavalue) & (dist1_mean != nodatavalue), 0, dist2_mean)\n",
        "      dist2_uncertainty = np.where((dist2_uncertainty == nodatavalue) & (dist1_uncertainty != nodatavalue), 0, dist2_uncertainty)\n",
        "      # Create disturbance arrays where the value is not 'nodatavalue' in disturbance (second array)\n",
        "      result_mean = np.where(dist2_mean == nodatavalue, nodatavalue, subtract_arrays(dist2_mean, dist1_mean))\n",
        "      result_uncertainty = np.where(dist2_mean == nodatavalue, nodatavalue,\n",
        "                                   propagate_uncertainty(dist2_mean, dist2_uncertainty, dist1_mean, dist1_uncertainty))\n",
        "      # Round arrays before export\n",
        "      result_mean_rounded = np.round(result_mean)\n",
        "      # Set uncertainty to 0 where rounded difference is 0\n",
        "      result_uncertainty = np.where(result_mean_rounded == 0, 0, result_uncertainty)\n",
        "      # Export disturbance rasters\n",
        "      export_array_as_tif(result_mean_rounded, mean_path, template=dist2_mean_path)\n",
        "      export_array_as_tif(result_uncertainty, uncertainty_path, template=dist2_mean_path)\n",
        "  else:\n",
        "      # Define filenames and paths of disturbance .tifs\n",
        "      output_filename = f\"{disturbance_name}__{selected_model}.tif\"\n",
        "      output_path = join(disturbance_dir, output_filename)\n",
        "      # Skip if file already exists\n",
        "      if exists(output_path):\n",
        "          progress_index += 1\n",
        "          progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "          continue\n",
        "      # Define disturbance paths, assert that both exist\n",
        "      dist1_path = join(disturbance_dir, f\"{dist1_name}__{selected_model}.tif\")\n",
        "      assert exists(dist1_path), f\"{dist1_path} does not exist.\"\n",
        "      dist2_path = join(disturbance_dir, f\"{dist2_name}__{selected_model}.tif\")\n",
        "      assert exists(dist2_path), f\"{dist2_path} does not exist.\"\n",
        "      # Read arrays\n",
        "      dist1_array_temp = gdal.Open(dist1_path).ReadAsArray()\n",
        "      dist2_array_temp = gdal.Open(dist2_path).ReadAsArray()\n",
        "      # Fill disturbance nodata values with 0 if they are not nodatavalues in the other disturbance\n",
        "      dist1_array = np.where((dist1_array_temp == nodatavalue) & (dist2_array_temp != nodatavalue), 0, dist1_array_temp)\n",
        "      dist2_array = np.where((dist2_array_temp == nodatavalue) & (dist1_array != nodatavalue), 0, dist2_array_temp)\n",
        "      # Create disturbance arrays where the value is not 'nodatavalue' in disturbance (second array)\n",
        "      result_array = np.where(dist2_array==nodatavalue, nodatavalue, subtract_arrays(dist2_array, dist1_array))\n",
        "      # Round array before export\n",
        "      result_array_rounded = np.round(result_array)\n",
        "      # Export disturbance raster\n",
        "      export_array_as_tif(result_array_rounded, output_path, template = dist2_path)\n",
        "\n",
        "  # Update progress\n",
        "  progress_index += 1\n",
        "  progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "# 4. Process specific year effects\n",
        "for dist_key, disturbance_name in specific_year_effects_dictionary.items():\n",
        "  if len(dist_key) == 1:  # Direct export operation (same-year effect)\n",
        "      source_name = dist_key[0]\n",
        "      if use_uncertainty:\n",
        "          # Define source and target filenames and paths\n",
        "          source_mean_filename = f\"mean__{source_name}__{selected_model}.tif\"\n",
        "          source_mean_path = join(disturbance_dir, source_mean_filename)\n",
        "          source_uncertainty_filename = f\"uncertainty__{source_name}__{selected_model}.tif\"\n",
        "          source_uncertainty_path = join(disturbance_dir, source_uncertainty_filename)\n",
        "          target_mean_filename = f\"mean__{disturbance_name}__{selected_model}.tif\"\n",
        "          target_mean_path = join(disturbance_dir, target_mean_filename)\n",
        "          target_uncertainty_filename = f\"uncertainty__{disturbance_name}__{selected_model}.tif\"\n",
        "          target_uncertainty_path = join(disturbance_dir, target_uncertainty_filename)\n",
        "          # Skip if both target files already exist\n",
        "          if exists(target_mean_path) and exists(target_uncertainty_path):\n",
        "              progress_index += 1\n",
        "              progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "              continue\n",
        "          # Assert source files exist\n",
        "          assert exists(source_mean_path), f\"{source_mean_path} does not exist.\"\n",
        "          assert exists(source_uncertainty_path), f\"{source_uncertainty_path} does not exist.\"\n",
        "          # Read arrays\n",
        "          source_mean = gdal.Open(source_mean_path).ReadAsArray()\n",
        "          source_uncertainty = gdal.Open(source_uncertainty_path).ReadAsArray()\n",
        "          # Round arrays before export\n",
        "          source_mean_rounded = np.round(source_mean)\n",
        "          # Set uncertainty to 0 where rounded difference is 0\n",
        "          source_uncertainty = np.where(source_mean_rounded == 0, 0, source_uncertainty)\n",
        "          # Export arrays directly\n",
        "          export_array_as_tif(source_mean_rounded, target_mean_path, template=source_mean_path)\n",
        "          export_array_as_tif(source_uncertainty, target_uncertainty_path, template=source_mean_path)\n",
        "      else:\n",
        "          # Define source and target filenames and paths\n",
        "          source_filename = f\"{source_name}__{selected_model}.tif\"\n",
        "          source_path = join(disturbance_dir, source_filename)\n",
        "          target_filename = f\"{disturbance_name}__{selected_model}.tif\"\n",
        "          target_path = join(disturbance_dir, target_filename)\n",
        "          # Skip if target file already exists\n",
        "          if exists(target_path):\n",
        "              progress_index += 1\n",
        "              progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "              continue\n",
        "          # Assert source file exists\n",
        "          assert exists(source_path), f\"{source_path} does not exist.\"\n",
        "          # Read array\n",
        "          source_array = gdal.Open(source_path).ReadAsArray()\n",
        "          # Round array before export\n",
        "          source_array_rounded = np.round(source_array)\n",
        "          # Export array directly\n",
        "          export_array_as_tif(source_array_rounded, target_path, template=source_path)\n",
        "  else:  # Subtraction operation (two-element tuple)\n",
        "      dist1_name, dist2_name = dist_key\n",
        "      if use_uncertainty:\n",
        "          # Define filenames and paths of disturbance .tifs\n",
        "          mean_filename = f\"mean__{disturbance_name}__{selected_model}.tif\"\n",
        "          mean_path = join(disturbance_dir, mean_filename)\n",
        "          uncertainty_filename = f\"uncertainty__{disturbance_name}__{selected_model}.tif\"\n",
        "          uncertainty_path = join(disturbance_dir, uncertainty_filename)\n",
        "          # Skip if both files already exist\n",
        "          if exists(mean_path) and exists(uncertainty_path):\n",
        "              progress_index += 1\n",
        "              progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "              continue\n",
        "          # Define disturbance paths, assert that both exist\n",
        "          dist1_mean_path = join(disturbance_dir, f\"mean__{dist1_name}__{selected_model}.tif\")\n",
        "          dist1_uncertainty_path = join(disturbance_dir, f\"uncertainty__{dist1_name}__{selected_model}.tif\")\n",
        "          dist2_mean_path = join(disturbance_dir, f\"mean__{dist2_name}__{selected_model}.tif\")\n",
        "          dist2_uncertainty_path = join(disturbance_dir, f\"uncertainty__{dist2_name}__{selected_model}.tif\")\n",
        "          assert exists(dist1_mean_path), f\"{dist1_mean_path} does not exist.\"\n",
        "          assert exists(dist1_uncertainty_path), f\"{dist1_uncertainty_path} does not exist.\"\n",
        "          assert exists(dist2_mean_path), f\"{dist2_mean_path} does not exist.\"\n",
        "          assert exists(dist2_uncertainty_path), f\"{dist2_uncertainty_path} does not exist.\"\n",
        "          # Read arrays\n",
        "          dist1_mean = gdal.Open(dist1_mean_path).ReadAsArray()\n",
        "          dist1_uncertainty = gdal.Open(dist1_uncertainty_path).ReadAsArray()\n",
        "          dist2_mean = gdal.Open(dist2_mean_path).ReadAsArray()\n",
        "          dist2_uncertainty = gdal.Open(dist2_uncertainty_path).ReadAsArray()\n",
        "          # Fill disturbance nodata values with 0 if they are not nodatavalues in the other disturbance\n",
        "          dist1_mean = np.where((dist1_mean == nodatavalue) & (dist2_mean != nodatavalue), 0, dist1_mean)\n",
        "          dist1_uncertainty = np.where((dist1_uncertainty == nodatavalue) & (dist2_uncertainty != nodatavalue), 0, dist1_uncertainty)\n",
        "          dist2_mean = np.where((dist2_mean == nodatavalue) & (dist1_mean != nodatavalue), 0, dist2_mean)\n",
        "          dist2_uncertainty = np.where((dist2_uncertainty == nodatavalue) & (dist1_uncertainty != nodatavalue), 0, dist2_uncertainty)\n",
        "          # Create disturbance arrays where the value is not 'nodatavalue' in both scenarios\n",
        "          result_mean = np.where(dist1_mean == nodatavalue, nodatavalue, subtract_arrays(dist1_mean, dist2_mean))\n",
        "          result_uncertainty = np.where(dist1_mean == nodatavalue, nodatavalue,\n",
        "                                       propagate_uncertainty(dist1_mean, dist1_uncertainty, dist2_mean, dist2_uncertainty))\n",
        "          # Round arrays before export\n",
        "          result_mean_rounded = np.round(result_mean)\n",
        "          # Set uncertainty to 0 where rounded difference is 0\n",
        "          result_uncertainty = np.where(result_mean_rounded == 0, 0, result_uncertainty)\n",
        "          # Export disturbance rasters\n",
        "          export_array_as_tif(result_mean_rounded, mean_path, template=dist1_mean_path)\n",
        "          export_array_as_tif(result_uncertainty, uncertainty_path, template=dist1_mean_path)\n",
        "      else:\n",
        "          # Define filenames and paths of disturbance .tifs\n",
        "          output_filename = f\"{disturbance_name}__{selected_model}.tif\"\n",
        "          output_path = join(disturbance_dir, output_filename)\n",
        "          # Skip if file already exists\n",
        "          if exists(output_path):\n",
        "              progress_index += 1\n",
        "              progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "              continue\n",
        "          # Define disturbance paths, assert that both exist\n",
        "          dist1_path = join(disturbance_dir, f\"{dist1_name}__{selected_model}.tif\")\n",
        "          assert exists(dist1_path), f\"{dist1_path} does not exist.\"\n",
        "          dist2_path = join(disturbance_dir, f\"{dist2_name}__{selected_model}.tif\")\n",
        "          assert exists(dist2_path), f\"{dist2_path} does not exist.\"\n",
        "          # Read arrays\n",
        "          dist1_array_temp = gdal.Open(dist1_path).ReadAsArray()\n",
        "          dist2_array_temp = gdal.Open(dist2_path).ReadAsArray()\n",
        "          # Fill disturbance nodata values with 0 if they are not nodatavalues in the other disturbance\n",
        "          dist1_array = np.where((dist1_array_temp == nodatavalue) & (dist2_array_temp != nodatavalue), 0, dist1_array_temp)\n",
        "          dist2_array = np.where((dist2_array_temp == nodatavalue) & (dist1_array != nodatavalue), 0, dist2_array_temp)\n",
        "          # Create disturbance arrays where the value is not 'nodatavalue' in both scenarios\n",
        "          result_array = np.where(dist1_array==nodatavalue, nodatavalue, subtract_arrays(dist1_array, dist2_array))\n",
        "          # Round array before export\n",
        "          result_array_rounded = np.round(result_array)\n",
        "          # Export disturbance raster\n",
        "          export_array_as_tif(result_array_rounded, output_path, template = dist1_path)\n",
        "\n",
        "  # Update progress\n",
        "  progress_index += 1\n",
        "  progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "# 5. Process area-based disturbances\n",
        "for (scenario1, scenario2), disturbance_name in area_based_dictionary.items():\n",
        "  if use_uncertainty:\n",
        "      mean_filename = f\"mean__{disturbance_name}__{selected_model}.tif\"\n",
        "      mean_path = join(disturbance_dir, mean_filename)\n",
        "      uncertainty_filename = f\"uncertainty__{disturbance_name}__{selected_model}.tif\"\n",
        "      uncertainty_path = join(disturbance_dir, uncertainty_filename)\n",
        "      if exists(mean_path) and exists(uncertainty_path):\n",
        "          progress_index += 1\n",
        "          progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "          continue\n",
        "      scenario1_base = f\"{scenario1}__{selected_model}\"\n",
        "      scenario2_base = f\"{scenario2}__{selected_model}\"\n",
        "      scenario1_mean_path = join(predictions_dir, f\"mean__{scenario1_base}.tif\")\n",
        "      scenario1_uncertainty_path = join(predictions_dir, f\"uncertainty__{scenario1_base}.tif\")\n",
        "      scenario2_mean_path = join(predictions_dir, f\"mean__{scenario2_base}.tif\")\n",
        "      scenario2_uncertainty_path = join(predictions_dir, f\"uncertainty__{scenario2_base}.tif\")\n",
        "      assert exists(scenario1_mean_path), f\"mean__{scenario1_base}.tif does not exist.\"\n",
        "      assert exists(scenario1_uncertainty_path), f\"uncertainty__{scenario1_base}.tif does not exist.\"\n",
        "      assert exists(scenario2_mean_path), f\"mean__{scenario2_base}.tif does not exist.\"\n",
        "      assert exists(scenario2_uncertainty_path), f\"uncertainty__{scenario2_base}.tif does not exist.\"\n",
        "      # Read arrays\n",
        "      scenario1_mean = gdal.Open(scenario1_mean_path).ReadAsArray()\n",
        "      scenario1_uncertainty = gdal.Open(scenario1_uncertainty_path).ReadAsArray()\n",
        "      scenario2_mean = gdal.Open(scenario2_mean_path).ReadAsArray()\n",
        "      scenario2_uncertainty = gdal.Open(scenario2_uncertainty_path).ReadAsArray()\n",
        "      scenario1_mean = np.where((scenario1_mean == nodatavalue) & (scenario2_mean != nodatavalue), 0, scenario1_mean)\n",
        "      scenario1_uncertainty = np.where((scenario1_uncertainty == nodatavalue) & (scenario2_uncertainty != nodatavalue), 0, scenario1_uncertainty)\n",
        "      scenario2_mean = np.where((scenario2_mean == nodatavalue) & (scenario1_mean != nodatavalue), 0, scenario2_mean)\n",
        "      scenario2_uncertainty = np.where((scenario2_uncertainty == nodatavalue) & (scenario1_uncertainty != nodatavalue), 0, scenario2_uncertainty)\n",
        "      dist_mean_array = np.where(scenario1_mean == nodatavalue, nodatavalue, subtract_arrays(scenario1_mean, scenario2_mean))\n",
        "      dist_uncertainty_array = np.where(scenario1_mean == nodatavalue, nodatavalue,\n",
        "                                       propagate_uncertainty(scenario1_mean, scenario1_uncertainty, scenario2_mean, scenario2_uncertainty))\n",
        "      # Round arrays before export\n",
        "      dist_mean_array_rounded = np.round(dist_mean_array)\n",
        "      # Set uncertainty to 0 where rounded difference is 0\n",
        "      dist_uncertainty_array = np.where(dist_mean_array_rounded == 0, 0, dist_uncertainty_array)\n",
        "      # Export disturbance rasters\n",
        "      export_array_as_tif(dist_mean_array_rounded, mean_path, template=scenario1_mean_path)\n",
        "      export_array_as_tif(dist_uncertainty_array, uncertainty_path, template=scenario1_mean_path)\n",
        "  else:\n",
        "      dist_filename = f\"{disturbance_name}__{selected_model}.tif\"\n",
        "      dist_path = join(disturbance_dir, dist_filename)\n",
        "      if exists(dist_path):\n",
        "          progress_index += 1\n",
        "          progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "          continue\n",
        "      scenario1_path = join(predictions_dir, f\"{scenario1}__{selected_model}.tif\")\n",
        "      assert exists(scenario1_path), f\"{scenario1_path} does not exist.\"\n",
        "      scenario2_path = join(predictions_dir, f\"{scenario2}__{selected_model}.tif\")\n",
        "      assert exists(scenario2_path), f\"{scenario2_path} does not exist.\"\n",
        "      # Read arrays\n",
        "      scenario1_array_temp = gdal.Open(scenario1_path).ReadAsArray()\n",
        "      scenario2_array_temp = gdal.Open(scenario2_path).ReadAsArray()\n",
        "      scenario1_array = np.where((scenario1_array_temp == nodatavalue) & (scenario2_array_temp != nodatavalue), 0, scenario1_array_temp)\n",
        "      scenario2_array = np.where((scenario2_array_temp == nodatavalue) & (scenario1_array != nodatavalue), 0, scenario2_array_temp)\n",
        "      dist_array = np.where(scenario1_array==nodatavalue, nodatavalue, subtract_arrays(scenario1_array, scenario2_array))\n",
        "      # Round array before export\n",
        "      dist_array_rounded = np.round(dist_array)\n",
        "      # Export disturbance raster\n",
        "      export_array_as_tif(dist_array_rounded, dist_path, template = scenario1_path)\n",
        "\n",
        "  progress_index += 1\n",
        "  progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "print(\"All disturbances calculated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81OCJi98NDwj"
      },
      "source": [
        "# Intactness"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Percentage loss"
      ],
      "metadata": {
        "id": "MJyxcVvLqCdW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsend_eeNIll"
      },
      "outputs": [],
      "source": [
        "# Intactness is measured as relative percentage loss of AGBD within an area of interest\n",
        "\n",
        "# Select which baseline and disturbance raster to use for calculating intactness\n",
        "# percentage and relative intactness. Ideally this is the scenario with the least disturbance\n",
        "# and the difference between that and the current reality.\n",
        "\n",
        "for baseline in os.listdir(predictions_dir):\n",
        "  if source_dir == scenarios_dir: print(f\"selected_baseline = '{baseline}'\")\n",
        "  if source_dir == uncertainty_dir:\n",
        "    if 'mean' in baseline: print(f\"selected_baseline = '{baseline}'\")\n",
        "for dist in os.listdir(disturbance_dir):\n",
        "  if source_dir == scenarios_dir: print(f\"selected_dist = '{dist}'\")\n",
        "  if source_dir == uncertainty_dir:\n",
        "    if 'mean' in dist:print(f\"selected_dist = '{dist}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87Q6aMSdNIye"
      },
      "outputs": [],
      "source": [
        "# selected_baseline = 'mean__2021_no_disturbance_since_1993__agbd_tekai_250625_003858.tif'\n",
        "# selected_dist = 'mean__2021_disturbance_since_1993__agbd_tekai_250625_003858.tif'\n",
        "# selected_baseline = 'mean__2021_oldgrowth_all_land__agbd_tekai_250625_003858.tif'\n",
        "# selected_dist = 'mean__2021_disturbance_since_oldgrowth__agbd_tekai_250625_003858.tif'\n",
        "# selected_baseline = 'mean__2024_no_disturbance_since_1996__agbd_tekai_250625_003858.tif'\n",
        "# selected_dist = 'mean__2024_disturbance_since_1996__agbd_tekai_250625_003858.tif'\n",
        "selected_baseline = 'mean__2024_oldgrowth_all_land__agbd_tekai_250625_003858.tif'\n",
        "selected_dist = 'mean__2024_disturbance_since_oldgrowth__agbd_tekai_250625_003858.tif'\n",
        "\n",
        "# Define the baseline name based on source directory\n",
        "if source_dir == scenarios_dir:\n",
        "  base_dist_name = f\"{selected_baseline.split('__')[0]}__{selected_dist.split('__')[0]}\"\n",
        "if source_dir == uncertainty_dir:\n",
        "  base_dist_name = f\"{selected_baseline.split('__')[1]}__{selected_dist.split('__')[1]}\"\n",
        "forest_mask_year = base_dist_name.split('_')[0]\n",
        "\n",
        "intactness_baseline_dist_dir = join(intactness_dir, base_dist_name)\n",
        "makedirs(intactness_baseline_dist_dir, exist_ok=True)\n",
        "\n",
        "percentage_filename = f\"percentage_change__{base_dist_name}__{selected_model}.tif\"\n",
        "percentage_path = join(intactness_baseline_dist_dir, percentage_filename)\n",
        "\n",
        "if not exists(percentage_path):\n",
        "  # Define filenames and directories\n",
        "  selected_baseline_path = join(predictions_dir, selected_baseline)\n",
        "  selected_dist_path = join(disturbance_dir, selected_dist)\n",
        "  selected_mask_path = join(masks_dir, f\"mask_forest_{forest_mask_year}.tif\")\n",
        "\n",
        "  # Convert to arrays\n",
        "  selected_baseline_array = gdal.Open(selected_baseline_path).ReadAsArray()\n",
        "  selected_dist_array = gdal.Open(selected_dist_path).ReadAsArray()\n",
        "  selected_mask_array = gdal.Open(selected_mask_path).ReadAsArray()\n",
        "\n",
        "  # Create percentage array where the value is not 'nodatavalue' in any of the inputs\n",
        "  percentage_array = np.where((selected_mask_array==nodatavalue) | (selected_baseline_array==nodatavalue) | (selected_dist_array==nodatavalue), nodatavalue,\n",
        "                              selected_dist_array/selected_baseline_array*100)\n",
        "  export_array_as_tif(percentage_array, percentage_path, template = selected_baseline_path)\n",
        "  print(f\"{percentage_filename} has been exported.\")\n",
        "\n",
        "else: print(f\"{percentage_filename} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantiles (relative intactness)"
      ],
      "metadata": {
        "id": "7ugXALeIqLkt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwXRZTew1r39"
      },
      "outputs": [],
      "source": [
        "# Use additional polygons for masking relative intactness quantiles\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "\n",
        "# Select baseline / disturbance pairs to measure relative intactness\n",
        "print(\"baseline_disturbance_pairs = [\")\n",
        "for dir in os.listdir(intactness_dir):\n",
        "  print(f\"'{dir}',\")\n",
        "print(\"]\\n\")\n",
        "\n",
        "# Select polygons to mask and calculate quantiles\n",
        "print(\"mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"'{polygon}',\")\n",
        "print(None)\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7-X8cXopVo3"
      },
      "outputs": [],
      "source": [
        "baseline_disturbance_pairs = [\n",
        "'2021_no_disturbance_since_1993__2021_disturbance_since_1993',\n",
        "'2021_oldgrowth_all_land__2021_disturbance_since_oldgrowth',\n",
        "'2024_no_disturbance_since_1996__2024_disturbance_since_1996',\n",
        "'2024_oldgrowth_all_land__2024_disturbance_since_oldgrowth',\n",
        "]\n",
        "\n",
        "mask_polygons = [\n",
        "# 'project_area.gpkg',\n",
        "# 'peninsular_malaysia.gpkg',\n",
        "# 'lu_yong.gpkg',\n",
        "# 'lu_yong_lipis.gpkg',\n",
        "# 'lu_berkelah_jerantut.gpkg',\n",
        "# 'lu_tekai_tembeling.gpkg',\n",
        "# 'lu_ais.gpkg',\n",
        "# 'lu_pa_taman_negara_krau.gpkg',\n",
        "# 'lu_tekam.gpkg',\n",
        "# 'lu_berkelah_temerloh.gpkg',\n",
        "# 'lu_remen_chereh.gpkg',\n",
        "# 'lu_berkelah_kuantan.gpkg',\n",
        "'forest_reserves.gpkg',\n",
        "# 'gedi_area.gpkg',\n",
        "# None\n",
        "]\n",
        "\n",
        "# Convert nodata values inside the mask to a score of 0 (representing non-forest areas)\n",
        "# Otherwise both non-forest and masked areas will be nodatavalue\n",
        "convert_non_forest_nodatavalue_to_0 = True\n",
        "\n",
        "# Define top score for intactness rating (e.g. 10 for 1 - 10 scale)\n",
        "top_score = 10\n",
        "\n",
        "# Margin for small variation in estimated change values due to uncertainty\n",
        "# Values above this margin are considered 'intact'\n",
        "top_score_margin = -0.01\n",
        "\n",
        "# Calculate actual number of quantiles for non-zero values\n",
        "num_quantiles = top_score - 1\n",
        "\n",
        "print(f\"Calculating {num_quantiles} quantiles for negative percentage change (scores 1-{num_quantiles}), with score {top_score} reserved for >= {top_score_margin} % change.\\n\")\n",
        "\n",
        "# Create polygon mask array using template tif\n",
        "template_array = gdal.Open(template_tif_path).ReadAsArray()\n",
        "polygon_mask_array = np.ones_like(template_array, dtype=bool)\n",
        "\n",
        "for mask_polygon in mask_polygons:\n",
        "  if mask_polygon is not None:\n",
        "    # Create an inverse project area path for masking\n",
        "    template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "    inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "    if not exists(inverse_polygon_path):\n",
        "      polygon_path = join(polygons_dir, mask_polygon)\n",
        "      template_polygon = gpd.read_file(template_polygon_path)\n",
        "      polygon_read = gpd.read_file(polygon_path)\n",
        "      polygon_crs = polygon_read.crs.to_epsg()\n",
        "      inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "      inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "      inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "      print(f\"An inverse masking polygon for {mask_polygon} has been created in {polygons_dir}.\")\n",
        "    else: print(f\"An inverse masking polygon for {mask_polygon} already exists.\")\n",
        "\n",
        "    # Apply polygon mask to the array\n",
        "    print(f\"Creating a polygon masking array.\")\n",
        "    inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "    temp_mask_path = join(intactness_dir, f\"temp_mask_{mask_polygon[:-5]}.tif\")\n",
        "    copyfile(template_tif_path, temp_mask_path)\n",
        "    burn_polygon_to_raster(temp_mask_path, inverse_polygon_path, fixed_value=nodatavalue, all_touched=False)\n",
        "    temp_mask_array = gdal.Open(temp_mask_path).ReadAsArray()\n",
        "    polygon_mask_array[temp_mask_array == nodatavalue] = False\n",
        "    os.remove(temp_mask_path)\n",
        "\n",
        "for base_dist_name in baseline_disturbance_pairs:\n",
        "  intactness_baseline_dist_dir = join(intactness_dir, base_dist_name)\n",
        "  percentage_filename = f\"percentage_change__{base_dist_name}__{selected_model}\"\n",
        "  percentage_path = join(intactness_baseline_dist_dir, f\"{percentage_filename}.tif\")\n",
        "\n",
        "  for mask_polygon in mask_polygons:\n",
        "\n",
        "    if mask_polygon is not None:\n",
        "      # Copy the percentage raster for potential masking\n",
        "      percentage_masked_filename = f\"{percentage_filename}__masked_{mask_polygon[:-5]}.tif\"\n",
        "      percentage_masked_path = join(intactness_baseline_dist_dir, percentage_masked_filename)\n",
        "      if not exists(percentage_masked_path):\n",
        "        print(f\"Copying {percentage_filename} for masking...\")\n",
        "        copyfile(percentage_path, percentage_masked_path)\n",
        "        print(f\"Masking {percentage_filename} with {mask_polygon}...\")\n",
        "        inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "        burn_polygon_to_raster(percentage_masked_path, inverse_polygon_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        # Recompress the prediction after burning the polygon masks\n",
        "        percentage_masked_array = gdal.Open(percentage_masked_path).ReadAsArray()\n",
        "        export_array_as_tif(percentage_masked_array, percentage_masked_path, compress = True)\n",
        "        print(f\"{percentage_filename} masked.\")\n",
        "      else: print(f\"{percentage_masked_filename} already exists.\")\n",
        "\n",
        "    # Define paths and arrays\n",
        "    if mask_polygon is None: relative_intactness_name = f'intactness__{top_score}_quantiles__{base_dist_name}__{selected_model}'\n",
        "    else: relative_intactness_name = f'intactness__{mask_polygon[:-5]}_{top_score}_quantiles__{base_dist_name}__{selected_model}'\n",
        "    relative_intactness_path = join(intactness_baseline_dist_dir, f'{relative_intactness_name}.tif')\n",
        "    if not exists(relative_intactness_path):\n",
        "      # Always track originally nodata pixels from the original percentage raster\n",
        "      original_percentage_array = gdal.Open(percentage_path).ReadAsArray()\n",
        "      originally_nodata_mask = original_percentage_array == nodatavalue\n",
        "\n",
        "      # Apply polygon masking to percentage array using pre-created mask\n",
        "      if mask_polygon is None:\n",
        "        percentage_array = original_percentage_array.copy()\n",
        "      else:\n",
        "        percentage_array = original_percentage_array.copy()\n",
        "        percentage_array[~polygon_mask_array] = nodatavalue\n",
        "\n",
        "      # Capture original data for histogram before conversions\n",
        "      original_valid_elements = percentage_array[percentage_array != nodatavalue]\n",
        "\n",
        "      relative_intactness_array = np.empty_like(percentage_array, dtype=object)\n",
        "\n",
        "      # Set all values above 0 to 0, assuming negative values are not intact\n",
        "      percentage_array[percentage_array > 0] = 0\n",
        "\n",
        "      # Set all values above the 'top score margin' to 0 to account for uncertainty\n",
        "      percentage_array[percentage_array > top_score_margin] = 0\n",
        "\n",
        "      # Separate valid and invalid (nodatavalue) elements\n",
        "      valid_elements = percentage_array[percentage_array != nodatavalue]\n",
        "      invalid_elements = percentage_array == nodatavalue\n",
        "\n",
        "      # Separate zero and non-zero values, excluding originally nodata pixels from quantile calculation\n",
        "      zero_elements = percentage_array == 0\n",
        "      # Exclude pixels that were originally nodata from quantile calculation\n",
        "      quantile_mask = (percentage_array != nodatavalue) & (~originally_nodata_mask) & (percentage_array != 0)\n",
        "      non_zero_valid_elements = percentage_array[quantile_mask]\n",
        "\n",
        "      # Calculate quantiles for non-zero valid elements only\n",
        "      quantiles = np.percentile(non_zero_valid_elements, np.linspace(0, 100, num_quantiles + 1)[1:-1]) if len(non_zero_valid_elements) > 0 else []\n",
        "\n",
        "      # Assign scores 1 to num_quantiles for non-zero values\n",
        "      for i in range(1, num_quantiles + 1):\n",
        "          lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "          upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "          relative_intactness_array[(percentage_array > lower_bound) & (percentage_array <= upper_bound) & (percentage_array != 0)] = i\n",
        "\n",
        "      # Set all zero values to top score\n",
        "      relative_intactness_array[zero_elements] = top_score\n",
        "\n",
        "      # Set areas outside polygon to nodatavalue using pre-created mask\n",
        "      if mask_polygon is not None:\n",
        "        relative_intactness_array[~polygon_mask_array] = nodatavalue\n",
        "\n",
        "      # Convert non-forest areas inside polygon to 0\n",
        "      if convert_non_forest_nodatavalue_to_0:\n",
        "        if mask_polygon is None:\n",
        "          non_forest_inside_polygon = originally_nodata_mask\n",
        "        else:\n",
        "          non_forest_inside_polygon = originally_nodata_mask & polygon_mask_array\n",
        "        relative_intactness_array[non_forest_inside_polygon] = 0\n",
        "\n",
        "      export_array_as_tif(relative_intactness_array, relative_intactness_path)\n",
        "\n",
        "      # Prepare data for CSV: Collect lower and upper bounds for each category\n",
        "      ranges_data = {'Score': [], 'Lower_Bound': [], 'Upper_Bound': []}\n",
        "\n",
        "      # Add ranges for scores 1 to num_quantiles (non-zero values)\n",
        "      for i in range(1, num_quantiles + 1):\n",
        "          lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "          if i == num_quantiles:\n",
        "            upper_bound = top_score_margin - 0.000000001\n",
        "          else:upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "          ranges_data['Score'].append(i)\n",
        "          ranges_data['Lower_Bound'].append(lower_bound)\n",
        "          ranges_data['Upper_Bound'].append(upper_bound)\n",
        "\n",
        "      # Add entry for top score (values from margin to 0)\n",
        "      ranges_data['Score'].append(top_score)\n",
        "      ranges_data['Lower_Bound'].append(top_score_margin)\n",
        "      ranges_data['Upper_Bound'].append(0)\n",
        "\n",
        "      # Create DataFrame and save to CSV\n",
        "      relative_intactness_df = pd.DataFrame(ranges_data)\n",
        "      relative_intactness_csv_path = os.path.join(intactness_baseline_dist_dir, f'{relative_intactness_name}.csv')\n",
        "      relative_intactness_df.to_csv(relative_intactness_csv_path, index=False)\n",
        "\n",
        "      # Generate and save histogram for converted data as .png\n",
        "      histogram_path = join(intactness_baseline_dist_dir, f'{relative_intactness_name}.png')\n",
        "      plt.figure()\n",
        "      counts, bins, patches = plt.hist(original_valid_elements.flatten(), bins='auto')\n",
        "\n",
        "      # Count how many values became 0 after conversions\n",
        "      zero_count_after_conversion = np.sum(valid_elements == 0)\n",
        "\n",
        "      # Find the zero bin and set its frequency to 0\n",
        "      zero_idx = next((i for i, (l, r) in enumerate(zip(bins[:-1], bins[1:])) if l <= 0 <= r), None)\n",
        "      if zero_idx is not None:\n",
        "          counts[zero_idx] = 0\n",
        "          plt.clf()\n",
        "          plt.bar(bins[:-1], counts, width=np.diff(bins), align='edge')\n",
        "          x_center = (bins.min() + bins.max()) / 2\n",
        "          y_max = max(counts)\n",
        "          plt.text(x_center, y_max * 0.9,\n",
        "                  f'0 value frequency = {zero_count_after_conversion:,}',\n",
        "                  ha='center', va='center', fontweight='bold',\n",
        "                  bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9))\n",
        "      plt.title(f'{relative_intactness_name} Histogram')\n",
        "      plt.xlabel('Value')\n",
        "      plt.ylabel('Frequency')\n",
        "      plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(histogram_path)\n",
        "      plt.close()\n",
        "\n",
        "    else: print(f\"{relative_intactness_name} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-7ctfgdoF2"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcuArY00N9Mc"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "QJ8jLRKRSfgb",
        "y_OkHaUkTCyB",
        "7ugXALeIqLkt"
      ],
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}