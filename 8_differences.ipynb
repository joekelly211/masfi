{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/8_differences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8jLRKRSfgb"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkvV803eSUQY"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "# Use '/content/drive/MyDrive/' for a personal drive\n",
        "# Use '/gdrive/Shareddrives/' for a shared drive (must be created first)\n",
        "\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "# base_dir = '/content/drive/MyDrive/masfi'\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "if base_dir.startswith('/gdrive/Shareddrives/'):\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "elif base_dir.startswith('/content/drive/MyDrive/'):\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "else: print(\"Create a base_dir beginning with '/gdrive/Shareddrives/' or '/content/drive/MyDrive/'.\")\n",
        "\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEpdIjOwSiB1"
      },
      "outputs": [],
      "source": [
        "# Capture outputs\n",
        "%%capture\n",
        "# Installs and upgrades\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!apt-get install -y gdal-bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQDSsvGeSi4u"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import geopandas as gpd\n",
        "from google.colab import runtime\n",
        "from os import makedirs\n",
        "from os.path import join, exists\n",
        "from osgeo import gdal, ogr\n",
        "gdal.UseExceptions()\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "from shutil import copyfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxgT3K1Sjum"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "masks_dir = join(scenarios_dir, \"scenario_masks\")\n",
        "uncertainty_dir = join(base_dir, \"7_uncertainty\")\n",
        "differences_dir = join(base_dir, \"8_differences\")\n",
        "\n",
        "# Create directories\n",
        "makedirs(differences_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96FAfeyooMLz"
      },
      "outputs": [],
      "source": [
        "# Global function: export an array as a .tif\n",
        "template_tif_path = join(areas_dir, \"template.tif\")\n",
        "nodatavalue = -11111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_tif_path, nodatavalue=nodatavalue, compress=compress, dtype=gdal.GDT_Float32):\n",
        "    template_ds = gdal.Open(template)\n",
        "    template_band = template_ds.GetRasterBand(1)\n",
        "    template_dimensions, template_projection = template_ds.GetGeoTransform(), template_ds.GetProjection()\n",
        "    if compress: options = ['COMPRESS=ZSTD', 'ZSTD_LEVEL=1'] # Good speed / size ratio\n",
        "    else: options = []\n",
        "    if input_array.dtype == 'int16': dtype = gdal.GDT_Int16\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, 1, dtype, options=options)\n",
        "    driver.GetRasterBand(1).WriteArray(input_array)\n",
        "    driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "    driver.SetGeoTransform(template_dimensions)\n",
        "    driver.SetProjection(template_projection)\n",
        "    template_ds = driver = None\n",
        "\n",
        "# Global function: burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = vector = None\n",
        "    try:\n",
        "        raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "        vector = ogr.Open(polygon_path)\n",
        "        if not raster or not vector:\n",
        "            raise ValueError(\"Cannot open input files\")\n",
        "        layer = vector.GetLayer()\n",
        "        options = [\"ALL_TOUCHED=TRUE\"] if all_touched else []\n",
        "        if fixed:\n",
        "            gdal.RasterizeLayer(raster, [1], layer, burn_values=[fixed_value], options=options)\n",
        "        else:\n",
        "            attr_name = column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()\n",
        "            options.append(f\"ATTRIBUTE={attr_name}\")\n",
        "            gdal.RasterizeLayer(raster, [1], layer, options=options)\n",
        "    finally:\n",
        "        if raster: raster.FlushCache()\n",
        "        raster = vector = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_OkHaUkTCyB"
      },
      "source": [
        "# Select source and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mB0XrPNYU_I"
      },
      "outputs": [],
      "source": [
        "# Select if to source predictions from scenarios_dir or uncertainty_dir\n",
        "# If available, uncertainty_dir should be selected so that uncertainty can\n",
        "# be propagated and scenario 'mean' iteration values used.\n",
        "\n",
        "# source_dir = uncertainty_dir\n",
        "source_dir = scenarios_dir\n",
        "\n",
        "print(f\"{source_dir.split('/')[-1]} has been selected as the source directory for predictions\")\n",
        "print(\"to calculate disturbance and intactness.\\n\")\n",
        "\n",
        "# If uncertainty selected, check it exists\n",
        "if not exists(uncertainty_dir) and source_dir == uncertainty_dir:\n",
        "  print(\"The uncertainty directory does not yet exist. Defaulting to scenarios directory.\")\n",
        "  source_dir = scenarios_dir\n",
        "\n",
        "source_dir_name = f\"{source_dir.split('_')[-1]}_dir\"\n",
        "\n",
        "# Select the model\n",
        "for subdir in os.listdir(source_dir):\n",
        "  if 'scenario_masks' not in subdir:\n",
        "    print(f\"selected_model = '{subdir}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbXY6iP_qjYc"
      },
      "outputs": [],
      "source": [
        "selected_model = 'agbd_251203_161707'\n",
        "\n",
        "selected_model_dir = join(source_dir, selected_model)\n",
        "if source_dir == scenarios_dir: predictions_dir = join(selected_model_dir, 'scenario_predictions')\n",
        "if source_dir == uncertainty_dir:\n",
        "  predictions_dir = join(selected_model_dir, 'uncertainty_predictions')\n",
        "  predictions_unmasked_dir = join(selected_model_dir, 'uncertainty_predictions_unmasked')\n",
        "\n",
        "# Check predictions exist to calculate differences\n",
        "if len(os.listdir(predictions_dir)) < 2: print(f\"At least 2 predictions must exist in {source_dir} to calculate differences.\")\n",
        "else:\n",
        "  model_differences_dir = join(differences_dir, f\"{selected_model}_{source_dir_name}\")\n",
        "  disturbance_dir = join(model_differences_dir, 'disturbance')\n",
        "  intactness_dir = join(model_differences_dir, 'intactness')\n",
        "  makedirs(model_differences_dir, exist_ok=True)\n",
        "  makedirs(disturbance_dir, exist_ok=True)\n",
        "  makedirs(intactness_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECHaTfrs0n7i"
      },
      "source": [
        "# Disturbance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define type and period"
      ],
      "metadata": {
        "id": "hgPVQgfTpy21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUaYCCymYI2Y"
      },
      "outputs": [],
      "source": [
        "# Disturbance is measured as absolute AGBD loss\n",
        "# This block builds dictionaries of disturbance options based on available files\n",
        "\n",
        "# Extract all available scenarios from scenarios predictions directory\n",
        "if source_dir == scenarios_dir:\n",
        "  scenarios = set()\n",
        "  for file in os.listdir(predictions_dir):\n",
        "      scenarios.add(file.split(\"__\")[0])\n",
        "\n",
        "# OR Extract all available scenarios from uncertainty predictions directory\n",
        "if source_dir == uncertainty_dir:\n",
        "  prediction_stats = {}\n",
        "  for file in os.listdir(predictions_dir):\n",
        "      parts = file.split(\"__\")\n",
        "      if len(parts) >= 2:\n",
        "          stat, scenario = parts[0], parts[1]\n",
        "          if scenario not in prediction_stats:\n",
        "              prediction_stats[scenario] = set()\n",
        "          prediction_stats[scenario].add(stat)\n",
        "  # Only keep scenarios that have both 'uncertainty' and 'mean' prediction stats\n",
        "  scenarios = {prediction for prediction, stats in prediction_stats.items()\n",
        "              if 'uncertainty' in stats and 'mean' in stats}\n",
        "\n",
        "# Categorise years from scenarios\n",
        "years = set()\n",
        "plain_years = set()\n",
        "oldgrowth_years = set()\n",
        "oldgrowth_all_land_years = set()\n",
        "for s in scenarios:\n",
        "    if s.isdigit():\n",
        "        years.add(int(s))\n",
        "        plain_years.add(int(s))\n",
        "    elif \"_oldgrowth_all_land\" in s:\n",
        "        year = s.split(\"_oldgrowth_all_land\")[0]\n",
        "        if year.isdigit():\n",
        "            years.add(int(year))\n",
        "            oldgrowth_all_land_years.add(int(year))\n",
        "    elif \"_oldgrowth\" in s:\n",
        "        year = s.split(\"_oldgrowth\")[0]\n",
        "        if year.isdigit():\n",
        "            years.add(int(year))\n",
        "            oldgrowth_years.add(int(year))\n",
        "    elif any(pattern in s for pattern in [\"_no_disturbance_since_\", \"_no_degradation_since_\"]):\n",
        "        year = s.split(\"_\")[0]\n",
        "        if year.isdigit():\n",
        "            years.add(int(year))\n",
        "        if \"_since_\" in s:\n",
        "            since_year = s.split(\"_since_\")[1]\n",
        "            if since_year.isdigit():\n",
        "                years.add(int(since_year) - 1)\n",
        "years_sorted = sorted(list(years))\n",
        "\n",
        "# Output dictionaries\n",
        "disturbance_since_dictionary = {}\n",
        "degradation_since_dictionary = {}\n",
        "deforestation_since_dictionary = {}\n",
        "print(\"disturbance_since_dictionary = {\")\n",
        "print(\"\")\n",
        "\n",
        "# 1. Process disturbance_since scenarios\n",
        "for year_a in sorted(years_sorted):\n",
        "    a_str = str(year_a)\n",
        "    for year_b in sorted(years_sorted):\n",
        "        if year_a <= year_b:\n",
        "            continue\n",
        "        b_str, b_plus1 = str(year_b), str(year_b + 1)\n",
        "\n",
        "        if year_a in plain_years and f\"{a_str}_no_disturbance_since_{b_plus1}\" in scenarios:\n",
        "            print(f\"# Disturbance in {a_str} caused by events since {b_plus1}\")\n",
        "            print(f\"  ('{a_str}', '{a_str}_no_disturbance_since_{b_plus1}'):\")\n",
        "            print(f\"    '{a_str}_disturbance_since_{b_plus1}',\")\n",
        "            print(\"\")\n",
        "            disturbance_since_dictionary[(a_str, f\"{a_str}_no_disturbance_since_{b_plus1}\")] = f\"{a_str}_disturbance_since_{b_plus1}\"\n",
        "# Process disturbance_since_oldgrowth scenarios\n",
        "for year in years_sorted:\n",
        "    y_str = str(year)\n",
        "    if (year in plain_years and year in oldgrowth_all_land_years and f\"{y_str}_oldgrowth_all_land\" in scenarios):\n",
        "        print(f\"# Disturbance in {y_str} caused by events since an oldgrowth state.\")\n",
        "        print(f\"  ('{y_str}', '{y_str}_oldgrowth_all_land'):\")\n",
        "        print(f\"    '{y_str}_disturbance_since_oldgrowth',\")\n",
        "        print(\"\")\n",
        "        disturbance_since_dictionary[(y_str, f\"{y_str}_oldgrowth_all_land\")] = f\"{y_str}_disturbance_since_oldgrowth\"\n",
        "print(\"}\\n\")\n",
        "\n",
        "# 2. Degradation since dictionary\n",
        "print(\"degradation_since_dictionary = {\\n\")\n",
        "for year_a in sorted(years_sorted):\n",
        "    a_str = str(year_a)\n",
        "    for year_b in sorted(years_sorted):\n",
        "        if year_a <= year_b:\n",
        "            continue\n",
        "        b_str, b_plus1 = str(year_b), str(year_b + 1)\n",
        "        if year_a in plain_years and f\"{a_str}_no_degradation_since_{b_plus1}\" in scenarios:\n",
        "            print(f\"# Degradation in {a_str} caused by events since {b_plus1}\")\n",
        "            print(f\"  ('{a_str}', '{a_str}_no_degradation_since_{b_plus1}'):\")\n",
        "            print(f\"    '{a_str}_degradation_since_{b_plus1}',\")\n",
        "            print(\"\")\n",
        "            degradation_since_dictionary[(a_str, f\"{a_str}_no_degradation_since_{b_plus1}\")] = f\"{a_str}_degradation_since_{b_plus1}\"\n",
        "# Process degradation_since_oldgrowth scenarios\n",
        "for year in years_sorted:\n",
        "    y_str = str(year)\n",
        "    if (year in plain_years and year in oldgrowth_years and f\"{y_str}_oldgrowth\" in scenarios):\n",
        "        print(f\"# Degradation in {y_str} caused by events since an old-growth state\")\n",
        "        print(f\"  ('{y_str}', '{y_str}_oldgrowth'):\")\n",
        "        print(f\"    '{y_str}_degradation_since_oldgrowth',\")\n",
        "        print(\"\")\n",
        "        degradation_since_dictionary[(y_str, f\"{y_str}_oldgrowth\")] = f\"{y_str}_degradation_since_oldgrowth\"\n",
        "print(\"}\\n\")\n",
        "\n",
        "# 3. Deforestation since dictionary\n",
        "print(\"deforestation_since_dictionary = {\\n\")\n",
        "for year_a in sorted(years_sorted):\n",
        "    a_str = str(year_a)\n",
        "    for year_b in sorted(years_sorted):\n",
        "        if year_a <= year_b:\n",
        "            continue\n",
        "        b_str, b_plus1 = str(year_b), str(year_b + 1)\n",
        "        deg_key = (a_str, f\"{a_str}_no_degradation_since_{b_plus1}\")\n",
        "        dist_key = (a_str, f\"{a_str}_no_disturbance_since_{b_plus1}\")\n",
        "        if deg_key in degradation_since_dictionary and dist_key in disturbance_since_dictionary:\n",
        "            deg_result = degradation_since_dictionary[deg_key]\n",
        "            dist_result = disturbance_since_dictionary[dist_key]\n",
        "            defor_result = f\"{a_str}_deforestation_since_{b_plus1}\"\n",
        "            print(f\"# Deforestation in {a_str} caused by events since {b_plus1}\")\n",
        "            print(f\"  ('{deg_result}', '{dist_result}'):\")\n",
        "            print(f\"    '{defor_result}',\")\n",
        "            print(\"\")\n",
        "            deforestation_since_dictionary[(deg_result, dist_result)] = defor_result\n",
        "# Process deforestation_since_oldgrowth scenarios\n",
        "for year in years_sorted:\n",
        "    y_str = str(year)\n",
        "    deg_key = (y_str, f\"{y_str}_oldgrowth\")\n",
        "    dist_key = (y_str, f\"{y_str}_oldgrowth_all_land\")\n",
        "\n",
        "    if deg_key in degradation_since_dictionary and dist_key in disturbance_since_dictionary:\n",
        "        print(f\"# Deforestation in {y_str} caused by events since an old-growth state\")\n",
        "        deg_result = degradation_since_dictionary[deg_key]\n",
        "        dist_result = disturbance_since_dictionary[dist_key]\n",
        "        defor_result = f\"{y_str}_deforestation_since_oldgrowth\"\n",
        "        print(f\"  ('{deg_result}', '{dist_result}'):\")\n",
        "        print(f\"    '{defor_result}',\")\n",
        "        print(\"\")\n",
        "        deforestation_since_dictionary[(deg_result, dist_result)] = defor_result\n",
        "print(\"}\\n\")\n",
        "\n",
        "\n",
        "# 4. Specific year effects dictionary\n",
        "print(\"specific_year_effects_dictionary = {\\n\")\n",
        "# Collect all since results and organise by year of interest and disturbance type\n",
        "effects_by_year = {}\n",
        "# Process degradation since results\n",
        "for result_name in degradation_since_dictionary.values():\n",
        "    if \"_degradation_since_\" in result_name and \"_oldgrowth\" not in result_name:\n",
        "        parts = result_name.split(\"_degradation_since_\")\n",
        "        year_of_interest = parts[0]\n",
        "        baseline_year = int(parts[1])\n",
        "        if year_of_interest not in effects_by_year:\n",
        "            effects_by_year[year_of_interest] = {}\n",
        "        if 'degradation' not in effects_by_year[year_of_interest]:\n",
        "            effects_by_year[year_of_interest]['degradation'] = {}\n",
        "        effects_by_year[year_of_interest]['degradation'][baseline_year] = result_name\n",
        "# Process disturbance since results\n",
        "for result_name in disturbance_since_dictionary.values():\n",
        "    if \"_disturbance_since_\" in result_name and \"_oldgrowth\" not in result_name:\n",
        "        parts = result_name.split(\"_disturbance_since_\")\n",
        "        year_of_interest = parts[0]\n",
        "        baseline_year = int(parts[1])\n",
        "        if year_of_interest not in effects_by_year:\n",
        "            effects_by_year[year_of_interest] = {}\n",
        "        if 'disturbance' not in effects_by_year[year_of_interest]:\n",
        "            effects_by_year[year_of_interest]['disturbance'] = {}\n",
        "        effects_by_year[year_of_interest]['disturbance'][baseline_year] = result_name\n",
        "# Process deforestation since results\n",
        "for result_name in deforestation_since_dictionary.values():\n",
        "    if \"_deforestation_since_\" in result_name and \"_oldgrowth\" not in result_name:\n",
        "        parts = result_name.split(\"_deforestation_since_\")\n",
        "        year_of_interest = parts[0]\n",
        "        baseline_year = int(parts[1])\n",
        "        if year_of_interest not in effects_by_year:\n",
        "            effects_by_year[year_of_interest] = {}\n",
        "        if 'deforestation' not in effects_by_year[year_of_interest]:\n",
        "            effects_by_year[year_of_interest]['deforestation'] = {}\n",
        "        effects_by_year[year_of_interest]['deforestation'][baseline_year] = result_name\n",
        "# Output dictionary entries grouped by year of interest and disturbance type\n",
        "specific_year_effects_dictionary = {}\n",
        "for year_of_interest in sorted(effects_by_year.keys()):\n",
        "    year_effects = effects_by_year[year_of_interest]\n",
        "\n",
        "    # Build all effects for this year first\n",
        "    year_has_effects = False\n",
        "    all_type_effects = {}\n",
        "\n",
        "    # Process each disturbance type\n",
        "    for dist_type in ['degradation', 'deforestation', 'disturbance']:\n",
        "        if dist_type in year_effects:\n",
        "            baseline_years = sorted(year_effects[dist_type].keys())\n",
        "            # Find consecutive year pairs for specific year effects\n",
        "            type_effects = []\n",
        "            for i in range(len(baseline_years) - 1):\n",
        "                current_year = baseline_years[i]\n",
        "                next_year = baseline_years[i + 1]\n",
        "                if next_year == current_year + 1:\n",
        "                    since_current = year_effects[dist_type][current_year]\n",
        "                    since_next = year_effects[dist_type][next_year]\n",
        "                    effect_name = f\"{year_of_interest}_effect_of_{dist_type}_in_{current_year}\"\n",
        "                    type_effects.append((since_current, since_next, effect_name, current_year))\n",
        "                    specific_year_effects_dictionary[(since_current, since_next)] = effect_name\n",
        "            # Add same-year effect (copy and rename)\n",
        "            if baseline_years:\n",
        "                last_year = max(baseline_years)\n",
        "                if last_year == int(year_of_interest):\n",
        "                    since_same_year = year_effects[dist_type][last_year]\n",
        "                    same_year_effect = f\"{year_of_interest}_effect_of_{dist_type}_in_{last_year}\"\n",
        "                    type_effects.append((since_same_year, None, same_year_effect, last_year))\n",
        "                    specific_year_effects_dictionary[(since_same_year,)] = same_year_effect\n",
        "\n",
        "            if type_effects:\n",
        "                all_type_effects[dist_type] = type_effects\n",
        "                year_has_effects = True\n",
        "\n",
        "    # Only print if there are effects for this year\n",
        "    if year_has_effects:\n",
        "        print(f\"# Effects in {year_of_interest}\")\n",
        "        for dist_type in ['degradation', 'deforestation', 'disturbance']:\n",
        "            if dist_type in all_type_effects:\n",
        "                print(f\"  # {dist_type.capitalize()} effects\")\n",
        "                # Sort by effect year chronologically\n",
        "                sorted_effects = sorted(all_type_effects[dist_type], key=lambda x: x[3])\n",
        "                for since_current, since_next, effect_name, effect_year in sorted_effects:\n",
        "                    if since_next is None:  # Same-year effect (copy and rename)\n",
        "                        print(f\"  ('{since_current}',):\")\n",
        "                        print(f\"    '{effect_name}',\")\n",
        "                    else:  # Regular subtraction effect\n",
        "                        print(f\"  ('{since_current}', '{since_next}'):\")\n",
        "                        print(f\"    '{effect_name}',\")\n",
        "                print(\"\")\n",
        "print(\"}\\n\")\n",
        "\n",
        "# 5. Area-based dictionary\n",
        "print(\"area_based_dictionary = {\")\n",
        "# Get polygon names from polygons directory\n",
        "polygon_names = set()\n",
        "if os.path.exists(polygons_dir):\n",
        "    for file in os.listdir(polygons_dir):\n",
        "        if file.endswith('.gpkg'):\n",
        "            polygon_names.add(file[:-5])\n",
        "area_based_entries = []\n",
        "for scenario in scenarios:\n",
        "    parts = scenario.split('_')\n",
        "    # Check for deforestation (ends with \"Xm_degradation_buffer\")\n",
        "    if len(parts) >= 5 and parts[-1] == 'buffer' and parts[-2] == 'degradation' and parts[-3].endswith('m'):\n",
        "        alt_year, year_affix, dist_type = parts[0], parts[-4], parts[-5]\n",
        "        polygon_name = '_'.join(parts[1:-5])\n",
        "        if polygon_name in polygon_names and dist_type == 'deforestation':\n",
        "            output_name = f\"{alt_year}_deforestation_of_{polygon_name}_{year_affix}\"\n",
        "            area_based_entries.append((scenario, alt_year, output_name))\n",
        "    # Check for degradation (ends with \"degradation_YYYY\")\n",
        "    elif len(parts) >= 3 and parts[-2] == 'degradation' and parts[-1].isdigit() and len(parts[-1]) == 4:\n",
        "        alt_year, year_affix = parts[0], parts[-1]\n",
        "        polygon_name = '_'.join(parts[1:-2])\n",
        "        if polygon_name in polygon_names:\n",
        "            output_name = f\"{alt_year}_degradation_of_{polygon_name}_{year_affix}\"\n",
        "            area_based_entries.append((scenario, alt_year, output_name))\n",
        "if area_based_entries:\n",
        "    print(\"\\n# Area-based disturbance from alternate scenarios\")\n",
        "    for scenario, alt_year, output_name in sorted(area_based_entries):\n",
        "        print(f\"  ('{scenario}', '{alt_year}'):\")\n",
        "        print(f\"    '{output_name}',\")\n",
        "print(\"}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disturbance_since_dictionary = {\n",
        "\n",
        "# Disturbance in 2024 caused by events since 1996\n",
        "  ('2024', '2024_no_disturbance_since_1996'):\n",
        "    '2024_disturbance_since_1996',\n",
        "\n",
        "# Disturbance in 2024 caused by events since an oldgrowth state.\n",
        "  ('2024', '2024_oldgrowth_all_land'):\n",
        "    '2024_disturbance_since_oldgrowth',\n",
        "\n",
        "}\n",
        "\n",
        "degradation_since_dictionary = {\n",
        "\n",
        "# Degradation in 2024 caused by events since 1996\n",
        "  ('2024', '2024_no_degradation_since_1996'):\n",
        "    '2024_degradation_since_1996',\n",
        "\n",
        "# Degradation in 2024 caused by events since an old-growth state\n",
        "  ('2024', '2024_oldgrowth'):\n",
        "    '2024_degradation_since_oldgrowth',\n",
        "\n",
        "}\n",
        "\n",
        "deforestation_since_dictionary = {\n",
        "\n",
        "# Deforestation in 2024 caused by events since 1996\n",
        "  ('2024_degradation_since_1996', '2024_disturbance_since_1996'):\n",
        "    '2024_deforestation_since_1996',\n",
        "\n",
        "# Deforestation in 2024 caused by events since an old-growth state\n",
        "  ('2024_degradation_since_oldgrowth', '2024_disturbance_since_oldgrowth'):\n",
        "    '2024_deforestation_since_oldgrowth',\n",
        "\n",
        "}\n",
        "\n",
        "specific_year_effects_dictionary = {\n",
        "\n",
        "}\n",
        "\n",
        "area_based_dictionary = {\n",
        "\n",
        "# Area-based disturbance from alternate scenarios\n",
        "  ('2024_road_mat_daling_deforestation_2023_30m_degradation_buffer', '2024'):\n",
        "    '2024_deforestation_of_road_mat_daling_2023',\n",
        "}\n"
      ],
      "metadata": {
        "id": "fQho_DuUMbGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFNUYdP3Fibv"
      },
      "outputs": [],
      "source": [
        "# disturbance_since_dictionary = {\n",
        "\n",
        "# # Disturbance in 2021 caused by events since 1993\n",
        "#   ('2021', '2021_no_disturbance_since_1993'):\n",
        "#     '2021_disturbance_since_1993',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 1996\n",
        "#   ('2024', '2024_no_disturbance_since_1996'):\n",
        "#     '2024_disturbance_since_1996',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 1997\n",
        "#   ('2024', '2024_no_disturbance_since_1997'):\n",
        "#     '2024_disturbance_since_1997',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 1998\n",
        "#   ('2024', '2024_no_disturbance_since_1998'):\n",
        "#     '2024_disturbance_since_1998',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 1999\n",
        "#   ('2024', '2024_no_disturbance_since_1999'):\n",
        "#     '2024_disturbance_since_1999',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2000\n",
        "#   ('2024', '2024_no_disturbance_since_2000'):\n",
        "#     '2024_disturbance_since_2000',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2001\n",
        "#   ('2024', '2024_no_disturbance_since_2001'):\n",
        "#     '2024_disturbance_since_2001',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2002\n",
        "#   ('2024', '2024_no_disturbance_since_2002'):\n",
        "#     '2024_disturbance_since_2002',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2003\n",
        "#   ('2024', '2024_no_disturbance_since_2003'):\n",
        "#     '2024_disturbance_since_2003',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2004\n",
        "#   ('2024', '2024_no_disturbance_since_2004'):\n",
        "#     '2024_disturbance_since_2004',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2005\n",
        "#   ('2024', '2024_no_disturbance_since_2005'):\n",
        "#     '2024_disturbance_since_2005',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2006\n",
        "#   ('2024', '2024_no_disturbance_since_2006'):\n",
        "#     '2024_disturbance_since_2006',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2007\n",
        "#   ('2024', '2024_no_disturbance_since_2007'):\n",
        "#     '2024_disturbance_since_2007',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2008\n",
        "#   ('2024', '2024_no_disturbance_since_2008'):\n",
        "#     '2024_disturbance_since_2008',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2009\n",
        "#   ('2024', '2024_no_disturbance_since_2009'):\n",
        "#     '2024_disturbance_since_2009',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2010\n",
        "#   ('2024', '2024_no_disturbance_since_2010'):\n",
        "#     '2024_disturbance_since_2010',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2011\n",
        "#   ('2024', '2024_no_disturbance_since_2011'):\n",
        "#     '2024_disturbance_since_2011',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2012\n",
        "#   ('2024', '2024_no_disturbance_since_2012'):\n",
        "#     '2024_disturbance_since_2012',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2013\n",
        "#   ('2024', '2024_no_disturbance_since_2013'):\n",
        "#     '2024_disturbance_since_2013',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2014\n",
        "#   ('2024', '2024_no_disturbance_since_2014'):\n",
        "#     '2024_disturbance_since_2014',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2015\n",
        "#   ('2024', '2024_no_disturbance_since_2015'):\n",
        "#     '2024_disturbance_since_2015',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2016\n",
        "#   ('2024', '2024_no_disturbance_since_2016'):\n",
        "#     '2024_disturbance_since_2016',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2017\n",
        "#   ('2024', '2024_no_disturbance_since_2017'):\n",
        "#     '2024_disturbance_since_2017',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2018\n",
        "#   ('2024', '2024_no_disturbance_since_2018'):\n",
        "#     '2024_disturbance_since_2018',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2019\n",
        "#   ('2024', '2024_no_disturbance_since_2019'):\n",
        "#     '2024_disturbance_since_2019',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2020\n",
        "#   ('2024', '2024_no_disturbance_since_2020'):\n",
        "#     '2024_disturbance_since_2020',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2021\n",
        "#   ('2024', '2024_no_disturbance_since_2021'):\n",
        "#     '2024_disturbance_since_2021',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2022\n",
        "#   ('2024', '2024_no_disturbance_since_2022'):\n",
        "#     '2024_disturbance_since_2022',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2023\n",
        "#   ('2024', '2024_no_disturbance_since_2023'):\n",
        "#     '2024_disturbance_since_2023',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since 2024\n",
        "#   ('2024', '2024_no_disturbance_since_2024'):\n",
        "#     '2024_disturbance_since_2024',\n",
        "\n",
        "# # Disturbance in 2021 caused by events since an oldgrowth state.\n",
        "#   ('2021', '2021_oldgrowth_all_land'):\n",
        "#     '2021_disturbance_since_oldgrowth',\n",
        "\n",
        "# # Disturbance in 2024 caused by events since an oldgrowth state.\n",
        "#   ('2024', '2024_oldgrowth_all_land'):\n",
        "#     '2024_disturbance_since_oldgrowth',\n",
        "\n",
        "# }\n",
        "\n",
        "# degradation_since_dictionary = {\n",
        "\n",
        "# # Degradation in 2021 caused by events since 1993\n",
        "#   ('2021', '2021_no_degradation_since_1993'):\n",
        "#     '2021_degradation_since_1993',\n",
        "\n",
        "# # Degradation in 2024 caused by events since 1996\n",
        "#   ('2024', '2024_no_degradation_since_1996'):\n",
        "#     '2024_degradation_since_1996',\n",
        "\n",
        "# # Degradation in 2021 caused by events since an old-growth state\n",
        "#   ('2021', '2021_oldgrowth'):\n",
        "#     '2021_degradation_since_oldgrowth',\n",
        "\n",
        "# # Degradation in 2024 caused by events since an old-growth state\n",
        "#   ('2024', '2024_oldgrowth'):\n",
        "#     '2024_degradation_since_oldgrowth',\n",
        "\n",
        "# }\n",
        "\n",
        "# deforestation_since_dictionary = {\n",
        "\n",
        "# # Deforestation in 2021 caused by events since 1993\n",
        "#   ('2021_degradation_since_1993', '2021_disturbance_since_1993'):\n",
        "#     '2021_deforestation_since_1993',\n",
        "\n",
        "# # Deforestation in 2024 caused by events since 1996\n",
        "#   ('2024_degradation_since_1996', '2024_disturbance_since_1996'):\n",
        "#     '2024_deforestation_since_1996',\n",
        "\n",
        "# # Deforestation in 2021 caused by events since an old-growth state\n",
        "#   ('2021_degradation_since_oldgrowth', '2021_disturbance_since_oldgrowth'):\n",
        "#     '2021_deforestation_since_oldgrowth',\n",
        "\n",
        "# # Deforestation in 2024 caused by events since an old-growth state\n",
        "#   ('2024_degradation_since_oldgrowth', '2024_disturbance_since_oldgrowth'):\n",
        "#     '2024_deforestation_since_oldgrowth',\n",
        "\n",
        "# }\n",
        "\n",
        "# specific_year_effects_dictionary = {\n",
        "\n",
        "# # Effects in 2024\n",
        "#   # Disturbance effects\n",
        "#   ('2024_disturbance_since_1996', '2024_disturbance_since_1997'):\n",
        "#     '2024_effect_of_disturbance_in_1996',\n",
        "#   ('2024_disturbance_since_1997', '2024_disturbance_since_1998'):\n",
        "#     '2024_effect_of_disturbance_in_1997',\n",
        "#   ('2024_disturbance_since_1998', '2024_disturbance_since_1999'):\n",
        "#     '2024_effect_of_disturbance_in_1998',\n",
        "#   ('2024_disturbance_since_1999', '2024_disturbance_since_2000'):\n",
        "#     '2024_effect_of_disturbance_in_1999',\n",
        "#   ('2024_disturbance_since_2000', '2024_disturbance_since_2001'):\n",
        "#     '2024_effect_of_disturbance_in_2000',\n",
        "#   ('2024_disturbance_since_2001', '2024_disturbance_since_2002'):\n",
        "#     '2024_effect_of_disturbance_in_2001',\n",
        "#   ('2024_disturbance_since_2002', '2024_disturbance_since_2003'):\n",
        "#     '2024_effect_of_disturbance_in_2002',\n",
        "#   ('2024_disturbance_since_2003', '2024_disturbance_since_2004'):\n",
        "#     '2024_effect_of_disturbance_in_2003',\n",
        "#   ('2024_disturbance_since_2004', '2024_disturbance_since_2005'):\n",
        "#     '2024_effect_of_disturbance_in_2004',\n",
        "#   ('2024_disturbance_since_2005', '2024_disturbance_since_2006'):\n",
        "#     '2024_effect_of_disturbance_in_2005',\n",
        "#   ('2024_disturbance_since_2006', '2024_disturbance_since_2007'):\n",
        "#     '2024_effect_of_disturbance_in_2006',\n",
        "#   ('2024_disturbance_since_2007', '2024_disturbance_since_2008'):\n",
        "#     '2024_effect_of_disturbance_in_2007',\n",
        "#   ('2024_disturbance_since_2008', '2024_disturbance_since_2009'):\n",
        "#     '2024_effect_of_disturbance_in_2008',\n",
        "#   ('2024_disturbance_since_2009', '2024_disturbance_since_2010'):\n",
        "#     '2024_effect_of_disturbance_in_2009',\n",
        "#   ('2024_disturbance_since_2010', '2024_disturbance_since_2011'):\n",
        "#     '2024_effect_of_disturbance_in_2010',\n",
        "#   ('2024_disturbance_since_2011', '2024_disturbance_since_2012'):\n",
        "#     '2024_effect_of_disturbance_in_2011',\n",
        "#   ('2024_disturbance_since_2012', '2024_disturbance_since_2013'):\n",
        "#     '2024_effect_of_disturbance_in_2012',\n",
        "#   ('2024_disturbance_since_2013', '2024_disturbance_since_2014'):\n",
        "#     '2024_effect_of_disturbance_in_2013',\n",
        "#   ('2024_disturbance_since_2014', '2024_disturbance_since_2015'):\n",
        "#     '2024_effect_of_disturbance_in_2014',\n",
        "#   ('2024_disturbance_since_2015', '2024_disturbance_since_2016'):\n",
        "#     '2024_effect_of_disturbance_in_2015',\n",
        "#   ('2024_disturbance_since_2016', '2024_disturbance_since_2017'):\n",
        "#     '2024_effect_of_disturbance_in_2016',\n",
        "#   ('2024_disturbance_since_2017', '2024_disturbance_since_2018'):\n",
        "#     '2024_effect_of_disturbance_in_2017',\n",
        "#   ('2024_disturbance_since_2018', '2024_disturbance_since_2019'):\n",
        "#     '2024_effect_of_disturbance_in_2018',\n",
        "#   ('2024_disturbance_since_2019', '2024_disturbance_since_2020'):\n",
        "#     '2024_effect_of_disturbance_in_2019',\n",
        "#   ('2024_disturbance_since_2020', '2024_disturbance_since_2021'):\n",
        "#     '2024_effect_of_disturbance_in_2020',\n",
        "#   ('2024_disturbance_since_2021', '2024_disturbance_since_2022'):\n",
        "#     '2024_effect_of_disturbance_in_2021',\n",
        "#   ('2024_disturbance_since_2022', '2024_disturbance_since_2023'):\n",
        "#     '2024_effect_of_disturbance_in_2022',\n",
        "#   ('2024_disturbance_since_2023', '2024_disturbance_since_2024'):\n",
        "#     '2024_effect_of_disturbance_in_2023',\n",
        "#   ('2024_disturbance_since_2024',):\n",
        "#     '2024_effect_of_disturbance_in_2024',\n",
        "\n",
        "# }\n",
        "\n",
        "# area_based_dictionary = {\n",
        "\n",
        "# # Area-based disturbance from alternate scenarios\n",
        "#   ('2024_road_mat_daling_deforestation_2023_30m_degradation_buffer', '2024'):\n",
        "#     '2024_deforestation_of_road_mat_daling_2023',\n",
        "# }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate disturbance"
      ],
      "metadata": {
        "id": "ozToZSzop-V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply floor constraint preventing degradation from exceeding total disturbance.\n",
        "# Conceptually, disturbance = degradation + deforestation.\n",
        "# This can happen in rare cases where edge effects from non-forest have a positive\n",
        "# predicted impact on AGBD, especially at high elevation.\n",
        "apply_degradation_floor = False\n",
        "\n",
        "# Caps all positive differences, again mainly from rare edge effect cases.\n",
        "# Conceptually this is disturbance loss rather than effect of disturbance.\n",
        "cap_positive_differences = False\n",
        "\n",
        "# Precision settings for output rasters\n",
        "mean_precision = 2\n",
        "ci_precision = 2\n",
        "uncertainty_precision = 2\n",
        "\n",
        "# Confidence interval level for CI filename pattern\n",
        "confidence_level = 95\n",
        "\n",
        "# Calculate AGBD loss between two scenarios (array1 - array2).\n",
        "# Returns negative values for disturbance losses.\n",
        "# Gains will be negligible artifacts of float precision if scenario dictionary correct.\n",
        "def subtract_arrays(array1, array2, cap_positive=False):\n",
        "    diff = array1 - array2\n",
        "    return np.where(diff > 0, 0, diff) if cap_positive else diff\n",
        "\n",
        "# Propagate uncertainty for forest AGBD loss calculations using confidence intervals.\n",
        "# Measures uncertainty of forest AGBD change from disturbance events only.\n",
        "\n",
        "# Mathematical basis: For difference Z = X - Y with confidence intervals CI_x, CI_y:\n",
        "# Combined CI: CI_z = √[CI_x² + CI_y²] (IPCC 2006, Eq. 3.2; 2019, Eq. 3.2A)\n",
        "# Relative uncertainty: CI_z / |Z| = CI_z / |X - Y|\n",
        "\n",
        "# Note: Liang et al. (2023) incorrectly used |X + Y| as denominator, violating standard\n",
        "# uncertainty propagation theory for differences. IPCC guidelines (2006 Section 3.2.3.1,\n",
        "# 2019 Section 3.2.3.1) specify the denominator must be the absolute value of the\n",
        "# difference |X - Y| for mathematically correct relative uncertainty calculations.\n",
        "\n",
        "# Limitation: This approach assumes independence between scenario uncertainties, but\n",
        "# scenarios using identical models and predictors are highly correlated. This results\n",
        "# in conservative (overestimated) uncertainty bounds. Liang et al. (2023) has the same\n",
        "# correlation limitation plus the mathematical error noted above.\n",
        "\n",
        "# Forest classification from external dataset determines data availability per scenario.\n",
        "# External disturbance classification determines whether forest AGBD change occurred.\n",
        "# Uncertainty quantifies confidence in magnitude of forest AGBD change from disturbance.\n",
        "\n",
        "# References:\n",
        "# - IPCC (2006) Guidelines Vol.1 Ch.3: Uncertainties, Section 3.2.3.1\n",
        "# - IPCC (2019) Refinement Vol.1 Ch.3: Uncertainties, Section 3.2.3.1\n",
        "# - Liang et al. (2023) Remote Sensing of Environment 284:113367\n",
        "\n",
        "# Propagate uncertainty function\n",
        "# mean1, mean2: Forest AGBD values for two scenarios/timepoints (Mg/ha)\n",
        "# ci1, ci2: Confidence interval half-widths (Mg/ha)\n",
        "# relative_uncertainty: Percentage (0-100)\n",
        "# ci_combined: Absolute CI for downstream calculations\n",
        "def propagate_uncertainty(mean1, ci1, mean2, ci2):\n",
        "    mean_diff = mean1 - mean2\n",
        "    # Handle forest/non-forest transitions where one scenario has nodata (converted to 0 mean, 0 CI).\n",
        "    # Uncertainty reflects confidence in original forest AGBD estimate, not the forest mask.\n",
        "    deforestation_case = (ci1 == 0) & (mean1 == 0) & (ci2 != 0) & (mean2 != 0)\n",
        "    reforestation_case = (ci1 != 0) & (mean1 != 0) & (ci2 == 0) & (mean2 == 0)\n",
        "    # Combine absolute uncertainties using IPCC error propagation formula.\n",
        "    # Applies when both scenarios contain forest AGBD estimates.\n",
        "    ci_combined = np.sqrt(np.square(ci1) + np.square(ci2))\n",
        "    # Calculate relative uncertainty using absolute difference as denominator (IPCC standard).\n",
        "    denominator = np.abs(mean_diff)\n",
        "    standard_rel_unc = np.divide(ci_combined, denominator,\n",
        "                                 out=np.zeros_like(ci_combined, dtype=np.float64),\n",
        "                                 where=(denominator != 0))\n",
        "    # Forest transition uncertainties.\n",
        "    defor_rel_unc = np.divide(ci2, np.abs(mean2),\n",
        "                              out=np.zeros_like(ci2, dtype=np.float64),\n",
        "                              where=(mean2 != 0))\n",
        "    refor_rel_unc = np.divide(ci1, np.abs(mean1),\n",
        "                              out=np.zeros_like(ci1, dtype=np.float64),\n",
        "                              where=(mean1 != 0))\n",
        "    # Apply uncertainty logic for forest AGBD change from disturbance measurements.\n",
        "    # Zero uncertainty when no disturbance classified: scenarios definitionally identical.\n",
        "    # Uncertainty measures confidence in forest AGBD change magnitude given disturbance occurred.\n",
        "    relative_uncertainty = np.where(\n",
        "        deforestation_case, defor_rel_unc,\n",
        "        np.where(reforestation_case, refor_rel_unc,\n",
        "                 np.where((mean_diff > 0) | (denominator == 0), 0, standard_rel_unc)))\n",
        "    return relative_uncertainty * 100.0, ci_combined\n",
        "\n",
        "# Load raster as numpy array.\n",
        "def load_raster(path):\n",
        "    ds = gdal.Open(path)\n",
        "    arr = ds.ReadAsArray()\n",
        "    ds = None\n",
        "    return arr\n",
        "\n",
        "# Fill nodata with zero where partner array has valid data.\n",
        "def fill_nodata_pair(arr1, arr2, nodata):\n",
        "    arr1 = np.where((arr1 == nodata) & (arr2 != nodata), 0, arr1)\n",
        "    arr2 = np.where((arr2 == nodata) & (arr1 != nodata), 0, arr2)\n",
        "    return arr1, arr2\n",
        "\n",
        "# Round array to precision; convert to int16 if precision is 0.\n",
        "def round_array(arr, precision):\n",
        "    rounded = np.round(arr, precision)\n",
        "    return rounded.astype(np.int16) if precision == 0 else rounded\n",
        "\n",
        "# Determine processing mode based on source directory\n",
        "use_uncertainty = source_dir == uncertainty_dir\n",
        "\n",
        "# Progress tracking\n",
        "total_operations = (len(disturbance_since_dictionary) + len(degradation_since_dictionary) +\n",
        "                    len(deforestation_since_dictionary) + len(specific_year_effects_dictionary) +\n",
        "                    len(area_based_dictionary))\n",
        "progress_index = 0\n",
        "progress_label = widgets.Label(f\"Disturbance calculation progress: {progress_index}/{total_operations}\")\n",
        "display(progress_label)\n",
        "\n",
        "\n",
        "# 1. Process disturbance_since calculations\n",
        "for (scenario1, scenario2), disturbance_name in disturbance_since_dictionary.items():\n",
        "    # Define output paths\n",
        "    if use_uncertainty:\n",
        "        mean_output_path = join(disturbance_dir, f\"mean__{disturbance_name}__{selected_model}.tif\")\n",
        "        ci_output_path = join(disturbance_dir, f\"ci_{confidence_level}__{disturbance_name}__{selected_model}.tif\")\n",
        "        unc_output_path = join(disturbance_dir, f\"uncertainty__{disturbance_name}__{selected_model}.tif\")\n",
        "        outputs_exist = exists(mean_output_path) and exists(ci_output_path) and exists(unc_output_path)\n",
        "    else:\n",
        "        output_path = join(disturbance_dir, f\"{disturbance_name}__{selected_model}.tif\")\n",
        "        outputs_exist = exists(output_path)\n",
        "    if outputs_exist:\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "        continue\n",
        "    # Define and validate input paths\n",
        "    if use_uncertainty:\n",
        "        scenario1_mean_path = join(predictions_dir, f\"mean__{scenario1}__{selected_model}.tif\")\n",
        "        scenario1_ci_path = join(predictions_dir, f\"ci_{confidence_level}__{scenario1}__{selected_model}.tif\")\n",
        "        scenario2_mean_path = join(predictions_dir, f\"mean__{scenario2}__{selected_model}.tif\")\n",
        "        scenario2_ci_path = join(predictions_dir, f\"ci_{confidence_level}__{scenario2}__{selected_model}.tif\")\n",
        "        assert exists(scenario1_mean_path), f\"Missing: mean__{scenario1}__{selected_model}.tif\"\n",
        "        assert exists(scenario1_ci_path), f\"Missing: ci_{confidence_level}__{scenario1}__{selected_model}.tif\"\n",
        "        assert exists(scenario2_mean_path), f\"Missing: mean__{scenario2}__{selected_model}.tif\"\n",
        "        assert exists(scenario2_ci_path), f\"Missing: ci_{confidence_level}__{scenario2}__{selected_model}.tif\"\n",
        "    else:\n",
        "        scenario1_path = join(predictions_dir, f\"{scenario1}__{selected_model}.tif\")\n",
        "        scenario2_path = join(predictions_dir, f\"{scenario2}__{selected_model}.tif\")\n",
        "        assert exists(scenario1_path), f\"Missing: {scenario1_path}\"\n",
        "        assert exists(scenario2_path), f\"Missing: {scenario2_path}\"\n",
        "    # Load input rasters and fill nodata\n",
        "    if use_uncertainty:\n",
        "        scenario1_mean = load_raster(scenario1_mean_path)\n",
        "        scenario2_mean = load_raster(scenario2_mean_path)\n",
        "        scenario1_ci = load_raster(scenario1_ci_path)\n",
        "        scenario2_ci = load_raster(scenario2_ci_path)\n",
        "        scenario1_mean, scenario2_mean = fill_nodata_pair(scenario1_mean, scenario2_mean, nodatavalue)\n",
        "        scenario1_ci, scenario2_ci = fill_nodata_pair(scenario1_ci, scenario2_ci, nodatavalue)\n",
        "        template_path = scenario1_mean_path\n",
        "    else:\n",
        "        scenario1_arr = load_raster(scenario1_path)\n",
        "        scenario2_arr = load_raster(scenario2_path)\n",
        "        scenario1_arr, scenario2_arr = fill_nodata_pair(scenario1_arr, scenario2_arr, nodatavalue)\n",
        "        template_path = scenario1_path\n",
        "    # Calculate disturbance difference and export\n",
        "    if use_uncertainty:\n",
        "        disturbance_mean = subtract_arrays(scenario1_mean, scenario2_mean, cap_positive_differences)\n",
        "        disturbance_mean = np.where(scenario1_mean == nodatavalue, nodatavalue, disturbance_mean)\n",
        "        disturbance_unc, disturbance_ci = propagate_uncertainty(\n",
        "            scenario1_mean, scenario1_ci, scenario2_mean, scenario2_ci)\n",
        "        disturbance_ci = np.where(scenario1_mean == nodatavalue, nodatavalue, disturbance_ci)\n",
        "        disturbance_unc = np.where(scenario1_mean == nodatavalue, nodatavalue, disturbance_unc)\n",
        "        # Zero uncertainty where mean rounds to zero\n",
        "        disturbance_mean_rounded = round_array(disturbance_mean, mean_precision)\n",
        "        zero_mean_mask = (disturbance_mean_rounded == 0)\n",
        "        disturbance_ci = np.where(zero_mean_mask, 0, disturbance_ci)\n",
        "        disturbance_unc = np.where(zero_mean_mask, 0, disturbance_unc)\n",
        "        export_array_as_tif(disturbance_mean_rounded, mean_output_path, template=template_path)\n",
        "        export_array_as_tif(round_array(disturbance_ci, ci_precision), ci_output_path, template=template_path)\n",
        "        export_array_as_tif(round_array(disturbance_unc, uncertainty_precision), unc_output_path, template=template_path)\n",
        "    else:\n",
        "        disturbance_arr = subtract_arrays(scenario1_arr, scenario2_arr, cap_positive_differences)\n",
        "        disturbance_arr = np.where(scenario1_arr == nodatavalue, nodatavalue, disturbance_arr)\n",
        "        export_array_as_tif(round_array(disturbance_arr, mean_precision), output_path, template=template_path)\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "\n",
        "# 2. Process degradation_since calculations\n",
        "for (scenario1, scenario2), disturbance_name in degradation_since_dictionary.items():\n",
        "    # Define output paths\n",
        "    if use_uncertainty:\n",
        "        mean_output_path = join(disturbance_dir, f\"mean__{disturbance_name}__{selected_model}.tif\")\n",
        "        ci_output_path = join(disturbance_dir, f\"ci_{confidence_level}__{disturbance_name}__{selected_model}.tif\")\n",
        "        unc_output_path = join(disturbance_dir, f\"uncertainty__{disturbance_name}__{selected_model}.tif\")\n",
        "        outputs_exist = exists(mean_output_path) and exists(ci_output_path) and exists(unc_output_path)\n",
        "    else:\n",
        "        output_path = join(disturbance_dir, f\"{disturbance_name}__{selected_model}.tif\")\n",
        "        outputs_exist = exists(output_path)\n",
        "    if outputs_exist:\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "        continue\n",
        "    # Define and validate input paths\n",
        "    if use_uncertainty:\n",
        "        scenario1_mean_path = join(predictions_dir, f\"mean__{scenario1}__{selected_model}.tif\")\n",
        "        scenario1_ci_path = join(predictions_dir, f\"ci_{confidence_level}__{scenario1}__{selected_model}.tif\")\n",
        "        scenario2_mean_path = join(predictions_dir, f\"mean__{scenario2}__{selected_model}.tif\")\n",
        "        scenario2_ci_path = join(predictions_dir, f\"ci_{confidence_level}__{scenario2}__{selected_model}.tif\")\n",
        "        assert exists(scenario1_mean_path), f\"Missing: mean__{scenario1}__{selected_model}.tif\"\n",
        "        assert exists(scenario1_ci_path), f\"Missing: ci_{confidence_level}__{scenario1}__{selected_model}.tif\"\n",
        "        assert exists(scenario2_mean_path), f\"Missing: mean__{scenario2}__{selected_model}.tif\"\n",
        "        assert exists(scenario2_ci_path), f\"Missing: ci_{confidence_level}__{scenario2}__{selected_model}.tif\"\n",
        "    else:\n",
        "        scenario1_path = join(predictions_dir, f\"{scenario1}__{selected_model}.tif\")\n",
        "        scenario2_path = join(predictions_dir, f\"{scenario2}__{selected_model}.tif\")\n",
        "        assert exists(scenario1_path), f\"Missing: {scenario1_path}\"\n",
        "        assert exists(scenario2_path), f\"Missing: {scenario2_path}\"\n",
        "    # Load input rasters and fill nodata\n",
        "    if use_uncertainty:\n",
        "        scenario1_mean = load_raster(scenario1_mean_path)\n",
        "        scenario2_mean = load_raster(scenario2_mean_path)\n",
        "        scenario1_ci = load_raster(scenario1_ci_path)\n",
        "        scenario2_ci = load_raster(scenario2_ci_path)\n",
        "        scenario1_mean, scenario2_mean = fill_nodata_pair(scenario1_mean, scenario2_mean, nodatavalue)\n",
        "        scenario1_ci, scenario2_ci = fill_nodata_pair(scenario1_ci, scenario2_ci, nodatavalue)\n",
        "        template_path = scenario1_mean_path\n",
        "    else:\n",
        "        scenario1_arr = load_raster(scenario1_path)\n",
        "        scenario2_arr = load_raster(scenario2_path)\n",
        "        scenario1_arr, scenario2_arr = fill_nodata_pair(scenario1_arr, scenario2_arr, nodatavalue)\n",
        "        template_path = scenario1_path\n",
        "    # Calculate degradation difference\n",
        "    if use_uncertainty:\n",
        "        disturbance_mean = subtract_arrays(scenario1_mean, scenario2_mean, cap_positive_differences)\n",
        "        disturbance_mean = np.where(scenario1_mean == nodatavalue, nodatavalue, disturbance_mean)\n",
        "        disturbance_unc, disturbance_ci = propagate_uncertainty(\n",
        "            scenario1_mean, scenario1_ci, scenario2_mean, scenario2_ci)\n",
        "        disturbance_ci = np.where(scenario1_mean == nodatavalue, nodatavalue, disturbance_ci)\n",
        "        disturbance_unc = np.where(scenario1_mean == nodatavalue, nodatavalue, disturbance_unc)\n",
        "        # Apply floor constraint: degradation cannot exceed total disturbance\n",
        "        if apply_degradation_floor:\n",
        "            equiv_name = disturbance_name.replace('degradation_since', 'disturbance_since')\n",
        "            equiv_mean_path = join(disturbance_dir, f\"mean__{equiv_name}__{selected_model}.tif\")\n",
        "            equiv_ci_path = join(disturbance_dir, f\"ci_{confidence_level}__{equiv_name}__{selected_model}.tif\")\n",
        "            equiv_unc_path = join(disturbance_dir, f\"uncertainty__{equiv_name}__{selected_model}.tif\")\n",
        "            if exists(equiv_mean_path) and exists(equiv_ci_path) and exists(equiv_unc_path):\n",
        "                print(f\"Applying floor constraint: {disturbance_name} constrained by {equiv_name}\")\n",
        "                equiv_mean = load_raster(equiv_mean_path)\n",
        "                equiv_ci = load_raster(equiv_ci_path)\n",
        "                equiv_unc = load_raster(equiv_unc_path)\n",
        "                floor_mask = (disturbance_mean != nodatavalue) & (equiv_mean != nodatavalue) & (disturbance_mean < equiv_mean)\n",
        "                disturbance_mean = np.where(floor_mask, equiv_mean, disturbance_mean)\n",
        "                disturbance_ci = np.where(floor_mask, equiv_ci, disturbance_ci)\n",
        "                disturbance_unc = np.where(floor_mask, equiv_unc, disturbance_unc)\n",
        "            else: print(f\"No floor constraint applied: {equiv_name} files not found\")\n",
        "        # Zero uncertainty where mean rounds to zero\n",
        "        disturbance_mean_rounded = round_array(disturbance_mean, mean_precision)\n",
        "        zero_mean_mask = (disturbance_mean_rounded == 0)\n",
        "        disturbance_ci = np.where(zero_mean_mask, 0, disturbance_ci)\n",
        "        disturbance_unc = np.where(zero_mean_mask, 0, disturbance_unc)\n",
        "        export_array_as_tif(disturbance_mean_rounded, mean_output_path, template=template_path)\n",
        "        export_array_as_tif(round_array(disturbance_ci, ci_precision), ci_output_path, template=template_path)\n",
        "        export_array_as_tif(round_array(disturbance_unc, uncertainty_precision), unc_output_path, template=template_path)\n",
        "    else:\n",
        "        disturbance_arr = subtract_arrays(scenario1_arr, scenario2_arr, cap_positive_differences)\n",
        "        disturbance_arr = np.where(scenario1_arr == nodatavalue, nodatavalue, disturbance_arr)\n",
        "        # Apply floor constraint\n",
        "        if apply_degradation_floor:\n",
        "            equiv_name = disturbance_name.replace('degradation_since', 'disturbance_since')\n",
        "            equiv_path = join(disturbance_dir, f\"{equiv_name}__{selected_model}.tif\")\n",
        "            if exists(equiv_path):\n",
        "                print(f\"Applying floor constraint: {disturbance_name} constrained by {equiv_name}\")\n",
        "                equiv_arr = load_raster(equiv_path)\n",
        "                disturbance_arr = np.where(\n",
        "                    (disturbance_arr != nodatavalue) & (equiv_arr != nodatavalue) & (disturbance_arr < equiv_arr),\n",
        "                    equiv_arr, disturbance_arr)\n",
        "            else: print(f\"No floor constraint applied: {equiv_name} file not found\")\n",
        "        export_array_as_tif(round_array(disturbance_arr, mean_precision), output_path, template=template_path)\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "\n",
        "# 3. Process deforestation_since calculations (disturbance - degradation)\n",
        "for (dist1_name, dist2_name), disturbance_name in deforestation_since_dictionary.items():\n",
        "    # Define output paths\n",
        "    if use_uncertainty:\n",
        "        mean_output_path = join(disturbance_dir, f\"mean__{disturbance_name}__{selected_model}.tif\")\n",
        "        ci_output_path = join(disturbance_dir, f\"ci_{confidence_level}__{disturbance_name}__{selected_model}.tif\")\n",
        "        unc_output_path = join(disturbance_dir, f\"uncertainty__{disturbance_name}__{selected_model}.tif\")\n",
        "        outputs_exist = exists(mean_output_path) and exists(ci_output_path) and exists(unc_output_path)\n",
        "    else:\n",
        "        output_path = join(disturbance_dir, f\"{disturbance_name}__{selected_model}.tif\")\n",
        "        outputs_exist = exists(output_path)\n",
        "    if outputs_exist:\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "        continue\n",
        "    # Define and validate input paths (from previously calculated disturbance outputs)\n",
        "    if use_uncertainty:\n",
        "        dist1_mean_path = join(disturbance_dir, f\"mean__{dist1_name}__{selected_model}.tif\")\n",
        "        dist1_ci_path = join(disturbance_dir, f\"ci_{confidence_level}__{dist1_name}__{selected_model}.tif\")\n",
        "        dist2_mean_path = join(disturbance_dir, f\"mean__{dist2_name}__{selected_model}.tif\")\n",
        "        dist2_ci_path = join(disturbance_dir, f\"ci_{confidence_level}__{dist2_name}__{selected_model}.tif\")\n",
        "        assert exists(dist1_mean_path), f\"Missing: mean__{dist1_name}__{selected_model}.tif\"\n",
        "        assert exists(dist1_ci_path), f\"Missing: ci_{confidence_level}__{dist1_name}__{selected_model}.tif\"\n",
        "        assert exists(dist2_mean_path), f\"Missing: mean__{dist2_name}__{selected_model}.tif\"\n",
        "        assert exists(dist2_ci_path), f\"Missing: ci_{confidence_level}__{dist2_name}__{selected_model}.tif\"\n",
        "    else:\n",
        "        dist1_path = join(disturbance_dir, f\"{dist1_name}__{selected_model}.tif\")\n",
        "        dist2_path = join(disturbance_dir, f\"{dist2_name}__{selected_model}.tif\")\n",
        "        assert exists(dist1_path), f\"Missing: {dist1_path}\"\n",
        "        assert exists(dist2_path), f\"Missing: {dist2_path}\"\n",
        "    # Load input rasters and fill nodata\n",
        "    if use_uncertainty:\n",
        "        dist1_mean = load_raster(dist1_mean_path)\n",
        "        dist2_mean = load_raster(dist2_mean_path)\n",
        "        dist1_ci = load_raster(dist1_ci_path)\n",
        "        dist2_ci = load_raster(dist2_ci_path)\n",
        "        dist1_mean, dist2_mean = fill_nodata_pair(dist1_mean, dist2_mean, nodatavalue)\n",
        "        dist1_ci, dist2_ci = fill_nodata_pair(dist1_ci, dist2_ci, nodatavalue)\n",
        "        template_path = dist2_mean_path\n",
        "    else:\n",
        "        dist1_arr = load_raster(dist1_path)\n",
        "        dist2_arr = load_raster(dist2_path)\n",
        "        dist1_arr, dist2_arr = fill_nodata_pair(dist1_arr, dist2_arr, nodatavalue)\n",
        "        template_path = dist2_path\n",
        "    # Calculate deforestation (dist2 - dist1) and export\n",
        "    if use_uncertainty:\n",
        "        result_mean = subtract_arrays(dist2_mean, dist1_mean, cap_positive_differences)\n",
        "        result_mean = np.where(dist2_mean == nodatavalue, nodatavalue, result_mean)\n",
        "        result_unc, result_ci = propagate_uncertainty(dist2_mean, dist2_ci, dist1_mean, dist1_ci)\n",
        "        result_ci = np.where(dist2_mean == nodatavalue, nodatavalue, result_ci)\n",
        "        result_unc = np.where(dist2_mean == nodatavalue, nodatavalue, result_unc)\n",
        "        # Zero uncertainty where mean rounds to zero\n",
        "        result_mean_rounded = round_array(result_mean, mean_precision)\n",
        "        zero_mean_mask = (result_mean_rounded == 0)\n",
        "        result_ci = np.where(zero_mean_mask, 0, result_ci)\n",
        "        result_unc = np.where(zero_mean_mask, 0, result_unc)\n",
        "        export_array_as_tif(result_mean_rounded, mean_output_path, template=template_path)\n",
        "        export_array_as_tif(round_array(result_ci, ci_precision), ci_output_path, template=template_path)\n",
        "        export_array_as_tif(round_array(result_unc, uncertainty_precision), unc_output_path, template=template_path)\n",
        "    else:\n",
        "        result_arr = subtract_arrays(dist2_arr, dist1_arr, cap_positive_differences)\n",
        "        result_arr = np.where(dist2_arr == nodatavalue, nodatavalue, result_arr)\n",
        "        export_array_as_tif(round_array(result_arr, mean_precision), output_path, template=template_path)\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "\n",
        "# 4. Process specific_year_effects calculations\n",
        "for dist_key, disturbance_name in specific_year_effects_dictionary.items():\n",
        "    if len(dist_key) == 1:\n",
        "        # Direct copy (same-year effect)\n",
        "        source_name = dist_key[0]\n",
        "        if use_uncertainty:\n",
        "            src_mean_path = join(disturbance_dir, f\"mean__{source_name}__{selected_model}.tif\")\n",
        "            src_ci_path = join(disturbance_dir, f\"ci_{confidence_level}__{source_name}__{selected_model}.tif\")\n",
        "            src_unc_path = join(disturbance_dir, f\"uncertainty__{source_name}__{selected_model}.tif\")\n",
        "            tgt_mean_path = join(disturbance_dir, f\"mean__{disturbance_name}__{selected_model}.tif\")\n",
        "            tgt_ci_path = join(disturbance_dir, f\"ci_{confidence_level}__{disturbance_name}__{selected_model}.tif\")\n",
        "            tgt_unc_path = join(disturbance_dir, f\"uncertainty__{disturbance_name}__{selected_model}.tif\")\n",
        "            outputs_exist = exists(tgt_mean_path) and exists(tgt_ci_path) and exists(tgt_unc_path)\n",
        "        else:\n",
        "            src_path = join(disturbance_dir, f\"{source_name}__{selected_model}.tif\")\n",
        "            tgt_path = join(disturbance_dir, f\"{disturbance_name}__{selected_model}.tif\")\n",
        "            outputs_exist = exists(tgt_path)\n",
        "        if outputs_exist:\n",
        "            progress_index += 1\n",
        "            progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "            continue\n",
        "        # Copy source to target (source files already rounded)\n",
        "        if use_uncertainty:\n",
        "            assert exists(src_mean_path), f\"Missing: mean__{source_name}__{selected_model}.tif\"\n",
        "            assert exists(src_ci_path), f\"Missing: ci_{confidence_level}__{source_name}__{selected_model}.tif\"\n",
        "            assert exists(src_unc_path), f\"Missing: uncertainty__{source_name}__{selected_model}.tif\"\n",
        "            src_mean = load_raster(src_mean_path)\n",
        "            src_ci = load_raster(src_ci_path)\n",
        "            src_unc = load_raster(src_unc_path)\n",
        "            # Zero uncertainty where mean rounds to zero\n",
        "            src_mean_rounded = round_array(src_mean, mean_precision)\n",
        "            zero_mean_mask = (src_mean_rounded == 0)\n",
        "            src_ci = np.where(zero_mean_mask, 0, src_ci)\n",
        "            src_unc = np.where(zero_mean_mask, 0, src_unc)\n",
        "            export_array_as_tif(src_mean_rounded, tgt_mean_path, template=src_mean_path)\n",
        "            export_array_as_tif(src_ci, tgt_ci_path, template=src_mean_path)\n",
        "            export_array_as_tif(src_unc, tgt_unc_path, template=src_mean_path)\n",
        "        else:\n",
        "            assert exists(src_path), f\"Missing: {src_path}\"\n",
        "            export_array_as_tif(round_array(load_raster(src_path), mean_precision), tgt_path, template=src_path)\n",
        "    else:\n",
        "        # Subtraction (dist1 - dist2)\n",
        "        dist1_name, dist2_name = dist_key\n",
        "        if use_uncertainty:\n",
        "            mean_output_path = join(disturbance_dir, f\"mean__{disturbance_name}__{selected_model}.tif\")\n",
        "            ci_output_path = join(disturbance_dir, f\"ci_{confidence_level}__{disturbance_name}__{selected_model}.tif\")\n",
        "            unc_output_path = join(disturbance_dir, f\"uncertainty__{disturbance_name}__{selected_model}.tif\")\n",
        "            outputs_exist = exists(mean_output_path) and exists(ci_output_path) and exists(unc_output_path)\n",
        "        else:\n",
        "            output_path = join(disturbance_dir, f\"{disturbance_name}__{selected_model}.tif\")\n",
        "            outputs_exist = exists(output_path)\n",
        "        if outputs_exist:\n",
        "            progress_index += 1\n",
        "            progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "            continue\n",
        "        # Define and validate input paths\n",
        "        if use_uncertainty:\n",
        "            dist1_mean_path = join(disturbance_dir, f\"mean__{dist1_name}__{selected_model}.tif\")\n",
        "            dist1_ci_path = join(disturbance_dir, f\"ci_{confidence_level}__{dist1_name}__{selected_model}.tif\")\n",
        "            dist2_mean_path = join(disturbance_dir, f\"mean__{dist2_name}__{selected_model}.tif\")\n",
        "            dist2_ci_path = join(disturbance_dir, f\"ci_{confidence_level}__{dist2_name}__{selected_model}.tif\")\n",
        "            assert exists(dist1_mean_path), f\"Missing: mean__{dist1_name}__{selected_model}.tif\"\n",
        "            assert exists(dist1_ci_path), f\"Missing: ci_{confidence_level}__{dist1_name}__{selected_model}.tif\"\n",
        "            assert exists(dist2_mean_path), f\"Missing: mean__{dist2_name}__{selected_model}.tif\"\n",
        "            assert exists(dist2_ci_path), f\"Missing: ci_{confidence_level}__{dist2_name}__{selected_model}.tif\"\n",
        "        else:\n",
        "            dist1_path = join(disturbance_dir, f\"{dist1_name}__{selected_model}.tif\")\n",
        "            dist2_path = join(disturbance_dir, f\"{dist2_name}__{selected_model}.tif\")\n",
        "            assert exists(dist1_path), f\"Missing: {dist1_path}\"\n",
        "            assert exists(dist2_path), f\"Missing: {dist2_path}\"\n",
        "        # Load input rasters and fill nodata\n",
        "        if use_uncertainty:\n",
        "            dist1_mean = load_raster(dist1_mean_path)\n",
        "            dist2_mean = load_raster(dist2_mean_path)\n",
        "            dist1_ci = load_raster(dist1_ci_path)\n",
        "            dist2_ci = load_raster(dist2_ci_path)\n",
        "            dist1_mean, dist2_mean = fill_nodata_pair(dist1_mean, dist2_mean, nodatavalue)\n",
        "            dist1_ci, dist2_ci = fill_nodata_pair(dist1_ci, dist2_ci, nodatavalue)\n",
        "            template_path = dist1_mean_path\n",
        "        else:\n",
        "            dist1_arr = load_raster(dist1_path)\n",
        "            dist2_arr = load_raster(dist2_path)\n",
        "            dist1_arr, dist2_arr = fill_nodata_pair(dist1_arr, dist2_arr, nodatavalue)\n",
        "            template_path = dist1_path\n",
        "        # Calculate difference (dist1 - dist2) and export\n",
        "        if use_uncertainty:\n",
        "            result_mean = subtract_arrays(dist1_mean, dist2_mean, cap_positive_differences)\n",
        "            result_mean = np.where(dist1_mean == nodatavalue, nodatavalue, result_mean)\n",
        "            result_unc, result_ci = propagate_uncertainty(dist1_mean, dist1_ci, dist2_mean, dist2_ci)\n",
        "            result_ci = np.where(dist1_mean == nodatavalue, nodatavalue, result_ci)\n",
        "            result_unc = np.where(dist1_mean == nodatavalue, nodatavalue, result_unc)\n",
        "            # Zero uncertainty where mean rounds to zero\n",
        "            result_mean_rounded = round_array(result_mean, mean_precision)\n",
        "            zero_mean_mask = (result_mean_rounded == 0)\n",
        "            result_ci = np.where(zero_mean_mask, 0, result_ci)\n",
        "            result_unc = np.where(zero_mean_mask, 0, result_unc)\n",
        "            export_array_as_tif(result_mean_rounded, mean_output_path, template=template_path)\n",
        "            export_array_as_tif(round_array(result_ci, ci_precision), ci_output_path, template=template_path)\n",
        "            export_array_as_tif(round_array(result_unc, uncertainty_precision), unc_output_path, template=template_path)\n",
        "        else:\n",
        "            result_arr = subtract_arrays(dist1_arr, dist2_arr, cap_positive_differences)\n",
        "            result_arr = np.where(dist1_arr == nodatavalue, nodatavalue, result_arr)\n",
        "            export_array_as_tif(round_array(result_arr, mean_precision), output_path, template=template_path)\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "\n",
        "# 5. Process area_based calculations\n",
        "for (scenario1, scenario2), disturbance_name in area_based_dictionary.items():\n",
        "    # Define output paths\n",
        "    if use_uncertainty:\n",
        "        mean_output_path = join(disturbance_dir, f\"mean__{disturbance_name}__{selected_model}.tif\")\n",
        "        ci_output_path = join(disturbance_dir, f\"ci_{confidence_level}__{disturbance_name}__{selected_model}.tif\")\n",
        "        unc_output_path = join(disturbance_dir, f\"uncertainty__{disturbance_name}__{selected_model}.tif\")\n",
        "        outputs_exist = exists(mean_output_path) and exists(ci_output_path) and exists(unc_output_path)\n",
        "    else:\n",
        "        output_path = join(disturbance_dir, f\"{disturbance_name}__{selected_model}.tif\")\n",
        "        outputs_exist = exists(output_path)\n",
        "    if outputs_exist:\n",
        "        progress_index += 1\n",
        "        progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "        continue\n",
        "    # Define and validate input paths\n",
        "    if use_uncertainty:\n",
        "        scenario1_mean_path = join(predictions_dir, f\"mean__{scenario1}__{selected_model}.tif\")\n",
        "        scenario1_ci_path = join(predictions_dir, f\"ci_{confidence_level}__{scenario1}__{selected_model}.tif\")\n",
        "        scenario2_mean_path = join(predictions_dir, f\"mean__{scenario2}__{selected_model}.tif\")\n",
        "        scenario2_ci_path = join(predictions_dir, f\"ci_{confidence_level}__{scenario2}__{selected_model}.tif\")\n",
        "        assert exists(scenario1_mean_path), f\"Missing: mean__{scenario1}__{selected_model}.tif\"\n",
        "        assert exists(scenario1_ci_path), f\"Missing: ci_{confidence_level}__{scenario1}__{selected_model}.tif\"\n",
        "        assert exists(scenario2_mean_path), f\"Missing: mean__{scenario2}__{selected_model}.tif\"\n",
        "        assert exists(scenario2_ci_path), f\"Missing: ci_{confidence_level}__{scenario2}__{selected_model}.tif\"\n",
        "    else:\n",
        "        scenario1_path = join(predictions_dir, f\"{scenario1}__{selected_model}.tif\")\n",
        "        scenario2_path = join(predictions_dir, f\"{scenario2}__{selected_model}.tif\")\n",
        "        assert exists(scenario1_path), f\"Missing: {scenario1_path}\"\n",
        "        assert exists(scenario2_path), f\"Missing: {scenario2_path}\"\n",
        "    # Load input rasters and fill nodata\n",
        "    if use_uncertainty:\n",
        "        scenario1_mean = load_raster(scenario1_mean_path)\n",
        "        scenario2_mean = load_raster(scenario2_mean_path)\n",
        "        scenario1_ci = load_raster(scenario1_ci_path)\n",
        "        scenario2_ci = load_raster(scenario2_ci_path)\n",
        "        scenario1_mean, scenario2_mean = fill_nodata_pair(scenario1_mean, scenario2_mean, nodatavalue)\n",
        "        scenario1_ci, scenario2_ci = fill_nodata_pair(scenario1_ci, scenario2_ci, nodatavalue)\n",
        "        template_path = scenario1_mean_path\n",
        "    else:\n",
        "        scenario1_arr = load_raster(scenario1_path)\n",
        "        scenario2_arr = load_raster(scenario2_path)\n",
        "        scenario1_arr, scenario2_arr = fill_nodata_pair(scenario1_arr, scenario2_arr, nodatavalue)\n",
        "        template_path = scenario1_path\n",
        "    # Calculate area-based disturbance difference and export\n",
        "    if use_uncertainty:\n",
        "        disturbance_mean = subtract_arrays(scenario1_mean, scenario2_mean, cap_positive_differences)\n",
        "        disturbance_mean = np.where(scenario1_mean == nodatavalue, nodatavalue, disturbance_mean)\n",
        "        disturbance_unc, disturbance_ci = propagate_uncertainty(\n",
        "            scenario1_mean, scenario1_ci, scenario2_mean, scenario2_ci)\n",
        "        disturbance_ci = np.where(scenario1_mean == nodatavalue, nodatavalue, disturbance_ci)\n",
        "        disturbance_unc = np.where(scenario1_mean == nodatavalue, nodatavalue, disturbance_unc)\n",
        "        # Zero uncertainty where mean rounds to zero\n",
        "        disturbance_mean_rounded = round_array(disturbance_mean, mean_precision)\n",
        "        zero_mean_mask = (disturbance_mean_rounded == 0)\n",
        "        disturbance_ci = np.where(zero_mean_mask, 0, disturbance_ci)\n",
        "        disturbance_unc = np.where(zero_mean_mask, 0, disturbance_unc)\n",
        "        export_array_as_tif(disturbance_mean_rounded, mean_output_path, template=template_path)\n",
        "        export_array_as_tif(round_array(disturbance_ci, ci_precision), ci_output_path, template=template_path)\n",
        "        export_array_as_tif(round_array(disturbance_unc, uncertainty_precision), unc_output_path, template=template_path)\n",
        "    else:\n",
        "        disturbance_arr = subtract_arrays(scenario1_arr, scenario2_arr, cap_positive_differences)\n",
        "        disturbance_arr = np.where(scenario1_arr == nodatavalue, nodatavalue, disturbance_arr)\n",
        "        export_array_as_tif(round_array(disturbance_arr, mean_precision), output_path, template=template_path)\n",
        "    progress_index += 1\n",
        "    progress_label.value = f\"Disturbance calculation progress: {progress_index}/{total_operations}\"\n",
        "\n",
        "\n",
        "print(\"All disturbances calculated.\")"
      ],
      "metadata": {
        "id": "41f6Y5-bz5E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81OCJi98NDwj"
      },
      "source": [
        "# Intactness"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Percentage loss"
      ],
      "metadata": {
        "id": "MJyxcVvLqCdW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsend_eeNIll"
      },
      "outputs": [],
      "source": [
        "# Intactness is measured as relative percentage loss of AGBD within an area of interest\n",
        "\n",
        "# Select which baseline and disturbance raster to use for calculating intactness\n",
        "# percentage and relative intactness. Ideally this is the scenario with the least disturbance\n",
        "# and the difference between that and the current reality.\n",
        "\n",
        "for baseline in os.listdir(predictions_dir):\n",
        "  if source_dir == scenarios_dir: print(f\"selected_baseline = '{'__'.join(baseline.split('__')[0:-1])}'\")\n",
        "  if source_dir == uncertainty_dir:\n",
        "    if 'mean' in baseline: print(f\"selected_baseline = '{'__'.join(baseline.split('__')[1:-1])}'\")\n",
        "for dist in os.listdir(disturbance_dir):\n",
        "  if source_dir == scenarios_dir: print(f\"selected_dist = '{'__'.join(dist.split('__')[0:-1])}'\")\n",
        "  if source_dir == uncertainty_dir:\n",
        "    if 'mean' in dist:print(f\"selected_dist = '{'__'.join(dist.split('__')[1:-1])}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87Q6aMSdNIye"
      },
      "outputs": [],
      "source": [
        "# selected_baseline = '2021_no_disturbance_since_1993'\n",
        "# selected_dist = '2021_disturbance_since_1993'\n",
        "# selected_baseline = '2021_oldgrowth_all_land'\n",
        "# selected_dist = '2021_disturbance_since_oldgrowth'\n",
        "# selected_baseline = '2024_no_disturbance_since_1996'\n",
        "# selected_dist = '2024_disturbance_since_1996'\n",
        "selected_baseline = '2024_oldgrowth_all_land'\n",
        "selected_dist = '2024_disturbance_since_oldgrowth'\n",
        "\n",
        "percentage_loss_precision = 0\n",
        "\n",
        "# Define the baseline name based on source directory\n",
        "if source_dir == scenarios_dir:\n",
        "  base_dist_name = f\"{selected_baseline.split('__')[0]}__{selected_dist.split('__')[0]}\"\n",
        "if source_dir == uncertainty_dir:\n",
        "  base_dist_name = f\"{selected_baseline.split('__')[0]}__{selected_dist.split('__')[0]}\"\n",
        "forest_mask_year = base_dist_name.split('_')[0]\n",
        "\n",
        "intactness_baseline_dist_dir = join(intactness_dir, base_dist_name)\n",
        "makedirs(intactness_baseline_dist_dir, exist_ok=True)\n",
        "\n",
        "percentage_filename = f\"percentage_change__{base_dist_name}__{selected_model}.tif\"\n",
        "percentage_path = join(intactness_baseline_dist_dir, percentage_filename)\n",
        "\n",
        "if not exists(percentage_path):\n",
        "  # Define filenames and directories\n",
        "  if source_dir == scenarios_dir:\n",
        "    selected_baseline_path = join(predictions_dir, f\"{selected_baseline}__{selected_model}.tif\")\n",
        "    selected_dist_path = join(disturbance_dir, f\"{selected_dist}__{selected_model}.tif\")\n",
        "  if source_dir == uncertainty_dir:\n",
        "    selected_baseline_path = join(predictions_dir, f\"mean__{selected_baseline}__{selected_model}.tif\")\n",
        "    selected_dist_path = join(disturbance_dir, f\"mean__{selected_dist}__{selected_model}.tif\")\n",
        "  selected_mask_path = join(masks_dir, f\"mask_forest_{forest_mask_year}.tif\")\n",
        "\n",
        "  # Convert to arrays\n",
        "  selected_baseline = gdal.Open(selected_baseline_path)\n",
        "  selected_baseline_array = selected_baseline.ReadAsArray()\n",
        "  selected_baseline = None\n",
        "  selected_dist = gdal.Open(selected_dist_path)\n",
        "  selected_dist_array = selected_dist.ReadAsArray()\n",
        "  selected_dist = None\n",
        "  selected_mask = gdal.Open(selected_mask_path)\n",
        "  selected_mask_array = selected_mask.ReadAsArray()\n",
        "  selected_mask = None\n",
        "\n",
        "  # Create percentage array where the value is not 'nodatavalue' in any of the inputs\n",
        "  percentage_array = np.full_like(selected_baseline_array, nodatavalue, dtype=np.float64)\n",
        "  valid_mask = (selected_mask_array!=nodatavalue) & (selected_baseline_array!=nodatavalue) & (selected_dist_array!=nodatavalue)\n",
        "  zero_baseline = valid_mask & (selected_baseline_array==0)\n",
        "  nonzero_baseline = valid_mask & (selected_baseline_array!=0)\n",
        "  percentage_array[zero_baseline] = 0\n",
        "  percentage_array[nonzero_baseline] = (selected_dist_array[nonzero_baseline] / selected_baseline_array[nonzero_baseline]) * 100\n",
        "  if percentage_loss_precision == 0: percentage_array = np.round(percentage_array, percentage_loss_precision).astype(np.int16)\n",
        "  else: percentage_array = np.round(percentage_array, percentage_loss_precision)\n",
        "  export_array_as_tif(percentage_array, percentage_path, template = selected_baseline_path)\n",
        "  print(f\"{percentage_filename} has been exported.\")\n",
        "\n",
        "else: print(f\"{percentage_filename} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantiles (relative intactness)"
      ],
      "metadata": {
        "id": "7ugXALeIqLkt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwXRZTew1r39"
      },
      "outputs": [],
      "source": [
        "# Use additional polygons for masking relative intactness quantiles\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area_buffered_bbox.gpkg']\n",
        "\n",
        "# Select baseline / disturbance pairs to measure relative intactness\n",
        "print(\"baseline_disturbance_pairs = [\")\n",
        "for dir in os.listdir(intactness_dir):\n",
        "  print(f\"'{dir}',\")\n",
        "print(\"]\\n\")\n",
        "\n",
        "# Select polygons to mask and calculate quantiles\n",
        "print(\"mask_polygons = [\")\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    if 'inverse' not in polygon:\n",
        "      print(f\"'{polygon}',\")\n",
        "print(None)\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7-X8cXopVo3"
      },
      "outputs": [],
      "source": [
        "baseline_disturbance_pairs = [\n",
        "'2024_oldgrowth_all_land__2024_disturbance_since_oldgrowth',\n",
        "]\n",
        "\n",
        "mask_polygons = [\n",
        "# 'project_area.gpkg',\n",
        "# 'peninsular_malaysia.gpkg',\n",
        "# 'lu_yong.gpkg',\n",
        "# 'lu_yong_lipis.gpkg',\n",
        "# 'lu_berkelah_jerantut.gpkg',\n",
        "# 'lu_tekai_tembeling.gpkg',\n",
        "# 'lu_ais.gpkg',\n",
        "# 'lu_pa_taman_negara_krau.gpkg',\n",
        "# 'lu_tekam.gpkg',\n",
        "# 'lu_berkelah_temerloh.gpkg',\n",
        "# 'lu_remen_chereh.gpkg',\n",
        "# 'lu_berkelah_kuantan.gpkg',\n",
        "'forest_reserves.gpkg',\n",
        "'gedi_area.gpkg',\n",
        "# None\n",
        "]\n",
        "\n",
        "# Convert nodata values inside the mask to a score of 0 (representing non-forest areas)\n",
        "# Otherwise both non-forest and masked areas will be nodatavalue\n",
        "convert_non_forest_nodatavalue_to_0 = True\n",
        "\n",
        "# Define top score for intactness rating (e.g. 10 for 1 - 10 scale)\n",
        "top_score = 10\n",
        "\n",
        "# Calculate actual number of quantiles for non-zero values\n",
        "num_quantiles = top_score - 1\n",
        "\n",
        "print(f\"Calculating {num_quantiles} quantiles for negative percentage change (scores 1-{num_quantiles}), with score {top_score} reserved for 0% change.\\n\")\n",
        "\n",
        "# Create polygon mask array using template tif\n",
        "template = gdal.Open(template_tif_path)\n",
        "template_array = template.ReadAsArray()\n",
        "template = None\n",
        "polygon_mask_array = np.ones_like(template_array, dtype=bool)\n",
        "\n",
        "polygon_masks = {}\n",
        "for mask_polygon in mask_polygons:\n",
        "  if mask_polygon is not None:\n",
        "    # Create an inverse project area path for masking\n",
        "    template_polygon_path = join(polygons_dir, \"template.gpkg\")\n",
        "    inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "    if not exists(inverse_polygon_path):\n",
        "      polygon_path = join(polygons_dir, mask_polygon)\n",
        "      template_polygon = gpd.read_file(template_polygon_path)\n",
        "      polygon_read = gpd.read_file(polygon_path)\n",
        "      polygon_crs = polygon_read.crs.to_epsg()\n",
        "      inverse_polygon = template_polygon['geometry'].difference(polygon_read['geometry']).iloc[0]\n",
        "      inverse_polygon_gdf = gpd.GeoDataFrame({'geometry': [inverse_polygon]}, crs=f\"EPSG:{polygon_crs}\")\n",
        "      inverse_polygon_gdf.to_file(inverse_polygon_path, driver=\"GPKG\")\n",
        "      print(f\"An inverse masking polygon for {mask_polygon} has been created in {polygons_dir}.\")\n",
        "    else: print(f\"An inverse masking polygon for {mask_polygon} already exists.\")\n",
        "\n",
        "    # Create and store individual mask for this polygon\n",
        "    print(f\"Creating polygon mask for {mask_polygon}.\")\n",
        "    inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "    temp_mask_path = join(intactness_dir, f\"temp_mask_{mask_polygon[:-5]}.tif\")\n",
        "    copyfile(template_tif_path, temp_mask_path)\n",
        "    burn_polygon_to_raster(temp_mask_path, inverse_polygon_path, fixed_value=nodatavalue, all_touched=False)\n",
        "    temp_mask_array = gdal.Open(temp_mask_path).ReadAsArray()\n",
        "    individual_mask = np.ones_like(template_array, dtype=bool)\n",
        "    individual_mask[temp_mask_array == nodatavalue] = False\n",
        "    polygon_masks[mask_polygon] = individual_mask\n",
        "    os.remove(temp_mask_path)\n",
        "\n",
        "for base_dist_name in baseline_disturbance_pairs:\n",
        "  intactness_baseline_dist_dir = join(intactness_dir, base_dist_name)\n",
        "  percentage_filename = f\"percentage_change__{base_dist_name}__{selected_model}\"\n",
        "  percentage_path = join(intactness_baseline_dist_dir, f\"{percentage_filename}.tif\")\n",
        "\n",
        "  for mask_polygon in mask_polygons:\n",
        "\n",
        "    if mask_polygon is not None:\n",
        "      # Copy the percentage raster for potential masking\n",
        "      percentage_masked_filename = f\"{percentage_filename}__masked_{mask_polygon[:-5]}.tif\"\n",
        "      percentage_masked_path = join(intactness_baseline_dist_dir, percentage_masked_filename)\n",
        "      if not exists(percentage_masked_path):\n",
        "        print(f\"Copying {percentage_filename} for masking...\")\n",
        "        copyfile(percentage_path, percentage_masked_path)\n",
        "        print(f\"Masking {percentage_filename} with {mask_polygon}...\")\n",
        "        inverse_polygon_path = join(polygons_dir, f\"{mask_polygon[:-5]}_inverse.gpkg\")\n",
        "        burn_polygon_to_raster(percentage_masked_path, inverse_polygon_path, fixed_value=nodatavalue, all_touched=False)\n",
        "        # Recompress the prediction after burning the polygon masks\n",
        "        percentage_masked = gdal.Open(percentage_masked_path)\n",
        "        percentage_masked_array = percentage_masked.ReadAsArray()\n",
        "        percentage_masked = None\n",
        "        export_array_as_tif(percentage_masked_array, percentage_masked_path, compress = True)\n",
        "        print(f\"{percentage_filename} masked.\")\n",
        "      else: print(f\"{percentage_masked_filename} already exists.\")\n",
        "\n",
        "    # Define paths and arrays\n",
        "    if mask_polygon is None: relative_intactness_name = f'intactness__{top_score}_quantiles__{base_dist_name}__{selected_model}'\n",
        "    else: relative_intactness_name = f'intactness__{mask_polygon[:-5]}_{top_score}_quantiles__{base_dist_name}__{selected_model}'\n",
        "    relative_intactness_path = join(intactness_baseline_dist_dir, f'{relative_intactness_name}.tif')\n",
        "    if not exists(relative_intactness_path):\n",
        "      # Always track originally nodata pixels from the original percentage raster\n",
        "      original_percentage = gdal.Open(percentage_path)\n",
        "      original_percentage_array = original_percentage.ReadAsArray()\n",
        "      original_percentage = None\n",
        "      originally_nodata_mask = original_percentage_array == nodatavalue\n",
        "\n",
        "      # Apply polygon masking to percentage array using pre-created mask\n",
        "      if mask_polygon is None:\n",
        "        percentage_array = original_percentage_array.copy()\n",
        "      else:\n",
        "        percentage_array = original_percentage_array.copy()\n",
        "        percentage_array[~polygon_masks[mask_polygon]] = nodatavalue\n",
        "\n",
        "      # Capture original data for histogram before conversions\n",
        "      original_valid_elements = percentage_array[percentage_array != nodatavalue]\n",
        "\n",
        "      relative_intactness_array = np.full_like(percentage_array, nodatavalue, dtype=np.int16)\n",
        "\n",
        "      # Set all values above 0 to 0\n",
        "      percentage_array[percentage_array > 0] = 0\n",
        "\n",
        "      # Separate valid and invalid (nodatavalue) elements\n",
        "      valid_elements = percentage_array[percentage_array != nodatavalue]\n",
        "\n",
        "      # Separate zero and non-zero values, excluding originally nodata pixels from quantile calculation\n",
        "      zero_elements = percentage_array == 0\n",
        "      # Exclude pixels that were originally nodata from quantile calculation\n",
        "      quantile_mask = (percentage_array != nodatavalue) & (~originally_nodata_mask) & (percentage_array != 0)\n",
        "      non_zero_valid_elements = percentage_array[quantile_mask]\n",
        "\n",
        "      # Calculate quantiles for non-zero valid elements only\n",
        "      quantiles = np.percentile(non_zero_valid_elements, np.linspace(0, 100, num_quantiles + 1)[1:-1]) if len(non_zero_valid_elements) > 0 else []\n",
        "\n",
        "      # Assign scores 1 to num_quantiles for non-zero values\n",
        "      for i in range(1, num_quantiles + 1):\n",
        "          lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "          upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "          relative_intactness_array[\n",
        "              (percentage_array > lower_bound) & (percentage_array <= upper_bound) &\n",
        "              (percentage_array != 0) & (percentage_array != nodatavalue)] = i\n",
        "\n",
        "      # Set all zero values to top score\n",
        "      relative_intactness_array[zero_elements] = top_score\n",
        "\n",
        "      # Set areas outside polygon to nodatavalue using pre-created mask\n",
        "      if mask_polygon is not None:\n",
        "        relative_intactness_array[~polygon_masks[mask_polygon]] = nodatavalue\n",
        "\n",
        "      # Convert non-forest areas inside polygon to 0\n",
        "      if convert_non_forest_nodatavalue_to_0:\n",
        "        if mask_polygon is None:\n",
        "          non_forest_inside_polygon = originally_nodata_mask\n",
        "        else:\n",
        "          non_forest_inside_polygon = originally_nodata_mask & polygon_masks[mask_polygon]\n",
        "        relative_intactness_array[non_forest_inside_polygon] = 0\n",
        "\n",
        "      export_array_as_tif(relative_intactness_array, relative_intactness_path)\n",
        "\n",
        "      # Prepare data for CSV: Collect lower and upper bounds for each category\n",
        "      ranges_data = {'Score': [], 'Lower_Bound': [], 'Upper_Bound': []}\n",
        "\n",
        "      # Add ranges for scores 1 to num_quantiles (non-zero values)\n",
        "      for i in range(1, num_quantiles + 1):\n",
        "          lower_bound = quantiles[i-2] if i > 1 and len(quantiles) >= i-1 else float('-inf')\n",
        "          if i == num_quantiles:\n",
        "            upper_bound = -0.000000001\n",
        "          else: upper_bound = quantiles[i-1] if len(quantiles) >= i else float('inf')\n",
        "          ranges_data['Score'].append(i)\n",
        "          ranges_data['Lower_Bound'].append(lower_bound)\n",
        "          ranges_data['Upper_Bound'].append(upper_bound)\n",
        "\n",
        "      # Add entry for top score (value of 0)\n",
        "      ranges_data['Score'].append(top_score)\n",
        "      ranges_data['Lower_Bound'].append(0)\n",
        "      ranges_data['Upper_Bound'].append(0)\n",
        "\n",
        "      # Create DataFrame and save to CSV\n",
        "      relative_intactness_df = pd.DataFrame(ranges_data)\n",
        "      relative_intactness_csv_path = os.path.join(intactness_baseline_dist_dir, f'{relative_intactness_name}.csv')\n",
        "      relative_intactness_df.to_csv(relative_intactness_csv_path, index=False)\n",
        "\n",
        "      # Generate and save histogram for converted data as .png\n",
        "      histogram_path = join(intactness_baseline_dist_dir, f'{relative_intactness_name}.png')\n",
        "      plt.figure()\n",
        "      counts, bins, patches = plt.hist(original_valid_elements.flatten(), bins='auto')\n",
        "\n",
        "      # Count how many values became 0 after conversions\n",
        "      zero_count_after_conversion = np.sum(valid_elements == 0)\n",
        "\n",
        "      # Find the zero bin and set its frequency to 0\n",
        "      zero_idx = next((i for i, (l, r) in enumerate(zip(bins[:-1], bins[1:])) if l <= 0 <= r), None)\n",
        "      if zero_idx is not None:\n",
        "          counts[zero_idx] = 0\n",
        "          plt.clf()\n",
        "          plt.bar(bins[:-1], counts, width=np.diff(bins), align='edge')\n",
        "          x_center = (bins.min() + bins.max()) / 2\n",
        "          y_max = max(counts)\n",
        "          plt.text(x_center, y_max * 0.9,\n",
        "                  f'0 value frequency = {zero_count_after_conversion:,}',\n",
        "                  ha='center', va='center', fontweight='bold',\n",
        "                  bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9))\n",
        "      plt.title(f'{relative_intactness_name} Histogram')\n",
        "      plt.xlabel('Value')\n",
        "      plt.ylabel('Frequency')\n",
        "      plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(histogram_path)\n",
        "      plt.close()\n",
        "\n",
        "    else: print(f\"{relative_intactness_name} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-7ctfgdoF2"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcuArY00N9Mc"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}