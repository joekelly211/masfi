{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joekelly211/masfi/blob/main/3_predictors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yk8CnJRCYVQ"
      },
      "source": [
        "# Imports, directories and global functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5USlWSaxqv9Y"
      },
      "outputs": [],
      "source": [
        "# Define base directory\n",
        "base_dir = \"/gdrive/Shareddrives/masfi\"\n",
        "\n",
        "# Mount Google Drive and set base directory\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "_path_to_add = os.path.realpath(base_dir)\n",
        "if _path_to_add not in sys.path:\n",
        "    sys.path.append(_path_to_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOfy6gWFwHEC"
      },
      "outputs": [],
      "source": [
        "# Installs\n",
        "%%capture\n",
        "!pip install astropy\n",
        "!pip install earthengine-api\n",
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTHaRSv8wICv"
      },
      "outputs": [],
      "source": [
        "!# Reload imports, replacing those in the cache\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# Imports\n",
        "from astropy.convolution import convolve, Gaussian2DKernel\n",
        "import csv\n",
        "import ee\n",
        "import geopandas as gpd\n",
        "import glob\n",
        "from google.colab import runtime, userdata\n",
        "import ipywidgets as widgets\n",
        "from math import sqrt, cos, radians\n",
        "import matplotlib.pyplot as plt\n",
        "from numba import jit\n",
        "import numpy as np\n",
        "from os import makedirs, remove\n",
        "from os.path import exists, join\n",
        "from osgeo import gdal, ogr, gdalconst\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from scipy import ndimage\n",
        "from scipy.ndimage import maximum_filter, minimum_filter, uniform_filter, distance_transform_edt\n",
        "from scipy.ndimage import label, sum as ndi_sum\n",
        "from shutil import copyfile\n",
        "from time import sleep\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMCq50kg36Br"
      },
      "outputs": [],
      "source": [
        "# 1_areas directories\n",
        "areas_dir = join(base_dir, \"1_areas\")\n",
        "polygons_dir = join(areas_dir, \"polygons\")\n",
        "masks_dir = join(areas_dir, \"masks\")\n",
        "template_dir = join(areas_dir, \"template.tif\")\n",
        "# 3_predictors directories\n",
        "predictors_dir = join(base_dir, \"3_predictors\")\n",
        "ee_dir = join(predictors_dir, \"earth_engine\")\n",
        "user_upload_dir = join(predictors_dir, \"user_upload\")\n",
        "glad_lcluc_dir = join(predictors_dir, 'glad_lcluc')\n",
        "resampled_dir = join(predictors_dir, \"resampled\")\n",
        "continuous_final_dir = join(predictors_dir, \"continuous_final\")\n",
        "binary_dir = join(predictors_dir, 'binary')\n",
        "edge_effects_dir = join(predictors_dir, 'binary_edge_effects')\n",
        "coast_dir = join(predictors_dir, 'coast')\n",
        "topography_temp_dir = join(predictors_dir, \"topo_temp\")\n",
        "topography_final_dir = join(predictors_dir, \"topo_final\")\n",
        "topography_corrected_temp_dir = join(predictors_dir, \"topo_corrected_temp\")\n",
        "topography_corrected_final_dir = join(predictors_dir, \"topo_corrected_final\")\n",
        "predictor_final_dir = join(predictors_dir, 'final')\n",
        "# 6_scenarios directories\n",
        "scenarios_dir = join(base_dir, \"6_scenarios\")\n",
        "# Create directories\n",
        "makedirs(ee_dir, exist_ok=True)\n",
        "makedirs(user_upload_dir, exist_ok=True)\n",
        "makedirs(glad_lcluc_dir, exist_ok=True)\n",
        "makedirs(resampled_dir, exist_ok=True)\n",
        "makedirs(continuous_final_dir, exist_ok=True)\n",
        "makedirs(binary_dir, exist_ok=True)\n",
        "makedirs(edge_effects_dir, exist_ok=True)\n",
        "makedirs(coast_dir, exist_ok=True)\n",
        "makedirs(topography_temp_dir, exist_ok=True)\n",
        "makedirs(topography_final_dir, exist_ok=True)\n",
        "makedirs(predictor_final_dir, exist_ok=True)\n",
        "makedirs(scenarios_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhkgXF4foXhx"
      },
      "outputs": [],
      "source": [
        "# export_array_as_tif function\n",
        "nodatavalue = -1111111\n",
        "compress = True\n",
        "def export_array_as_tif(input_array, output_tif, template=template_dir, nodatavalue=nodatavalue, compress=compress):\n",
        "  template = gdal.Open(template)\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  if compress: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "  if compress == False: driver = gdal.GetDriverByName(\"GTiff\").Create(output_tif, template_band.XSize, template_band.YSize, bands=1, eType=gdal.GDT_Float32)\n",
        "  driver.GetRasterBand(1).WriteArray(input_array)\n",
        "  driver.GetRasterBand(1).SetNoDataValue(nodatavalue)\n",
        "  driver.SetGeoTransform(template_dimensions)\n",
        "  driver.SetProjection(template_projection)\n",
        "\n",
        "# Burn a polygon to raster\n",
        "def burn_polygon_to_raster(raster_path, polygon_path, fixed=True, fixed_value=1, column_name=None, all_touched=True):\n",
        "    raster = gdal.Open(raster_path, gdal.GA_Update)\n",
        "    vector = ogr.Open(polygon_path)\n",
        "    layer = vector.GetLayer()\n",
        "    if all_touched: options = [\"ALL_TOUCHED=TRUE\"]\n",
        "    else: options = []\n",
        "    if not fixed: options.append(f\"ATTRIBUTE={column_name or layer.GetLayerDefn().GetFieldDefn(0).GetName()}\")\n",
        "    gdal.RasterizeLayer(raster, [1], layer,\n",
        "                        burn_values=[fixed_value] if fixed else None,\n",
        "                        options=options)\n",
        "    raster.FlushCache()\n",
        "    raster = vector = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSuevzw7xyil"
      },
      "source": [
        "# Download Earth Engine rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_wg4IqHpFXT"
      },
      "outputs": [],
      "source": [
        "# Enable Google Earth Engine API at Google Cloud https://console.cloud.google.com/apis/dashboard\n",
        "# See here for walkthrough: https://github.com/googlecolab/colabtools/issues/4228#issuecomment-1859068706\n",
        "# Set project ID under 'secrets' tab on the left with the name 'google_cloud_project'\n",
        "ee_project = userdata.get('google_cloud_project')\n",
        "\n",
        "# Authenticate Earth Engine\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project=ee_project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddVd9lPMbLOL"
      },
      "outputs": [],
      "source": [
        "# Edit this section to change which Earth Engine datasets are downloaded.\n",
        "\n",
        "# Warning: Earth Engine uses 'nearest neighbour' to resample rasters to the desired extent and resolution before exporting.\n",
        "# This creates artifacts if the data is continuous, such as DEMs (elevation) or other topographic metrics.\n",
        "# These should be downloaded from the original source, uploaded to '/user_upload' and resampled in the next section, checking the option for 'bilinear'.\n",
        "\n",
        "# Check datasets in https://code.earthengine.google.com/ with:\n",
        "# var assetList = ee.data.listAssets(\"projects/JRC/TMF/v1_2022/\");\n",
        "# print(assetList);\n",
        "\n",
        "ee_datasets = [\n",
        "\n",
        "    {\n",
        "        \"ee_dataset_name\": \"tmf\",\n",
        "        \"ee_dataset_type\": \"ImageCollection\",\n",
        "        \"ee_paths\": [\n",
        "            \"projects/JRC/TMF/v1_2023/AnnualChanges\",\n",
        "            \"projects/JRC/TMF/v1_2023/AnnualDisruptionObs2023\",\n",
        "            \"projects/JRC/TMF/v1_2023/TransitionMap_MainClasses\",\n",
        "            \"projects/JRC/TMF/v1_2023/TransitionMap_Subtypes\",\n",
        "            \"projects/JRC/TMF/v1_2023/Ndisturb_C2_1982_2022\",\n",
        "        ],\n",
        "    }\n",
        "    # {\n",
        "    #     \"ee_dataset_name\": \"glad\",\n",
        "    #     \"ee_dataset_type\": \"Image\",\n",
        "    #     \"ee_paths\": [\n",
        "    #                 # 'projects/glad/GLCLU2020/Forest_gain',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_disturbance',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_netgain',\n",
        "    #                 'projects/glad/GLCLU2020/Forest_height_netloss',\n",
        "    #                 # 'projects/glad/GLCLU2020/Forest_loss',\n",
        "    #                 # 'projects/glad/GLCLU2020/Forest_type',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC_2000',\n",
        "    #                 'projects/glad/GLCLU2020/LCLUC_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_2020',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_gain',\n",
        "    #                 'projects/glad/GLCLU2020/Vegetation_cover_loss',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2000',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2005',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2010',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2015',\n",
        "    #                 'projects/glad/GLCLU2020/Water_2020',\n",
        "    #                 # 'projects/glad/GLCLU2020/Water_dynamics',\n",
        "    #                 # 'projects/glad/GLCLU2020/Water_dynamics_classes',\n",
        "    #     ]\n",
        "    # }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIlnNYhQcb10"
      },
      "outputs": [],
      "source": [
        "# Verify Earth Engine rasters that will be downloaded\n",
        "ee_raster_list = []\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "    for ee_path in ee_paths:\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image = ee.ImageCollection(ee_path)\n",
        "            ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        else:\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "        for ee_band in ee_bands:\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_raster_list.append(ee_tif_filename)\n",
        "\n",
        "ee_raster_list = list(reversed(ee_raster_list))\n",
        "ee_raster_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qislgFIYUnqi"
      },
      "outputs": [],
      "source": [
        "# Check whether using mydrive as opposed to a shared gdrive\n",
        "mydrive_mounted = False\n",
        "\n",
        "# Earth Engine download progress\n",
        "ee_progress_index = 0\n",
        "ee_progress_label = widgets.Label(f\"Earth Engine download progress: {ee_progress_index}/{len(ee_raster_list)}\")\n",
        "display(ee_progress_label)\n",
        "\n",
        "# Load template and set Earth Engine geometry\n",
        "template_polygon_dir = join(polygons_dir, 'template.gpkg')\n",
        "template_area = gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0]\n",
        "template_coords = list(gpd.read_file(template_polygon_dir)[\"geometry\"].iloc[0].exterior.coords)\n",
        "ee_geometry = ee.Geometry.Polygon(template_coords)\n",
        "\n",
        "# Download Earth Engine datasets\n",
        "for ee_dataset in ee_datasets:\n",
        "    ee_dataset_name = ee_dataset['ee_dataset_name']\n",
        "    ee_dataset_type = ee_dataset['ee_dataset_type']\n",
        "    ee_paths = ee_dataset['ee_paths']\n",
        "    # Loop through Earth Engine paths\n",
        "    for ee_path in ee_paths:\n",
        "        # identify bands\n",
        "        if ee_dataset_type == 'ImageCollection':\n",
        "            ee_image = ee.ImageCollection(ee_path)\n",
        "            ee_bands = [b['id'] for b in ee_image.getInfo()['features'][0]['bands']]\n",
        "        elif ee_dataset_type == 'Image':\n",
        "            ee_image = ee.Image(ee_path)\n",
        "            ee_bands = ee_image.bandNames().getInfo()\n",
        "        # Loop through bands\n",
        "        for ee_band in reversed(ee_bands):\n",
        "            # Set filename and directory of downloaded raster and check if exists\n",
        "            ee_tif_filename = f\"{ee_dataset_name}_{ee_path.split('/')[-1]}_{ee_band}.tif\"\n",
        "            ee_tif_dir = join(ee_dir, ee_tif_filename)\n",
        "            ee_temp_dir = join(\"/gdrive/MyDrive\", ee_tif_filename)\n",
        "            if mydrive_mounted: ee_temp_dir = join(\"/content/drive/MyDrive/\", ee_tif_filename)\n",
        "            # Check if temporary raster exists and needs copying\n",
        "            if exists(ee_temp_dir):\n",
        "              copyfile(ee_temp_dir, ee_tif_dir)\n",
        "              remove(ee_temp_dir)\n",
        "            # Check if copied raster exists, and if not download from Earth Engine.\n",
        "            if not exists(ee_tif_dir):\n",
        "              if ee_dataset_type == 'ImageCollection':\n",
        "                image_selected = ee_image.qualityMosaic(ee_band).select([ee_band])\n",
        "                resolution = ee_image.first().projection().nominalScale().getInfo()\n",
        "              if ee_dataset_type == 'Image':\n",
        "                image_selected = ee_image.select([ee_band])\n",
        "                resolution = ee_image.select(0).projection().nominalScale().getInfo()\n",
        "              ee_task = ee.batch.Export.image.toDrive(image=image_selected.toFloat(),\n",
        "                                                    description=ee_tif_filename[:-4],\n",
        "                                                    scale=resolution,\n",
        "                                                    region=ee_geometry,\n",
        "                                                    maxPixels=10000000000,\n",
        "                                                    fileNamePrefix=ee_tif_filename[:-4],\n",
        "                                                    crs='EPSG:4326',\n",
        "                                                    fileFormat='GeoTIFF')\n",
        "              ee_task.start()\n",
        "              # Check whether the raster has downloaded yet\n",
        "              while not exists(ee_temp_dir):\n",
        "                  ee_task_status = ee_task.status()\n",
        "                  # If the task is completed, continue\n",
        "                  if ee_task_status[\"state\"] == 'COMPLETED': break\n",
        "                  # If it has failed or been cancelled, show an error\n",
        "                  elif ee_task_status['state'] == 'FAILED' or ee_task_status['state'] == 'CANCELLED':\n",
        "                      print(f\"{ee_tif_filename}:{ee_task_status['error_message']}\")\n",
        "                      try: remove(ee_temp_dir)\n",
        "                      except: pass\n",
        "                      break\n",
        "                  sleep(1)\n",
        "              # Copy the raster to intended directory and remove the temporary raster\n",
        "              while not exists(ee_temp_dir):\n",
        "                sleep(1)\n",
        "              copyfile(ee_temp_dir, ee_tif_dir)\n",
        "              remove(ee_temp_dir)\n",
        "            # Update Earth Engine download progress\n",
        "            ee_progress_index += 1\n",
        "            ee_progress_label.value = f\"Earth Engine download progress: {ee_progress_index}/{len(ee_raster_list)}\"\n",
        "\n",
        "# Check Earth Engine tasks here: https://code.earthengine.google.com/tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIBbo-gsS3nD"
      },
      "source": [
        "# GLAD LCLUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W90YInS-S5I5"
      },
      "outputs": [],
      "source": [
        "# LCLUC contains several land cover and land use types, each with continuous metrics.\n",
        "# This splits them into categories for better modelling, based on the legend:\n",
        "# https://glad.umd.edu/sites/default/files/legend_0.xlsx\n",
        "# Should do before resampling.\n",
        "\n",
        "lcluc_dict = {\n",
        "    'terra_vegetation_cover_percent': (0, 24),\n",
        "    'terra_stable_tree_m': (25, 48),\n",
        "    'wetland_vegetation_cover_percent': (100, 124),\n",
        "    'wetland_stable_tree_m': (125, 148),\n",
        "    'open_surface_water_percent_of_year': (200, 207),\n",
        "    'snow_ice': (241, 241),\n",
        "    'cropland': (244, 244),\n",
        "    'built_up': (250, 250),\n",
        "    'ocean': (254, 254),\n",
        "}\n",
        "\n",
        "for lcluc_raster in os.listdir(ee_dir):\n",
        "  if 'LCLUC' in lcluc_raster:\n",
        "    lcluc_path = join(ee_dir, lcluc_raster)\n",
        "    luluc_array = gdal.Open(lcluc_path).ReadAsArray()\n",
        "    for key, (lower, upper) in lcluc_dict.items():\n",
        "        split_luluc_filename = f\"{lcluc_raster[:-4]}_{key}.tif\"\n",
        "        split_luluc_filename_binary = f\"{lcluc_raster[:-4]}_{key}_binary.tif\"\n",
        "        split_luluc_dir = join(glad_lcluc_dir, split_luluc_filename)\n",
        "        split_luluc_dir_binary = join(glad_lcluc_dir, split_luluc_filename_binary)\n",
        "        if not exists(split_luluc_dir) and not exists(split_luluc_dir_binary):\n",
        "          split_luluc_mask = np.logical_and(luluc_array >= lower, luluc_array <= upper)\n",
        "          split_luluc_array = np.where(split_luluc_mask, luluc_array, 0) # outside the range set to 0\n",
        "          non_zero_percentage = np.count_nonzero(split_luluc_array) / split_luluc_array.size * 100\n",
        "          if non_zero_percentage >= 0.1:\n",
        "            # Check if there's only one unique non-zero value, and convert to a 1-0 binary raster if true\n",
        "            unique_non_zero_values = np.unique(split_luluc_array[split_luluc_array > 0])\n",
        "            if len(unique_non_zero_values) == 1:\n",
        "                split_luluc_array = np.where(split_luluc_array > 0, 1, 0)\n",
        "                split_luluc_dir = split_luluc_dir_binary\n",
        "            export_array_as_tif(split_luluc_array, split_luluc_dir, template=lcluc_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-UOVsvRuHNS"
      },
      "source": [
        "# Resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5yEcU40E8eD"
      },
      "outputs": [],
      "source": [
        "# Create dictionary of all tifs in Earth Engine and user upload directory\n",
        "resample_dict = {}\n",
        "for resample_raster in os.listdir(ee_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'categorical'\"})\n",
        "for resample_raster in os.listdir(user_upload_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'categorical'\"})\n",
        "for resample_raster in os.listdir(glad_lcluc_dir):\n",
        "    resample_dict.update({f'{resample_raster}':\"'continuous'\"})\n",
        "resample_dict = {key: value for key, value in sorted(resample_dict.items())}\n",
        "\n",
        "# Select rasters for resampling and verify data type (categorical or continuous)\n",
        "print(\"selected_original_rasters = {\")\n",
        "for key, value in resample_dict.items():\n",
        "    print(f'\"{key}\": {value},')\n",
        "print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfxPFqlxE0Ok"
      },
      "outputs": [],
      "source": [
        "selected_original_rasters = {\n",
        "\"tmf_AnnualChanges_Dec1990.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1991.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1992.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1993.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1994.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1995.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1996.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1997.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1998.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec1999.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2000.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2001.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2002.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2003.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2004.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2005.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2006.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2007.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2008.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2009.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2010.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2011.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2012.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2013.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2014.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2015.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2016.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2017.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2018.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2019.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2020.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2021.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2022.tif\": 'categorical',\n",
        "\"tmf_AnnualChanges_Dec2023.tif\": 'categorical',\n",
        "\"tmf_AnnualDisruptionObs2023_y2023.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1982.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1983.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1984.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1985.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1986.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1987.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1988.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1989.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1990.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1991.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1992.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1993.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1994.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1995.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1996.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1997.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1998.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y1999.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2000.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2001.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2002.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2003.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2004.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2005.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2006.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2007.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2008.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2009.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2010.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2011.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2012.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2013.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2014.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2015.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2016.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2017.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2018.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2019.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2020.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2021.tif\": 'categorical',\n",
        "\"tmf_Ndisturb_C2_1982_2022_y2022.tif\": 'categorical',\n",
        "\"tmf_TransitionMap_MainClasses_TransitionMap_MainClasses.tif\": 'categorical',\n",
        "\"tmf_TransitionMap_Subtypes_TransitionMap_Subtypes.tif\": 'categorical',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEZwd6XYXqRk"
      },
      "outputs": [],
      "source": [
        "# Set resample algorithms for different raster types\n",
        "# See https://gdal.org/programs/gdalwarp.html\n",
        "categorical_alg = 'near'\n",
        "continuous_alg = 'bilinear'\n",
        "\n",
        "template = gdal.Open(template_dir)\n",
        "template_dimensions = template.GetGeoTransform()\n",
        "xres, yres = template_dimensions[1], -template_dimensions[5]\n",
        "xmin = template_dimensions[0]\n",
        "ymin = template_dimensions[3] - template.RasterYSize * yres\n",
        "xmax = xmin + template.RasterXSize * xres\n",
        "ymax = template_dimensions[3]\n",
        "\n",
        "# Resample progress\n",
        "resample_progress_index = 0\n",
        "resample_progress_label = widgets.Label(f\"Resample progress: {resample_progress_index}/{len(selected_original_rasters.items())}\")\n",
        "display(resample_progress_label)\n",
        "\n",
        "# Iterate over selected rasters\n",
        "for original_raster_name, data_type in selected_original_rasters.items():\n",
        "  resampled_raster_dir = join(resampled_dir, original_raster_name)\n",
        "  if not exists(resampled_raster_dir):\n",
        "    original_raster_dir = join(ee_dir, original_raster_name)\n",
        "    if not exists(original_raster_dir): original_raster_dir = join(user_upload_dir, original_raster_name)\n",
        "    if not exists(original_raster_dir): original_raster_dir = join(glad_lcluc_dir, original_raster_name)\n",
        "    # Set resample type\n",
        "    if data_type == 'categorical': resample_alg = categorical_alg\n",
        "    if data_type == 'continuous': resample_alg = continuous_alg\n",
        "    src = gdal.Warp(\n",
        "        resampled_raster_dir,\n",
        "        original_raster_dir,\n",
        "        xRes=xres, yRes=yres,\n",
        "        outputBounds=(xmin, ymin, xmax, ymax),\n",
        "        resampleAlg=resample_alg,\n",
        "        outputType=gdalconst.GDT_Float32)\n",
        "    # Compress and close\n",
        "    driver = gdal.GetDriverByName(\"GTiff\")\n",
        "    src = driver.CreateCopy(resampled_raster_dir, src, 0, options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "    src = None\n",
        "  # Update resample progress\n",
        "  resample_progress_index += 1\n",
        "  resample_progress_label.value = f\"Resample progress: {resample_progress_index}/{len(selected_original_rasters.items())}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D35niESIwR9"
      },
      "outputs": [],
      "source": [
        "# Determine continous predictor precision\n",
        "\n",
        "override_max_unique_values = False\n",
        "max_unique_values = 5000 # Should be >=10\n",
        "\n",
        "if override_max_unique_values == False:\n",
        "  dem_base_path = join(areas_dir, \"base_dem.tif\")\n",
        "  dem_base_array = gdal.Open(dem_base_path).ReadAsArray()\n",
        "  max_unique_values = int(np.ptp(dem_base_array)) # Precision based on elevation variance\n",
        "resampled_precision_dict = {}\n",
        "\n",
        "for resampled_predictor, resample_type in selected_original_rasters.items():\n",
        "  if resample_type == 'continuous':\n",
        "    resampled_predictor_path = join(resampled_dir, resampled_predictor)\n",
        "    print(f\"Reading {resampled_predictor}...\")\n",
        "    # Read raster as array\n",
        "    resampled_predictor_array = gdal.Open(resampled_predictor_path).ReadAsArray()\n",
        "    # Convert 'nodata' values to nan\n",
        "    resampled_predictor_array[resampled_predictor_array == nodatavalue] = np.nan\n",
        "    resampled_predictor_array_masked = np.ma.array(resampled_predictor_array, mask=np.isnan(resampled_predictor_array))\n",
        "    # Count unique values in raster\n",
        "    unique_values = len(np.unique(resampled_predictor_array_masked))\n",
        "    print(f\"There are {unique_values} unique values in {resampled_predictor}\")\n",
        "    # Generate histogram from 100,000 random points\n",
        "    random_selection = np.random.choice(resampled_predictor_array_masked.ravel(), size = 100_000, replace = False)\n",
        "    _ = plt.hist(random_selection, bins='auto')  # arguments are passed to np.histogram\n",
        "    plt.title(f\"{resampled_predictor}\")\n",
        "    plt.show()\n",
        "    # Remove 0 values for log10\n",
        "    resampled_predictor_array_masked[resampled_predictor_array_masked == 0] = np.nan\n",
        "    resampled_predictor_array_masked = np.ma.array(resampled_predictor_array, mask=np.isnan(resampled_predictor_array))\n",
        "    # Create log10 array for determining positions for rounding\n",
        "    array_log10 = np.log10(abs(resampled_predictor_array_masked))\n",
        "    place_value_decimal = int(abs(np.min(array_log10)))\n",
        "    place_value_integer = int(0 - np.max(array_log10))\n",
        "    # Iterate down precision levels to determine optimal number of unique values\n",
        "    min_starting_precision = len(str(max_unique_values))\n",
        "    for precision in reversed(range(place_value_integer, max(min_starting_precision, place_value_decimal +1))):\n",
        "      rounded_array = np.round(resampled_predictor_array, decimals=precision)\n",
        "      round_unique_values = len(np.unique(rounded_array))\n",
        "      optimal_precision = None\n",
        "      if round_unique_values <= max_unique_values:\n",
        "        optimal_precision = precision\n",
        "        print(f\"The optimal precison for {resampled_predictor} is {optimal_precision}, with {round_unique_values} unique values.\")\n",
        "        resampled_precision_dict.update({f'{resampled_predictor}':f'{optimal_precision}'})\n",
        "        break\n",
        "    if optimal_precision == None: print(\"There's a problem with setting precision.\")\n",
        "    print(\"___________________\\n\")\n",
        "\n",
        "print(\"Dictionary for optimal rounding values:\")\n",
        "resampled_precision_dict\n",
        "\n",
        "precision_dict_csv_path = join(resampled_dir, 'rounding_dictionary.csv')\n",
        "# Save rounding dictionary to CSV\n",
        "with open(precision_dict_csv_path, 'w', newline='') as precision_dict_csv:\n",
        "    writer = csv.writer(precision_dict_csv)\n",
        "    writer.writerow(resampled_precision_dict.keys())\n",
        "    writer.writerow(resampled_precision_dict.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UjcjQ52NoT7"
      },
      "outputs": [],
      "source": [
        "# Open rounding dictionary and verify\n",
        "with open(precision_dict_csv_path, 'r') as file:\n",
        "    keys, values = list(csv.reader(file))\n",
        "    topo_precision_dict = dict(zip(keys, values))\n",
        "\n",
        "# Verify precision and correct if necessary\n",
        "print(\"topo_precision_dict = {\")\n",
        "for key, value in topo_precision_dict.items():\n",
        "    print(f'\"{key}\": {value},')\n",
        "print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2kquWfYOeS-"
      },
      "outputs": [],
      "source": [
        "topo_precision_dict = {\n",
        "\"agbd_2022.tif\": 0,\n",
        "\"agbd_diff_disturbance_1990_2022.tif\": 0,\n",
        "\"agbd_diff_disturbance_before_1990.tif\": 0,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn-nfKEPOi9d"
      },
      "outputs": [],
      "source": [
        "# Set smoothing kernel\n",
        "kernel = Gaussian2DKernel(x_stddev=1, y_stddev=1)\n",
        "\n",
        "# Continuous progress\n",
        "continuous_progress_index = 0\n",
        "continuous_progress_label = widgets.Label(f\"Continuous progress: {continuous_progress_index}/{len(topo_precision_dict.items())}\")\n",
        "display(continuous_progress_label)\n",
        "\n",
        "# Iterate over selected continuous rasters\n",
        "for continuous, precision in topo_precision_dict.items():\n",
        "  cont_raster_resampled_path = join(resampled_dir, continuous)\n",
        "  cont_raster_resampled_array = gdal.Open(cont_raster_resampled_path).ReadAsArray()\n",
        "  # Convert nodata values to 0\n",
        "  cont_raster_resampled_array[cont_raster_resampled_array == nodatavalue] = 0\n",
        "  # Set path and check if exists\n",
        "  cont_raster_unsmoothed_filename = f\"{continuous[:-4]}_unsmooth.tif\"\n",
        "  cont_raster_unsmoothed_path = join(continuous_final_dir, cont_raster_unsmoothed_filename)\n",
        "  if not exists(cont_raster_unsmoothed_path):\n",
        "    # Round and export unsmoothed continuous raster\n",
        "    cont_raster_unsmoothed_rounded = np.round(cont_raster_resampled_array, decimals=int(precision))\n",
        "    export_array_as_tif(cont_raster_unsmoothed_rounded, cont_raster_unsmoothed_path)\n",
        "  # Smooth using 2D spatial convolution\n",
        "  cont_raster_smoothed_filename = f\"{continuous[:-4]}_smooth.tif\"\n",
        "  cont_raster_smoothed_path = join(continuous_final_dir, cont_raster_smoothed_filename)\n",
        "  if not exists(cont_raster_smoothed_path):\n",
        "    cont_raster_smoothed = convolve(cont_raster_resampled_array, kernel, boundary='extend')\n",
        "    # Round and export smoothed continuous raster\n",
        "    cont_raster_smoothed_rounded = np.round(cont_raster_smoothed, decimals=int(precision))\n",
        "    export_array_as_tif(cont_raster_smoothed_rounded, cont_raster_smoothed_path)\n",
        "  # Update continuous progress\n",
        "  continuous_progress_index += 1\n",
        "  continuous_progress_label.value = f\"Continuous progress: {continuous_progress_index}/{len(topo_precision_dict.items())}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhukvPP02gBm"
      },
      "source": [
        "# TMF binary predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdGYp7HAXkD4"
      },
      "outputs": [],
      "source": [
        "# Check TMF data users guide for classification. https://forobs.jrc.ec.europa.eu/static/tmf/TMF_DataUsersGuide.pdf\n",
        "\n",
        "cell_size_x = gdal.Open(join(areas_dir, 'cell_size_x.tif')).ReadAsArray()\n",
        "cell_size_y = gdal.Open(join(areas_dir, 'cell_size_y.tif')).ReadAsArray()\n",
        "cell_size_ha = np.mean(cell_size_x) * np.mean(cell_size_y) / 10_000\n",
        "sieve_size = int(np.ceil(0.5/cell_size_ha)) # Removes all forest patches smaller than 0.5 ha\n",
        "print(f\"Forest binary sieve size (>0.5 ha) is {sieve_size} pixels.\")\n",
        "\n",
        "# Generate list of valid TMF rasters to convert to binary\n",
        "binary_list = []\n",
        "for resampled_raster in os.listdir(resampled_dir):\n",
        "  # Verify these are in the filenames\n",
        "  if 'DisruptionObs' in resampled_raster or 'AnnualChanges' in resampled_raster or 'Ndisturb' in resampled_raster:\n",
        "    # Verify this is the position of the year in the filename\n",
        "    year = resampled_raster[-8:-4]  # Data prior to 1990 is poor\n",
        "    if int(year) >= 1990: binary_list.append(resampled_raster)\n",
        "\n",
        "# Binary progress\n",
        "binary_progress_index = 0\n",
        "binary_progress_label = widgets.Label(f\"Binary progress: {binary_progress_index}/{len(binary_list)}\")\n",
        "display(binary_progress_label)\n",
        "\n",
        "for resampled_raster in binary_list:\n",
        "  year = resampled_raster[-8:-4]\n",
        "  # Forest binary\n",
        "  if 'AnnualChanges' in resampled_raster:\n",
        "    forest_binary_path = join(binary_dir, f\"forest_binary_{year}.tif\")\n",
        "    if not exists(forest_binary_path):\n",
        "      ac_raster_path = join(resampled_dir, resampled_raster)\n",
        "      ac_array = gdal.Open(ac_raster_path).ReadAsArray()\n",
        "      # Set classes 1 & 2 as 1, all else as 0\n",
        "      forest_binary_array = np.where((ac_array == 1) | (ac_array == 2), 1, 0)\n",
        "\n",
        "      # Sieve to 0.5 ha, using 8-connectedness (3, 3)\n",
        "      fb_array_labelled, fb_array_features = label(forest_binary_array, structure=np.ones((3, 3)))\n",
        "      # Determine the size of each patch\n",
        "      fb_array_sizes = ndi_sum(forest_binary_array, fb_array_labelled, range(fb_array_features + 1))\n",
        "      # Create a mask to remove patches smaller than the threshold\n",
        "      fb_array_mask_sizes = fb_array_sizes >= sieve_size\n",
        "      fb_array_mask_sizes[0] = 0 # Ensure non-forest (0) is excluded\n",
        "      fb_array_mask = fb_array_mask_sizes[fb_array_labelled]\n",
        "      # Apply the mask to the forest binary array and export\n",
        "      fb_array_sieved = forest_binary_array * fb_array_mask\n",
        "      export_array_as_tif(fb_array_sieved, forest_binary_path)\n",
        "\n",
        "  # Disturbance binary\n",
        "  if 'DisruptionObs' in resampled_raster or 'Ndisturb' in resampled_raster:\n",
        "    disturbance_binary_path = join(binary_dir, f\"disturbance_binary_{year}.tif\")\n",
        "    if not exists(disturbance_binary_path):\n",
        "      ac_raster_path = glob.glob(f\"{resampled_dir}/*AnnualChanges*{year}*\")\n",
        "      ac_array = gdal.Open(ac_raster_path[0]).ReadAsArray()\n",
        "      do_raster_path = join(resampled_dir, resampled_raster)\n",
        "      do_array = gdal.Open(do_raster_path).ReadAsArray()\n",
        "      # Set all disruption events to '1' if they're not classed as undisturbed forest or water in AnnualChanges\n",
        "      disturbance_binary_array = np.where((do_array >= 1) & ((ac_array != 1) & (ac_array != 5)), 1, 0)\n",
        "      export_array_as_tif(disturbance_binary_array, disturbance_binary_path)\n",
        "\n",
        "  # Update binary progress\n",
        "  binary_progress_index += 1\n",
        "  binary_progress_label.value = f\"Binary progress: {binary_progress_index}/{len(binary_list)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJnV0l75FpPN"
      },
      "source": [
        "# PA binary predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vigY_9XqXhCQ"
      },
      "outputs": [],
      "source": [
        "# Selected the 'protected area' polygon.\n",
        "# This can be multiple combined PAs / polygons that have no or minimal history of human disturbance\n",
        "\n",
        "polygons_to_exclude = ['template.gpkg', 'project_area.gpkg', 'project_area_buffered_bbox.gpkg', 'gedi_area.gpkg']\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    print(f\"protected_area_polygon = '{polygon}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th4OTc7P4Xmz"
      },
      "outputs": [],
      "source": [
        "protected_area_polygon = 'pa_ais.gpkg'\n",
        "\n",
        "# Set paths\n",
        "pa_polygon_path = join(polygons_dir, protected_area_polygon)\n",
        "pa_binary_path = join(binary_dir, f\"{protected_area_polygon[:-5]}_binary.tif\")\n",
        "# Convert all template values to 'nodata'\n",
        "template_tif = gdal.Open(template_dir)\n",
        "template_mask_array = gdal.Open(template_dir).ReadAsArray()\n",
        "template_mask_array[template_mask_array != None] = 0\n",
        "export_array_as_tif(template_mask_array, pa_binary_path)\n",
        "# Burn the value '1' where it overlaps with the project area polygon\n",
        "burn_polygon_to_raster(pa_binary_path, pa_polygon_path, fixed=True, fixed_value=1, all_touched=False)\n",
        "print(f\"PA binary raster has been created: {pa_binary_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCv0mNkb2pIY"
      },
      "source": [
        "# Binary predictor edge effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tqkninr4qy-g"
      },
      "outputs": [],
      "source": [
        "# Set smoothing kernel\n",
        "kernel = Gaussian2DKernel(x_stddev=3, y_stddev=3)\n",
        "# Set precision\n",
        "precision = 2\n",
        "\n",
        "binary_list = []\n",
        "for binary_raster in os.listdir(binary_dir) + os.listdir(resampled_dir):\n",
        "  if \"binary\" in binary_raster:\n",
        "    binary_list.append(binary_raster)\n",
        "\n",
        "# Edge effect progress\n",
        "edge_effect_progress_index = 0\n",
        "edge_effect_progress_label = widgets.Label(f\"Edge effect progress: {edge_effect_progress_index}/{len(binary_list)}\")\n",
        "display(edge_effect_progress_label)\n",
        "\n",
        "for binary_raster in binary_list:\n",
        "  if \"binary\" in binary_raster:\n",
        "    edge_effects_filename = binary_raster.replace('binary', 'with_edge_effects')\n",
        "    edge_effects_path = join(edge_effects_dir, edge_effects_filename)\n",
        "    if not exists(edge_effects_path):\n",
        "      binary_raster_path = join(binary_dir, binary_raster)\n",
        "      if not exists(binary_raster_path): binary_raster_path = join(resampled_dir, binary_raster)\n",
        "      binary_array = gdal.Open(binary_raster_path).ReadAsArray()\n",
        "      # Reclassify for binary differentiation after proximity conversion\n",
        "      differentiator_array = binary_array.copy()\n",
        "      differentiator_array[differentiator_array == 1] = 10\n",
        "      # Positive proximity\n",
        "      positive_distances = distance_transform_edt(binary_array == 0) # target pixels\n",
        "      positive_proximity_array = np.where(positive_distances > 2, 0, positive_distances) # max distance 2\n",
        "      # Negative proximity\n",
        "      negative_distances = distance_transform_edt(binary_array == 1) # target pixels\n",
        "      negative_proximity_array = np.where(negative_distances > 2, 0, negative_distances) # max distance 2\n",
        "      # Sum proximities and differentiator\n",
        "      pixel_prox_summed =  differentiator_array + positive_proximity_array + negative_proximity_array\n",
        "      # Reclassify for better semantic understanding of pixel proximity\n",
        "      pixel_prox_reclassed = pixel_prox_summed.copy()\n",
        "      pixel_prox_reclass_table = [(0, 0, -4), (1, 1, -1), (1.4, 1.5, -2), (2, 2, -3), (10, 10, 3), (11, 11, 0), (11.4, 11.5, 1), (12, 12, 2)]\n",
        "      for min_value, max_value, new_value in pixel_prox_reclass_table:\n",
        "        pixel_prox_reclassed[(pixel_prox_reclassed >= min_value) & (pixel_prox_reclassed <= max_value)] = new_value\n",
        "      # Smooth binary array using 2D convolution\n",
        "      binary_smoothed = convolve(binary_array, kernel, boundary='extend')\n",
        "      # Sum pixel proximity and smoothed binary array\n",
        "      edge_effects_array = np.round(pixel_prox_reclassed + binary_smoothed, precision)\n",
        "      # Export edge effects predictors\n",
        "      export_array_as_tif(edge_effects_array, edge_effects_path)\n",
        "\n",
        "  # Update binary progress\n",
        "  edge_effect_progress_index += 1\n",
        "  edge_effect_progress_label.value = f\"Edge effect progress: {edge_effect_progress_index}/{len(binary_list)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mow4bIz0EBw"
      },
      "source": [
        "# Distance from coast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7PZCLj4HCqf"
      },
      "outputs": [],
      "source": [
        "# Download global coast data from https://osmdata.openstreetmap.de/data/coastlines.html\n",
        "coastlines_url = 'https://osmdata.openstreetmap.de/download/coastlines-split-4326.zip'\n",
        "coastlines_global_file_path = join(coast_dir, 'coastlines-split-4326.zip')\n",
        "if not exists(coastlines_global_file_path):\n",
        "  request = requests.get(coastlines_url, allow_redirects=True)\n",
        "  open(coastlines_global_file_path, 'wb').write(request.content)\n",
        "\n",
        "coastlines_global_dir = join(coast_dir, 'coastlines-split-4326')\n",
        "if not exists(coastlines_global_dir):\n",
        "  with zipfile.ZipFile(coastlines_global_file_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(coast_dir)\n",
        "\n",
        "# Upload and select a polygon with full coastline extent.\n",
        "# It must be the template.gpkg polygon OR a polygon that entirely contains template.gpkg.\n",
        "polygons_to_exclude = ['project_area.gpkg', 'project_area_buffered_bbox.gpkg', 'gedi_area.gpkg']\n",
        "for polygon in os.listdir(polygons_dir):\n",
        "  if polygon not in polygons_to_exclude:\n",
        "    print(f\"coastline_extent_polygon = '{polygon}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8IUkGsiFfYs"
      },
      "outputs": [],
      "source": [
        "coastline_extent_polygon = 'peninsular_malaysia.gpkg'\n",
        "\n",
        "# Get extent of polygon\n",
        "coastline_extent_polygon_path = join(polygons_dir, coastline_extent_polygon)\n",
        "coastline_extent_bounds = gpd.read_file(coastline_extent_polygon_path).total_bounds\n",
        "coastline_min_x, coastline_max_x = coastline_extent_bounds[0], coastline_extent_bounds[2]\n",
        "coastline_min_y, coastline_max_y = coastline_extent_bounds[1], coastline_extent_bounds[3]\n",
        "\n",
        "# Set precision (in km) of distance\n",
        "precision = 1\n",
        "\n",
        "# Load template\n",
        "template = gdal.Open(template_dir)\n",
        "\n",
        "# Path of the new coast raster (where coastline will be rasterized)\n",
        "rasterized_coast_path = join(coast_dir, 'rasterized_coast.tif')\n",
        "\n",
        "# Create a new empty raster based on the coastline extent polygon\n",
        "if coastline_extent_polygon != 'template.gpkg':\n",
        "  # Get dimensions of template\n",
        "  template_band = template.GetRasterBand(1)\n",
        "  template_dimensions, template_projection = template.GetGeoTransform(), template.GetProjection()\n",
        "  template_size_x, template_size_y = template_band.XSize, template_band.YSize\n",
        "  template_res_x, template_res_y = template_dimensions[1], -template_dimensions[5]\n",
        "\n",
        "  # Calculate the minimum x and y of the template\n",
        "  template_min_x = template_dimensions[0]\n",
        "  template_max_y = template_dimensions[3]\n",
        "\n",
        "  # Calculate the centre x and y of the template\n",
        "  template_centre_x = template_min_x + ((template_size_x / 2) * template_res_x)\n",
        "  template_centre_y = template_max_y - ((template_size_y / 2) * template_res_y)\n",
        "\n",
        "  # Calculate the size (in pixels) difference between the polygon minimum and template minimum\n",
        "  coastline_min_diff_x = template_min_x - coastline_min_x\n",
        "  coastline_max_diff_y = coastline_max_y - template_max_y\n",
        "  coastline_min_diff_x_size = int(np.ceil(coastline_min_diff_x / template_res_x))\n",
        "  coastline_max_diff_y_size = int(np.ceil(coastline_max_diff_y / template_res_y))\n",
        "\n",
        "  # Calculate when the coastline raster should start while maintaining template resolution and position\n",
        "  coastline_start_x = template_min_x - (coastline_min_diff_x_size * template_res_x)\n",
        "  coastline_start_y = template_max_y + (coastline_max_diff_y_size * template_res_y)\n",
        "\n",
        "  # Calculate the size of the coastline raster\n",
        "  coastline_size_x = int(np.ceil((coastline_max_x - coastline_start_x)/template_res_x))\n",
        "  coastline_size_y = int(np.ceil((coastline_start_y - coastline_min_y)/template_res_y))\n",
        "\n",
        "  if not exists(rasterized_coast_path):\n",
        "    # Create coast raster dataset\n",
        "    driver = gdal.GetDriverByName(\"GTiff\").Create(rasterized_coast_path, coastline_size_x, coastline_size_y, bands=1, eType=gdal.GDT_Float32,\n",
        "                                                    options=[\"COMPRESS=DEFLATE\",\"PREDICTOR=2\",\"ZLEVEL=9\"])\n",
        "    driver.SetProjection(template_projection)\n",
        "    driver.SetGeoTransform((coastline_start_x, template_res_x, 0, coastline_start_y, 0, -template_res_y))\n",
        "\n",
        "    #  Create and write array (all pixels with value 1)\n",
        "    raster_data = np.ones((coastline_size_y, coastline_size_x), dtype=np.float32)\n",
        "    driver.GetRasterBand(1).WriteArray(raster_data)\n",
        "\n",
        "    # Close coast raster dataset\n",
        "    driver.FlushCache()\n",
        "    driver = None\n",
        "    print(\"A blank raster at the extent of coastlines polygon has been generated, ready for rasterization.\")\n",
        "  else: print(\"A rasterization raster already exists.\")\n",
        "\n",
        "else: # If just using the template area, copy the template.\n",
        "  if not exists(rasterized_coast_path):\n",
        "    template_array = template.ReadAsArray()\n",
        "    export_array_as_tif(template_array, rasterized_coast_path)\n",
        "  else: print(\"A rasterization raster already exists.\")\n",
        "\n",
        "# Clip coastlines polygon to the extent (speeds up rasterization)\n",
        "coastlines_clipped_path = join(coast_dir, 'clipped_coastlines.gpkg')\n",
        "if not exists(coastlines_clipped_path):\n",
        "  coastlines_shp_path = join(coastlines_global_dir, 'lines.shp')\n",
        "  coastlines_shp_df = gpd.read_file(coastlines_shp_path)\n",
        "  coastlines_clipped_df = gpd.clip(coastlines_shp_df, coastline_extent_bounds)\n",
        "  coastlines_clipped_df.to_file(coastlines_clipped_path, driver='GPKG')\n",
        "  print(f\"Coastlines clipped to the polygon: {coastlines_clipped_path}\")\n",
        "else: print(f\"Coastlines have already been clipped to the polygon: {coastlines_clipped_path}\")\n",
        "\n",
        "# Rasterize coastlines (2), if not already\n",
        "rasterized_coast_array = gdal.Open(rasterized_coast_path).ReadAsArray()\n",
        "if not np.any(rasterized_coast_array == 2):\n",
        "  burn_polygon_to_raster(rasterized_coast_path, coastlines_clipped_path, fixed_value=2)\n",
        "  print(f\"Coastlines rasterized: {rasterized_coast_path}\")\n",
        "else: print(f\"Coastlines have already been rasterized: {rasterized_coast_path}\")\n",
        "\n",
        "# Calculate proximity in pixels\n",
        "cost_proximity_pixels_path = join(coast_dir, \"coast_proximity_pixels.tif\")\n",
        "if not exists(cost_proximity_pixels_path):\n",
        "  rasterized_coast_array = gdal.Open(rasterized_coast_path).ReadAsArray()\n",
        "  # Target '0' pixels, away from the coastal '1' pixels. May require high RAM.\n",
        "  coast_proximity_pixels = distance_transform_edt(rasterized_coast_array != 2)\n",
        "  # Clip the array to the template size\n",
        "  if coastline_extent_polygon != 'template.gpkg':\n",
        "    clip_start_x = coastline_min_diff_x_size-1\n",
        "    clip_start_y = coastline_max_diff_y_size-1\n",
        "    clip_size_x = template_size_x\n",
        "    clip_size_y = template_size_y\n",
        "    coast_proximity_pixels = coast_proximity_pixels[clip_start_y:clip_start_y + clip_size_y, clip_start_x:clip_start_x + clip_size_x]\n",
        "  export_array_as_tif(coast_proximity_pixels, cost_proximity_pixels_path)\n",
        "  print(f\"A proximity (pixel number) raster has been generated at: {cost_proximity_pixels_path}\")\n",
        "else: print(f\"A proximity (pixel number) raster already exists at: {cost_proximity_pixels_path}\")\n",
        "\n",
        "# Convert proximity to km (distance from coast)\n",
        "coast_distance_path = join(coast_dir, \"coast_proximity_km.tif\")\n",
        "if not exists(coast_distance_path):\n",
        "  coast_proximity_array = gdal.Open(cost_proximity_pixels_path).ReadAsArray()\n",
        "  cell_size_x = gdal.Open(join(areas_dir, 'cell_size_x.tif')).ReadAsArray()\n",
        "  cell_size_y = gdal.Open(join(areas_dir, 'cell_size_y.tif')).ReadAsArray()\n",
        "  cell_size_mean_km = ((np.mean(cell_size_x) + np.mean(cell_size_y)) / 2) / 1000\n",
        "  coast_proximity_km = coast_proximity_array * cell_size_mean_km\n",
        "  coast_proximity_round = np.round(coast_proximity_km, precision)\n",
        "  export_array_as_tif(coast_proximity_round, coast_distance_path)\n",
        "  print(f\"A distance from coast (km) raster has been generated at: {coast_distance_path}\")\n",
        "else: print(f\"A distance from coast (km) raster already exists at: {coast_distance_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUGmSRY1pVNG"
      },
      "source": [
        "# Topography"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9x_KcII90Cw"
      },
      "source": [
        "## Define base DEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DMoW0_un4dOA"
      },
      "outputs": [],
      "source": [
        "# If using a GEDI corrected DEM, copy the prediction from scenarios directory\n",
        "enable_gedi_corrected = False\n",
        "\n",
        "# Select which prediction path to use as the GEDI corrected DEM\n",
        "if enable_gedi_corrected:\n",
        "  gedi_corrected_dem_exists = False\n",
        "  for subdir, dirs, files in os.walk(scenarios_dir):\n",
        "    for raster in files:\n",
        "      if raster.endswith('.tif'):\n",
        "        if 'elevation_corrected' in raster:\n",
        "          print(f'gedi_corrected_dem_path = \"{subdir}/{raster}\"')\n",
        "          gedi_corrected_dem_exists = True\n",
        "  if not gedi_corrected_dem_exists: print(\"No GEDI corrected DEM found in scenarios folder. Defaulting to uncorrected DEM.\")\n",
        "else: print(f\"Using the uncorrected DEM in {areas_dir}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JuGr-V0ViwgE"
      },
      "outputs": [],
      "source": [
        "gedi_corrected_dem_path = \"/gdrive/Shareddrives/masec/6_scenarios/elevation_corrected_230822_125308/taman/scenario_predictions_unmasked/2014__taman_elevation_corrected_230822_125308_unmasked.tif\"\n",
        "\n",
        "dem_base_path = join(areas_dir, \"base_dem.tif\")\n",
        "dem_base_corrected_path = join(areas_dir, \"base_dem_corrected.tif\")\n",
        "# Copy GEDI corrected DEM to areas directory\n",
        "if enable_gedi_corrected and gedi_corrected_dem_exists:\n",
        "  if not exists(dem_base_corrected_path):\n",
        "    # Sea level is post-processed back to 0 m, and areas that might have been predicted\n",
        "    # Below this. At ~sea level, the original DEM was likely the true terrain height.\n",
        "\n",
        "    # Define a low transition zone between original and corrected DEM.\n",
        "    # This delineates a transition from 0 to 100 % corrected DEM values.\n",
        "    low_transition_lower_limit = 0\n",
        "    low_transition_upper_limit = 5\n",
        "\n",
        "    # Higher elevations are poorly predicted in some study areas due to low sample size.\n",
        "    # Therefore original values should be used, as regardless there's minimal difference\n",
        "    # between surface height (vegetation) and terrain height.\n",
        "\n",
        "    # Define a montane transition zone between original and corrected DEM.\n",
        "    # This delineates a transition from 0 to 100 % original DEM values.\n",
        "    montane_transition_lower_limit = 1500\n",
        "    montane_transition_upper_limit = 1800\n",
        "\n",
        "    # Read original base DEM\n",
        "    dem_base_array = gdal.Open(dem_base_path).ReadAsArray()\n",
        "\n",
        "    # Scale <=0 to >=5 m values to a ratio (0 - 1) of corrected DEM values\n",
        "    dem_base_array_low_ratio = dem_base_array.copy()\n",
        "    dem_base_array_low_ratio[dem_base_array_low_ratio <= low_transition_lower_limit] = low_transition_lower_limit\n",
        "    dem_base_array_low_ratio[dem_base_array_low_ratio >= low_transition_upper_limit] = low_transition_upper_limit\n",
        "    dem_base_array_low_ratio = dem_base_array_low_ratio / low_transition_upper_limit\n",
        "\n",
        "    # Scale <=0 to >=5 m values to a ratio (0 - 1) of corrected DEM values\n",
        "    dem_base_array_montane_ratio = dem_base_array.copy()\n",
        "    dem_base_array_montane_ratio[dem_base_array_montane_ratio <= montane_transition_lower_limit] = montane_transition_lower_limit\n",
        "    dem_base_array_montane_ratio[dem_base_array_montane_ratio >= montane_transition_upper_limit] = montane_transition_upper_limit\n",
        "    dem_base_array_montane_ratio = (montane_transition_upper_limit - dem_base_array_montane_ratio) / (montane_transition_upper_limit - montane_transition_lower_limit)\n",
        "\n",
        "    # Use original DEM values for surface water.\n",
        "    # The Copernicus DEM rounds all surface water values to 1 or 0 decimal places.\n",
        "    # This is used to differentiate them from land values, creating a 'land binary'.\n",
        "    dem_base_array_land_binary = dem_base_array.copy()\n",
        "    dem_base_array_land_binary = np.floor(dem_base_array_land_binary * 10) / 10 # Round DOWN 1 decimal place\n",
        "    dem_base_array_land_binary = dem_base_array - dem_base_array_land_binary\n",
        "    dem_base_array_land_binary[dem_base_array_land_binary > 0] = 1\n",
        "    # Invert the binary array to target 0 values for sieving single water pixels (usually erroneous)\n",
        "    dem_base_array_land_binary_inverted = np.logical_not(dem_base_array_land_binary)\n",
        "    # Sieve to 0.5 ha, using 8-connectedness (3, 3)\n",
        "    lb_array_labelled, lb_array_features = label(dem_base_array_land_binary_inverted, structure=np.ones((3, 3)))\n",
        "    # Determine the size of each patch\n",
        "    lb_array_sizes = ndi_sum(dem_base_array_land_binary_inverted, lb_array_labelled, range(lb_array_features + 1))\n",
        "    # Create a mask to remove patches smaller than the threshold\n",
        "    lb_array_mask_sizes = lb_array_sizes >= 2\n",
        "    lb_array_mask_sizes[0] = 0 # Ensure non-target values are excluded\n",
        "    lb_array_mask = lb_array_mask_sizes[lb_array_labelled]\n",
        "    # Apply the mask to the inverted binary array\n",
        "    lb_array_sieved_inverted = dem_base_array_land_binary_inverted * lb_array_mask\n",
        "    # Invert the array back to original representation\n",
        "    dem_base_array_land_binary = np.logical_not(lb_array_sieved_inverted)\n",
        "\n",
        "    # Read the corrected base DEM and create the final modifier\n",
        "    gedi_corrected_dem_array = gdal.Open(gedi_corrected_dem_path).ReadAsArray()\n",
        "    dem_base_corrected_array_modifier = gedi_corrected_dem_array.copy()\n",
        "    # Change all corrected DEM values < sea level to 0 (most are erroneous)\n",
        "    dem_base_corrected_array_modifier[dem_base_corrected_array_modifier < 0] = 0\n",
        "    # Sutract corrected DEM from original as the 'corrected' modifier\n",
        "    dem_base_corrected_array_modifier = dem_base_array - dem_base_corrected_array_modifier\n",
        "    # Multiply the corrected modifier by low ratio, montane ratio and land binary\n",
        "    dem_base_corrected_array_modifier = dem_base_corrected_array_modifier * dem_base_array_low_ratio * dem_base_array_montane_ratio * dem_base_array_land_binary\n",
        "\n",
        "    # Apply the modifier\n",
        "    dem_base_corrected_array = dem_base_array - dem_base_corrected_array_modifier\n",
        "\n",
        "    # Export uncompressed for further topographic metrics\n",
        "    export_array_as_tif(dem_base_corrected_array, dem_base_corrected_path, compress=False)\n",
        "    print(f\"GEDI corrected DEM has been postprocessed and uncompressed to: {dem_base_corrected_path}\")\n",
        "\n",
        "  else: print(f\"A corrected base DEM already exists, first remove from {areas_dir} for replacement.\")\n",
        "\n",
        "else: print(f\"A GEDI corrected DEM does not exist in the scenarios directory. Proceeding with uncorrected base DEM.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "obnUPMpUit18"
      },
      "outputs": [],
      "source": [
        "# Define base DEM and properties\n",
        "if enable_gedi_corrected:\n",
        "  print(\"Post-processed GEDI corrected base DEM enabled.\")\n",
        "  if not exists(dem_base_corrected_path):\n",
        "    print(\"A post-processed GEDI corrected based DEM does not exist. Defaulting to the original base DEM.\")\n",
        "    dem_base = gdal.Open(dem_base_path)\n",
        "    topography_temp_dir = join(predictors_dir, 'topo_temp')\n",
        "    topography_final_dir = join(predictors_dir, 'topo_final')\n",
        "  else:\n",
        "    dem_base = gdal.Open(dem_base_corrected_path)\n",
        "    topography_temp_dir = join(predictors_dir, 'topo_corrected_temp')\n",
        "    topography_final_dir = join(predictors_dir, 'topo_corrected_final')\n",
        "    makedirs(topography_temp_dir, exist_ok=True)\n",
        "    makedirs(topography_final_dir, exist_ok=True)\n",
        "else:\n",
        "  print(\"Post-processed GEDI corrected based DEM disabled. Using the original base DEM.\")\n",
        "  dem_base = gdal.Open(dem_base_path)\n",
        "  topography_temp_dir = join(predictors_dir, 'topo_temp')\n",
        "  topography_final_dir = join(predictors_dir, 'topo_final')\n",
        "\n",
        "# Get base DEM attributes\n",
        "dem_base_array = dem_base.ReadAsArray()\n",
        "dem_dimensions = dem_base.GetGeoTransform()\n",
        "y_origin, pixel_height, raster_height = dem_dimensions[3], dem_dimensions[5], len(dem_base_array)\n",
        "dem_central_latitude = y_origin + (raster_height // 2) * pixel_height"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izy46g_asfrV"
      },
      "source": [
        "## Empty temporary dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjMke1nLtM7C"
      },
      "outputs": [],
      "source": [
        "# for raster in Path(topography_temp_dir).glob(\"**/*\"):\n",
        "#   if raster.is_file(): raster.unlink()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc0og_zgpQHG"
      },
      "source": [
        "## Whitebox algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_CDy-BuWQbSU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install whitebox\n",
        "import whitebox\n",
        "wbt = whitebox.WhiteboxTools()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6RY2j4dh5etQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# https://www.whiteboxgeo.com/manual/wbt_book/preface.html\n",
        "\n",
        "# Elevation\n",
        "elevation_path_temp = join(topography_temp_dir, \"elevation.tif\")\n",
        "if not exists(elevation_path_temp):\n",
        "  elevation = dem_base_array\n",
        "  export_array_as_tif(elevation, elevation_path_temp, compress=False)\n",
        "\n",
        "# Slope\n",
        "slope_path_temp = join(topography_temp_dir, \"slope.tif\")\n",
        "if not exists(slope_path_temp):\n",
        "  wbt.slope(elevation_path_temp, slope_path_temp, units = \"degrees\")\n",
        "\n",
        "# Aspect\n",
        "aspect_path_temp = join(topography_temp_dir, \"aspect.tif\")\n",
        "if not exists(aspect_path_temp):\n",
        "  wbt.aspect(elevation_path_temp, aspect_path_temp)\n",
        "\n",
        "# Profile Curvature\n",
        "profile_curvature_path_temp = join(topography_temp_dir, \"profile_curvature.tif\")\n",
        "if not exists(profile_curvature_path_temp):\n",
        "  wbt.profile_curvature(elevation_path_temp, profile_curvature_path_temp, log=False)\n",
        "\n",
        "# Tangential Curvature\n",
        "tangential_curvature_path_temp = join(topography_temp_dir, \"tangential_curvature.tif\")\n",
        "if not exists(tangential_curvature_path_temp):\n",
        "  wbt.tangential_curvature(elevation_path_temp, tangential_curvature_path_temp, log=False)\n",
        "\n",
        "# Topographic Ruggedness Index\n",
        "topographic_ruggedness_index_path_temp = join(topography_temp_dir, \"topographic_ruggedness_index.tif\")\n",
        "if not exists(topographic_ruggedness_index_path_temp):\n",
        "  wbt.ruggedness_index(elevation_path_temp, topographic_ruggedness_index_path_temp)\n",
        "\n",
        "# Deviation from Mean Elevation\n",
        "dev_kernel_sizes = [3, 7, 11]\n",
        "for kernel_size in dev_kernel_sizes:\n",
        "  deviation_mean_elevation_path_temp = join(topography_temp_dir, f\"deviation_mean_elevation_{str(kernel_size).rjust(2, '0')}.tif\")\n",
        "  if not exists(deviation_mean_elevation_path_temp):\n",
        "    wbt.dev_from_mean_elev(elevation_path_temp, deviation_mean_elevation_path_temp, filterx=kernel_size, filtery=kernel_size)\n",
        "\n",
        "# Circular Variance of Aspect\n",
        "cva_kernel_sizes = [3, 7, 11]\n",
        "for kernel_size in cva_kernel_sizes:\n",
        "  circular_variance_aspect_path_temp = join(topography_temp_dir, f\"circular_variance_aspect_{str(kernel_size).rjust(2, '0')}.tif\")\n",
        "  if not exists(circular_variance_aspect_path_temp):\n",
        "    wbt.circular_variance_of_aspect(elevation_path_temp, circular_variance_aspect_path_temp, filter=kernel_size)\n",
        "\n",
        "# Fill Single Cell Pits for Breach Depressions\n",
        "dem_fill_single_cell_pits_path_temp = join(topography_temp_dir, \"dem_fill_single_cell_pits.tif\")\n",
        "if not exists(dem_fill_single_cell_pits_path_temp):\n",
        "  wbt.fill_single_cell_pits(elevation_path_temp, dem_fill_single_cell_pits_path_temp)\n",
        "  # Raw output doesn't work, needs to be saved again.\n",
        "  dem_fill_single_cell_pits = gdal.Open(dem_fill_single_cell_pits_path_temp).ReadAsArray()\n",
        "  export_array_as_tif(dem_fill_single_cell_pits, dem_fill_single_cell_pits_path_temp, compress=False)\n",
        "\n",
        "# Breach Depressions for Specific Contributing Area\n",
        "max_search_dist = 2 # Maximum search distance for breach paths in cells (pixels)\n",
        "dem_breach_depressions_path_temp = join(topography_temp_dir, \"dem_breach_depressions.tif\")\n",
        "if not exists(dem_breach_depressions_path_temp):\n",
        "  wbt.breach_depressions_least_cost(dem_fill_single_cell_pits_path_temp, dem_breach_depressions_path_temp, dist=max_search_dist)\n",
        "\n",
        "# Specific Contributing Area (Qin) (for TWI and SPI)\n",
        "specific_contributing_area_qin_path_temp = join(topography_temp_dir, \"specific_contributing_area_qin.tif\")\n",
        "if not exists(specific_contributing_area_qin_path_temp):\n",
        "  wbt.qin_flow_accumulation(dem_breach_depressions_path_temp, specific_contributing_area_qin_path_temp, out_type=\"specific contributing area\")\n",
        "\n",
        "# Topographic Wetness Index (TWI)\n",
        "topographic_wetness_index_path_temp = join(topography_temp_dir, \"topographic_wetness_index.tif\")\n",
        "if not exists(topographic_wetness_index_path_temp):\n",
        "  wbt.wetness_index(specific_contributing_area_qin_path_temp, slope_path_temp, topographic_wetness_index_path_temp)\n",
        "\n",
        "# Stream Power Index (SPI)\n",
        "exponent = 1.0\n",
        "stream_power_index_path_temp = join(topography_temp_dir, \"stream_power_index.tif\")\n",
        "if not exists(stream_power_index_path_temp):\n",
        "  wbt.stream_power_index(specific_contributing_area_qin_path_temp, slope_path_temp, stream_power_index_path_temp, exponent=exponent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1L8Y-vV5vyy"
      },
      "source": [
        "## Further algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "deEmzSWmlH_o"
      },
      "outputs": [],
      "source": [
        "# Arrays to use\n",
        "elevation_path_temp = join(topography_temp_dir, \"elevation.tif\")\n",
        "slope_path_temp = join(topography_temp_dir, \"slope.tif\")\n",
        "aspect_path_temp = join(topography_temp_dir, \"aspect.tif\")\n",
        "stream_power_index_path_temp = join(topography_temp_dir, \"stream_power_index.tif\")\n",
        "\n",
        "# Calculate Aspect Sine\n",
        "aspect_sine_path_temp = join(topography_temp_dir,\"aspect_sine.tif\")\n",
        "if not exists(aspect_sine_path_temp):\n",
        "  aspect = gdal.Open(aspect_path_temp).ReadAsArray()\n",
        "  aspect_sine = np.sin(np.radians(aspect))\n",
        "  export_array_as_tif(aspect_sine, aspect_sine_path_temp, compress=False)\n",
        "\n",
        "# Calculate Aspect Cosine\n",
        "aspect_cosine_path_temp = join(topography_temp_dir,\"aspect_cosine.tif\")\n",
        "if not exists(aspect_cosine_path_temp):\n",
        "  aspect = gdal.Open(aspect_path_temp).ReadAsArray()\n",
        "  aspect_cosine = np.cos(np.radians(aspect))\n",
        "  export_array_as_tif(aspect_cosine, aspect_cosine_path_temp, compress=False)\n",
        "\n",
        "# Calculate Eastness\n",
        "eastness_path_temp = join(topography_temp_dir,\"eastness.tif\")\n",
        "if not exists(eastness_path_temp):\n",
        "  slope = gdal.Open(slope_path_temp).ReadAsArray()\n",
        "  aspect_sine = gdal.Open(aspect_sine_path_temp).ReadAsArray()\n",
        "  eastness = aspect_sine * np.sin(np.radians(slope))\n",
        "  export_array_as_tif(eastness, eastness_path_temp, compress=False)\n",
        "\n",
        "# Calculate Northness\n",
        "northness_temp = join(topography_temp_dir,\"northness.tif\")\n",
        "if not exists(northness_temp):\n",
        "  slope = gdal.Open(slope_path_temp).ReadAsArray()\n",
        "  aspect_cosine = gdal.Open(aspect_cosine_path_temp).ReadAsArray()\n",
        "  northness = aspect_cosine * np.sin(np.radians(slope))\n",
        "  export_array_as_tif(northness, northness_temp, compress=False)\n",
        "\n",
        "elevation = gdal.Open(elevation_path_temp).ReadAsArray()\n",
        "\n",
        "# Calculate Roughness\n",
        "roughness_kernel_sizes = [3, 7, 11]\n",
        "for kernel_size in roughness_kernel_sizes:\n",
        "  roughness_path_temp = join(topography_temp_dir,f\"roughness_{str(kernel_size).rjust(2, '0')}.tif\")\n",
        "  if not exists(roughness_path_temp):\n",
        "    roughness = maximum_filter(elevation, size=kernel_size) - minimum_filter(elevation, size=kernel_size)\n",
        "    export_array_as_tif(roughness, roughness_path_temp, compress=False)\n",
        "\n",
        "# Calculate Topographic Position Index (TPI)\n",
        "tpi_kernel_sizes = [3, 7, 11]\n",
        "for kernel_size in tpi_kernel_sizes:\n",
        "  tpi_path_temp = join(topography_temp_dir, f\"topographic_position_index_{str(kernel_size).rjust(2, '0')}.tif\")\n",
        "  if not exists(tpi_path_temp):\n",
        "    topographic_position_index = elevation - uniform_filter(elevation, size=kernel_size)\n",
        "    export_array_as_tif(topographic_position_index, tpi_path_temp, compress=False)\n",
        "\n",
        "# Calculate Stream Power Index (SPI) log10\n",
        "spi_log10_path_temp = join(topography_temp_dir,\"stream_power_index_log10.tif\")\n",
        "if not exists(spi_log10_path_temp):\n",
        "  stream_power_index = gdal.Open(stream_power_index_path_temp).ReadAsArray()\n",
        "  stream_power_index[stream_power_index == 0] = 1.0e-30 # Convert 0 values to one that's below any other non-zero value\n",
        "  stream_power_index_log10 = np.log10(stream_power_index)\n",
        "  export_array_as_tif(stream_power_index_log10, spi_log10_path_temp, compress=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXdHf1e5pM73"
      },
      "source": [
        "## Surface Area Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q-9sNBe1NcOd"
      },
      "outputs": [],
      "source": [
        "# The whitebox algorithm below currently not working correctly.\n",
        "# wbt.surface_area_ratio(elevation_path_temp, surface_area_ratio_path_temp)\n",
        "\n",
        "# Based on source code of whitebox SAR (algorithm doesn't output correctly):\n",
        "# https://github.com/jblindsay/whitebox-tools/blob/master/whitebox-tools-app/src/tools/terrain_analysis/surface_area_ratio.rs\n",
        "\n",
        "elevation_path_temp = join(topography_temp_dir, \"elevation.tif\")\n",
        "surface_area_ratio_path_temp = join(topography_temp_dir, \"surface_area_ratio.tif\")\n",
        "\n",
        "elevation_raster = gdal.Open(elevation_path_temp)\n",
        "transform = elevation_raster.GetGeoTransform()\n",
        "elevation_array = elevation_raster.ReadAsArray()\n",
        "\n",
        "@jit(nopython=True)\n",
        "def calculate_surface_area_ratio(dem, transform, nodata):\n",
        "    resx, resy = transform[1], -transform[5]\n",
        "    output = np.full(dem.shape, nodata, dtype=np.float32)\n",
        "\n",
        "    for i in range(1, dem.shape[0]-1):\n",
        "        mid_lat = transform[3] + i*transform[5]\n",
        "        resx_adjusted = abs(resx) * 111_111.0 * cos(radians(mid_lat))\n",
        "        resy_adjusted = abs(resy) * 111_111.0\n",
        "        res_diag = sqrt(resx_adjusted**2 + resy_adjusted**2)\n",
        "        cell_area = resx_adjusted * resy_adjusted\n",
        "        eigth_area = cell_area / 8.0\n",
        "        for j in range(1, dem.shape[1]-1):\n",
        "            if dem[i, j] == nodata:\n",
        "                continue\n",
        "\n",
        "            window = dem[i-1:i+2, j-1:j+2]\n",
        "\n",
        "            dx = np.array([-1, 0, 1, -1, 0, 1, -1, 0, 1])\n",
        "            dy = np.array([-1, -1, -1, 0, 0, 0, 1, 1, 1])\n",
        "            zvals = np.array([window[dy[k]+1, dx[k]+1] for k in range(9)])\n",
        "            dist_planar = np.array([resx_adjusted]*6 + [resy_adjusted]*6 + [res_diag]*4)\n",
        "            dist_pairs = [(0, 1), (1, 2), (3, 4), (4, 5), (6, 7), (7, 8), (0, 3), (1, 4), (2, 5), (3, 6), (4, 7), (5, 8), (0, 4), (2, 4), (6, 4), (8, 4)]\n",
        "            distances = np.array([sqrt(dist_planar[k]**2 + (zvals[i] - zvals[j])**2) / 2 for k, (i, j) in enumerate(dist_pairs) if zvals[i] != nodata and zvals[j] != nodata])\n",
        "\n",
        "            triangle_sides = [(0, 7, 12), (1, 7, 13), (2, 6, 12), (3, 8, 13), (2, 9, 14), (3, 11, 15), (4, 10, 14), (5, 10, 15)]\n",
        "            area = 0.0\n",
        "            cell_area2 = cell_area\n",
        "            for a, b, c in triangle_sides:\n",
        "                if a < len(distances) and b < len(distances) and c < len(distances):\n",
        "                    s = (distances[a] + distances[b] + distances[c]) / 2.0\n",
        "                    area += sqrt(s * (s - distances[a]) * (s - distances[b]) * (s - distances[c]))\n",
        "                else:\n",
        "                    cell_area2 -= eigth_area\n",
        "\n",
        "            if cell_area2 > 0.0:\n",
        "                output[i, j] = area / cell_area2\n",
        "\n",
        "        resx, resy = transform[1], -transform[5]\n",
        "\n",
        "    return output\n",
        "\n",
        "# Run the jitted function on the entire array\n",
        "surface_area_ratio_array = calculate_surface_area_ratio(elevation_array, transform, nodatavalue)\n",
        "\n",
        "export_array_as_tif(surface_area_ratio_array, surface_area_ratio_path_temp, template=elevation_path_temp, compress=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kJl14SJ7z_E"
      },
      "source": [
        "## Further hydrography (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CNrsnOutaCO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "minimum_length_of_stream_m = 100\n",
        "\n",
        "dem_breach_depressions_path_temp = join(topography_temp_dir, \"dem_breach_depressions.tif\")\n",
        "\n",
        "# # d8 Pointer (for basins)\n",
        "d8_pointer_path_temp = join(topography_temp_dir, \"d8_pointer.tif\")\n",
        "wbt.d8_pointer(dem_breach_depressions_path_temp, d8_pointer_path_temp)\n",
        "\n",
        "# # Basins\n",
        "basins_path_temp = join(topography_temp_dir, \"basins.tif\")\n",
        "wbt.basins(d8_pointer_path_temp, basins_path_temp)\n",
        "\n",
        "# # Specific Contributing Area (d8) (for streams)\n",
        "specific_contributing_area_d8_path_temp = join(topography_temp_dir, \"specific_contributing_area_d8.tif\")\n",
        "wbt.d8_flow_accumulation(dem_breach_depressions_path_temp, specific_contributing_area_d8_path_temp, out_type=\"specific contributing area\")\n",
        "\n",
        "# # Streams\n",
        "flow_accumulation_threshold = 0.01 # Lowest FA (usually 0 - 0.5) to include as stream\n",
        "streams_path_temp = join(topography_temp_dir, \"streams.tif\")\n",
        "wbt.extract_streams(specific_contributing_area_d8_path_temp, streams_path_temp, threshold=flow_accumulation_threshold)\n",
        "\n",
        "# Streams - short removed\n",
        "minimum_length_of_stream = minimum_length_of_stream_m / 111_111\n",
        "streams_short_removed_path_temp = join(topography_temp_dir, \"streams_short_removed.tif\")\n",
        "wbt.remove_short_streams(d8_pointer_path_temp, streams_path_temp, streams_short_removed_path_temp, min_length=minimum_length_of_stream)\n",
        "\n",
        "# Strahler stream order\n",
        "strahler_stream_order_path_temp = join(topography_temp_dir, \"strahler_stream_order.tif\")\n",
        "wbt.strahler_stream_order(d8_pointer_path_temp, streams_short_removed_path_temp, strahler_stream_order_path_temp)\n",
        "\n",
        "# River Centre Lines (not working)\n",
        "# minimum_river_length = 5\n",
        "# minimum_pixel_search_radius_connection = 2\n",
        "# river_centre_lines_path_temp = join(topography_temp_dir, \"river_centre_lines.tif\")\n",
        "# wbt.river_centerlines(streams_path_temp, river_centre_lines_path_temp,\n",
        "#                       min_length=minimum_river_length, radius=minimum_pixel_search_radius_connection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jp49GET7sd1j"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# May need to experiment with order depending on study area\n",
        "\n",
        "# Base paths\n",
        "d8_pointer_path_temp = join(topography_temp_dir, \"d8_pointer.tif\")\n",
        "\n",
        "# Strahler stream order limit (for sub-basins)\n",
        "strahler_order = 6\n",
        "strahler_stream_order_path_temp = join(topography_temp_dir, \"strahler_stream_order.tif\")\n",
        "strahler_stream_order_limit_path_temp = join(topography_temp_dir, \"strahler_stream_order_limit.tif\")\n",
        "strahler_stream_order = gdal.Open(strahler_stream_order_path_temp).ReadAsArray()\n",
        "strahler_stream_order_limit = strahler_stream_order\n",
        "mask_below = strahler_stream_order_limit < strahler_order\n",
        "strahler_stream_order_limit[strahler_stream_order_limit >= strahler_order] = 1\n",
        "strahler_stream_order_limit[mask_below] = np.nan\n",
        "export_array_as_tif(strahler_stream_order_limit, strahler_stream_order_limit_path_temp, compress=False)\n",
        "\n",
        "# Sub-basins\n",
        "subbasins_path_temp = join(topography_temp_dir, \"subbasins.tif\")\n",
        "wbt.subbasins(d8_pointer_path_temp, strahler_stream_order_limit_path_temp, subbasins_path_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xRwX0hSxBjw"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# # To determine pour point (lowest point) of a basin:\n",
        "# # 1. Download depressionless DEM. Determine range of values in depression (e.g. 50.2 - 50.3)\n",
        "# # 2. Reclassify so that this range = 1, all else 0, with 0 as nodata.\n",
        "# # 3. Convert pixels to points, and remove any outside the basin. Now sample raster values of DEM.\n",
        "# # 4. Sort the points by lowest raster value, check it's within basin. This is the pour point needed.\n",
        "\n",
        "# # Create watershed by pour points\n",
        "# pour_points = join(base_dir, \"pour_points/pour_points.shp\")\n",
        "# snap_distance_m = 5\n",
        "\n",
        "# # Breached depressions DEM and d8 pointer paths\n",
        "# dem_breach_depressions_path_temp = join(topography_temp_dir, \"dem_breach_depressions.tif\")\n",
        "# d8_pointer_path_temp = join(topography_temp_dir, \"d8_pointer.tif\")\n",
        "# streams_path_temp = join(topography_temp_dir, \"streams.tif\")\n",
        "\n",
        "# #  Jenson snap pour points\n",
        "# snap_distance = snap_distance_m / 111_111\n",
        "# jenson_snap_pour_points_dir_temp = join(topography_temp_dir, 'jenson_snap_pour_points')\n",
        "# makedirs(jenson_snap_pour_points_dir_temp, exist_ok=True)\n",
        "# jenson_snap_pour_points_path_temp = join(jenson_snap_pour_points_dir_temp, \"jenson_snap_pour_points.shp\")\n",
        "# wbt.jenson_snap_pour_points(pour_points, streams_path_temp, jenson_snap_pour_points_path_temp, snap_dist=snap_distance)\n",
        "\n",
        "# # Watershed\n",
        "# watershed_path_temp = join(topography_temp_dir, \"watershed.tif\")\n",
        "# wbt.watershed(d8_pointer_path_temp, jenson_snap_pour_points_path_temp, watershed_path_temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWbOUh8KtqI-"
      },
      "source": [
        "## Visualise temporary dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sl2SbGN5ONS"
      },
      "outputs": [],
      "source": [
        "# Get a list of all files in the directory\n",
        "raster_files = [os.path.join(topography_temp_dir, file) for file in os.listdir(topography_temp_dir) if file.endswith('.tif')]\n",
        "\n",
        "for raster_file in raster_files:\n",
        "    # Open the raster file\n",
        "    ds = gdal.Open(raster_file)\n",
        "    if ds is None:\n",
        "        print('Could not open ' + raster_file)\n",
        "        continue\n",
        "    # Read the raster data\n",
        "    band = ds.GetRasterBand(1)\n",
        "    raster_data = band.ReadAsArray()\n",
        "    # Close the dataset\n",
        "    ds = None\n",
        "\n",
        "    # Compute the 2% and 98% percentiles\n",
        "    p2, p98 = np.percentile(raster_data, [2, 98])\n",
        "\n",
        "    # Create a new figure\n",
        "    plt.figure()\n",
        "    # Display the data\n",
        "    plt.imshow(raster_data, cmap='viridis', vmin=p2, vmax=p98)\n",
        "    # Add a colorbar\n",
        "    plt.colorbar()\n",
        "    # Add a title\n",
        "    plt.title(os.path.basename(raster_file))\n",
        "    # Show the figure\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vPCgfSN2652"
      },
      "source": [
        "## Automate precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SsFuhGRoPhb8"
      },
      "outputs": [],
      "source": [
        "topography_list = []\n",
        "for temp_tif in os.listdir(topography_temp_dir):\n",
        "    topography_list.append(str(temp_tif))\n",
        "topography_list = sorted(topography_list)\n",
        "print(\"topography_final_list = [\")\n",
        "for topography in topography_list:\n",
        "    print(f\"'{topography}',\")\n",
        "print(\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UABWfbsgS9BJ"
      },
      "outputs": [],
      "source": [
        "topography_final_list = [\n",
        "# 'aspect.tif',\n",
        "'aspect_cosine.tif',\n",
        "'aspect_sine.tif',\n",
        "# 'basins.tif',\n",
        "'circular_variance_aspect_03.tif',\n",
        "'circular_variance_aspect_07.tif',\n",
        "'circular_variance_aspect_11.tif',\n",
        "# 'd8_pointer.tif',\n",
        "# 'dem_breach_depressions.tif',\n",
        "# 'dem_fill_single_cell_pits.tif',\n",
        "'deviation_mean_elevation_03.tif',\n",
        "'deviation_mean_elevation_07.tif',\n",
        "'deviation_mean_elevation_11.tif',\n",
        "'eastness.tif',\n",
        "'elevation.tif',\n",
        "'northness.tif',\n",
        "'profile_curvature.tif',\n",
        "'roughness_03.tif',\n",
        "'roughness_07.tif',\n",
        "'roughness_11.tif',\n",
        "'slope.tif',\n",
        "# 'specific_contributing_area_d8.tif',\n",
        "# 'specific_contributing_area_qin.tif',\n",
        "# 'strahler_stream_order.tif',\n",
        "# 'strahler_stream_order_limit.tif',\n",
        "# 'stream_power_index.tif',\n",
        "'stream_power_index_log10.tif',\n",
        "# 'streams.tif',\n",
        "# 'streams_short_removed.tif',\n",
        "# 'subbasins.tif',\n",
        "'surface_area_ratio.tif',\n",
        "'tangential_curvature.tif',\n",
        "'topographic_position_index_03.tif',\n",
        "'topographic_position_index_07.tif',\n",
        "'topographic_position_index_11.tif',\n",
        "'topographic_ruggedness_index.tif',\n",
        "'topographic_wetness_index.tif',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l6Og5v7BU5GW"
      },
      "outputs": [],
      "source": [
        "override_max_unique_values = False\n",
        "max_unique_values = 5000 # Should be >=10\n",
        "\n",
        "if override_max_unique_values == False:\n",
        "  max_unique_values = int(np.ptp(dem_base_array)) # Precision based on elevation variance\n",
        "topo_precision_dict = {}\n",
        "\n",
        "\n",
        "for topography_final in topography_final_list:\n",
        "  print(f\"Reading {topography_final}...\")\n",
        "  # Read raster as array\n",
        "  topography_raster_path = join(topography_temp_dir, topography_final)\n",
        "  topography_raster_array = gdal.Open(topography_raster_path).ReadAsArray()\n",
        "  # Convert 'nodata' values to nan\n",
        "  topography_raster_array[topography_raster_array == nodatavalue] = np.nan\n",
        "  topography_raster_array_masked = np.ma.array(topography_raster_array, mask=np.isnan(topography_raster_array))\n",
        "  # Count unique values in raster\n",
        "  unique_values = len(np.unique(topography_raster_array_masked))\n",
        "  print(f\"There are {unique_values} unique values in {topography_final}\")\n",
        "  # Generate histogram from 100,000 random points\n",
        "  random_selection = np.random.choice(topography_raster_array_masked.ravel(), size = 100_000, replace = False)\n",
        "  _ = plt.hist(random_selection, bins='auto')  # arguments are passed to np.histogram\n",
        "  plt.title(f\"{topography_final}\")\n",
        "  plt.show()\n",
        "  # Remove 0 values for log10\n",
        "  topography_raster_array_masked[topography_raster_array_masked == 0] = np.nan\n",
        "  topography_raster_array_masked = np.ma.array(topography_raster_array, mask=np.isnan(topography_raster_array))\n",
        "  # Create log10 array for determining positions for rounding\n",
        "  array_log10 = np.log10(abs(topography_raster_array_masked))\n",
        "  place_value_decimal = int(abs(np.min(array_log10)))\n",
        "  place_value_integer = int(0 - np.max(array_log10))\n",
        "  # Iterate down precision levels to determine optimal number of unique values\n",
        "  min_starting_precision = len(str(max_unique_values))\n",
        "  for precision in reversed(range(place_value_integer, max(min_starting_precision, place_value_decimal +1))):\n",
        "    rounded_array = np.round(topography_raster_array, decimals=precision)\n",
        "    round_unique_values = len(np.unique(rounded_array))\n",
        "    optimal_precision = None\n",
        "    if round_unique_values <= max_unique_values:\n",
        "      optimal_precision = precision\n",
        "      print(f\"The optimal precison for {topography_final} is {optimal_precision}, with {round_unique_values} unique values.\")\n",
        "      topo_precision_dict.update({f'{topography_final}':f'{optimal_precision}'})\n",
        "      break\n",
        "  if optimal_precision == None: print(\"There's a problem with setting precision.\")\n",
        "  print(\"___________________\\n\")\n",
        "\n",
        "print(\"Dictionary for optimal rounding values:\")\n",
        "topo_precision_dict\n",
        "\n",
        "precision_dict_csv_path = join(topography_temp_dir, 'rounding_dictionary.csv')\n",
        "# Save rounding dictionary to CSV\n",
        "with open(precision_dict_csv_path, 'w', newline='') as precision_dict_csv:\n",
        "    writer = csv.writer(precision_dict_csv)\n",
        "    writer.writerow(topo_precision_dict.keys())\n",
        "    writer.writerow(topo_precision_dict.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsX4t1x7dGm4"
      },
      "source": [
        "## Round and smooth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eYoXZMcn3Ais"
      },
      "outputs": [],
      "source": [
        "# Open rounding dictionary and verify\n",
        "with open(precision_dict_csv_path, 'r') as file:\n",
        "    keys, values = list(csv.reader(file))\n",
        "    topo_precision_dict = dict(zip(keys, values))\n",
        "\n",
        "# Verify precision and correct if necessary\n",
        "print(\"topo_precision_dict = {\")\n",
        "for key, value in topo_precision_dict.items():\n",
        "    print(f'\"{key}\": {value},')\n",
        "print(\"}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JVCcwLGOeYa4"
      },
      "outputs": [],
      "source": [
        "topo_precision_dict = {\n",
        "\"aspect_cosine.tif\": 2,\n",
        "\"aspect_sine.tif\": 2,\n",
        "\"circular_variance_aspect_03.tif\": 3,\n",
        "\"circular_variance_aspect_07.tif\": 3,\n",
        "\"circular_variance_aspect_11.tif\": 3,\n",
        "\"deviation_mean_elevation_03.tif\": 2,\n",
        "\"deviation_mean_elevation_07.tif\": 2,\n",
        "\"deviation_mean_elevation_11.tif\": 2,\n",
        "\"eastness.tif\": 2,\n",
        "\"elevation.tif\": 0,\n",
        "\"northness.tif\": 2,\n",
        "\"profile_curvature.tif\": 4,\n",
        "\"roughness_03.tif\": 1,\n",
        "\"roughness_07.tif\": 0,\n",
        "\"roughness_11.tif\": 0,\n",
        "\"slope.tif\": 1,\n",
        "\"stream_power_index_log10.tif\": 1,\n",
        "\"surface_area_ratio.tif\": 3,\n",
        "\"tangential_curvature.tif\": 4,\n",
        "\"topographic_position_index_03.tif\": 1,\n",
        "\"topographic_position_index_07.tif\": 1,\n",
        "\"topographic_position_index_11.tif\": 1,\n",
        "\"topographic_ruggedness_index.tif\": 1,\n",
        "\"topographic_wetness_index.tif\": 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oyv3GuQz3JEO"
      },
      "outputs": [],
      "source": [
        "# Set smoothing kernel\n",
        "kernel = Gaussian2DKernel(x_stddev=1, y_stddev=1)\n",
        "\n",
        "# Topography progress\n",
        "topography_progress_index = 0\n",
        "topography_progress_label = widgets.Label(f\"Topography progress: {topography_progress_index}/{len(topo_precision_dict.items())}\")\n",
        "display(topography_progress_label)\n",
        "\n",
        "# Iterate over selected topography rasters\n",
        "for topography, precision in topo_precision_dict.items():\n",
        "  topo_raster_temp_path = join(topography_temp_dir, topography)\n",
        "  topo_raster_temp_array = gdal.Open(topo_raster_temp_path).ReadAsArray()\n",
        "  # Convert nodata values to 0\n",
        "  topo_raster_temp_array[topo_raster_temp_array == nodatavalue] = 0\n",
        "  # Set path and check if exists\n",
        "  if topography_temp_dir.endswith(\"corrected_temp\"): topo_raster_unsmoothed_filename = f\"topo_cor_unsmooth_{topography}\"\n",
        "  else: topo_raster_unsmoothed_filename = f\"topo_uncor_unsmooth_{topography}\"\n",
        "  topo_raster_unsmoothed_path = join(topography_final_dir, topo_raster_unsmoothed_filename)\n",
        "  if not exists(topo_raster_unsmoothed_path):\n",
        "    # Round and export unsmoothed topography raster\n",
        "    topo_raster_unsmoothed_rounded = np.round(topo_raster_temp_array, decimals=int(precision))\n",
        "    export_array_as_tif(topo_raster_unsmoothed_rounded, topo_raster_unsmoothed_path)\n",
        "  # Smooth using 2D spatial convolution\n",
        "  if topography_temp_dir.endswith(\"corrected_temp\"): topo_raster_smoothed_filename = f\"topo_cor_smooth_{topography}\"\n",
        "  else: topo_raster_smoothed_filename = f\"topo_uncor_smooth_{topography}\"\n",
        "  topo_raster_smoothed_path = join(topography_final_dir, topo_raster_smoothed_filename)\n",
        "  if not exists(topo_raster_smoothed_path):\n",
        "    topo_raster_smoothed = convolve(topo_raster_temp_array, kernel, boundary='extend')\n",
        "    # Round and export smoothed topography raster\n",
        "    topo_raster_smoothed_rounded = np.round(topo_raster_smoothed, decimals=int(precision))\n",
        "    export_array_as_tif(topo_raster_smoothed_rounded, topo_raster_smoothed_path)\n",
        "  # Update topography progress\n",
        "  topography_progress_index += 1\n",
        "  topography_progress_label.value = f\"Topography progress: {topography_progress_index}/{len(topo_precision_dict.items())}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8s-2Jiu9rBK"
      },
      "source": [
        "# Finalise predictor rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xikCuMTCywMS"
      },
      "outputs": [],
      "source": [
        "# Round latitude and longitude rasters and finalise\n",
        "precision = 3\n",
        "coordinates = ['latitude.tif', 'longitude.tif']\n",
        "for raster in coordinates:\n",
        "  raster_path = join(areas_dir, raster)\n",
        "  array = gdal.Open(raster_path).ReadAsArray()\n",
        "  np.round(array, precision)\n",
        "  export_array_as_tif(array, join(predictor_final_dir, raster))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3mfnsS3-RIr"
      },
      "outputs": [],
      "source": [
        "# Compile and verify final predictor list\n",
        "predictor_list = [join(coast_dir,'coast_proximity_km.tif')]\n",
        "for predictor in os.listdir(continuous_final_dir):\n",
        "    predictor_list.append(join(continuous_final_dir, predictor))\n",
        "for predictor in os.listdir(edge_effects_dir):\n",
        "  predictor_list.append(join(edge_effects_dir, predictor))\n",
        "for predictor in os.listdir(topography_final_dir):\n",
        "  predictor_list.append(join(topography_final_dir, predictor))\n",
        "if exists(topography_corrected_final_dir):\n",
        "  for predictor in os.listdir(topography_corrected_final_dir):\n",
        "    predictor_list.append(join(topography_corrected_final_dir, predictor))\n",
        "predictor_list = sorted(predictor_list)\n",
        "\n",
        "print(\"predictor_list = [\")\n",
        "for predictor in predictor_list:\n",
        "  print(f\"'{predictor}',\")\n",
        "print(']')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRTxRQcefZbx"
      },
      "outputs": [],
      "source": [
        "predictor_list = [\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_1990.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_1991.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_1992.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_1993.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_1994.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_1995.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_1996.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_1997.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_1998.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_1999.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2000.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2001.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2002.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2003.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2004.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2005.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2006.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2007.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2008.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2009.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2010.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2011.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2012.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2013.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2014.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2015.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2016.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2017.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2018.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2019.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2020.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2021.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2022.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/disturbance_with_edge_effects_2023.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_1990.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_1991.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_1992.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_1993.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_1994.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_1995.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_1996.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_1997.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_1998.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_1999.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2000.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2001.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2002.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2003.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2004.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2005.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2006.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2007.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2008.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2009.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2010.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2011.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2012.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2013.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2014.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2015.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2016.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2017.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2018.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2019.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2020.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2021.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2022.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/forest_with_edge_effects_2023.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/pa_ais_with_edge_effects.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/pa_taman_krau_ais_with_edge_effects.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/binary_edge_effects/pa_taman_krau_with_edge_effects.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/coast/coast_proximity_km.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_aspect_cosine.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_aspect_sine.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_circular_variance_aspect_03.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_circular_variance_aspect_07.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_circular_variance_aspect_11.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_deviation_mean_elevation_03.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_deviation_mean_elevation_07.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_deviation_mean_elevation_11.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_eastness.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_elevation.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_northness.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_profile_curvature.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_roughness_03.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_roughness_07.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_roughness_11.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_slope.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_stream_power_index_log10.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_surface_area_ratio.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_tangential_curvature.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_topographic_position_index_03.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_topographic_position_index_07.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_topographic_position_index_11.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_topographic_ruggedness_index.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_smooth_topographic_wetness_index.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_aspect_cosine.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_aspect_sine.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_circular_variance_aspect_03.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_circular_variance_aspect_07.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_circular_variance_aspect_11.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_deviation_mean_elevation_03.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_deviation_mean_elevation_07.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_deviation_mean_elevation_11.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_eastness.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_elevation.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_northness.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_profile_curvature.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_roughness_03.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_roughness_07.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_roughness_11.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_slope.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_stream_power_index_log10.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_surface_area_ratio.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_tangential_curvature.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_topographic_position_index_03.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_topographic_position_index_07.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_topographic_position_index_11.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_topographic_ruggedness_index.tif',\n",
        "'/gdrive/Shareddrives/masfi/3_predictors/topo_final/topo_uncor_unsmooth_topographic_wetness_index.tif',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OILl1sFy9t_t"
      },
      "outputs": [],
      "source": [
        "for predictor in predictor_list:\n",
        "  predictor_name = predictor.split('/')[-1]\n",
        "  predictor_destination = join(predictor_final_dir, predictor_name)\n",
        "  if not exists(predictor_destination):\n",
        "    copyfile(predictor, predictor_destination)\n",
        "print(\"All predictors finalised.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6r7JXbijM50"
      },
      "source": [
        "# Disconnect runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU_AC6MjNfTN"
      },
      "outputs": [],
      "source": [
        "# Useful for stopping background execution\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-yk8CnJRCYVQ",
        "kSuevzw7xyil",
        "XIBbo-gsS3nD",
        "XJnV0l75FpPN",
        "1mow4bIz0EBw",
        "CUGmSRY1pVNG"
      ],
      "toc_visible": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}